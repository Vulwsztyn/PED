{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "comments_all = pickle.load(open( \"../../pickle/comments_all/comments_all.pkl\", \"rb\" ))\n",
    "\n",
    "comments_mix = comments_all\n",
    "comments_mix.pop('post_id', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../../data/reddit_wsb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = {}\n",
    "for id in comments_mix:\n",
    "    times[id] = []\n",
    "    created = df.loc[df['id'] == id].iloc[0].created\n",
    "    for comment in comments_mix[id]:\n",
    "        times[id].append(comment.created_utc - created)\n",
    "for id in times.keys():\n",
    "    times[id].sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "bins_count = 12\n",
    "bin_timeframe = 3600\n",
    "\n",
    "buckets={}\n",
    "for id in comments_all:\n",
    "    buckets[id] = []\n",
    "    cur_idx = 0\n",
    "    for i in range(bins_count):\n",
    "        buckets[id].append(0)\n",
    "        for j in range(cur_idx, len(times[id])):\n",
    "            if times[id][j]<bin_timeframe*(i+1):\n",
    "                cur_idx += 1\n",
    "                buckets[id][i] += 1\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "    if np.sum(buckets[id])>len(times[id]):\n",
    "        print(buckets[id])\n",
    "        print(times[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x  = [i*bin_timeframe/60/60 for i in range(bins_count)]\n",
    "\n",
    "plt.plot(x, buckets['l8azdz'])\n",
    "plt.bar(x, buckets['l8azdz'], width=0.25)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x, buckets['l922ub'])\n",
    "plt.bar(x, buckets['l922ub'], width=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9242509912773307\n"
     ]
    }
   ],
   "source": [
    "bucket_sum = 0\n",
    "for bucket in buckets:\n",
    "    bucket_sum += np.sum(buckets[bucket])\n",
    "\n",
    "times_sum = 0\n",
    "for time in times:\n",
    "    times_sum += len(times[time])\n",
    "\n",
    "print(bucket_sum/times_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id                                            buckets  buckets_1  \\\n",
      "0      l6ycbd               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]          0   \n",
      "1      l6xfee               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]          0   \n",
      "2      l706i4               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]          0   \n",
      "3      l67xdl               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]          0   \n",
      "4      l6yxnx               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]          0   \n",
      "...       ...                                                ...        ...   \n",
      "38696  l87hhd  [211, 178, 534, 643, 352, 217, 148, 50, 49, 39...        211   \n",
      "38697  l68xm1  [113, 1041, 605, 249, 80, 40, 26, 4, 24, 4, 4, 7]        113   \n",
      "38698  l78duc  [312, 739, 376, 190, 107, 57, 29, 10, 15, 11, ...        312   \n",
      "38699  l7dt3n  [184, 270, 525, 355, 176, 92, 71, 50, 15, 15, ...        184   \n",
      "38700  l69jz5  [234, 606, 766, 256, 103, 59, 7, 20, 19, 11, 9...        234   \n",
      "\n",
      "       buckets_2  buckets_3  buckets_4  buckets_5  \n",
      "0              0          0          0          0  \n",
      "1              0          0          0          0  \n",
      "2              0          0          0          0  \n",
      "3              0          0          0          0  \n",
      "4              0          0          0          0  \n",
      "...          ...        ...        ...        ...  \n",
      "38696        178        534        643        352  \n",
      "38697       1041        605        249         80  \n",
      "38698        739        376        190        107  \n",
      "38699        270        525        355        176  \n",
      "38700        606        766        256        103  \n",
      "\n",
      "[38701 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "buckets_df = pd.DataFrame(buckets.items(), columns=['id', 'buckets'])\n",
    "buckets_df['buckets_1'] = buckets_df['buckets'].apply(lambda x: x[0])\n",
    "buckets_df['buckets_2'] = buckets_df['buckets'].apply(lambda x: x[1])\n",
    "buckets_df['buckets_3'] = buckets_df['buckets'].apply(lambda x: x[2])\n",
    "buckets_df['buckets_4'] = buckets_df['buckets'].apply(lambda x: x[3])\n",
    "buckets_df['buckets_5'] = buckets_df['buckets'].apply(lambda x: x[4])\n",
    "print(buckets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = pickle.load(open( \"../../pickle/text_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
    "attributes = pd.merge(attributes, buckets_df, on=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.09554525e+01, 3.82747307e+00, 3.44269533e+00, ...,\n",
       "        4.31824307e-01, 4.72250905e-01, 3.06592197e-01],\n",
       "       [6.42500000e-01, 1.85518868e-01, 1.70801887e-01, ...,\n",
       "        2.66981132e-01, 1.01415094e-02, 8.96226415e-03],\n",
       "       [4.53235226e+00, 9.41744922e-01, 2.77072000e+00, ...,\n",
       "        2.04000000e+00, 5.16000000e+00, 0.00000000e+00],\n",
       "       ...,\n",
       "       [1.02841262e+01, 1.83190870e+00, 1.50599880e+00, ...,\n",
       "        1.81502027e-01, 1.20153682e-01, 1.87452776e-01],\n",
       "       [3.11533333e+01, 5.02680303e+00, 1.61000000e+00, ...,\n",
       "        4.00000000e-02, 0.00000000e+00, 2.00000000e-02],\n",
       "       [8.33898562e+00, 6.38790545e-01, 2.44202899e-02, ...,\n",
       "        2.82482000e-01, 2.82031052e-01, 2.81377421e-01]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_linnerud\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
    "\n",
    "train, test = train_test_split(attributes, test_size=0.2, random_state=123)\n",
    "\n",
    "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets', 'buckets_1', 'buckets_2', 'buckets_3', 'buckets_4', 'buckets_5']\n",
    "train_without_exculded = exclude(train)\n",
    "test_without_excluded = exclude(test)\n",
    "attributes_without_excluded = exclude(attributes)\n",
    "\n",
    "\n",
    "clf = MultiOutputRegressor(RandomForestRegressor(random_state=123)).fit(train_without_exculded, train['buckets'].to_list())\n",
    "clf.predict(test_without_excluded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    " clf_com_numms = RandomForestRegressor(random_state=123).fit(train_without_exculded, train['comms_num'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reddit comment count:  1305\n",
      "actual comment count:  1289\n",
      "timeslot comment count:  1153\n",
      "old predicted comments 809.23\n",
      "new predicted comments 708.6314166666667\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reddit comment count:  49\n",
      "actual comment count:  41\n",
      "timeslot comment count:  41\n",
      "old predicted comments 36.93206926406926\n",
      "new predicted comments 29.371579004329007\n"
     ]
    }
   ],
   "source": [
    "def probe_post(post_id,clf,attributes_excluded ):\n",
    "    new_prediction = clf.predict(attributes_excluded[attributes['id']==post_id])[0]\n",
    "    plt.plot(x, new_prediction, color='blue')\n",
    "    plt.plot(x, buckets[post_id], color='red')\n",
    "    plt.show()\n",
    "    print('reddit comment count: ', attributes[attributes['id']==post_id]['comms_num'].to_list()[0])\n",
    "    print('actual comment count: ', len(times[post_id]))\n",
    "    print('timeslot comment count: ', np.sum(buckets[post_id]))\n",
    "    print('old predicted comments', clf_com_numms.predict(attributes_without_excluded[attributes['id']==post_id])[0])\n",
    "    print('new predicted comments', np.sum(new_prediction))\n",
    "\n",
    "    \n",
    "probe_post('l8azdz', clf, attributes_without_excluded)\n",
    "probe_post('l922ub', clf, attributes_without_excluded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets', 'buckets_2', 'buckets_3', 'buckets_4', 'buckets_5']\n",
    "train_without_exculded_1 = exclude(train)\n",
    "test_without_excluded_1 = exclude(test)\n",
    "attributes_without_excluded_1 = exclude(attributes)\n",
    "\n",
    "clf_1 = MultiOutputRegressor(RandomForestRegressor(random_state=123)).fit(train_without_exculded_1, train['buckets'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reddit comment count:  1305\n",
      "actual comment count:  1289\n",
      "timeslot comment count:  1153\n",
      "old predicted comments 809.23\n",
      "new predicted comments 966.96\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reddit comment count:  49\n",
      "actual comment count:  41\n",
      "timeslot comment count:  41\n",
      "old predicted comments 36.93206926406926\n",
      "new predicted comments 43.91266666666667\n"
     ]
    }
   ],
   "source": [
    "probe_post('l8azdz', clf_1, attributes_without_excluded_1)\n",
    "probe_post('l922ub', clf_1, attributes_without_excluded_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets', 'buckets_1', 'buckets_5']\n",
    "train_without_exculded_3 = exclude(train)\n",
    "test_without_excluded_3 = exclude(test)\n",
    "attributes_without_excluded_3 = exclude(attributes)\n",
    "\n",
    "clf_3 = MultiOutputRegressor(RandomForestRegressor(random_state=123)).fit(train_without_exculded_3, train['buckets'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reddit comment count:  1305\n",
      "actual comment count:  1289\n",
      "timeslot comment count:  1153\n",
      "old predicted comments 809.23\n",
      "new predicted comments 1171.63\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reddit comment count:  49\n",
      "actual comment count:  41\n",
      "timeslot comment count:  41\n",
      "old predicted comments 36.93206926406926\n",
      "new predicted comments 44.720000000000006\n"
     ]
    }
   ],
   "source": [
    "probe_post('l8azdz', clf_3, attributes_without_excluded_3)\n",
    "probe_post('l922ub', clf_3, attributes_without_excluded_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
    "train_without_exculded_5 = exclude(train)\n",
    "test_without_excluded_5 = exclude(test)\n",
    "attributes_without_excluded_5 = exclude(attributes)\n",
    "\n",
    "\n",
    "clf_5 = MultiOutputRegressor(RandomForestRegressor(random_state=123)).fit(train_without_exculded_5, train['buckets'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reddit comment count:  1305\n",
      "actual comment count:  1289\n",
      "timeslot comment count:  1153\n",
      "old predicted comments 809.23\n",
      "new predicted comments 1244.6799999999998\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reddit comment count:  49\n",
      "actual comment count:  41\n",
      "timeslot comment count:  41\n",
      "old predicted comments 36.93206926406926\n",
      "new predicted comments 40.71\n"
     ]
    }
   ],
   "source": [
    "probe_post('l8azdz', clf_5, attributes_without_excluded_5)\n",
    "probe_post('l922ub', clf_5, attributes_without_excluded_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/1:\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "from sklearn.metrics import classification_report, auc, roc_auc_score\n",
      " 1/2:\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "from sklearn.metrics import classification_report, auc, roc_auc_score\n",
      " 1/3:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      " 2/1:\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "from sklearn.metrics import classification_report, auc, roc_auc_score\n",
      " 2/2:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      " 2/3:\n",
      "# skorzystaj z metody read_csv w module pandas\n",
      "# poniewa nadalismy temu moduowi alias, musisz wywoa\n",
      "# pd.read_csv()\n",
      "# wyszukaj w Internecie jakie parametry przyjmuje ta funkcja\n",
      "# wynik wywoania przypisz do zmiennej df\n",
      "df = pd.read_csv('titanic.csv')\n",
      " 2/4: df.head(10)\n",
      " 2/5:\n",
      "df_filtered = df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
      "df_filtered.head(10)\n",
      " 2/6:\n",
      "df_complete = df_filtered.dropna()\n",
      "df_complete.head(10)\n",
      " 2/7:\n",
      "# tu troch pomog...\n",
      "# musimy zamieni (sowo klucz) wartoci tekstowe na liczby\n",
      "# zrb co takiego dla kolumny Sex (zignoruj ostrzeenie):\n",
      "#\n",
      "# df_complete.loc[:,\"Sex\"].replace({\"female\": 0, \"male\": 1}, inplace=True)\n",
      "# df_complete.head(10)\n",
      "#\n",
      "# jeli to si uda - wymyl co podobnego dla kolumny Embarked\n",
      "df_complete.loc[:,\"Sex\"].replace({\"female\": 0, \"male\": 1}, inplace=True)\n",
      "df_complete.loc[:,\"Embarked\"].replace({\"S\": 0, \"C\": 1, \"Q\": 2}, inplace=True)\n",
      "df_complete.head(10)\n",
      " 2/8:\n",
      "\n",
      "X = df_complete.drop(['Survived'], axis=1)\n",
      "y = df_complete.Survived\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\n",
      " 2/9:\n",
      "# spjrz na slajdy do zaj, znajdujce si tam fragmenty kodu powinny Ci naprowadzi na rozwizanie\n",
      "# uwaaj... bdziesz musia zaimportowa dodatkow bibliotek\n",
      "# upewnij si te, e Twj eksperyment jest powtarzalny...\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.model_selection import StratifiedKFold\n",
      "\n",
      "skf = StratifiedKFold(n_splits=10)\n",
      "\n",
      "clf = GridSearchCV(RandomForestClassifier(),{\n",
      "    'criterion': ['gini','entropy'],\n",
      "    'n_estimators': [1,100,1000],\n",
      "}, scoring='roc_auc', cv = skf, n_jobs=-1, verbose=10, refit = True)\n",
      "clf.fit(X_train, y_train)\n",
      "print(clf.cvresults['mean_test_score'])\n",
      "2/10:\n",
      "# spjrz na slajdy do zaj, znajdujce si tam fragmenty kodu powinny Ci naprowadzi na rozwizanie\n",
      "# uwaaj... bdziesz musia zaimportowa dodatkow bibliotek\n",
      "# upewnij si te, e Twj eksperyment jest powtarzalny...\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.model_selection import StratifiedKFold\n",
      "\n",
      "skf = StratifiedKFold(n_splits=10)\n",
      "\n",
      "clf = GridSearchCV(RandomForestClassifier(),{\n",
      "    'criterion': ['gini','entropy'],\n",
      "    'n_estimators': [1,100,1000],\n",
      "}, scoring='roc_auc', cv = skf, n_jobs=-1, verbose=10, refit = True)\n",
      "clf.fit(X_train, y_train)\n",
      "print(clf)\n",
      "2/11:\n",
      "y_true, y_pred = y_test, clf.predict(X_test)\n",
      "print(classification_report(y_true, y_pred))\n",
      "2/12:\n",
      "# najpierw odrobina magii\n",
      "# jeli chcemy eby wykresy rysoway nam si w raporcie to musimy wykona instrukcj\n",
      "%matplotlib inline\n",
      "2/13:\n",
      "# dodatkowo zaimportujmy przydatne biblioteki i ustawmy domylny styl wykresw\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set(style=\"whitegrid\")\n",
      "2/14:\n",
      "sns.violinplot(x=\"Pclass\", y=\"Fare\", data=df, palette=\"Paired\")\n",
      "Set2, Set3, muted\n",
      "2/15: sns.violinplot(x=\"Pclass\", y=\"Fare\", data=df, palette=\"muted\")\n",
      " 3/1:\n",
      "from plotnine import ggplot, aes, geom_line\n",
      "\n",
      "    ggplot(df)  # What data to use\n",
      "    + aes(x=\"Pclass\", y=\"Sex\")  # What variable to use\n",
      " 3/2:\n",
      "from plotnine import ggplot, aes, geom_line\n",
      "\n",
      "ggplot(df)  # What data to use\n",
      "    + aes(x=\"Pclass\", y=\"Sex\")  # What variable to use\n",
      " 3/3:\n",
      "from plotnine import ggplot, aes, geom_line\n",
      "\n",
      "ggplot(df) + aes(x=\"Pclass\", y=\"Sex\")\n",
      " 4/1:\n",
      "import re\n",
      "import pandas as pd\n",
      "import nltk\n",
      "\n",
      "RE_SPACES = re.compile(\"\\s+\")\n",
      "RE_HASHTAG = re.compile(\"[@#][_a-z0-9]+\")\n",
      "RE_EMOTICONS = re.compile(\"(:-?\\))|(:p)|(:d+)|(:-?\\()|(:/)|(;-?\\))|(<3)|(=\\))|(\\)-?:)|(:'\\()|(8\\))\")\n",
      "RE_HTTP = re.compile(\"http(s)?://[/\\.a-z0-9]+\")\n",
      " 4/2:\n",
      "import re\n",
      "import pandas as pd\n",
      "import nltk\n",
      "\n",
      "RE_SPACES = re.compile(\"\\s+\")\n",
      "RE_HASHTAG = re.compile(\"[@#][_a-z0-9]+\")\n",
      "RE_EMOTICONS = re.compile(\"(:-?\\))|(:p)|(:d+)|(:-?\\()|(:/)|(;-?\\))|(<3)|(=\\))|(\\)-?:)|(:'\\()|(8\\))\")\n",
      "RE_HTTP = re.compile(\"http(s)?://[/\\.a-z0-9]+\")\n",
      " 4/3: tweets = DataFrame.from_csv('tweets_train.tsv', sep='\\t')\n",
      " 4/4: tweets = pd.read_csv('tweets_train.tsv', sep='\\t')\n",
      " 4/5:\n",
      "tweets = pd.read_csv('tweets_train.tsv', sep='\\t')[-1][1:]\n",
      "\n",
      "print(tweets[0])\n",
      " 4/6:\n",
      "tweets = pd.read_csv('tweets_train.tsv', sep='\\t')\n",
      "\n",
      "print(tweets[0])\n",
      " 4/7:\n",
      "tweets = pd.read_csv('tweets_train.tsv', sep='\\t')\n",
      "\n",
      "print(tweets)\n",
      " 4/8:\n",
      "tweets = pd.read_csv('tweets_train.tsv', sep='\\t')\n",
      "tweets.head()\n",
      " 4/9:\n",
      "tweets = pd.read_csv('tweets_train.tsv', sep='\\t', header=0)\n",
      "tweets.head()\n",
      "4/10:\n",
      "class Tokenizer():\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        pass\n",
      "    \n",
      "class BeforeTokenizationNormalizer():\n",
      "    @staticmethod\n",
      "    def normalize(text):\n",
      "        text = text.strip().lower()\n",
      "        text = text.replace('&nbsp;', ' ')\n",
      "        text = text.replace('&lt;', '<')\n",
      "        text = text.replace('&gt;', '>')\n",
      "        text = text.replace('&amp;', '&')\n",
      "        text = text.replace('&pound;', u'')\n",
      "        text = text.replace('&euro;', u'')\n",
      "        text = text.replace('&copy;', u'')\n",
      "        text = text.replace('&reg;', u'')\n",
      "        return text\n",
      "4/11:\n",
      "tweets = pd.read_csv('tweets_train.tsv', sep='\\t', header=None, usecols=[2])\n",
      "tweets.head()\n",
      "4/12:\n",
      "tweets = pd.read_csv('tweets_train.tsv', sep='\\t', header=None, usecols=[2])\n",
      "tweets.head(6)\n",
      "4/13:\n",
      "tweets = pd.read_csv('tweets_train.tsv', sep='\\t', header=None, usecols=[2], ames=['text'])\n",
      "tweets.head(6)\n",
      "4/14:\n",
      "tweets = pd.read_csv('tweets_train.tsv', sep='\\t', header=None, usecols=[2], names=['text'])\n",
      "tweets.head(6)\n",
      "4/15:\n",
      "class Tokenizer():\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        pass\n",
      "    \n",
      "class BeforeTokenizationNormalizer():\n",
      "    @staticmethod\n",
      "    def normalize(text):\n",
      "        text = text.strip().lower()\n",
      "        text = text.replace('&nbsp;', ' ')\n",
      "        text = text.replace('&lt;', '<')\n",
      "        text = text.replace('&gt;', '>')\n",
      "        text = text.replace('&amp;', '&')\n",
      "        text = text.replace('&pound;', u'')\n",
      "        text = text.replace('&euro;', u'')\n",
      "        text = text.replace('&copy;', u'')\n",
      "        text = text.replace('&reg;', u'')\n",
      "        return text\n",
      "4/16:\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i])\n",
      "    print(tweet)\n",
      "4/17:\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.values.tolist().iat[i])\n",
      "    print(tweet)\n",
      "4/18:\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i])\n",
      "    print(tweet)\n",
      "4/19:\n",
      "tweets = pd.read_csv('tweets_train.tsv', sep='\\t', header=None, usecols=[2], names=['col'])\n",
      "tweets.head(6)\n",
      "4/20:\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i])\n",
      "    print(tweet)\n",
      "4/21:\n",
      "for i in tweets.index:\n",
      "    print(i)\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i])\n",
      "    print(tweet)\n",
      "4/22:\n",
      "for i in tweets.index:\n",
      "    print(tweets.iat)\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i])\n",
      "    print(tweet)\n",
      "4/23:\n",
      "for i in tweets.index:\n",
      "    print(tweets.iat)\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[0][i])\n",
      "    print(tweet)\n",
      "4/24:\n",
      "for i in tweets.index:\n",
      "    print(tweets.iat[0])\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[0][i])\n",
      "    print(tweet)\n",
      "4/25:\n",
      "for i in tweets.index:\n",
      "    print(tweets.iat[0,0])\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[0][i])\n",
      "    print(tweet)\n",
      "4/26:\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[0,i])\n",
      "    print(tweet)\n",
      "4/27:\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i,0])\n",
      "    print(tweet)\n",
      " 5/1:\n",
      "print('PyDev console: using IPython 7.19.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['C:\\\\Users\\\\luker\\\\Documents\\\\Github\\\\EMD2', 'C:/Users/luker/Documents/Github/EMD2'])\n",
      " 6/1:\n",
      "import re\n",
      "import pandas as pd\n",
      "import nltk\n",
      "\n",
      "RE_SPACES = re.compile(\"\\s+\")\n",
      "RE_HASHTAG = re.compile(\"[@#][_a-z0-9]+\")\n",
      "RE_EMOTICONS = re.compile(\"(:-?\\))|(:p)|(:d+)|(:-?\\()|(:/)|(;-?\\))|(<3)|(=\\))|(\\)-?:)|(:'\\()|(8\\))\")\n",
      "RE_HTTP = re.compile(\"http(s)?://[/\\.a-z0-9]+\")\n",
      " 6/2:\n",
      "tweets = pd.read_csv('tweets_train.tsv', sep='\\t', header=None, usecols=[2], names=['text'])\n",
      "tweets.head(6)\n",
      " 6/3:\n",
      "class Tokenizer():\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        pass\n",
      "    \n",
      "class BeforeTokenizationNormalizer():\n",
      "    @staticmethod\n",
      "    def normalize(text):\n",
      "        text = text.strip().lower()\n",
      "        text = text.replace('&nbsp;', ' ')\n",
      "        text = text.replace('&lt;', '<')\n",
      "        text = text.replace('&gt;', '>')\n",
      "        text = text.replace('&amp;', '&')\n",
      "        text = text.replace('&pound;', u'')\n",
      "        text = text.replace('&euro;', u'')\n",
      "        text = text.replace('&copy;', u'')\n",
      "        text = text.replace('&reg;', u'')\n",
      "        return text\n",
      " 6/4:\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i,0])\n",
      "    print(tweet)\n",
      " 6/5:\n",
      "class SimpleTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        return text.split()\n",
      "        pass\n",
      " 6/6:\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i,0])\n",
      "    words_simple = SimpleTokenizer.tokenize(tweet)\n",
      "    print(words_simple)\n",
      " 6/7:\n",
      "class NltkTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        return nltk.word_tokenize(text)\n",
      "        pass\n",
      " 6/8:\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i,0])\n",
      "    words_nltk = NltkTokenizer.tokenize(tweet)\n",
      "    print(words_nltk)\n",
      " 6/9:\n",
      "nltk.download('punkt')\n",
      "class NltkTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        return nltk.word_tokenize(text)\n",
      "        pass\n",
      "6/10:\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i,0])\n",
      "    words_nltk = NltkTokenizer.tokenize(tweet)\n",
      "    print(words_nltk)\n",
      "6/11:\n",
      "class TweetTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        tokens = SimpleTokenizer.tokenize(text)\n",
      "        i = 0\n",
      "        while i < len(tokens):\n",
      "            token = tokens[i]\n",
      "            match = RE_EMOTICONS.match(token)\n",
      "            if match is not None:\n",
      "                print(match)\n",
      "                pass\n",
      "                # wydziel emotikon lub hashtag jako token a reszt tekstu rozpatrz ponownie\n",
      "            else:\n",
      "                del tokens[i]\n",
      "                tokens[i:i] = NltkTokenizer.tokenize(token)\n",
      "            i += 1\n",
      "            \n",
      "        # stwrz stemmer i w ptli stemmuj wszystkie tokeny\n",
      "        return tokens\n",
      "6/12:\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i,0])\n",
      "    words_tweet = TweetTokenizer.tokenize(tweet)\n",
      "    print(words_tweet)\n",
      "6/13:\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i,0])\n",
      "    words_tweet = TweetTokenizer.tokenize(tweet)\n",
      "    #print(words_tweet)\n",
      "6/14:\n",
      "class TweetTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        tokens = SimpleTokenizer.tokenize(text)\n",
      "        i = 0\n",
      "        while i < len(tokens):\n",
      "            token = tokens[i]\n",
      "            match = RE_EMOTICONS.match(token) or RE_HASHTAG.match(token)\n",
      "            if match is not None:\n",
      "                print(match)\n",
      "                pass\n",
      "                # wydziel emotikon lub hashtag jako token a reszt tekstu rozpatrz ponownie\n",
      "            else:\n",
      "                del tokens[i]\n",
      "                tokens[i:i] = NltkTokenizer.tokenize(token)\n",
      "            i += 1\n",
      "            \n",
      "        # stwrz stemmer i w ptli stemmuj wszystkie tokeny\n",
      "        return tokens\n",
      "6/15:\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i,0])\n",
      "    words_tweet = TweetTokenizer.tokenize(tweet)\n",
      "    #print(words_tweet)\n",
      "6/16:\n",
      "class TweetTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        tokens = SimpleTokenizer.tokenize(text)\n",
      "        i = 0\n",
      "        while i < len(tokens):\n",
      "            token = tokens[i]\n",
      "            match = RE_EMOTICONS.match(token) or RE_HASHTAG.match(token)\n",
      "            if match is not None:\n",
      "                print(token)\n",
      "                print(match)\n",
      "                pass\n",
      "                # wydziel emotikon lub hashtag jako token a reszt tekstu rozpatrz ponownie\n",
      "            else:\n",
      "                del tokens[i]\n",
      "                tokens[i:i] = NltkTokenizer.tokenize(token)\n",
      "            i += 1\n",
      "            \n",
      "        # stwrz stemmer i w ptli stemmuj wszystkie tokeny\n",
      "        return tokens\n",
      "6/17:\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i,0])\n",
      "    words_tweet = TweetTokenizer.tokenize(tweet)\n",
      "    #print(words_tweet)\n",
      "6/18:\n",
      "class SimpleTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        #wiem e chodzi o regex, ale to jest tak adnie wykonalne tym splitem, e a al nie skorzysta\n",
      "        text.split(RE_SPACES);\n",
      "        return text.split()\n",
      "        pass\n",
      "6/19:\n",
      "class SimpleTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        #wiem e chodzi o regex, ale to jest tak adnie wykonalne tym splitem, e a al nie skorzysta\n",
      "        return text.split(RE_SPACES);\n",
      "        pass\n",
      "6/20:\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i,0])\n",
      "    words_simple = SimpleTokenizer.tokenize(tweet)\n",
      "    print(words_simple)\n",
      "6/21:\n",
      "class SimpleTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        #wiem e chodzi o regex, ale to jest tak adnie wykonalne tym splitem, e a al nie skorzysta\n",
      "        return text.split()\n",
      "        pass\n",
      "6/22:\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i,0])\n",
      "    words_simple = SimpleTokenizer.tokenize(tweet)\n",
      "    print(words_simple)\n",
      "6/23:\n",
      "class TweetTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        tokens = SimpleTokenizer.tokenize(text)\n",
      "        i = 0\n",
      "        while i < len(tokens):\n",
      "            token = tokens[i]\n",
      "            match = RE_EMOTICONS.match(token) or RE_HASHTAG.match(token)\n",
      "            if match is not None:\n",
      "                tokens[i:i] = match\n",
      "                pass\n",
      "                # wydziel emotikon lub hashtag jako token a reszt tekstu rozpatrz ponownie\n",
      "            else:\n",
      "                del tokens[i]\n",
      "                tokens[i:i] = NltkTokenizer.tokenize(token)\n",
      "            i += 1\n",
      "            \n",
      "        # stwrz stemmer i w ptli stemmuj wszystkie tokeny\n",
      "        return tokens\n",
      "6/24:\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i,0])\n",
      "    words_tweet = TweetTokenizer.tokenize(tweet)\n",
      "    #print(words_tweet)\n",
      "6/25:\n",
      "class TweetTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        tokens = SimpleTokenizer.tokenize(text)\n",
      "        i = 0\n",
      "        while i < len(tokens):\n",
      "            token = tokens[i]\n",
      "            match = RE_EMOTICONS.match(token) or RE_HASHTAG.match(token)\n",
      "            if match is not None:\n",
      "                tokens[i:i] = match.match\n",
      "                pass\n",
      "                # wydziel emotikon lub hashtag jako token a reszt tekstu rozpatrz ponownie\n",
      "            else:\n",
      "                del tokens[i]\n",
      "                tokens[i:i] = NltkTokenizer.tokenize(token)\n",
      "            i += 1\n",
      "            \n",
      "        # stwrz stemmer i w ptli stemmuj wszystkie tokeny\n",
      "        return tokens\n",
      "6/26:\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i,0])\n",
      "    words_tweet = TweetTokenizer.tokenize(tweet)\n",
      "    print(words_tweet)\n",
      "6/27:\n",
      "class TweetTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        tokens = SimpleTokenizer.tokenize(text)\n",
      "        i = 0\n",
      "        while i < len(tokens):\n",
      "            token = tokens[i]\n",
      "            match = RE_EMOTICONS.match(token) or RE_HASHTAG.match(token)\n",
      "            if match is not None:\n",
      "                tokens[i:i] = match.match[0]\n",
      "                pass\n",
      "                # wydziel emotikon lub hashtag jako token a reszt tekstu rozpatrz ponownie\n",
      "            else:\n",
      "                del tokens[i]\n",
      "                tokens[i:i] = NltkTokenizer.tokenize(token)\n",
      "            i += 1\n",
      "            \n",
      "        # stwrz stemmer i w ptli stemmuj wszystkie tokeny\n",
      "        return tokens\n",
      "6/28:\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i,0])\n",
      "    words_tweet = TweetTokenizer.tokenize(tweet)\n",
      "    print(words_tweet)\n",
      "6/29:\n",
      "class TweetTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        tokens = SimpleTokenizer.tokenize(text)\n",
      "        i = 0\n",
      "        while i < len(tokens):\n",
      "            token = tokens[i]\n",
      "            match = RE_EMOTICONS.match(token) or RE_HASHTAG.match(token)\n",
      "            if match is not None:\n",
      "                tokens[i:i] = match[0]\n",
      "                pass\n",
      "                # wydziel emotikon lub hashtag jako token a reszt tekstu rozpatrz ponownie\n",
      "            else:\n",
      "                del tokens[i]\n",
      "                tokens[i:i] = NltkTokenizer.tokenize(token)\n",
      "            i += 1\n",
      "            \n",
      "        # stwrz stemmer i w ptli stemmuj wszystkie tokeny\n",
      "        return tokens\n",
      "6/30:\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i,0])\n",
      "    words_tweet = TweetTokenizer.tokenize(tweet)\n",
      "    print(words_tweet)\n",
      "6/31:\n",
      "class TweetTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        tokens = SimpleTokenizer.tokenize(text)\n",
      "        i = 0\n",
      "        while i < len(tokens):\n",
      "            token = tokens[i]\n",
      "            match = RE_EMOTICONS.match(token) or RE_HASHTAG.match(token)\n",
      "            print(i:i)\n",
      "            if match is not None:\n",
      "                tokens[i:i] = match[0]\n",
      "                pass\n",
      "                # wydziel emotikon lub hashtag jako token a reszt tekstu rozpatrz ponownie\n",
      "            else:\n",
      "                del tokens[i]\n",
      "                tokens[i:i] = NltkTokenizer.tokenize(token)\n",
      "            i += 1\n",
      "            \n",
      "        # stwrz stemmer i w ptli stemmuj wszystkie tokeny\n",
      "        return tokens\n",
      "6/32:\n",
      "class TweetTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        tokens = SimpleTokenizer.tokenize(text)\n",
      "        i = 0\n",
      "        while i < len(tokens):\n",
      "            token = tokens[i]\n",
      "            match = RE_EMOTICONS.match(token) or RE_HASHTAG.match(token)\n",
      "            print([i:i])\n",
      "            if match is not None:\n",
      "                tokens[i:i] = match[0]\n",
      "                pass\n",
      "                # wydziel emotikon lub hashtag jako token a reszt tekstu rozpatrz ponownie\n",
      "            else:\n",
      "                del tokens[i]\n",
      "                tokens[i:i] = NltkTokenizer.tokenize(token)\n",
      "            i += 1\n",
      "            \n",
      "        # stwrz stemmer i w ptli stemmuj wszystkie tokeny\n",
      "        return tokens\n",
      "6/33:\n",
      "class TweetTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        tokens = SimpleTokenizer.tokenize(text)\n",
      "        i = 0\n",
      "        while i < len(tokens):\n",
      "            token = tokens[i]\n",
      "            match = RE_EMOTICONS.match(token) or RE_HASHTAG.match(token)\n",
      "            print([i:i])\n",
      "            if match is not None:\n",
      "                tokens[i:i] = match[0]\n",
      "                pass\n",
      "                # wydziel emotikon lub hashtag jako token a reszt tekstu rozpatrz ponownie\n",
      "            else:\n",
      "                del tokens[i]\n",
      "                print(tokens[i:i],token)\n",
      "                tokens[i:i] = NltkTokenizer.tokenize(token)\n",
      "            i += 1\n",
      "            \n",
      "        # stwrz stemmer i w ptli stemmuj wszystkie tokeny\n",
      "        return tokens\n",
      "6/34:\n",
      "class TweetTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        tokens = SimpleTokenizer.tokenize(text)\n",
      "        i = 0\n",
      "        while i < len(tokens):\n",
      "            token = tokens[i]\n",
      "            match = RE_EMOTICONS.match(token) or RE_HASHTAG.match(token)\n",
      "            if match is not None:\n",
      "                tokens[i:i] = match[0]\n",
      "                pass\n",
      "                # wydziel emotikon lub hashtag jako token a reszt tekstu rozpatrz ponownie\n",
      "            else:\n",
      "                del tokens[i]\n",
      "                print(tokens[i:i],token)\n",
      "                tokens[i:i] = NltkTokenizer.tokenize(token)\n",
      "            i += 1\n",
      "            \n",
      "        # stwrz stemmer i w ptli stemmuj wszystkie tokeny\n",
      "        return tokens\n",
      "6/35:\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i,0])\n",
      "    words_tweet = TweetTokenizer.tokenize(tweet)\n",
      "    print(words_tweet)\n",
      "6/36:\n",
      "class TweetTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        tokens = SimpleTokenizer.tokenize(text)\n",
      "        i = 0\n",
      "        while i < len(tokens):\n",
      "            token = tokens[i]\n",
      "            match = RE_EMOTICONS.match(token) or RE_HASHTAG.match(token)\n",
      "            if match is not None:\n",
      "                #tokens[i:i] = match[0]\n",
      "                pass\n",
      "                # wydziel emotikon lub hashtag jako token a reszt tekstu rozpatrz ponownie\n",
      "            else:\n",
      "                del tokens[i]\n",
      "                print(tokens[i:i],token)\n",
      "                tokens[i:i] = NltkTokenizer.tokenize(token)\n",
      "            i += 1\n",
      "            \n",
      "        # stwrz stemmer i w ptli stemmuj wszystkie tokeny\n",
      "        return tokens\n",
      "6/37:\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i,0])\n",
      "    words_tweet = TweetTokenizer.tokenize(tweet)\n",
      "    print(words_tweet)\n",
      "6/38:\n",
      "class TweetTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        tokens = SimpleTokenizer.tokenize(text)\n",
      "        i = 0\n",
      "        while i < len(tokens):\n",
      "            token = tokens[i]\n",
      "            match = RE_EMOTICONS.match(token) or RE_HASHTAG.match(token)\n",
      "            if match is not None:\n",
      "                print(match[0])\n",
      "                #tokens[i:i] = match[0]\n",
      "                pass\n",
      "                # wydziel emotikon lub hashtag jako token a reszt tekstu rozpatrz ponownie\n",
      "            else:\n",
      "                del tokens[i]\n",
      "                print(tokens[i:i],token)\n",
      "                tokens[i:i] = NltkTokenizer.tokenize(token)\n",
      "            i += 1\n",
      "            \n",
      "        # stwrz stemmer i w ptli stemmuj wszystkie tokeny\n",
      "        return tokens\n",
      "6/39:\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i,0])\n",
      "    words_tweet = TweetTokenizer.tokenize(tweet)\n",
      "    print(words_tweet)\n",
      "6/40:\n",
      "class TweetTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        tokens = SimpleTokenizer.tokenize(text)\n",
      "        i = 0\n",
      "        while i < len(tokens):\n",
      "            token = tokens[i]\n",
      "            match = RE_EMOTICONS.match(token) or RE_HASHTAG.match(token)\n",
      "            if match is not None:\n",
      "                del tokens[i]\n",
      "                tokens[i:i] = match[0]\n",
      "                pass\n",
      "                # wydziel emotikon lub hashtag jako token a reszt tekstu rozpatrz ponownie\n",
      "            else:\n",
      "                del tokens[i]\n",
      "                print(tokens[i:i],token)\n",
      "                tokens[i:i] = NltkTokenizer.tokenize(token)\n",
      "            i += 1\n",
      "            \n",
      "        # stwrz stemmer i w ptli stemmuj wszystkie tokeny\n",
      "        return tokens\n",
      "6/41:\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i,0])\n",
      "    words_tweet = TweetTokenizer.tokenize(tweet)\n",
      "    print(words_tweet)\n",
      "6/42:\n",
      "class TweetTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        tokens = SimpleTokenizer.tokenize(text)\n",
      "        i = 0\n",
      "        while i < len(tokens):\n",
      "            token = tokens[i]\n",
      "            match = RE_EMOTICONS.match(token) or RE_HASHTAG.match(token)\n",
      "            if match is not None:\n",
      "                tokens[i] = match[0]\n",
      "                pass\n",
      "                # wydziel emotikon lub hashtag jako token a reszt tekstu rozpatrz ponownie\n",
      "            else:\n",
      "                del tokens[i]\n",
      "                print(tokens[i:i],token)\n",
      "                tokens[i:i] = NltkTokenizer.tokenize(token)\n",
      "            i += 1\n",
      "            \n",
      "        # stwrz stemmer i w ptli stemmuj wszystkie tokeny\n",
      "        return tokens\n",
      "6/43:\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i,0])\n",
      "    words_tweet = TweetTokenizer.tokenize(tweet)\n",
      "    print(words_tweet)\n",
      "6/44:\n",
      "class TweetTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        tokens = SimpleTokenizer.tokenize(text)\n",
      "        i = 0\n",
      "        while i < len(tokens):\n",
      "            token = tokens[i]\n",
      "            match = RE_EMOTICONS.match(token) or RE_HASHTAG.match(token)\n",
      "            if match is not None:\n",
      "                tokens[i] = match[0]\n",
      "                pass\n",
      "                # wydziel emotikon lub hashtag jako token a reszt tekstu rozpatrz ponownie\n",
      "            else:\n",
      "                del tokens[i]\n",
      "                tokens[i:i] = NltkTokenizer.tokenize(token)\n",
      "            i += 1\n",
      "            \n",
      "        # stwrz stemmer i w ptli stemmuj wszystkie tokeny\n",
      "        return tokens\n",
      "6/45:\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i,0])\n",
      "    words_tweet = TweetTokenizer.tokenize(tweet)\n",
      "    print(words_tweet)\n",
      "6/46:\n",
      "class TweetTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        tokens = SimpleTokenizer.tokenize(text)\n",
      "        i = 0\n",
      "        while i < len(tokens):\n",
      "            token = tokens[i]\n",
      "            match = RE_EMOTICONS.match(token) or RE_HASHTAG.match(token)\n",
      "            if match is not None:\n",
      "                tokens[i] = match[0]\n",
      "                pass\n",
      "                # wydziel emotikon lub hashtag jako token a reszt tekstu rozpatrz ponownie\n",
      "            else:\n",
      "                del tokens[i]\n",
      "                tokens[i:i] = NltkTokenizer.tokenize(token)\n",
      "            i += 1\n",
      "            \n",
      "        # stwrz stemmer i w ptli stemmuj wszystkie tokeny\n",
      "        return tokens\n",
      "6/47:\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i,0])\n",
      "    words_tweet = TweetTokenizer.tokenize(tweet)\n",
      "    print(words_tweet)\n",
      "6/48:\n",
      "class TweetTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        tokens = SimpleTokenizer.tokenize(text)\n",
      "        i = 0\n",
      "        while i < len(tokens):\n",
      "            token = tokens[i]\n",
      "            match = RE_EMOTICONS.match(token) or RE_HASHTAG.match(token)\n",
      "            if match is not None:\n",
      "                print(tokens[i])\n",
      "                tokens[i] = match[0]\n",
      "                pass\n",
      "                # wydziel emotikon lub hashtag jako token a reszt tekstu rozpatrz ponownie\n",
      "            else:\n",
      "                print(tokens[i])\n",
      "                del tokens[i]\n",
      "                tokens[i:i] = NltkTokenizer.tokenize(token)\n",
      "            i += 1\n",
      "            \n",
      "        # stwrz stemmer i w ptli stemmuj wszystkie tokeny\n",
      "        return tokens\n",
      "6/49:\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i,0])\n",
      "    words_tweet = TweetTokenizer.tokenize(tweet)\n",
      "    print(words_tweet)\n",
      "6/50:\n",
      "class TweetTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        tokens = SimpleTokenizer.tokenize(text)\n",
      "        i = 0\n",
      "        while i < len(tokens):\n",
      "            token = tokens[i]\n",
      "            match = RE_EMOTICONS.match(token) or RE_HASHTAG.match(token)\n",
      "            if match is not None:\n",
      "                print(tokens[i], 'match')\n",
      "                tokens[i] = match[0]\n",
      "                pass\n",
      "                # wydziel emotikon lub hashtag jako token a reszt tekstu rozpatrz ponownie\n",
      "            else:\n",
      "                print(tokens[i], 'match')\n",
      "                del tokens[i]\n",
      "                tokens[i:i] = NltkTokenizer.tokenize(token)\n",
      "            i += 1\n",
      "            \n",
      "        # stwrz stemmer i w ptli stemmuj wszystkie tokeny\n",
      "        return tokens\n",
      "6/51:\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i,0])\n",
      "    words_tweet = TweetTokenizer.tokenize(tweet)\n",
      "    print(words_tweet)\n",
      "6/52:\n",
      "class TweetTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        tokens = SimpleTokenizer.tokenize(text)\n",
      "        i = 0\n",
      "        while i < len(tokens):\n",
      "            token = tokens[i]\n",
      "            match = RE_EMOTICONS.match(token) or RE_HASHTAG.match(token)\n",
      "            if match is not None:\n",
      "                print(tokens[i], 'match')\n",
      "                tokens[i] = match[0]\n",
      "                pass\n",
      "                # wydziel emotikon lub hashtag jako token a reszt tekstu rozpatrz ponownie\n",
      "            else:\n",
      "                print(tokens[i], 'no match')\n",
      "                del tokens[i]\n",
      "                tokens[i:i] = NltkTokenizer.tokenize(token)\n",
      "            i += 1\n",
      "            \n",
      "        # stwrz stemmer i w ptli stemmuj wszystkie tokeny\n",
      "        return tokens\n",
      "6/53:\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i,0])\n",
      "    words_tweet = TweetTokenizer.tokenize(tweet)\n",
      "    print(words_tweet)\n",
      "6/54:\n",
      "class TweetTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        tokens = SimpleTokenizer.tokenize(text)\n",
      "        i = 0\n",
      "        while i < len(tokens):\n",
      "            token = tokens[i]\n",
      "            print(token, 'token')\n",
      "            match = RE_EMOTICONS.match(token) or RE_HASHTAG.match(token)\n",
      "            if match is not None:\n",
      "                print(tokens[i], 'match')\n",
      "                tokens[i] = match[0]\n",
      "                pass\n",
      "                # wydziel emotikon lub hashtag jako token a reszt tekstu rozpatrz ponownie\n",
      "            else:\n",
      "                print(tokens[i], 'no match')\n",
      "                del tokens[i]\n",
      "                tokens[i:i] = NltkTokenizer.tokenize(token)\n",
      "            i += 1\n",
      "            \n",
      "        # stwrz stemmer i w ptli stemmuj wszystkie tokeny\n",
      "        return tokens\n",
      "6/55:\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i,0])\n",
      "    words_tweet = TweetTokenizer.tokenize(tweet)\n",
      "    print(words_tweet)\n",
      "6/56:\n",
      "class TweetTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        tokens = SimpleTokenizer.tokenize(text)\n",
      "        i = 0\n",
      "        while i < len(tokens):\n",
      "            token = tokens[i]\n",
      "            print(token, 'token')\n",
      "            match = RE_EMOTICONS.match(token) or RE_HASHTAG.match(token)\n",
      "            if match is not None:\n",
      "                tokens[i] = match[0]\n",
      "                #troch nie rozumiem co tu jeszcze mamy wykona\n",
      "            else:\n",
      "                del tokens[i]\n",
      "                tokens[i:i] = NltkTokenizer.tokenize(token)\n",
      "            i += 1\n",
      "            \n",
      "        # stwrz stemmer i w ptli stemmuj wszystkie tokeny\n",
      "        porter = nltk.PorterStemmer() \n",
      "        porter.stem()\n",
      "        return porter.stem(tokens)\n",
      "6/57:\n",
      "class TweetTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        tokens = SimpleTokenizer.tokenize(text)\n",
      "        i = 0\n",
      "        while i < len(tokens):\n",
      "            token = tokens[i]\n",
      "            print(token, 'token')\n",
      "            match = RE_EMOTICONS.match(token) or RE_HASHTAG.match(token)\n",
      "            if match is not None:\n",
      "                tokens[i] = match[0]\n",
      "                #troch nie rozumiem co tu jeszcze mamy wykona\n",
      "            else:\n",
      "                del tokens[i]\n",
      "                tokens[i:i] = NltkTokenizer.tokenize(token)\n",
      "            i += 1\n",
      "            \n",
      "        porter = nltk.PorterStemmer() \n",
      "        porter.stem()\n",
      "        return porter.stem(tokens)\n",
      "6/58:\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i,0])\n",
      "    words_tweet = TweetTokenizer.tokenize(tweet)\n",
      "    print(words_tweet)\n",
      "6/59:\n",
      "class TweetTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        tokens = SimpleTokenizer.tokenize(text)\n",
      "        i = 0\n",
      "        while i < len(tokens):\n",
      "            token = tokens[i]\n",
      "            match = RE_EMOTICONS.match(token) or RE_HASHTAG.match(token)\n",
      "            if match is not None:\n",
      "                tokens[i] = match[0]\n",
      "                #troch nie rozumiem co tu jeszcze mamy wykona\n",
      "            else:\n",
      "                del tokens[i]\n",
      "                tokens[i:i] = NltkTokenizer.tokenize(token)\n",
      "            i += 1\n",
      "            \n",
      "        porter = nltk.PorterStemmer() \n",
      "        porter.stem()\n",
      "        return porter.stem(tokens)\n",
      "6/60:\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i,0])\n",
      "    words_tweet = TweetTokenizer.tokenize(tweet)\n",
      "    print(words_tweet)\n",
      "6/61:\n",
      "class TweetTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        tokens = SimpleTokenizer.tokenize(text)\n",
      "        i = 0\n",
      "        while i < len(tokens):\n",
      "            token = tokens[i]\n",
      "            match = RE_EMOTICONS.match(token) or RE_HASHTAG.match(token)\n",
      "            if match is not None:\n",
      "                tokens[i] = match[0]\n",
      "                #troch nie rozumiem co tu jeszcze mamy wykona\n",
      "            else:\n",
      "                del tokens[i]\n",
      "                tokens[i:i] = NltkTokenizer.tokenize(token)\n",
      "            i += 1\n",
      "            \n",
      "        porter = nltk.PorterStemmer() \n",
      "        return porter.stem(tokens)\n",
      "6/62:\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i,0])\n",
      "    words_tweet = TweetTokenizer.tokenize(tweet)\n",
      "    print(words_tweet)\n",
      "6/63:\n",
      "class TweetTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        tokens = SimpleTokenizer.tokenize(text)\n",
      "        i = 0\n",
      "        while i < len(tokens):\n",
      "            token = tokens[i]\n",
      "            match = RE_EMOTICONS.match(token) or RE_HASHTAG.match(token)\n",
      "            if match is not None:\n",
      "                tokens[i] = match[0]\n",
      "                #troch nie rozumiem co tu jeszcze mamy wykona\n",
      "            else:\n",
      "                del tokens[i]\n",
      "                tokens[i:i] = NltkTokenizer.tokenize(token)\n",
      "            i += 1\n",
      "            \n",
      "        porter = nltk.PorterStemmer() \n",
      "        tokens = [porter.stem(x) for x in tokens\n",
      "        return tokens\n",
      "6/64:\n",
      "class TweetTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        tokens = SimpleTokenizer.tokenize(text)\n",
      "        i = 0\n",
      "        while i < len(tokens):\n",
      "            token = tokens[i]\n",
      "            match = RE_EMOTICONS.match(token) or RE_HASHTAG.match(token)\n",
      "            if match is not None:\n",
      "                tokens[i] = match[0]\n",
      "                #troch nie rozumiem co tu jeszcze mamy wykona\n",
      "            else:\n",
      "                del tokens[i]\n",
      "                tokens[i:i] = NltkTokenizer.tokenize(token)\n",
      "            i += 1\n",
      "            \n",
      "        porter = nltk.PorterStemmer() \n",
      "        tokens = [porter.stem(x) for x in tokens]\n",
      "        return tokens\n",
      "6/65:\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i,0])\n",
      "    words_tweet = TweetTokenizer.tokenize(tweet)\n",
      "    print(words_tweet)\n",
      "6/66:\n",
      "class TweetTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        tokens = SimpleTokenizer.tokenize(text)\n",
      "        i = 0\n",
      "        while i < len(tokens):\n",
      "            token = tokens[i]\n",
      "            match = RE_EMOTICONS.match(token) or RE_HASHTAG.match(token)\n",
      "            if match is not None:\n",
      "                token = token.replace(match[0], '')\n",
      "                del tokens[i]\n",
      "                tokens[i:i] = NltkTokenizer.tokenize(token) + match[0]\n",
      "                print(tokens[i])\n",
      "            else:\n",
      "                del tokens[i]\n",
      "                tokens[i:i] = NltkTokenizer.tokenize(token)\n",
      "            i += 1\n",
      "            \n",
      "        porter = nltk.PorterStemmer() \n",
      "        tokens = [porter.stem(x) for x in tokens]\n",
      "        return tokens\n",
      "6/67:\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i,0])\n",
      "    words_tweet = TweetTokenizer.tokenize(tweet)\n",
      "    print(words_tweet)\n",
      "6/68:\n",
      "class TweetTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        tokens = SimpleTokenizer.tokenize(text)\n",
      "        i = 0\n",
      "        while i < len(tokens):\n",
      "            token = tokens[i]\n",
      "            match = RE_EMOTICONS.match(token) or RE_HASHTAG.match(token)\n",
      "            if match is not None:\n",
      "                tokens[i] = match[0]\n",
      "                tokens.insert(i+1, token.replace(match[0], ''))\n",
      "            else:\n",
      "                del tokens[i]\n",
      "                tokens[i:i] = NltkTokenizer.tokenize(token)\n",
      "            i += 1\n",
      "            \n",
      "        porter = nltk.PorterStemmer() \n",
      "        tokens = [porter.stem(x) for x in tokens]\n",
      "        return tokens\n",
      "6/69:\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i,0])\n",
      "    words_tweet = TweetTokenizer.tokenize(tweet)\n",
      "    print(words_tweet)\n",
      "6/70:\n",
      "%matplotlib inline\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from collections import Counter\n",
      "\n",
      "sns.set(style=\"whitegrid\")\n",
      "sns.set_color_codes(\"muted\")\n",
      "\n",
      "def show_histogram(word_counts, title=None):\n",
      "    plot_df = pd.DataFrame.from_dict(word_counts).rename(columns={0:'Token', 1:'Count'})\n",
      "    \n",
      "    f, ax = plt.subplots(figsize=(12, 15))\n",
      "    p = sns.barplot(x=\"Count\", y=\"Token\", data=plot_df, color=\"b\")\n",
      "    p.set(xlabel=\"Count\", ylabel=\"\", title=title)\n",
      "6/71:\n",
      "words = Counter()\n",
      "\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i,0])\n",
      "    words.update(TweetTokenizer.tokenize(tweet))\n",
      "6/72: print(words)\n",
      "6/73: print(words[:50])\n",
      "6/74: print(words[0])\n",
      "6/75: print(words)\n",
      "6/76: print(words.counter)\n",
      "6/77: print(words[50])\n",
      "6/78: print(words)\n",
      "6/79:\n",
      "words = Counter()\n",
      "\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i,0])\n",
      "    words.update(TweetTokenizer.tokenize(tweet))\n",
      "6/80: print(words[\"the\"])\n",
      "6/81:\n",
      "for key in words:\n",
      "    print(key)\n",
      "6/82:\n",
      "for key in words:\n",
      "    print(words[key])\n",
      "6/83: words.most_common(50)\n",
      "6/84:\n",
      "RE_PUNCT = re.compile(\"\\W\");\n",
      "for key in words:\n",
      "    if !RE_PUNCT.match(key):\n",
      "        del words[key]\n",
      "6/85:\n",
      "RE_PUNCT = re.compile(\"\\W\");\n",
      "for key in words:\n",
      "    if not RE_PUNCT.match(key):\n",
      "        del words[key]\n",
      "6/86:\n",
      "RE_PUNCT = re.compile(\"\\W\");\n",
      "newWords = {}\n",
      "for key in words:\n",
      "    if not RE_PUNCT.match(key):\n",
      "        newWords[words[key]] = words[key]\n",
      "6/87:\n",
      "RE_PUNCT = re.compile(\"\\W\");\n",
      "newWords = {}\n",
      "for key in words:\n",
      "    if not RE_PUNCT.match(key):\n",
      "        newWords[words[key]] = words[key]\n",
      "        \n",
      "newWords.most_common(50)\n",
      "6/88:\n",
      "RE_PUNCT = re.compile(\"\\W\");\n",
      "newWords = {}\n",
      "for key in words:\n",
      "    if not RE_PUNCT.match(key):\n",
      "        newWords[words[key]] = words[key]\n",
      "\n",
      "print([(l,k) for k,l in sorted([(j,i) for i,j in newWords.items()], reverse=True)])\n",
      "6/89:\n",
      "RE_PUNCT = re.compile(\"\\W\");\n",
      "newWords = {}\n",
      "for key in words:\n",
      "    if not RE_PUNCT.match(key):\n",
      "        newWords[words[key]] = words[key]\n",
      "\n",
      "print(newWords)\n",
      "print([(l,k) for k,l in sorted([(j,i) for i,j in newWords.items()], reverse=True)]\n",
      "6/90:\n",
      "RE_PUNCT = re.compile(\"\\W\");\n",
      "newWords = {}\n",
      "for key in words:\n",
      "    if not RE_PUNCT.match(key):\n",
      "        newWords[words[key]] = words[key]\n",
      "\n",
      "print(newWords)\n",
      "print([(l,k) for k,l in sorted([(j,i) for i,j in newWords.items()], reverse=True)])\n",
      "6/91:\n",
      "RE_PUNCT = re.compile(\"\\W\");\n",
      "newWords = {}\n",
      "for key in words:\n",
      "    if not RE_PUNCT.match(key):\n",
      "        newWords[key] = words[key]\n",
      "\n",
      "print(newWords)\n",
      "print([(l,k) for k,l in sorted([(j,i) for i,j in newWords.items()], reverse=True)])\n",
      "6/92:\n",
      "RE_PUNCT = re.compile(\"\\W\");\n",
      "newWords = {}\n",
      "for key in words:\n",
      "    if not RE_PUNCT.match(key):\n",
      "        newWords[key] = words[key]\n",
      "\n",
      "print([(l,k) for k,l in sorted([(j,i) for i,j in newWords.items()], reverse=True)])\n",
      "6/93:\n",
      "RE_PUNCT = re.compile(\"\\W\");\n",
      "newWords = {}\n",
      "for key in words:\n",
      "    if not RE_PUNCT.match(key):\n",
      "        newWords[key] = words[key]\n",
      "\n",
      "newWords = [(l,k) for k,l in sorted([(j,i) for i,j in newWords.items()], reverse=True)]\n",
      "print(newWords)\n",
      "6/94:\n",
      "RE_PUNCT = re.compile(\"\\W\");\n",
      "newWords = {}\n",
      "for key in words:\n",
      "    if not RE_PUNCT.match(key):\n",
      "        newWords[key] = words[key]\n",
      "\n",
      "result = [(l,k) for k,l in sorted([(j,i) for i,j in newWords.items()], reverse=True)]\n",
      "print(result[:50])\n",
      "6/95:\n",
      "newWords = {}\n",
      "for key in words:\n",
      "    if key not in stopwords:\n",
      "        newWords[key] = words[key]\n",
      "        \n",
      "result = [(l,k) for k,l in sorted([(j,i) for i,j in newWords.items()], reverse=True)]\n",
      "print(result[:50])\n",
      "6/96:\n",
      "stopwords = [\"a\", \"about\", \"after\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\",\n",
      "            \"before\", \"being\", \"between\", \"both\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"during\", \"each\",\n",
      "            \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\",\n",
      "            \"himself\", \"his\", \"how\", \"i\", \"in\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"let\", \"me\", \"more\", \"most\", \"my\",\n",
      "            \"myself\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"own\", \"sha\",\n",
      "            \"she\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\",\n",
      "            \"then\", \"there\", \"there's\", \"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"until\", \"up\", \"very\",\n",
      "            \"was\", \"we\", \"were\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\",\"whom\", \"with\", \"would\", \"you\",\n",
      "            \"your\", \"yours\", \"yourself\", \"yourselves\",\n",
      "            \"n't\", \"'s\", \"'ll\", \"'re\", \"'d\", \"'m\", \"'ve\",\n",
      "            \"above\", \"again\", \"against\", \"below\", \"but\", \"cannot\", \"down\", \"few\", \"if\", \"no\", \"nor\", \"not\", \"off\",\n",
      "            \"out\", \"over\", \"same\", \"too\", \"under\", \"why\"]\n",
      "6/97:\n",
      "newWords = {}\n",
      "for key in words:\n",
      "    if key not in stopwords:\n",
      "        newWords[key] = words[key]\n",
      "        \n",
      "result = [(l,k) for k,l in sorted([(j,i) for i,j in newWords.items()], reverse=True)]\n",
      "print(result[:50])\n",
      "6/98:\n",
      "RE_PUNCT = re.compile(\"\\W\");\n",
      "newWords = {}\n",
      "for key in words:\n",
      "    if not RE_PUNCT.match(key):\n",
      "        newWords[key] = words[key]\n",
      "\n",
      "result = [(l,k) for k,l in sorted([(j,i) for i,j in newWords.items()], reverse=True)]\n",
      "print(result[:50])\n",
      "6/99:\n",
      "stopwords = [\"a\", \"about\", \"after\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\",\n",
      "            \"before\", \"being\", \"between\", \"both\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"during\", \"each\",\n",
      "            \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\",\n",
      "            \"himself\", \"his\", \"how\", \"i\", \"in\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"let\", \"me\", \"more\", \"most\", \"my\",\n",
      "            \"myself\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"own\", \"sha\",\n",
      "            \"she\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\",\n",
      "            \"then\", \"there\", \"there's\", \"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"until\", \"up\", \"very\",\n",
      "            \"was\", \"we\", \"were\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\",\"whom\", \"with\", \"would\", \"you\",\n",
      "            \"your\", \"yours\", \"yourself\", \"yourselves\",\n",
      "            \"n't\", \"'s\", \"'ll\", \"'re\", \"'d\", \"'m\", \"'ve\",\n",
      "            \"above\", \"again\", \"against\", \"below\", \"but\", \"cannot\", \"down\", \"few\", \"if\", \"no\", \"nor\", \"not\", \"off\",\n",
      "            \"out\", \"over\", \"same\", \"too\", \"under\", \"why\"]\n",
      "6/100:\n",
      "newnewWords = {}\n",
      "for key in newWords:\n",
      "    if key not in stopwords:\n",
      "        newnewWords[key] = newWords[key]\n",
      "        \n",
      "result = [(l,k) for k,l in sorted([(j,i) for i,j in newnewWords.items()], reverse=True)]\n",
      "print(result[:50])\n",
      "6/101:\n",
      "def create_bow(documents, features):\n",
      "    row = []\n",
      "    col = []\n",
      "    data = []\n",
      "\n",
      "    labels = []\n",
      "\n",
      "    for i in documents.index:\n",
      "        tweet = BeforeTokenizationNormalizer.normalize(documents.iloc[i, 2])\n",
      "        label = documents.iloc[i, 1]\n",
      "        tweet_tokens = TweetTokenizer.tokenize(tweet)\n",
      "\n",
      "        labels.append(label)\n",
      "        for token in set(tweet_tokens):\n",
      "            if token not in features:\n",
      "                continue\n",
      "            row.append(i)\n",
      "            col.append(features[token])\n",
      "            data.append(1)\n",
      "    return csr_matrix((data, (row, col)), shape=(len(documents), len(features))), labels\n",
      "6/102:\n",
      "from scipy.sparse import csr_matrix\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.metrics import f1_score, precision_score, recall_score\n",
      "\n",
      "min_word_count = 5\n",
      "\n",
      "train_tweets = pd.read_csv(\"tweets_train.tsv\", sep=\"\\t\", header=None)\n",
      "test_tweets = pd.read_csv(\"tweets_test.tsv\", sep=\"\\t\", header=None)\n",
      "common_words = list([k for k, v in words.most_common() if v > min_word_count])\n",
      "\n",
      "feature_dict = {}\n",
      "for word in common_words:\n",
      "    feature_dict[word] = len(feature_dict)\n",
      "\n",
      "print(\"Training classifier...\")\n",
      "X_train, y_train = create_bow(train_tweets, feature_dict)\n",
      "list_of_labels = list(set(y_train))\n",
      "classifier = RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=23)\n",
      "classifier.fit(X_train, y_train)\n",
      "\n",
      "print(\"Testing...\")\n",
      "test_tweets = pd.read_csv(\"tweets_test.tsv\", sep=\"\\t\", header=None)\n",
      "X_test, y_test = create_bow(test_tweets, feature_dict)\n",
      "predicted = classifier.predict(X_test)\n",
      "\n",
      "print(\"=================== Results ===================\")\n",
      "print(\"            Positive    Neutral     Negative   \")\n",
      "print(\"F1       \", f1_score(y_test, predicted, average=None, pos_label=None, labels=list_of_labels))\n",
      "print(\"Precision\", precision_score(y_test, predicted, average=None, pos_label=None, labels=list_of_labels))\n",
      "print(\"Recall   \", recall_score(y_test, predicted, average=None, pos_label=None, labels=list_of_labels))\n",
      "6/103: result[-100:]\n",
      "6/104:\n",
      "newnewWords = {}\n",
      "for key in newWords:\n",
      "    if key not in stopwords:\n",
      "        newnewWords[key] = newWords[key]\n",
      "        \n",
      "result = [(l,k) for k,l in sorted([(j,i) for i,j in newnewWords.items()], reverse=True)]\n",
      "result[:50]\n",
      "6/105:\n",
      "from scipy.sparse import csr_matrix\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.metrics import f1_score, precision_score, recall_score\n",
      "\n",
      "min_word_count = 1\n",
      "\n",
      "train_tweets = pd.read_csv(\"tweets_train.tsv\", sep=\"\\t\", header=None)\n",
      "test_tweets = pd.read_csv(\"tweets_test.tsv\", sep=\"\\t\", header=None)\n",
      "common_words = list([k for k, v in words.most_common() if v > min_word_count])\n",
      "\n",
      "feature_dict = {}\n",
      "for word in common_words:\n",
      "    feature_dict[word] = len(feature_dict)\n",
      "\n",
      "print(\"Training classifier...\")\n",
      "X_train, y_train = create_bow(train_tweets, feature_dict)\n",
      "list_of_labels = list(set(y_train))\n",
      "classifier = RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=23)\n",
      "classifier.fit(X_train, y_train)\n",
      "\n",
      "print(\"Testing...\")\n",
      "test_tweets = pd.read_csv(\"tweets_test.tsv\", sep=\"\\t\", header=None)\n",
      "X_test, y_test = create_bow(test_tweets, feature_dict)\n",
      "predicted = classifier.predict(X_test)\n",
      "\n",
      "print(\"=================== Results ===================\")\n",
      "print(\"            Positive    Neutral     Negative   \")\n",
      "print(\"F1       \", f1_score(y_test, predicted, average=None, pos_label=None, labels=list_of_labels))\n",
      "print(\"Precision\", precision_score(y_test, predicted, average=None, pos_label=None, labels=list_of_labels))\n",
      "print(\"Recall   \", recall_score(y_test, predicted, average=None, pos_label=None, labels=list_of_labels))\n",
      "6/106:\n",
      "from scipy.sparse import csr_matrix\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.metrics import f1_score, precision_score, recall_score\n",
      "\n",
      "min_word_count = 5\n",
      "\n",
      "train_tweets = pd.read_csv(\"tweets_train.tsv\", sep=\"\\t\", header=None)\n",
      "test_tweets = pd.read_csv(\"tweets_test.tsv\", sep=\"\\t\", header=None)\n",
      "common_words = list([k for k, v in words.most_common() if v > min_word_count])\n",
      "\n",
      "feature_dict = {}\n",
      "for word in common_words:\n",
      "    feature_dict[word] = len(feature_dict)\n",
      "\n",
      "print(\"Training classifier...\")\n",
      "X_train, y_train = create_bow(train_tweets, feature_dict)\n",
      "list_of_labels = list(set(y_train))\n",
      "classifier = RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=23)\n",
      "classifier.fit(X_train, y_train)\n",
      "\n",
      "print(\"Testing...\")\n",
      "test_tweets = pd.read_csv(\"tweets_test.tsv\", sep=\"\\t\", header=None)\n",
      "X_test, y_test = create_bow(test_tweets, feature_dict)\n",
      "predicted = classifier.predict(X_test)\n",
      "\n",
      "print(\"=================== Results ===================\")\n",
      "print(\"            Positive    Neutral     Negative   \")\n",
      "print(\"F1       \", f1_score(y_test, predicted, average=None, pos_label=None, labels=list_of_labels))\n",
      "print(\"Precision\", precision_score(y_test, predicted, average=None, pos_label=None, labels=list_of_labels))\n",
      "print(\"Recall   \", recall_score(y_test, predicted, average=None, pos_label=None, labels=list_of_labels))\n",
      "6/107:\n",
      "from scipy.sparse import csr_matrix\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.metrics import f1_score, precision_score, recall_score\n",
      "\n",
      "min_word_count = 10\n",
      "\n",
      "train_tweets = pd.read_csv(\"tweets_train.tsv\", sep=\"\\t\", header=None)\n",
      "test_tweets = pd.read_csv(\"tweets_test.tsv\", sep=\"\\t\", header=None)\n",
      "common_words = list([k for k, v in words.most_common() if v > min_word_count])\n",
      "\n",
      "feature_dict = {}\n",
      "for word in common_words:\n",
      "    feature_dict[word] = len(feature_dict)\n",
      "\n",
      "print(\"Training classifier...\")\n",
      "X_train, y_train = create_bow(train_tweets, feature_dict)\n",
      "list_of_labels = list(set(y_train))\n",
      "classifier = RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=23)\n",
      "classifier.fit(X_train, y_train)\n",
      "\n",
      "print(\"Testing...\")\n",
      "test_tweets = pd.read_csv(\"tweets_test.tsv\", sep=\"\\t\", header=None)\n",
      "X_test, y_test = create_bow(test_tweets, feature_dict)\n",
      "predicted = classifier.predict(X_test)\n",
      "\n",
      "print(\"=================== Results ===================\")\n",
      "print(\"            Positive    Neutral     Negative   \")\n",
      "print(\"F1       \", f1_score(y_test, predicted, average=None, pos_label=None, labels=list_of_labels))\n",
      "print(\"Precision\", precision_score(y_test, predicted, average=None, pos_label=None, labels=list_of_labels))\n",
      "print(\"Recall   \", recall_score(y_test, predicted, average=None, pos_label=None, labels=list_of_labels))\n",
      " 8/1:\n",
      "class SimpleTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        return re.split(RE_SPACES, text)\n",
      "        pass\n",
      " 8/2:\n",
      "import re\n",
      "import pandas as pd\n",
      "import nltk\n",
      "\n",
      "RE_SPACES = re.compile(\"\\s+\")\n",
      "RE_HASHTAG = re.compile(\"[@#][_a-z0-9]+\")\n",
      "RE_EMOTICONS = re.compile(\"(:-?\\))|(:p)|(:d+)|(:-?\\()|(:/)|(;-?\\))|(<3)|(=\\))|(\\)-?:)|(:'\\()|(8\\))\")\n",
      "RE_HTTP = re.compile(\"http(s)?://[/\\.a-z0-9]+\")\n",
      " 8/3:\n",
      "tweets = pd.read_csv('tweets_train.tsv', sep='\\t', header=None, usecols=[2], names=['text'])\n",
      "tweets.head(6)\n",
      " 8/4:\n",
      "class Tokenizer():\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        pass\n",
      "    \n",
      "class BeforeTokenizationNormalizer():\n",
      "    @staticmethod\n",
      "    def normalize(text):\n",
      "        text = text.strip().lower()\n",
      "        text = text.replace('&nbsp;', ' ')\n",
      "        text = text.replace('&lt;', '<')\n",
      "        text = text.replace('&gt;', '>')\n",
      "        text = text.replace('&amp;', '&')\n",
      "        text = text.replace('&pound;', u'')\n",
      "        text = text.replace('&euro;', u'')\n",
      "        text = text.replace('&copy;', u'')\n",
      "        text = text.replace('&reg;', u'')\n",
      "        return text\n",
      " 8/5:\n",
      "class SimpleTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        return re.split(RE_SPACES, text)\n",
      "        pass\n",
      " 8/6:\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i,0])\n",
      "    words_simple = SimpleTokenizer.tokenize(tweet)\n",
      "    print(words_simple)\n",
      " 8/7:\n",
      "class NltkTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        return nltk.word_tokenize(text)\n",
      "        pass\n",
      " 7/1:\n",
      "# from nltk.tokenize import word_tokenize\n",
      "class NltkTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        # Napisz tokenizator korzystajcy z funkcji word_tokenize() z biblioteki NLTK.\n",
      "        # Czy w przypadku tweetw wszystkie sowa zostay poprawnie rozdzielone?\n",
      "        return nltk.tokenize.word_tokenize(text)\n",
      " 8/8:\n",
      "class TweetTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        tokens = SimpleTokenizer.tokenize(text)\n",
      "        i = 0\n",
      "        while i < len(tokens):\n",
      "            token = tokens[i]\n",
      "            match = RE_EMOTICONS.match(token) or RE_HASHTAG.match(token)\n",
      "            if match is not None:\n",
      "                tokens[i] = match[0]\n",
      "                tokens.insert(i+1, token.replace(match[0], ''))\n",
      "            else:\n",
      "                del tokens[i]\n",
      "                tokens[i:i] = NltkTokenizer.tokenize(token)\n",
      "            i += 1\n",
      "            \n",
      "        porter = nltk.PorterStemmer() \n",
      "        tokens = [porter.stem(x) for x in tokens]\n",
      "        return tokens\n",
      " 8/9:\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i,0])\n",
      "    words_tweet = TweetTokenizer.tokenize(tweet)\n",
      "    print(words_tweet)\n",
      "8/10:\n",
      "%matplotlib inline\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from collections import Counter\n",
      "\n",
      "sns.set(style=\"whitegrid\")\n",
      "sns.set_color_codes(\"muted\")\n",
      "\n",
      "def show_histogram(word_counts, title=None):\n",
      "    plot_df = pd.DataFrame.from_dict(word_counts).rename(columns={0:'Token', 1:'Count'})\n",
      "    \n",
      "    f, ax = plt.subplots(figsize=(12, 15))\n",
      "    p = sns.barplot(x=\"Count\", y=\"Token\", data=plot_df, color=\"b\")\n",
      "    p.set(xlabel=\"Count\", ylabel=\"\", title=title)\n",
      "8/11:\n",
      "words = Counter()\n",
      "\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i,0])\n",
      "    words.update(TweetTokenizer.tokenize(tweet))\n",
      "8/12:\n",
      "top = words.most_common(50)\n",
      "show_histogram(top, 'Most common 50 words')\n",
      "#Nie wszystkie sowa powinny by uwzgldnione. Trzeba usun znaki interpunkcyjne i stopwords\n",
      "8/13:\n",
      "stopwords = [\"a\", \"about\", \"after\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\",\n",
      "            \"before\", \"being\", \"between\", \"both\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"during\", \"each\",\n",
      "            \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\",\n",
      "            \"himself\", \"his\", \"how\", \"i\", \"in\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"let\", \"me\", \"more\", \"most\", \"my\",\n",
      "            \"myself\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"own\", \"sha\",\n",
      "            \"she\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\",\n",
      "            \"then\", \"there\", \"there's\", \"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"until\", \"up\", \"very\",\n",
      "            \"was\", \"we\", \"were\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\",\"whom\", \"with\", \"would\", \"you\",\n",
      "            \"your\", \"yours\", \"yourself\", \"yourselves\",\n",
      "            \"n't\", \"'s\", \"'ll\", \"'re\", \"'d\", \"'m\", \"'ve\",\n",
      "            \"above\", \"again\", \"against\", \"below\", \"but\", \"cannot\", \"down\", \"few\", \"if\", \"no\", \"nor\", \"not\", \"off\",\n",
      "            \"out\", \"over\", \"same\", \"too\", \"under\", \"why\"]\n",
      "8/14:\n",
      "newnewWords = {}\n",
      "for key in newWords:\n",
      "    if key not in stopwords:\n",
      "        newnewWords[key] = newWords[key]\n",
      "        \n",
      "result = [(l,k) for k,l in sorted([(j,i) for i,j in newnewWords.items()], reverse=True)]\n",
      "result[:50]\n",
      "\n",
      "#Nienaley usuwa wszystkich stopwords. Sowa takie jak not albo cannot mog by nonikiem opini.\n",
      "8/15:\n",
      "RE_PUNCT = re.compile(\"\\W\");\n",
      "newWords = {}\n",
      "for key in words:\n",
      "    if not RE_PUNCT.match(key):\n",
      "        newWords[key] = words[key]\n",
      "\n",
      "result = [(l,k) for k,l in sorted([(j,i) for i,j in newWords.items()], reverse=True)]\n",
      "print(result[:50])\n",
      "\n",
      "#Mona zostawi na przykad wykrzynkini, bo one mog wzmacnia wyraon opinie.\n",
      "8/16:\n",
      "stopwords = [\"a\", \"about\", \"after\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\",\n",
      "            \"before\", \"being\", \"between\", \"both\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"during\", \"each\",\n",
      "            \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\",\n",
      "            \"himself\", \"his\", \"how\", \"i\", \"in\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"let\", \"me\", \"more\", \"most\", \"my\",\n",
      "            \"myself\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"own\", \"sha\",\n",
      "            \"she\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\",\n",
      "            \"then\", \"there\", \"there's\", \"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"until\", \"up\", \"very\",\n",
      "            \"was\", \"we\", \"were\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\",\"whom\", \"with\", \"would\", \"you\",\n",
      "            \"your\", \"yours\", \"yourself\", \"yourselves\",\n",
      "            \"n't\", \"'s\", \"'ll\", \"'re\", \"'d\", \"'m\", \"'ve\",\n",
      "            \"above\", \"again\", \"against\", \"below\", \"but\", \"cannot\", \"down\", \"few\", \"if\", \"no\", \"nor\", \"not\", \"off\",\n",
      "            \"out\", \"over\", \"same\", \"too\", \"under\", \"why\"]\n",
      "8/17:\n",
      "newnewWords = {}\n",
      "for key in newWords:\n",
      "    if key not in stopwords:\n",
      "        newnewWords[key] = newWords[key]\n",
      "        \n",
      "result = [(l,k) for k,l in sorted([(j,i) for i,j in newnewWords.items()], reverse=True)]\n",
      "result[:50]\n",
      "\n",
      "#Nienaley usuwa wszystkich stopwords. Sowa takie jak not albo cannot mog by nonikiem opini.\n",
      "8/18:\n",
      "class TweetTokenizer(Tokenizer):\n",
      "    @staticmethod\n",
      "    def tokenize(text):\n",
      "        tokens = SimpleTokenizer.tokenize(text)\n",
      "        i = 0\n",
      "        while i < len(tokens):\n",
      "            token = tokens[i]\n",
      "            match = RE_EMOTICONS.match(token) or RE_HASHTAG.match(token) or RE_HTTP.search(token)\n",
      "            if match is not None:\n",
      "                tokens[i] = match[0]\n",
      "                tokens.insert(i+1, token.replace(match[0], ''))\n",
      "            else:\n",
      "                del tokens[i]\n",
      "                tokens[i:i] = NltkTokenizer.tokenize(token)\n",
      "            i += 1\n",
      "            \n",
      "        porter = nltk.PorterStemmer() \n",
      "        tokens = [porter.stem(x) for x in tokens]\n",
      "        return tokens\n",
      "8/19:\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i,0])\n",
      "    words_tweet = TweetTokenizer.tokenize(tweet)\n",
      "    print(words_tweet)\n",
      "8/20:\n",
      "%matplotlib inline\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from collections import Counter\n",
      "\n",
      "sns.set(style=\"whitegrid\")\n",
      "sns.set_color_codes(\"muted\")\n",
      "\n",
      "def show_histogram(word_counts, title=None):\n",
      "    plot_df = pd.DataFrame.from_dict(word_counts).rename(columns={0:'Token', 1:'Count'})\n",
      "    \n",
      "    f, ax = plt.subplots(figsize=(12, 15))\n",
      "    p = sns.barplot(x=\"Count\", y=\"Token\", data=plot_df, color=\"b\")\n",
      "    p.set(xlabel=\"Count\", ylabel=\"\", title=title)\n",
      "8/21:\n",
      "words = Counter()\n",
      "\n",
      "for i in tweets.index:\n",
      "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i,0])\n",
      "    words.update(TweetTokenizer.tokenize(tweet))\n",
      "8/22:\n",
      "top = words.most_common(50)\n",
      "show_histogram(top, 'Most common 50 words')\n",
      "#Nie wszystkie sowa powinny by uwzgldnione. Trzeba usun znaki interpunkcyjne i stopwords\n",
      "8/23:\n",
      "RE_PUNCT = re.compile(\"\\W\");\n",
      "newWords = {}\n",
      "for key in words:\n",
      "    if not RE_PUNCT.match(key):\n",
      "        newWords[key] = words[key]\n",
      "\n",
      "result = [(l,k) for k,l in sorted([(j,i) for i,j in newWords.items()], reverse=True)]\n",
      "print(result[:50])\n",
      "\n",
      "#Mona zostawi na przykad wykrzynkini, bo one mog wzmacnia wyraon opinie.\n",
      "8/24:\n",
      "newnewWords = {}\n",
      "for key in newWords:\n",
      "    if key not in stopwords:\n",
      "        newnewWords[key] = newWords[key]\n",
      "        \n",
      "result = [(l,k) for k,l in sorted([(j,i) for i,j in newnewWords.items()], reverse=True)]\n",
      "result[:50]\n",
      "\n",
      "#Nienaley usuwa wszystkich stopwords. Sowa takie jak not albo cannot mog by nonikiem opini.\n",
      "8/25:\n",
      "result[-100:]\n",
      "\n",
      "# Te tokeny faktycznie s do rzadkie\n",
      "# tokeny zawierajce liczb mona wyfiltrowa. Mona skorzysta z regexa, ktry wychwyci sowa zawierajce liczby i wtedy mona je usun.\n",
      "10/1:\n",
      "import logging\n",
      "import gensim\n",
      "\n",
      "# Wczamy logowanie, eby ledzi postpy algorytmu (to akurat nie bdzie dziaa w Jupyter Notebooku,\n",
      "# ale warto o tym wspomnie)\n",
      "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
      "\n",
      "# Odczytujemy z pliku mapowanie/sownik id->sowo\n",
      "id2word = gensim.corpora.Dictionary.load_from_text('wiki_wordids.txt.bz2')\n",
      "\n",
      "# Odczytujemy z pliku reprezentacj wektorow korpusu (macierz wetkorw TF-IDF)\n",
      "mm = gensim.corpora.MmCorpus('wiki_tfidf.mm')\n",
      "print(mm)\n",
      "\n",
      "# Tworzymy model LDA z 20 grupami wykonujc 20 iteracji na caym zbiorze\n",
      "lda = gensim.models.LdaMulticore(corpus=mm, id2word=id2word, num_topics=20, passes=20, workers=4)\n",
      "# Alternatywnie w razie problemw z wielowtkowoci:\n",
      "# lda = gensim.models.ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=20, update_every=0, passes=20)\n",
      "lda.print_topics(20)\n",
      "10/2:\n",
      "import logging\n",
      "import gensim\n",
      "\n",
      "# Wczamy logowanie, eby ledzi postpy algorytmu (to akurat nie bdzie dziaa w Jupyter Notebooku,\n",
      "# ale warto o tym wspomnie)\n",
      "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
      "\n",
      "# Odczytujemy z pliku mapowanie/sownik id->sowo\n",
      "id2word = gensim.corpora.Dictionary.load_from_text('wiki_wordids.txt.bz2')\n",
      "\n",
      "# Odczytujemy z pliku reprezentacj wektorow korpusu (macierz wetkorw TF-IDF)\n",
      "mm = gensim.corpora.MmCorpus('wiki_tfidf.mm')\n",
      "print(mm)\n",
      "\n",
      "# Tworzymy model LDA z 20 grupami wykonujc 20 iteracji na caym zbiorze\n",
      "lda = gensim.models.LdaMulticore(corpus=mm, id2word=id2word, num_topics=20, passes=20, workers=4)\n",
      "# Alternatywnie w razie problemw z wielowtkowoci:\n",
      "# lda = gensim.models.ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=20, update_every=0, passes=20)\n",
      "lda.print_topics(20)\n",
      "10/3:\n",
      "import logging\n",
      "import gensim\n",
      "\n",
      "# Wczamy logowanie, eby ledzi postpy algorytmu (to akurat nie bdzie dziaa w Jupyter Notebooku,\n",
      "# ale warto o tym wspomnie)\n",
      "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
      "\n",
      "# Odczytujemy z pliku mapowanie/sownik id->sowo\n",
      "id2word = gensim.corpora.Dictionary.load_from_text('wiki_wordids.txt.bz2')\n",
      "\n",
      "# Odczytujemy z pliku reprezentacj wektorow korpusu (macierz wetkorw TF-IDF)\n",
      "mm = gensim.corpora.MmCorpus('wiki_tfidf.mm')\n",
      "print(mm)\n",
      "\n",
      "# Tworzymy model LDA z 20 grupami wykonujc 20 iteracji na caym zbiorze\n",
      "lda = gensim.models.LdaMulticore(corpus=mm, id2word=id2word, num_topics=20, passes=20, workers=4)\n",
      "# Alternatywnie w razie problemw z wielowtkowoci:\n",
      "# lda = gensim.models.ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=20, update_every=0, passes=20)\n",
      "lda.print_topics(20)\n",
      "12/1:\n",
      "import logging\n",
      "import gensim\n",
      "\n",
      "# Wczamy logowanie, eby ledzi postpy algorytmu (to akurat nie bdzie dziaa w Jupyter Notebooku,\n",
      "# ale warto o tym wspomnie)\n",
      "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
      "\n",
      "# Odczytujemy z pliku mapowanie/sownik id->sowo\n",
      "id2word = gensim.corpora.Dictionary.load_from_text('wiki_wordids.txt.bz2')\n",
      "\n",
      "# Odczytujemy z pliku reprezentacj wektorow korpusu (macierz wetkorw TF-IDF)\n",
      "mm = gensim.corpora.MmCorpus('wiki_tfidf.mm')\n",
      "print(mm)\n",
      "\n",
      "# Tworzymy model LDA z 20 grupami wykonujc 20 iteracji na caym zbiorze\n",
      "lda = gensim.models.LdaMulticore(corpus=mm, id2word=id2word, num_topics=20, passes=20, workers=4)\n",
      "# Alternatywnie w razie problemw z wielowtkowoci:\n",
      "# lda = gensim.models.ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=20, update_every=0, passes=20)\n",
      "lda.print_topics(20)\n",
      "12/2:\n",
      "from nltk.tokenize import word_tokenize\n",
      "import nltk \n",
      "    \n",
      "texts = [word_tokenize(tweets.iat[i]) for i in tweets.index] \n",
      "texts = [[word for word in text if word not in stopwords]\n",
      "         for text in texts]\n",
      "print(texts)\n",
      "12/3:\n",
      "import gensim\n",
      "import logging\n",
      "import nltk\n",
      "import re\n",
      "import pandas as pd\n",
      "\n",
      "stopwords = [\"a\", \"about\", \"after\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\",\n",
      "            \"before\", \"being\", \"between\", \"both\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"during\", \"each\",\n",
      "            \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\",\n",
      "            \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\",\n",
      "            \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"of\",\n",
      "            \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"own\", \"shan't\", \"she\", \"she'd\",\n",
      "            \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\",\n",
      "            \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\",\n",
      "            \"they've\", \"this\", \"those\", \"through\", \"to\", \"until\", \"up\", \"very\", \"was\", \"wasn't\", \"we\", \"we'd\",\n",
      "            \"we'll\", \"we're\", \"we've\", \"were\", \"weren't\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\",\n",
      "            \"which\", \"while\", \"who\", \"who's\", \"whom\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\",\n",
      "            \"your\", \"yours\", \"yourself\", \"yourselves\", \"above\", \"again\", \"against\", \"aren't\", \"below\", \"but\", \"can't\",\n",
      "            \"cannot\", \"couldn't\", \"didn't\", \"doesn't\", \"don't\", \"down\", \"few\", \"hadn't\", \"hasn't\", \"haven't\", \"if\",\n",
      "            \"isn't\", \"mustn't\", \"no\", \"nor\", \"not\", \"off\", \"out\", \"over\", \"shouldn't\", \"same\", \"too\", \"under\", \"why\",\n",
      "            \"why's\", \"won't\", \"wouldn't\"]\n",
      "\n",
      "tweets = pd.read_csv('tweets.tsv', sep='\\t', header=None)\n",
      "tweets= tweets[2]\n",
      "tweets.head(6)\n",
      "12/4:\n",
      "from nltk.tokenize import word_tokenize\n",
      "import nltk \n",
      "    \n",
      "texts = [word_tokenize(tweets.iat[i]) for i in tweets.index] \n",
      "texts = [[word for word in text if word not in stopwords]\n",
      "         for text in texts]\n",
      "print(texts)\n",
      "12/5:\n",
      "from gensim import corpora\n",
      "\n",
      "id2word = corpora.Dictionary(texts)\n",
      "print(id2word)\n",
      "i = 0\n",
      "for k,v in id2word.items():\n",
      "  print(k,v)\n",
      "  i+=1\n",
      "  if i > 10:\n",
      "    break\n",
      "12/6:\n",
      "from gensim import corpora\n",
      "\n",
      "id2word = corpora.Dictionary(texts)\n",
      "print(id2word)\n",
      "12/7: mm = [id2word.doc2bow(text) for text in texts]\n",
      "12/8:\n",
      "# Wczamy logowanie, eby ledzi postpy algorytmu (to akurat nie bdzie dziaa w Jupyter Notebooku,\n",
      "# ale warto o tym wspomnie)\n",
      "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
      "\n",
      "lda = gensim.models.ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=10, update_every=0, passes=20)\n",
      "# alternatywnie lda = gensim.models.LdaMulticore(corpus=mm, id2word=id2word, num_topics=10, passes=20)\n",
      "lda.print_topics(10)\n",
      "12/9:\n",
      "# # Poniszy kod korzysta moduu pyLDAvis z githuba:\n",
      "# # https://github.com/bmabey/pyLDAvis\n",
      "import pyLDAvis\n",
      "import pyLDAvis.gensim\n",
      "pyLDAvis.enable_notebook()\n",
      "\n",
      "pyLDAvis.display(pyLDAvis.gensim.prepare(lda, mm, id2word))\n",
      "12/10:\n",
      "# # Poniszy kod korzysta moduu pyLDAvis z githuba:\n",
      "# # https://github.com/bmabey/pyLDAvis\n",
      "import pyLDAvis\n",
      "import pyLDAvis.gensim\n",
      "pyLDAvis.enable_notebook()\n",
      "\n",
      "pyLDAvis.display(pyLDAvis.gensim.prepare(lda, mm, id2word))\n",
      "12/11:\n",
      "from nltk.tokenize import word_tokenize\n",
      "import nltk \n",
      "    \n",
      "texts = [word_tokenize(tweet) for tweet in tweets.iat] \n",
      "texts = [[word for word in text if word not in stopwords]for text in texts]\n",
      "print(texts)\n",
      "12/12:\n",
      "from gensim import corpora\n",
      "\n",
      "id2word = corpora.Dictionary(texts)\n",
      "print(id2word)\n",
      "12/13:\n",
      "from nltk.tokenize import word_tokenize\n",
      "import nltk \n",
      "    \n",
      "texts = [word_tokenize(tweet) for tweet in tweets.iat] \n",
      "texts = [[token for token in text if token not in stopwords]for text in texts]\n",
      "print(texts)\n",
      "14/1:\n",
      "import gensim, logging, re, nltk\n",
      "import pandas as pd\n",
      "\n",
      "RE_SPACES = re.compile(\"\\s+\")\n",
      "RE_HASHTAG = re.compile(\"[@#][_a-z0-9]+\")\n",
      "RE_EMOTICONS = re.compile(\"(:-?\\))|(:p)|(:d+)|(:-?\\()|(:/)|(;-?\\))|(<3)|(=\\))|(\\)-?:)|(:'\\()|(8\\))\")\n",
      "RE_HTTP = re.compile(\"http(s)?://[/\\.a-z0-9]+\")\n",
      "\n",
      "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
      " \n",
      "song = \"\"\"Gdzie strumyk pynie z wolna,\n",
      "Rozsiewa zioa maj,\n",
      "Stokrotka rosa polna,\n",
      "A nad ni szumia gaj,\n",
      "Stokrotka rosa polna,\n",
      "A nad ni szumia gaj,\n",
      "Zielony gaj.\n",
      "\n",
      "W tym gaju tak ponuro,\n",
      "e a przeraa mnie,\n",
      "Ptaszta za wysoko,\n",
      "A mnie samotnej le,\n",
      "Ptaszta za wysoko,\n",
      "A mnie samotnej le,\n",
      "samotnej le.\n",
      "\n",
      "Wtem harcerz idzie z wolna.\n",
      "Stokrotko, witam ci,\n",
      "Twj urok mnie zachwyca,\n",
      "Czy chcesz by m, czy nie?\n",
      "\"Twj urok mnie zachwyca,\n",
      "Czy chcesz by m, czy nie?\n",
      "Czy nie, czy nie?\n",
      "\n",
      "Stokrotka si zgodzia\n",
      "I poszli w ciemny las,\n",
      "A harcerz taki gapa\n",
      "e a w pokrzywy wlaz,\n",
      "A harcerz taki gapa\n",
      "e a w pokrzywy wlaz,\n",
      "w pokrzywy wlaz.\n",
      "\n",
      "A ona, ona, ona,\n",
      "C biedna robi ma,\n",
      "Nad gap pochylona\n",
      "I mieje si: ha, ha,\n",
      "Nad gap pochylona\n",
      "I mieje: si ha, ha,\n",
      "ha, ha, ha, ha.\"\"\"\n",
      "14/2:\n",
      "sentences = None\n",
      "print(sentences)\n",
      "14/3:\n",
      "sentences = song.split(' ')\n",
      "print(sentences)\n",
      "14/4:\n",
      "sentences = song.split('\\n').split(' ')\n",
      "print(sentences)\n",
      "14/5:\n",
      "sentences = [verse.split(' ') for verse in song.split('\\n')]\n",
      "print(sentences)\n",
      "14/6:\n",
      "model = gensim.models.Word2Vec(sentences, iter=5, min_count=1)\n",
      "print(model)\n",
      "print(model.vocabulary)\n",
      "\n",
      "model.wv.doesnt_match(\"las harcerz gaj zioa\".split())\n",
      "14/7:\n",
      "import smart_open, os\n",
      "\n",
      "if not os.path.exists('./data/'):\n",
      "    os.makedirs('./data/')\n",
      "\n",
      "filenames = ['./data/f' + str(i) +'.txt' for i in range(39)]\n",
      "\n",
      "if sentences is not None:\n",
      "    for i, fname in enumerate(filenames):\n",
      "        with smart_open.smart_open(fname, 'w') as fout:\n",
      "            for line in sentences[i]:\n",
      "                fout.write(line + ' ')\n",
      "14/8:\n",
      "class MySentences(object):\n",
      "    def __init__(self, dirname):\n",
      "        self.dirname = dirname\n",
      " \n",
      "    def __iter__(self):\n",
      "        for fname in os.listdir(self.dirname):\n",
      "            if fname.endswith('.txt'):\n",
      "                for line in open(os.path.join(self.dirname, fname)):\n",
      "                    pass # TODO\n",
      "\n",
      "# Do odkomentowania:\n",
      "sentences = MySentences('./data/')\n",
      "model = gensim.models.Word2Vec(sentences, iter=10, min_count=1)\n",
      "print(model)\n",
      "\n",
      "model.wv.doesnt_match(\"las harcerz gaj zioa\".split())\n",
      "14/9:\n",
      "class MySentences(object):\n",
      "    def __init__(self, dirname):\n",
      "        self.dirname = dirname\n",
      " \n",
      "    def __iter__(self):\n",
      "        for fname in os.listdir(self.dirname):\n",
      "            if fname.endswith('.txt'):\n",
      "                for line in open(os.path.join(self.dirname, fname)):\n",
      "                    x = line.translate(str.maketrans('', '', string.punctuation+'')).lower().strip()\n",
      "                    yield x.split(' ') # 50 min spdziem, eby wpa na to, e split\n",
      "\n",
      "# Do odkomentowania:\n",
      "sentences = MySentences('./data/')\n",
      "model = gensim.models.Word2Vec(sentences, iter=10, min_count=1)\n",
      "print(model)\n",
      "\n",
      "model.wv.doesnt_match(\"las harcerz gaj zioa\".split())\n",
      "14/10:\n",
      "class MySentences(object):\n",
      "    def __init__(self, dirname):\n",
      "        self.dirname = dirname\n",
      " \n",
      "    def __iter__(self):\n",
      "        for fname in os.listdir(self.dirname):\n",
      "            if fname.endswith('.txt'):\n",
      "                for line in open(os.path.join(self.dirname, fname)):\n",
      "                    x = line.translate(str.maketrans('', '', string.punctuation+'')).lower().strip()\n",
      "                    yield x.split(' ') # 50 min spdziem, eby wpa na to, e split\n",
      "\n",
      "# Do odkomentowania:\n",
      "sentences = MySentences('./data/')\n",
      "model = gensim.models.Word2Vec(sentences, iter=10, min_count=1)\n",
      "print(model)\n",
      "\n",
      "model.wv.doesnt_match(\"las harcerz gaj zioa\".split())\n",
      "15/1:\n",
      "import string\n",
      "class MySentences(object):\n",
      "    def __init__(self, dirname):\n",
      "        self.dirname = dirname\n",
      " \n",
      "    def __iter__(self):\n",
      "        for fname in os.listdir(self.dirname):\n",
      "            if fname.endswith('.txt'):\n",
      "                for line in open(os.path.join(self.dirname, fname)):\n",
      "                    x = line.translate(str.maketrans('', '', string.punctuation)).lower().strip()\n",
      "                    yield x.split(' ')\n",
      "\n",
      "# Do odkomentowania:\n",
      "sentences = MySentences('./data/')\n",
      "model = gensim.models.Word2Vec(sentences, iter=10, min_count=1)\n",
      "print(model)\n",
      "print(model.wv.vocab)\n",
      "model.wv.doesnt_match(\"las harcerz gaj zioa\".split())\n",
      "14/11:\n",
      "class MySentences(object):\n",
      "    def __init__(self, dirname):\n",
      "        self.dirname = dirname\n",
      " \n",
      "    def __iter__(self):\n",
      "        for fname in os.listdir(self.dirname):\n",
      "            if fname.endswith('.txt'):\n",
      "                for line in open(os.path.join(self.dirname, fname)):\n",
      "                    x = line.translate(str.maketrans('', '', string.punctuation)).lower().strip()\n",
      "                    return x.split(' ')\n",
      "\n",
      "# Do odkomentowania:\n",
      "sentences = MySentences('./data/')\n",
      "model = gensim.models.Word2Vec(sentences, iter=10, min_count=1)\n",
      "print(model)\n",
      "\n",
      "model.wv.doesnt_match(\"las harcerz gaj zioa\".split())\n",
      "14/12:\n",
      "class MySentences(object):\n",
      "    def __init__(self, dirname):\n",
      "        self.dirname = dirname\n",
      " \n",
      "    def __iter__(self):\n",
      "        for fname in os.listdir(self.dirname):\n",
      "            if fname.endswith('.txt'):\n",
      "                for line in open(os.path.join(self.dirname, fname)):\n",
      "                    x = line.translate(str.maketrans('', '', string.punctuation)).lower().strip()\n",
      "                    yield x.split(' ')\n",
      "\n",
      "# Do odkomentowania:\n",
      "sentences = MySentences('./data/')\n",
      "model = gensim.models.Word2Vec(sentences, iter=10, min_count=1)\n",
      "print(model)\n",
      "\n",
      "model.wv.doesnt_match(\"las harcerz gaj zioa\".split())\n",
      "14/13:\n",
      "class MySentences(object):\n",
      "    def __init__(self, dirname):\n",
      "        self.dirname = dirname\n",
      " \n",
      "    def __iter__(self):\n",
      "        for fname in os.listdir(self.dirname):\n",
      "            if fname.endswith('.txt'):\n",
      "                for line in open(os.path.join(self.dirname, fname)):\n",
      "                    x = line.translate(str.maketrans('', '', string.punctuation)).lower().strip()\n",
      "                    yield x.split(' ')\n",
      "\n",
      "# Do odkomentowania:\n",
      "sentences = MySentences('./data/')\n",
      "model = gensim.models.Word2Vec(sentences, iter=10, min_count=1)\n",
      "print(model)\n",
      "\n",
      "model.wv.doesnt_match(\"las harcerz gaj zioa\".split())\n",
      "14/14:\n",
      "import smart_open, os\n",
      "\n",
      "if not os.path.exists('./data/'):\n",
      "    os.makedirs('./data/')\n",
      "\n",
      "filenames = ['./data/f' + str(i) +'.txt' for i in range(39)]\n",
      "\n",
      "if sentences is not None:\n",
      "    for i, fname in enumerate(filenames):\n",
      "        with smart_open.smart_open(fname, 'w') as fout:\n",
      "            for line in sentences[i]:\n",
      "                fout.write(line + ' ')\n",
      "14/15:\n",
      "import smart_open, os\n",
      "\n",
      "if not os.path.exists('./data/'):\n",
      "    os.makedirs('./data/')\n",
      "\n",
      "filenames = ['./data/f' + str(i) +'.txt' for i in range(39)]\n",
      "\n",
      "if sentences is not None:\n",
      "    for i, fname in enumerate(filenames):\n",
      "        with smart_open.smart_open(fname, 'w') as fout:\n",
      "            for line in sentences[i]:\n",
      "                fout.write(line + ' ')\n",
      "14/16:\n",
      "model = gensim.models.Word2Vec(sentences, iter=5, min_count=1)\n",
      "print(model)\n",
      "print(model.vocabulary)\n",
      "\n",
      "model.wv.doesnt_match(\"las harcerz gaj zioa\".split())\n",
      "14/17:\n",
      "import gensim, logging, re, nltk\n",
      "import pandas as pd\n",
      "\n",
      "RE_SPACES = re.compile(\"\\s+\")\n",
      "RE_HASHTAG = re.compile(\"[@#][_a-z0-9]+\")\n",
      "RE_EMOTICONS = re.compile(\"(:-?\\))|(:p)|(:d+)|(:-?\\()|(:/)|(;-?\\))|(<3)|(=\\))|(\\)-?:)|(:'\\()|(8\\))\")\n",
      "RE_HTTP = re.compile(\"http(s)?://[/\\.a-z0-9]+\")\n",
      "\n",
      "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
      " \n",
      "song = \"\"\"Gdzie strumyk pynie z wolna,\n",
      "Rozsiewa zioa maj,\n",
      "Stokrotka rosa polna,\n",
      "A nad ni szumia gaj,\n",
      "Stokrotka rosa polna,\n",
      "A nad ni szumia gaj,\n",
      "Zielony gaj.\n",
      "\n",
      "W tym gaju tak ponuro,\n",
      "e a przeraa mnie,\n",
      "Ptaszta za wysoko,\n",
      "A mnie samotnej le,\n",
      "Ptaszta za wysoko,\n",
      "A mnie samotnej le,\n",
      "samotnej le.\n",
      "\n",
      "Wtem harcerz idzie z wolna.\n",
      "Stokrotko, witam ci,\n",
      "Twj urok mnie zachwyca,\n",
      "Czy chcesz by m, czy nie?\n",
      "\"Twj urok mnie zachwyca,\n",
      "Czy chcesz by m, czy nie?\n",
      "Czy nie, czy nie?\n",
      "\n",
      "Stokrotka si zgodzia\n",
      "I poszli w ciemny las,\n",
      "A harcerz taki gapa\n",
      "e a w pokrzywy wlaz,\n",
      "A harcerz taki gapa\n",
      "e a w pokrzywy wlaz,\n",
      "w pokrzywy wlaz.\n",
      "\n",
      "A ona, ona, ona,\n",
      "C biedna robi ma,\n",
      "Nad gap pochylona\n",
      "I mieje si: ha, ha,\n",
      "Nad gap pochylona\n",
      "I mieje: si ha, ha,\n",
      "ha, ha, ha, ha.\"\"\"\n",
      "14/18:\n",
      "sentences = [verse.split(' ') for verse in song.split('\\n')]\n",
      "print(sentences)\n",
      "14/19:\n",
      "model = gensim.models.Word2Vec(sentences, iter=5, min_count=1)\n",
      "print(model)\n",
      "print(model.vocabulary)\n",
      "\n",
      "model.wv.doesnt_match(\"las harcerz gaj zioa\".split())\n",
      "14/20:\n",
      "import smart_open, os\n",
      "\n",
      "if not os.path.exists('./data/'):\n",
      "    os.makedirs('./data/')\n",
      "\n",
      "filenames = ['./data/f' + str(i) +'.txt' for i in range(39)]\n",
      "\n",
      "if sentences is not None:\n",
      "    for i, fname in enumerate(filenames):\n",
      "        with smart_open.smart_open(fname, 'w') as fout:\n",
      "            for line in sentences[i]:\n",
      "                fout.write(line + ' ')\n",
      "14/21:\n",
      "class MySentences(object):\n",
      "    def __init__(self, dirname):\n",
      "        self.dirname = dirname\n",
      " \n",
      "    def __iter__(self):\n",
      "        for fname in os.listdir(self.dirname):\n",
      "            if fname.endswith('.txt'):\n",
      "                for line in open(os.path.join(self.dirname, fname)):\n",
      "                    x = line.translate(str.maketrans('', '', string.punctuation)).lower().strip()\n",
      "                    yield x.split(' ')\n",
      "\n",
      "# Do odkomentowania:\n",
      "sentences = MySentences('./data/')\n",
      "model = gensim.models.Word2Vec(sentences, iter=10, min_count=1)\n",
      "print(model)\n",
      "\n",
      "model.wv.doesnt_match(\"las harcerz gaj zioa\".split())\n",
      "14/22:\n",
      "model = gensim.models.Word2Vec(sentences, iter=5, min_count=1)\n",
      "print(model)\n",
      "print(model.vocabulary)\n",
      "\n",
      "model.wv.doesnt_match(\"las harcerz gaj zioa\".split())\n",
      "14/23:\n",
      "import string\n",
      "class MySentences(object):\n",
      "    def __init__(self, dirname):\n",
      "        self.dirname = dirname\n",
      " \n",
      "    def __iter__(self):\n",
      "        for fname in os.listdir(self.dirname):\n",
      "            if fname.endswith('.txt'):\n",
      "                for line in open(os.path.join(self.dirname, fname)):\n",
      "                    x = line.translate(str.maketrans('', '', string.punctuation)).lower().strip()\n",
      "                    yield x.split(' ')\n",
      "\n",
      "# Do odkomentowania:\n",
      "sentences = MySentences('./data/')\n",
      "model = gensim.models.Word2Vec(sentences, iter=10, min_count=1)\n",
      "print(model)\n",
      "\n",
      "model.wv.doesnt_match(\"las harcerz gaj zioa\".split())\n",
      "14/24:\n",
      "test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data']) + os.sep\n",
      "lee_train_file = test_data_dir + 'lee_background.cor'\n",
      "\n",
      "class MyText(object):\n",
      "    def __iter__(self):\n",
      "        for line in open(lee_train_file):\n",
      "            # Za, e kada linia to dokument, zmie litery na mae,\n",
      "            # usu podstawowe znaki interpunkcyjne i podziel wedug biaych znakw\n",
      "            yield line.translate(str.maketrans('', '', string.punctuation)).lower().strip().split(' ')\n",
      "\n",
      "sentences = MyText()\n",
      "14/25:\n",
      "model = None\n",
      "print(model)\n",
      "14/26:\n",
      "model = gensim.models.Word2Vec(sentences, size=200, iter=100, min_count=5)\n",
      "print(model)\n",
      "14/27: model.wv.most_similar(positive=['human', 'crime'], negative=['party'], topn=1)\n",
      "14/28: model.wv.doesnt_match(\"input is lunch he sentence cat\".split())\n",
      "14/29:\n",
      "print(model.wv.similarity('human', 'tree'))\n",
      "print(model.wv.similarity('crime', 'murder'))\n",
      "14/30:\n",
      "model = gensim.models.Word2Vec(sentences, iter=5, min_count=1)\n",
      "print(model)\n",
      "print(model.vocabulary)\n",
      "\n",
      "model.wv.doesnt_match(\"las harcerz gaj zioa\".split())\n",
      "14/31:\n",
      "import gensim, logging, re, nltk\n",
      "import pandas as pd\n",
      "\n",
      "RE_SPACES = re.compile(\"\\s+\")\n",
      "RE_HASHTAG = re.compile(\"[@#][_a-z0-9]+\")\n",
      "RE_EMOTICONS = re.compile(\"(:-?\\))|(:p)|(:d+)|(:-?\\()|(:/)|(;-?\\))|(<3)|(=\\))|(\\)-?:)|(:'\\()|(8\\))\")\n",
      "RE_HTTP = re.compile(\"http(s)?://[/\\.a-z0-9]+\")\n",
      "\n",
      "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
      " \n",
      "song = \"\"\"Gdzie strumyk pynie z wolna,\n",
      "Rozsiewa zioa maj,\n",
      "Stokrotka rosa polna,\n",
      "A nad ni szumia gaj,\n",
      "Stokrotka rosa polna,\n",
      "A nad ni szumia gaj,\n",
      "Zielony gaj.\n",
      "\n",
      "W tym gaju tak ponuro,\n",
      "e a przeraa mnie,\n",
      "Ptaszta za wysoko,\n",
      "A mnie samotnej le,\n",
      "Ptaszta za wysoko,\n",
      "A mnie samotnej le,\n",
      "samotnej le.\n",
      "\n",
      "Wtem harcerz idzie z wolna.\n",
      "Stokrotko, witam ci,\n",
      "Twj urok mnie zachwyca,\n",
      "Czy chcesz by m, czy nie?\n",
      "\"Twj urok mnie zachwyca,\n",
      "Czy chcesz by m, czy nie?\n",
      "Czy nie, czy nie?\n",
      "\n",
      "Stokrotka si zgodzia\n",
      "I poszli w ciemny las,\n",
      "A harcerz taki gapa\n",
      "e a w pokrzywy wlaz,\n",
      "A harcerz taki gapa\n",
      "e a w pokrzywy wlaz,\n",
      "w pokrzywy wlaz.\n",
      "\n",
      "A ona, ona, ona,\n",
      "C biedna robi ma,\n",
      "Nad gap pochylona\n",
      "I mieje si: ha, ha,\n",
      "Nad gap pochylona\n",
      "I mieje: si ha, ha,\n",
      "ha, ha, ha, ha.\"\"\"\n",
      "14/32:\n",
      "sentences = [verse.split(' ') for verse in song.split('\\n')]\n",
      "print(sentences)\n",
      "14/33:\n",
      "model = gensim.models.Word2Vec(sentences, iter=5, min_count=1)\n",
      "print(model)\n",
      "print(model.vocabulary)\n",
      "\n",
      "model.wv.doesnt_match(\"las harcerz gaj zioa\".split())\n",
      "14/34:\n",
      "import smart_open, os\n",
      "\n",
      "if not os.path.exists('./data/'):\n",
      "    os.makedirs('./data/')\n",
      "\n",
      "filenames = ['./data/f' + str(i) +'.txt' for i in range(39)]\n",
      "\n",
      "if sentences is not None:\n",
      "    for i, fname in enumerate(filenames):\n",
      "        with smart_open.smart_open(fname, 'w') as fout:\n",
      "            for line in sentences[i]:\n",
      "                fout.write(line + ' ')\n",
      "14/35:\n",
      "import string\n",
      "class MySentences(object):\n",
      "    def __init__(self, dirname):\n",
      "        self.dirname = dirname\n",
      " \n",
      "    def __iter__(self):\n",
      "        for fname in os.listdir(self.dirname):\n",
      "            if fname.endswith('.txt'):\n",
      "                for line in open(os.path.join(self.dirname, fname)):\n",
      "                    x = line.translate(str.maketrans('', '', string.punctuation)).lower().strip()\n",
      "                    yield x.split(' ')\n",
      "\n",
      "# Do odkomentowania:\n",
      "sentences = MySentences('./data/')\n",
      "model = gensim.models.Word2Vec(sentences, iter=10, min_count=1)\n",
      "print(model)\n",
      "\n",
      "model.wv.doesnt_match(\"las harcerz gaj zioa\".split())\n",
      "14/36:\n",
      "test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data']) + os.sep\n",
      "lee_train_file = test_data_dir + 'lee_background.cor'\n",
      "\n",
      "class MyText(object):\n",
      "    def __iter__(self):\n",
      "        for line in open(lee_train_file):\n",
      "            # Za, e kada linia to dokument, zmie litery na mae,\n",
      "            # usu podstawowe znaki interpunkcyjne i podziel wedug biaych znakw\n",
      "            yield line.translate(str.maketrans('', '', string.punctuation)).lower().strip().split(' ')\n",
      "\n",
      "sentences = MyText()\n",
      "14/37:\n",
      "model = gensim.models.Word2Vec(sentences, size=200, iter=100, min_count=5)\n",
      "print(model)\n",
      "14/38: model.wv.most_similar(positive=['human', 'crime'], negative=['party'], topn=1)\n",
      "14/39: model.wv.doesnt_match(\"input is lunch he sentence cat\".split())\n",
      "14/40:\n",
      "print(model.wv.similarity('human', 'tree'))\n",
      "print(model.wv.similarity('crime', 'murder'))\n",
      "14/41:\n",
      "%%time \n",
      "wv = gensim.models.KeyedVectors.load_word2vec_format(\"data/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
      "wv.init_sims(replace=True)\n",
      "14/42:\n",
      "%%time \n",
      "wv = gensim.models.KeyedVectors.load_word2vec_format(\"data/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
      "wv.init_sims(replace=True)\n",
      "14/43: wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
      "14/44: wv.doesnt_match(\"breakfast cereal dinner lunch\".split())\n",
      "14/45:\n",
      "print(wv.similarity('woman', 'man'))\n",
      "print(wv.similarity('woman', 'cat'))\n",
      "14/46:\n",
      "def word_averaging(wv, words):\n",
      "    all_words, mean = set(), []\n",
      "    \n",
      "    for word in words:\n",
      "        if isinstance(word, np.ndarray):\n",
      "            mean.append(word)\n",
      "        elif word in wv.vocab:\n",
      "            mean.append(wv.vectors_norm[wv.vocab[word].index])\n",
      "            all_words.add(wv.vocab[word].index)\n",
      "\n",
      "    if not mean:\n",
      "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
      "        return np.zeros(wv.layer_size,)\n",
      "\n",
      "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
      "    return mean\n",
      "\n",
      "def  word_averaging_list(wv, text_list):\n",
      "    return np.vstack([word_averaging(wv, review) for review in text_list ])\n",
      "\n",
      "def w2v_tokenize_text(text):\n",
      "    tokens = []\n",
      "    for sent in nltk.sent_tokenize(text, language='english'):\n",
      "        for word in nltk.word_tokenize(sent, language='english'):\n",
      "            if len(word) < 2:\n",
      "                continue\n",
      "            tokens.append(word)\n",
      "    return tokens\n",
      "14/47:\n",
      "%matplotlib inline\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "df = pd.read_csv('data/tagged_plots_movielens.csv')\n",
      "df = df.dropna()\n",
      "\n",
      "print(df.head())\n",
      "df.tag.value_counts().plot(kind=\"bar\", rot=0)\n",
      "14/48: train_data, test_data = train_test_split(df, test_size=0.1, random_state=42)\n",
      "14/49:\n",
      "test_tokenized = test_data.apply(lambda r: w2v_tokenize_text(r['plot']), axis=1).values\n",
      "train_tokenized = train_data.apply(lambda r: w2v_tokenize_text(r['plot']), axis=1).values\n",
      "X_train_word_average = word_averaging_list(wv,train_tokenized)\n",
      "X_test_word_average = word_averaging_list(wv,test_tokenized)\n",
      "14/50:\n",
      "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
      "\n",
      "logreg = logreg.fit(X_train_word_average, train_data['tag'])\n",
      "predicted = logreg.predict(X_test_word_average)\n",
      "14/51:\n",
      "print('Trafno klasyfikacji %s' % accuracy_score(test_data.tag, predicted))\n",
      "cm = confusion_matrix(test_data.tag, predicted)\n",
      "print('Macierz pomyek\\n %s' % cm)\n",
      "16/1:\n",
      "print('PyDev console: using IPython 7.19.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['C:\\\\Users\\\\luker\\\\Desktop\\\\RL_pysc2-master', 'C:/Users/luker/Desktop/RL_pysc2-master'])\n",
      "16/2: pip install torch===1.7.1+cu110 torchvision===0.8.2+cu110 torchaudio===0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
      "16/3: pip install torch===1.7.1+cu110 torchvision===0.8.2+cu110 torchaudio===0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
      "16/4: pip install torch===1.7.1+cu110\n",
      "16/5: !pip install torch===1.7.1+cu110\n",
      "17/1:\n",
      "print('PyDev console: using IPython 7.19.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['C:\\\\Users\\\\luker\\\\Desktop\\\\RL_pysc2-master', 'C:/Users/luker/Desktop/RL_pysc2-master'])\n",
      "18/1:\n",
      "print('PyDev console: using IPython 7.19.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['C:\\\\Users\\\\luker\\\\Documents\\\\Github\\\\SkaiWiD', 'C:/Users/luker/Documents/Github/SkaiWiD'])\n",
      "19/1:\n",
      "from google.colab import drive\n",
      "\n",
      "drive.mount('/content/gdrive')\n",
      "19/2:\n",
      "import pandas\n",
      "from collections import Counter\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import nltk\n",
      "nltk.download('punkt')\n",
      "%matplotlib inline\n",
      "\n",
      "df = pandas.read_csv('../data/reddit_wsb.csv')\n",
      "df[\"body\"] = df[\"body\"].fillna('')\n",
      "df[\"title\"] = df[\"title\"].fillna('')\n",
      "\n",
      "df\n",
      "19/3:\n",
      "# from google.colab import drive\n",
      "\n",
      "# drive.mount('/content/gdrive')\n",
      "19/4:\n",
      "# !ls gdrive/MyDrive/PED\n",
      "# df = pandas.read_csv('gdrive/MyDrive/PED/reddit_wsb.csv')\n",
      "19/5:\n",
      "import pandas\n",
      "from collections import Counter\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import nltk\n",
      "nltk.download('punkt')\n",
      "%matplotlib inline\n",
      "\n",
      "df = pandas.read_csv('../data/reddit_wsb.csv')\n",
      "df[\"body\"] = df[\"body\"].fillna('')\n",
      "df[\"title\"] = df[\"title\"].fillna('')\n",
      "\n",
      "df\n",
      "19/6:\n",
      "seconds_in_day = 60 * 60 * 24.\n",
      "df['sin_time'] = np.sin(2*np.pi*(df.created % seconds_in_day)/seconds_in_day)\n",
      "df['cos_time'] = np.cos(2*np.pi*(df.created % seconds_in_day)/seconds_in_day)\n",
      "19/7: df.describe()\n",
      "19/8: df.sample(5000).plot.scatter('sin_time','cos_time').set_aspect('equal')\n",
      "19/9: df.columns\n",
      "19/10:\n",
      "plt.hist(df['score'], bins=30)\n",
      "plt.ylabel('Count')\n",
      "plt.yscale('log')\n",
      "plt.xlabel('Data');\n",
      "19/11:\n",
      "plt.hist(df['created'], bins=30)\n",
      "plt.ylabel('Count')\n",
      "plt.yscale('log')\n",
      "plt.xlabel('Data');\n",
      "19/12:\n",
      "!pip install emoji\n",
      "from collections import Counter\n",
      "from nltk import word_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
      "\n",
      "nltk.download('stopwords')\n",
      "# tokenizer = RegexpTokenizer(r'\\w+')\n",
      "from nltk.corpus import stopwords\n",
      "from collections import Counter\n",
      "19/13:\n",
      "# from google.colab import drive\n",
      "\n",
      "# drive.mount('/content/gdrive')\n",
      "19/14:\n",
      "# !ls gdrive/MyDrive/PED\n",
      "# df = pandas.read_csv('gdrive/MyDrive/PED/reddit_wsb.csv')\n",
      "19/15:\n",
      "import pandas\n",
      "from collections import Counter\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import nltk\n",
      "nltk.download('punkt')\n",
      "%matplotlib inline\n",
      "\n",
      "df = pandas.read_csv('../data/reddit_wsb.csv')\n",
      "df[\"body\"] = df[\"body\"].fillna('')\n",
      "df[\"title\"] = df[\"title\"].fillna('')\n",
      "\n",
      "df\n",
      "19/16:\n",
      "seconds_in_day = 60 * 60 * 24.\n",
      "df['sin_time'] = np.sin(2*np.pi*(df.created % seconds_in_day)/seconds_in_day)\n",
      "df['cos_time'] = np.cos(2*np.pi*(df.created % seconds_in_day)/seconds_in_day)\n",
      "19/17: df.describe()\n",
      "19/18: df.sample(5000).plot.scatter('sin_time','cos_time').set_aspect('equal')\n",
      "19/19: df.columns\n",
      "19/20:\n",
      "plt.hist(df['score'], bins=30)\n",
      "plt.ylabel('Count')\n",
      "plt.yscale('log')\n",
      "plt.xlabel('Data');\n",
      "19/21:\n",
      "plt.hist(df['created'], bins=30)\n",
      "plt.ylabel('Count')\n",
      "plt.yscale('log')\n",
      "plt.xlabel('Data');\n",
      "19/22:\n",
      "#!pip install emoji\n",
      "from collections import Counter\n",
      "from nltk import word_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
      "\n",
      "nltk.download('stopwords')\n",
      "# tokenizer = RegexpTokenizer(r'\\w+')\n",
      "from nltk.corpus import stopwords\n",
      "from collections import Counter\n",
      "19/23:\n",
      "body_most_common = Counter(word_tokenize(\" \".join(df[\"body\"])))\n",
      "for key, cnts in list(body_most_common.items()):\n",
      "  if not key in emoji.UNICODE_EMOJI and (key in stopwords.words('english') or not re.match(r'\\w+',key)):\n",
      "    del body_most_common[key]\n",
      "body_most_common = body_most_common.most_common(100)\n",
      "body_most_common\n",
      "19/24:\n",
      "title_most_common = Counter(word_tokenize(\" \".join(df[\"title\"])))\n",
      "for key, cnts in list(title_most_common.items()):\n",
      "  if not key in emoji.UNICODE_EMOJI and (key in stopwords.words('english') or not re.match(r'\\w+',key)):\n",
      "    del title_most_common[key]\n",
      "title_most_common = title_most_common.most_common(100)\n",
      "title_most_common\n",
      "19/25:\n",
      "additional_keywords = ['gme', 'hold', 'buy', 'retard', 'wife', 'hedgies', '', 'moon','','', '', '','','', '', '']\n",
      "count = lambda l1,l2: sum([1 for x in l1 if x in l2])\n",
      "text_attributes = {}\n",
      "for key in additional_keywords: \n",
      "  text_attributes['title_' + key] = df[\"title\"].str.lower().str.count(key)\n",
      "  text_attributes['body_' + key] = df[\"body\"].str.lower().str.count(key)\n",
      "19/26:\n",
      "for key, cnts in body_most_common: \n",
      "  text_attributes['body_' + key.lower()] = df[\"body\"].str.lower().str.count(key.lower())\n",
      "\n",
      "\n",
      "for key, cnts in title_most_common: \n",
      "  text_attributes['title_' + key.lower()] = df[\"title\"].str.lower().str.count(key.lower())\n",
      "19/27:\n",
      "\n",
      "text_attributes['body_punctuation'] = df['body'].apply(lambda s: count(s, string.punctuation))\n",
      "text_attributes['title_punctuation'] = df['title'].apply(lambda s: count(s, string.punctuation))\n",
      "\n",
      "text_attributes['body_cap_ratio'] = df['body'].str.count(r'[A-Z]')/df['body'].str.count(r'[a-zA-Z]')\n",
      "text_attributes['title_cap_ratio'] = df['title'].str.count(r'[A-Z]')/df['title'].str.count(r'[a-zA-Z]')\n",
      "\n",
      "text_attributes['time_of_day'] = pandas.to_datetime(df['timestamp']).dt.hour + pandas.to_datetime(df['timestamp']).dt.minute/60\n",
      "\n",
      "text_attributes['title_length'] = df['title'].apply(len)\n",
      "text_attributes['title_non_alphanumeric_ratio'] = df['title'].str.count(r'[^A-Za-z0-9]')/df['title'].apply(len)\n",
      "19/28:\n",
      "text_attributes['link'] = df['body'].apply(lambda s: len(re.findall(r'(https?://[^\\s]+)', s)))\n",
      "text_attributes['reddit_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?redd\\.?it[^\\s]+)', s)))\n",
      "text_attributes['yt_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?youtu\\.?be[^\\s]+)', s)))\n",
      "text_attributes['tweet_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?twitter[^\\s]+)', s)))\n",
      "text_attributes['facebook_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?facebook[^\\s]+)', s)))\n",
      "\n",
      "text_attributes = pandas.DataFrame(text_attributes)\n",
      "text_attributes\n",
      "19/29: d.describe()\n",
      "20/1:\n",
      "import efficientnet_builder\n",
      "features, endpoints = efficientnet_builder.build_model_base(images, 'efficientnet-b4')\n",
      "20/2:\n",
      "import efficientnet.keras as efn \n",
      "model = efn.EfficientNetB4(weights='imagenet')\n",
      "20/3:\n",
      "from tensorflow.keras.applications import EfficientNetB0\n",
      "model = EfficientNetB0(weights='imagenet')\n",
      "21/1:\n",
      "print('PyDev console: using IPython 7.19.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['C:\\\\Users\\\\luker\\\\Documents\\\\Github\\\\PED', 'C:/Users/luker/Documents/Github/PED'])\n",
      "20/4:\n",
      "from tensorflow.keras.applications import EfficientNetB0\n",
      "model = EfficientNetB0(weights='imagenet')\n",
      "20/5:\n",
      "import os\n",
      "for file in os.listdir(\"../data/images\"):\n",
      "    print(os.path.join(\"../data/images\", file))\n",
      "# model.predict()\n",
      "20/6:\n",
      "import os\n",
      "for file in os.listdir(\"../data/images\"):\n",
      "    print(model.predict(file))\n",
      "20/7:\n",
      "import os\n",
      "for file in os.listdir(\"../data/images\"):\n",
      "    print(model.predict([file]))\n",
      "20/8:\n",
      "import os\n",
      "for file in os.listdir(\"../data/images\"):\n",
      "    print(file)\n",
      "    \n",
      "model.predict([file])\n",
      "20/9:\n",
      "import os\n",
      "from PIL import Image\n",
      "image = Image.open(\"image_path.jpg\")\n",
      "image.show()\n",
      "for file in os.listdir(\"../data/images\"):\n",
      "    print(file)\n",
      "    \n",
      "model.predict([file])\n",
      "20/10:\n",
      "import os\n",
      "from PIL import Image\n",
      "\n",
      "for file in os.listdir(\"../data/images\"):\n",
      "    image = Image.open(file)\n",
      "    image.show()\n",
      "    \n",
      "model.predict([file])\n",
      "20/11:\n",
      "import os\n",
      "from PIL import Image\n",
      "\n",
      "for file in os.listdir(\"../data/images\"):\n",
      "    image = Image.open(\"../data/images\" + file)\n",
      "    image.show()\n",
      "    \n",
      "model.predict([file])\n",
      "20/12:\n",
      "import os\n",
      "from PIL import Image\n",
      "\n",
      "for file in os.listdir(\"../data/images/\"):\n",
      "    image = Image.open(\"../data/images/\" + file)\n",
      "    image.show()\n",
      "    \n",
      "model.predict([file])\n",
      "20/13:\n",
      "import os\n",
      "from PIL import Image\n",
      "\n",
      "for file in os.listdir(\"../data/images/\"):\n",
      "    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "        image = Image.open(\"../data/images/\" + file)\n",
      "        image.show()\n",
      "    \n",
      "model.predict([file])\n",
      "20/14:\n",
      "import os\n",
      "import cv2\n",
      "\n",
      "for file in os.listdir(\"../data/images/\"):\n",
      "    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "        image = cv2.imread(\"../data/images/\" + file)\n",
      "        model.predict(image)\n",
      "20/15:\n",
      "import os\n",
      "from PIL import Image\n",
      "\n",
      "for file in os.listdir(\"../data/images/\"):\n",
      "    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "        image = Image.open(\"../data/images/\" + file)\n",
      "        model.predict(image)\n",
      "    \n",
      "model.predict([file])\n",
      "20/16:\n",
      "import os\n",
      "import cv2\n",
      "\n",
      "for file in os.listdir(\"../data/images/\"):\n",
      "    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "        image = cv2.imread(\"../data/images/\" + file)\n",
      "        model.predict(image)\n",
      "    \n",
      "model.predict([file])\n",
      "20/17:\n",
      "import os\n",
      "import cv2\n",
      "\n",
      "for file in os.listdir(\"../data/images/\"):\n",
      "    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "        image = cv2.imread(\"../data/images/\" + file)\n",
      "        image = cv2.resize(image, (224,224), interpolation = cv2.INTER_AREA)\n",
      "        print(model.predict(image))\n",
      "    \n",
      "model.predict([file])\n",
      "20/18:\n",
      "import os\n",
      "import cv2\n",
      "\n",
      "for file in os.listdir(\"../data/images/\"):\n",
      "    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "        image = cv2.imread(\"../data/images/\" + file)\n",
      "        image = cv2.resize(image, (224,224), interpolation = cv2.INTER_AREA)\n",
      "        cv2.imshow(image)\n",
      "        print(model.predict(image))\n",
      "    \n",
      "model.predict([file])\n",
      "20/19:\n",
      "import os\n",
      "import cv2\n",
      "\n",
      "for file in os.listdir(\"../data/images/\"):\n",
      "    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "        image = cv2.imread(\"../data/images/\" + file)\n",
      "        image = cv2.resize(image, (224,224), interpolation = cv2.INTER_AREA)\n",
      "        cv2.imshow(\"Resized image\", image)\n",
      "        print(model.predict(image))\n",
      "    \n",
      "model.predict([file])\n",
      "20/20:\n",
      "import os\n",
      "import cv2\n",
      "\n",
      "for file in os.listdir(\"../data/images/\"):\n",
      "    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "        print(file)\n",
      "        image = cv2.imread(\"../data/images/\" + file)\n",
      "        image = cv2.resize(image, (224,224), interpolation = cv2.INTER_AREA)\n",
      "        cv2.imshow(\"Resized image\", image)\n",
      "        print(model.predict(image))\n",
      "    \n",
      "model.predict([file])\n",
      "20/21:\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "image = cv2.resize(image, (224,224), interpolation = cv2.INTER_AREA)\n",
      "cv2.imshow(\"Resized image\", image)\n",
      "print(model.predict(image))\n",
      "20/22:\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "image = cv2.resize(image, (224,224), interpolation = cv2.INTER_AREA)\n",
      "cv2.imshow(\"Resized image\", image)\n",
      "# print(model.predict(image))\n",
      "20/23:\n",
      "import os\n",
      "import cv2\n",
      "import matplotlib as plt\n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "image = cv2.resize(image, (224,224), interpolation = cv2.INTER_AREA)\n",
      "plt.imshow(image)\n",
      "# print(model.predict(image))\n",
      "20/24:\n",
      "import os\n",
      "import cv2\n",
      "import matplotlib\n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "image = cv2.resize(image, (224,224), interpolation = cv2.INTER_AREA)\n",
      "plt.imshow(image)\n",
      "# print(model.predict(image))\n",
      "20/25:\n",
      "import os\n",
      "import cv2\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "image = cv2.resize(image, (224,224), interpolation = cv2.INTER_AREA)\n",
      "plt.imshow(image)\n",
      "# print(model.predict(image))\n",
      "20/26:\n",
      "import os\n",
      "import cv2\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "image = cv2.resize(image, (224,224), interpolation = cv2.INTER_AREA)\n",
      "plt.imshow(image)\n",
      "print(model.predict(image))\n",
      "20/27:\n",
      "import os\n",
      "import cv2\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "image = cv2.resize(image, (224,224), interpolation = cv2.INTER_AREA)\n",
      "plt.imshow(image)\n",
      "print(np.shape(image))\n",
      "print(model.predict(image))\n",
      "20/28:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "image = cv2.resize(image, (224,224), interpolation = cv2.INTER_AREA)\n",
      "plt.imshow(image)\n",
      "print(np.shape(image))\n",
      "print(model.predict(image))\n",
      "20/29:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "image = cv2.resize(image, (224,224), interpolation = cv2.INTER_AREA)\n",
      "plt.imshow(image)\n",
      "print(np.shape(image))\n",
      "print(model.predict([image]))\n",
      "20/30:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "image = cv2.resize(image, (224,224), interpolation = cv2.INTER_AREA)\n",
      "plt.imshow(image)\n",
      "print(np.shape(image))\n",
      "print(model.predict([[image]]))\n",
      "20/31:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "image = cv2.resize(image, (224,224), interpolation = cv2.INTER_AREA)\n",
      "plt.imshow(image)\n",
      "print(np.shape(image))\n",
      "print(model.predict([image]))\n",
      "20/32:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "image = tf.image.resize(image, size\n",
      "plt.imshow(image)\n",
      "print(np.shape(image))\n",
      "print(model.predict([image]))\n",
      "20/33:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "image = tf.image.resize(image, size\n",
      "plt.imshow(image)\n",
      "print(np.shape(image))\n",
      "print(model.predict([image]))\n",
      "20/34:\n",
      "for file in os.listdir(\"../data/images/\"):\n",
      "    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "        print(file)\n",
      "        image = cv2.imread(\"../data/images/\" + file)\n",
      "        image = cv2.resize(image, (224,224), interpolation = cv2.INTER_AREA)\n",
      "        cv2.imshow(\"Resized image\", image)\n",
      "        print(model.predict(image))\n",
      "    \n",
      "model.predict([file])\n",
      "20/35:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "image = tf.image.resize(image, size\n",
      "plt.imshow(image)\n",
      "print(np.shape(image))\n",
      "print(model.predict([image]))\n",
      "20/36:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "image = tf.image.resize(image, size)\n",
      "#plt.imshow(image)\n",
      "print(np.shape(image))\n",
      "print(model.predict([image]))\n",
      "20/37:\n",
      "from tensorflow.keras.applications import EfficientNetB0\n",
      "from tensorflow.image import resize\n",
      "model = EfficientNetB0(weights='imagenet')\n",
      "20/38:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "image = resize(image, size)\n",
      "plt.imshow(image)\n",
      "print(np.shape(image))\n",
      "print(model.predict([image]))\n",
      "20/39:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "image = resize(image, 224)\n",
      "plt.imshow(image)\n",
      "print(np.shape(image))\n",
      "print(model.predict([image]))\n",
      "20/40:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 224\n",
      "\n",
      "size = (IMG_SIZE, IMG_SIZE)\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "image = resize(image, size)\n",
      "plt.imshow(image)\n",
      "print(np.shape(image))\n",
      "print(model.predict([image]))\n",
      "20/41:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 224\n",
      "\n",
      "size = (IMG_SIZE, IMG_SIZE)\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "image = resize(image, size)\n",
      "plt.imshow(image)\n",
      "print(np.shape(image))\n",
      "print(model.predict(image))\n",
      "20/42:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 224\n",
      "\n",
      "size = (IMG_SIZE, IMG_SIZE)\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "image = resize(image, size)\n",
      "plt.imshow(image.numpy().astype(\"uint8\"))\n",
      "print(np.shape(image))\n",
      "print(model.predict(image))\n",
      "20/43:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 224\n",
      "\n",
      "size = (None, 224, 224, 3)\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "image = resize(image, size)\n",
      "plt.imshow(image.numpy().astype(\"uint8\"))\n",
      "print(np.shape(image))\n",
      "print(model.predict(image))\n",
      "20/44:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 224\n",
      "\n",
      "size = (IMG_SIZE, IMG_SIZE)\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "image = resize(image, size)\n",
      "plt.imshow(image.numpy().astype(\"uint8\"))\n",
      "print(np.shape(image))\n",
      "print(model.predict(image))\n",
      "20/45:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 224\n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "image = cv2.resize(image,(IMG_SIZE,IMG_SIZE))     # resize image to match model's expected sizing\n",
      "image = image.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "plt.imshow(image.numpy().astype(\"uint8\"))\n",
      "print(np.shape(image))\n",
      "print(model.predict(image))\n",
      "20/46:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 224\n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "image = cv2.resize(image,(IMG_SIZE,IMG_SIZE))     # resize image to match model's expected sizing\n",
      "image = image.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "plt.imshow(image)\n",
      "print(np.shape(image))\n",
      "print(model.predict(image))\n",
      "20/47:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 224\n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "image = cv2.resize(image,(IMG_SIZE,IMG_SIZE))     # resize image to match model's expected sizing\n",
      "image = image.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "plt.imshow(image[0])\n",
      "print(np.shape(image))\n",
      "print(model.predict(image))\n",
      "20/48:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 224\n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "image = cv2.resize(image,(IMG_SIZE,IMG_SIZE))     # resize image to match model's expected sizing\n",
      "image = image.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "plt.imshow(image[0])\n",
      "print(np.shape(image))\n",
      "print(np.argmax(model.predict(image), axis=1))\n",
      "20/49:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 224\n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "image = cv2.resize(image,(IMG_SIZE,IMG_SIZE))     # resize image to match model's expected sizing\n",
      "image = image.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "plt.imshow(image[0])\n",
      "print(np.shape(image))\n",
      "print(classes[np.argmax(model.predict(image), axis=1)])\n",
      "20/50:\n",
      "from tensorflow.keras.applications import EfficientNetB0\n",
      "from tensorflow.image import resize\n",
      "model = EfficientNetB0(weights='imagenet')\n",
      "\n",
      "class_dict = {0: 'tench, Tinca tinca',\n",
      " 1: 'goldfish, Carassius auratus',\n",
      " 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n",
      " 3: 'tiger shark, Galeocerdo cuvieri',\n",
      " 4: 'hammerhead, hammerhead shark',\n",
      " 5: 'electric ray, crampfish, numbfish, torpedo',\n",
      " 6: 'stingray',\n",
      " 7: 'cock',\n",
      " 8: 'hen',\n",
      " 9: 'ostrich, Struthio camelus',\n",
      " 10: 'brambling, Fringilla montifringilla',\n",
      " 11: 'goldfinch, Carduelis carduelis',\n",
      " 12: 'house finch, linnet, Carpodacus mexicanus',\n",
      " 13: 'junco, snowbird',\n",
      " 14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea',\n",
      " 15: 'robin, American robin, Turdus migratorius',\n",
      " 16: 'bulbul',\n",
      " 17: 'jay',\n",
      " 18: 'magpie',\n",
      " 19: 'chickadee',\n",
      " 20: 'water ouzel, dipper',\n",
      " 21: 'kite',\n",
      " 22: 'bald eagle, American eagle, Haliaeetus leucocephalus',\n",
      " 23: 'vulture',\n",
      " 24: 'great grey owl, great gray owl, Strix nebulosa',\n",
      " 25: 'European fire salamander, Salamandra salamandra',\n",
      " 26: 'common newt, Triturus vulgaris',\n",
      " 27: 'eft',\n",
      " 28: 'spotted salamander, Ambystoma maculatum',\n",
      " 29: 'axolotl, mud puppy, Ambystoma mexicanum',\n",
      " 30: 'bullfrog, Rana catesbeiana',\n",
      " 31: 'tree frog, tree-frog',\n",
      " 32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui',\n",
      " 33: 'loggerhead, loggerhead turtle, Caretta caretta',\n",
      " 34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea',\n",
      " 35: 'mud turtle',\n",
      " 36: 'terrapin',\n",
      " 37: 'box turtle, box tortoise',\n",
      " 38: 'banded gecko',\n",
      " 39: 'common iguana, iguana, Iguana iguana',\n",
      " 40: 'American chameleon, anole, Anolis carolinensis',\n",
      " 41: 'whiptail, whiptail lizard',\n",
      " 42: 'agama',\n",
      " 43: 'frilled lizard, Chlamydosaurus kingi',\n",
      " 44: 'alligator lizard',\n",
      " 45: 'Gila monster, Heloderma suspectum',\n",
      " 46: 'green lizard, Lacerta viridis',\n",
      " 47: 'African chameleon, Chamaeleo chamaeleon',\n",
      " 48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis',\n",
      " 49: 'African crocodile, Nile crocodile, Crocodylus niloticus',\n",
      " 50: 'American alligator, Alligator mississipiensis',\n",
      " 51: 'triceratops',\n",
      " 52: 'thunder snake, worm snake, Carphophis amoenus',\n",
      " 53: 'ringneck snake, ring-necked snake, ring snake',\n",
      " 54: 'hognose snake, puff adder, sand viper',\n",
      " 55: 'green snake, grass snake',\n",
      " 56: 'king snake, kingsnake',\n",
      " 57: 'garter snake, grass snake',\n",
      " 58: 'water snake',\n",
      " 59: 'vine snake',\n",
      " 60: 'night snake, Hypsiglena torquata',\n",
      " 61: 'boa constrictor, Constrictor constrictor',\n",
      " 62: 'rock python, rock snake, Python sebae',\n",
      " 63: 'Indian cobra, Naja naja',\n",
      " 64: 'green mamba',\n",
      " 65: 'sea snake',\n",
      " 66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus',\n",
      " 67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus',\n",
      " 68: 'sidewinder, horned rattlesnake, Crotalus cerastes',\n",
      " 69: 'trilobite',\n",
      " 70: 'harvestman, daddy longlegs, Phalangium opilio',\n",
      " 71: 'scorpion',\n",
      " 72: 'black and gold garden spider, Argiope aurantia',\n",
      " 73: 'barn spider, Araneus cavaticus',\n",
      " 74: 'garden spider, Aranea diademata',\n",
      " 75: 'black widow, Latrodectus mactans',\n",
      " 76: 'tarantula',\n",
      " 77: 'wolf spider, hunting spider',\n",
      " 78: 'tick',\n",
      " 79: 'centipede',\n",
      " 80: 'black grouse',\n",
      " 81: 'ptarmigan',\n",
      " 82: 'ruffed grouse, partridge, Bonasa umbellus',\n",
      " 83: 'prairie chicken, prairie grouse, prairie fowl',\n",
      " 84: 'peacock',\n",
      " 85: 'quail',\n",
      " 86: 'partridge',\n",
      " 87: 'African grey, African gray, Psittacus erithacus',\n",
      " 88: 'macaw',\n",
      " 89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n",
      " 90: 'lorikeet',\n",
      " 91: 'coucal',\n",
      " 92: 'bee eater',\n",
      " 93: 'hornbill',\n",
      " 94: 'hummingbird',\n",
      " 95: 'jacamar',\n",
      " 96: 'toucan',\n",
      " 97: 'drake',\n",
      " 98: 'red-breasted merganser, Mergus serrator',\n",
      " 99: 'goose',\n",
      " 100: 'black swan, Cygnus atratus',\n",
      " 101: 'tusker',\n",
      " 102: 'echidna, spiny anteater, anteater',\n",
      " 103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n",
      " 104: 'wallaby, brush kangaroo',\n",
      " 105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n",
      " 106: 'wombat',\n",
      " 107: 'jellyfish',\n",
      " 108: 'sea anemone, anemone',\n",
      " 109: 'brain coral',\n",
      " 110: 'flatworm, platyhelminth',\n",
      " 111: 'nematode, nematode worm, roundworm',\n",
      " 112: 'conch',\n",
      " 113: 'snail',\n",
      " 114: 'slug',\n",
      " 115: 'sea slug, nudibranch',\n",
      " 116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore',\n",
      " 117: 'chambered nautilus, pearly nautilus, nautilus',\n",
      " 118: 'Dungeness crab, Cancer magister',\n",
      " 119: 'rock crab, Cancer irroratus',\n",
      " 120: 'fiddler crab',\n",
      " 121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica',\n",
      " 122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus',\n",
      " 123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish',\n",
      " 124: 'crayfish, crawfish, crawdad, crawdaddy',\n",
      " 125: 'hermit crab',\n",
      " 126: 'isopod',\n",
      " 127: 'white stork, Ciconia ciconia',\n",
      " 128: 'black stork, Ciconia nigra',\n",
      " 129: 'spoonbill',\n",
      " 130: 'flamingo',\n",
      " 131: 'little blue heron, Egretta caerulea',\n",
      " 132: 'American egret, great white heron, Egretta albus',\n",
      " 133: 'bittern',\n",
      " 134: 'crane',\n",
      " 135: 'limpkin, Aramus pictus',\n",
      " 136: 'European gallinule, Porphyrio porphyrio',\n",
      " 137: 'American coot, marsh hen, mud hen, water hen, Fulica americana',\n",
      " 138: 'bustard',\n",
      " 139: 'ruddy turnstone, Arenaria interpres',\n",
      " 140: 'red-backed sandpiper, dunlin, Erolia alpina',\n",
      " 141: 'redshank, Tringa totanus',\n",
      " 142: 'dowitcher',\n",
      " 143: 'oystercatcher, oyster catcher',\n",
      " 144: 'pelican',\n",
      " 145: 'king penguin, Aptenodytes patagonica',\n",
      " 146: 'albatross, mollymawk',\n",
      " 147: 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus',\n",
      " 148: 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca',\n",
      " 149: 'dugong, Dugong dugon',\n",
      " 150: 'sea lion',\n",
      " 151: 'Chihuahua',\n",
      " 152: 'Japanese spaniel',\n",
      " 153: 'Maltese dog, Maltese terrier, Maltese',\n",
      " 154: 'Pekinese, Pekingese, Peke',\n",
      " 155: 'Shih-Tzu',\n",
      " 156: 'Blenheim spaniel',\n",
      " 157: 'papillon',\n",
      " 158: 'toy terrier',\n",
      " 159: 'Rhodesian ridgeback',\n",
      " 160: 'Afghan hound, Afghan',\n",
      " 161: 'basset, basset hound',\n",
      " 162: 'beagle',\n",
      " 163: 'bloodhound, sleuthhound',\n",
      " 164: 'bluetick',\n",
      " 165: 'black-and-tan coonhound',\n",
      " 166: 'Walker hound, Walker foxhound',\n",
      " 167: 'English foxhound',\n",
      " 168: 'redbone',\n",
      " 169: 'borzoi, Russian wolfhound',\n",
      " 170: 'Irish wolfhound',\n",
      " 171: 'Italian greyhound',\n",
      " 172: 'whippet',\n",
      " 173: 'Ibizan hound, Ibizan Podenco',\n",
      " 174: 'Norwegian elkhound, elkhound',\n",
      " 175: 'otterhound, otter hound',\n",
      " 176: 'Saluki, gazelle hound',\n",
      " 177: 'Scottish deerhound, deerhound',\n",
      " 178: 'Weimaraner',\n",
      " 179: 'Staffordshire bullterrier, Staffordshire bull terrier',\n",
      " 180: 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier',\n",
      " 181: 'Bedlington terrier',\n",
      " 182: 'Border terrier',\n",
      " 183: 'Kerry blue terrier',\n",
      " 184: 'Irish terrier',\n",
      " 185: 'Norfolk terrier',\n",
      " 186: 'Norwich terrier',\n",
      " 187: 'Yorkshire terrier',\n",
      " 188: 'wire-haired fox terrier',\n",
      " 189: 'Lakeland terrier',\n",
      " 190: 'Sealyham terrier, Sealyham',\n",
      " 191: 'Airedale, Airedale terrier',\n",
      " 192: 'cairn, cairn terrier',\n",
      " 193: 'Australian terrier',\n",
      " 194: 'Dandie Dinmont, Dandie Dinmont terrier',\n",
      " 195: 'Boston bull, Boston terrier',\n",
      " 196: 'miniature schnauzer',\n",
      " 197: 'giant schnauzer',\n",
      " 198: 'standard schnauzer',\n",
      " 199: 'Scotch terrier, Scottish terrier, Scottie',\n",
      " 200: 'Tibetan terrier, chrysanthemum dog',\n",
      " 201: 'silky terrier, Sydney silky',\n",
      " 202: 'soft-coated wheaten terrier',\n",
      " 203: 'West Highland white terrier',\n",
      " 204: 'Lhasa, Lhasa apso',\n",
      " 205: 'flat-coated retriever',\n",
      " 206: 'curly-coated retriever',\n",
      " 207: 'golden retriever',\n",
      " 208: 'Labrador retriever',\n",
      " 209: 'Chesapeake Bay retriever',\n",
      " 210: 'German short-haired pointer',\n",
      " 211: 'vizsla, Hungarian pointer',\n",
      " 212: 'English setter',\n",
      " 213: 'Irish setter, red setter',\n",
      " 214: 'Gordon setter',\n",
      " 215: 'Brittany spaniel',\n",
      " 216: 'clumber, clumber spaniel',\n",
      " 217: 'English springer, English springer spaniel',\n",
      " 218: 'Welsh springer spaniel',\n",
      " 219: 'cocker spaniel, English cocker spaniel, cocker',\n",
      " 220: 'Sussex spaniel',\n",
      " 221: 'Irish water spaniel',\n",
      " 222: 'kuvasz',\n",
      " 223: 'schipperke',\n",
      " 224: 'groenendael',\n",
      " 225: 'malinois',\n",
      " 226: 'briard',\n",
      " 227: 'kelpie',\n",
      " 228: 'komondor',\n",
      " 229: 'Old English sheepdog, bobtail',\n",
      " 230: 'Shetland sheepdog, Shetland sheep dog, Shetland',\n",
      " 231: 'collie',\n",
      " 232: 'Border collie',\n",
      " 233: 'Bouvier des Flandres, Bouviers des Flandres',\n",
      " 234: 'Rottweiler',\n",
      " 235: 'German shepherd, German shepherd dog, German police dog, alsatian',\n",
      " 236: 'Doberman, Doberman pinscher',\n",
      " 237: 'miniature pinscher',\n",
      " 238: 'Greater Swiss Mountain dog',\n",
      " 239: 'Bernese mountain dog',\n",
      " 240: 'Appenzeller',\n",
      " 241: 'EntleBucher',\n",
      " 242: 'boxer',\n",
      " 243: 'bull mastiff',\n",
      " 244: 'Tibetan mastiff',\n",
      " 245: 'French bulldog',\n",
      " 246: 'Great Dane',\n",
      " 247: 'Saint Bernard, St Bernard',\n",
      " 248: 'Eskimo dog, husky',\n",
      " 249: 'malamute, malemute, Alaskan malamute',\n",
      " 250: 'Siberian husky',\n",
      " 251: 'dalmatian, coach dog, carriage dog',\n",
      " 252: 'affenpinscher, monkey pinscher, monkey dog',\n",
      " 253: 'basenji',\n",
      " 254: 'pug, pug-dog',\n",
      " 255: 'Leonberg',\n",
      " 256: 'Newfoundland, Newfoundland dog',\n",
      " 257: 'Great Pyrenees',\n",
      " 258: 'Samoyed, Samoyede',\n",
      " 259: 'Pomeranian',\n",
      " 260: 'chow, chow chow',\n",
      " 261: 'keeshond',\n",
      " 262: 'Brabancon griffon',\n",
      " 263: 'Pembroke, Pembroke Welsh corgi',\n",
      " 264: 'Cardigan, Cardigan Welsh corgi',\n",
      " 265: 'toy poodle',\n",
      " 266: 'miniature poodle',\n",
      " 267: 'standard poodle',\n",
      " 268: 'Mexican hairless',\n",
      " 269: 'timber wolf, grey wolf, gray wolf, Canis lupus',\n",
      " 270: 'white wolf, Arctic wolf, Canis lupus tundrarum',\n",
      " 271: 'red wolf, maned wolf, Canis rufus, Canis niger',\n",
      " 272: 'coyote, prairie wolf, brush wolf, Canis latrans',\n",
      " 273: 'dingo, warrigal, warragal, Canis dingo',\n",
      " 274: 'dhole, Cuon alpinus',\n",
      " 275: 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus',\n",
      " 276: 'hyena, hyaena',\n",
      " 277: 'red fox, Vulpes vulpes',\n",
      " 278: 'kit fox, Vulpes macrotis',\n",
      " 279: 'Arctic fox, white fox, Alopex lagopus',\n",
      " 280: 'grey fox, gray fox, Urocyon cinereoargenteus',\n",
      " 281: 'tabby, tabby cat',\n",
      " 282: 'tiger cat',\n",
      " 283: 'Persian cat',\n",
      " 284: 'Siamese cat, Siamese',\n",
      " 285: 'Egyptian cat',\n",
      " 286: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n",
      " 287: 'lynx, catamount',\n",
      " 288: 'leopard, Panthera pardus',\n",
      " 289: 'snow leopard, ounce, Panthera uncia',\n",
      " 290: 'jaguar, panther, Panthera onca, Felis onca',\n",
      " 291: 'lion, king of beasts, Panthera leo',\n",
      " 292: 'tiger, Panthera tigris',\n",
      " 293: 'cheetah, chetah, Acinonyx jubatus',\n",
      " 294: 'brown bear, bruin, Ursus arctos',\n",
      " 295: 'American black bear, black bear, Ursus americanus, Euarctos americanus',\n",
      " 296: 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus',\n",
      " 297: 'sloth bear, Melursus ursinus, Ursus ursinus',\n",
      " 298: 'mongoose',\n",
      " 299: 'meerkat, mierkat',\n",
      " 300: 'tiger beetle',\n",
      " 301: 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle',\n",
      " 302: 'ground beetle, carabid beetle',\n",
      " 303: 'long-horned beetle, longicorn, longicorn beetle',\n",
      " 304: 'leaf beetle, chrysomelid',\n",
      " 305: 'dung beetle',\n",
      " 306: 'rhinoceros beetle',\n",
      " 307: 'weevil',\n",
      " 308: 'fly',\n",
      " 309: 'bee',\n",
      " 310: 'ant, emmet, pismire',\n",
      " 311: 'grasshopper, hopper',\n",
      " 312: 'cricket',\n",
      " 313: 'walking stick, walkingstick, stick insect',\n",
      " 314: 'cockroach, roach',\n",
      " 315: 'mantis, mantid',\n",
      " 316: 'cicada, cicala',\n",
      " 317: 'leafhopper',\n",
      " 318: 'lacewing, lacewing fly',\n",
      " 319: \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\",\n",
      " 320: 'damselfly',\n",
      " 321: 'admiral',\n",
      " 322: 'ringlet, ringlet butterfly',\n",
      " 323: 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus',\n",
      " 324: 'cabbage butterfly',\n",
      " 325: 'sulphur butterfly, sulfur butterfly',\n",
      " 326: 'lycaenid, lycaenid butterfly',\n",
      " 327: 'starfish, sea star',\n",
      " 328: 'sea urchin',\n",
      " 329: 'sea cucumber, holothurian',\n",
      " 330: 'wood rabbit, cottontail, cottontail rabbit',\n",
      " 331: 'hare',\n",
      " 332: 'Angora, Angora rabbit',\n",
      " 333: 'hamster',\n",
      " 334: 'porcupine, hedgehog',\n",
      " 335: 'fox squirrel, eastern fox squirrel, Sciurus niger',\n",
      " 336: 'marmot',\n",
      " 337: 'beaver',\n",
      " 338: 'guinea pig, Cavia cobaya',\n",
      " 339: 'sorrel',\n",
      " 340: 'zebra',\n",
      " 341: 'hog, pig, grunter, squealer, Sus scrofa',\n",
      " 342: 'wild boar, boar, Sus scrofa',\n",
      " 343: 'warthog',\n",
      " 344: 'hippopotamus, hippo, river horse, Hippopotamus amphibius',\n",
      " 345: 'ox',\n",
      " 346: 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis',\n",
      " 347: 'bison',\n",
      " 348: 'ram, tup',\n",
      " 349: 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis',\n",
      " 350: 'ibex, Capra ibex',\n",
      " 351: 'hartebeest',\n",
      " 352: 'impala, Aepyceros melampus',\n",
      " 353: 'gazelle',\n",
      " 354: 'Arabian camel, dromedary, Camelus dromedarius',\n",
      " 355: 'llama',\n",
      " 356: 'weasel',\n",
      " 357: 'mink',\n",
      " 358: 'polecat, fitch, foulmart, foumart, Mustela putorius',\n",
      " 359: 'black-footed ferret, ferret, Mustela nigripes',\n",
      " 360: 'otter',\n",
      " 361: 'skunk, polecat, wood pussy',\n",
      " 362: 'badger',\n",
      " 363: 'armadillo',\n",
      " 364: 'three-toed sloth, ai, Bradypus tridactylus',\n",
      " 365: 'orangutan, orang, orangutang, Pongo pygmaeus',\n",
      " 366: 'gorilla, Gorilla gorilla',\n",
      " 367: 'chimpanzee, chimp, Pan troglodytes',\n",
      " 368: 'gibbon, Hylobates lar',\n",
      " 369: 'siamang, Hylobates syndactylus, Symphalangus syndactylus',\n",
      " 370: 'guenon, guenon monkey',\n",
      " 371: 'patas, hussar monkey, Erythrocebus patas',\n",
      " 372: 'baboon',\n",
      " 373: 'macaque',\n",
      " 374: 'langur',\n",
      " 375: 'colobus, colobus monkey',\n",
      " 376: 'proboscis monkey, Nasalis larvatus',\n",
      " 377: 'marmoset',\n",
      " 378: 'capuchin, ringtail, Cebus capucinus',\n",
      " 379: 'howler monkey, howler',\n",
      " 380: 'titi, titi monkey',\n",
      " 381: 'spider monkey, Ateles geoffroyi',\n",
      " 382: 'squirrel monkey, Saimiri sciureus',\n",
      " 383: 'Madagascar cat, ring-tailed lemur, Lemur catta',\n",
      " 384: 'indri, indris, Indri indri, Indri brevicaudatus',\n",
      " 385: 'Indian elephant, Elephas maximus',\n",
      " 386: 'African elephant, Loxodonta africana',\n",
      " 387: 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens',\n",
      " 388: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca',\n",
      " 389: 'barracouta, snoek',\n",
      " 390: 'eel',\n",
      " 391: 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch',\n",
      " 392: 'rock beauty, Holocanthus tricolor',\n",
      " 393: 'anemone fish',\n",
      " 394: 'sturgeon',\n",
      " 395: 'gar, garfish, garpike, billfish, Lepisosteus osseus',\n",
      " 396: 'lionfish',\n",
      " 397: 'puffer, pufferfish, blowfish, globefish',\n",
      " 398: 'abacus',\n",
      " 399: 'abaya',\n",
      " 400: \"academic gown, academic robe, judge's robe\",\n",
      " 401: 'accordion, piano accordion, squeeze box',\n",
      " 402: 'acoustic guitar',\n",
      " 403: 'aircraft carrier, carrier, flattop, attack aircraft carrier',\n",
      " 404: 'airliner',\n",
      " 405: 'airship, dirigible',\n",
      " 406: 'altar',\n",
      " 407: 'ambulance',\n",
      " 408: 'amphibian, amphibious vehicle',\n",
      " 409: 'analog clock',\n",
      " 410: 'apiary, bee house',\n",
      " 411: 'apron',\n",
      " 412: 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin',\n",
      " 413: 'assault rifle, assault gun',\n",
      " 414: 'backpack, back pack, knapsack, packsack, rucksack, haversack',\n",
      " 415: 'bakery, bakeshop, bakehouse',\n",
      " 416: 'balance beam, beam',\n",
      " 417: 'balloon',\n",
      " 418: 'ballpoint, ballpoint pen, ballpen, Biro',\n",
      " 419: 'Band Aid',\n",
      " 420: 'banjo',\n",
      " 421: 'bannister, banister, balustrade, balusters, handrail',\n",
      " 422: 'barbell',\n",
      " 423: 'barber chair',\n",
      " 424: 'barbershop',\n",
      " 425: 'barn',\n",
      " 426: 'barometer',\n",
      " 427: 'barrel, cask',\n",
      " 428: 'barrow, garden cart, lawn cart, wheelbarrow',\n",
      " 429: 'baseball',\n",
      " 430: 'basketball',\n",
      " 431: 'bassinet',\n",
      " 432: 'bassoon',\n",
      " 433: 'bathing cap, swimming cap',\n",
      " 434: 'bath towel',\n",
      " 435: 'bathtub, bathing tub, bath, tub',\n",
      " 436: 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon',\n",
      " 437: 'beacon, lighthouse, beacon light, pharos',\n",
      " 438: 'beaker',\n",
      " 439: 'bearskin, busby, shako',\n",
      " 440: 'beer bottle',\n",
      " 441: 'beer glass',\n",
      " 442: 'bell cote, bell cot',\n",
      " 443: 'bib',\n",
      " 444: 'bicycle-built-for-two, tandem bicycle, tandem',\n",
      " 445: 'bikini, two-piece',\n",
      " 446: 'binder, ring-binder',\n",
      " 447: 'binoculars, field glasses, opera glasses',\n",
      " 448: 'birdhouse',\n",
      " 449: 'boathouse',\n",
      " 450: 'bobsled, bobsleigh, bob',\n",
      " 451: 'bolo tie, bolo, bola tie, bola',\n",
      " 452: 'bonnet, poke bonnet',\n",
      " 453: 'bookcase',\n",
      " 454: 'bookshop, bookstore, bookstall',\n",
      " 455: 'bottlecap',\n",
      " 456: 'bow',\n",
      " 457: 'bow tie, bow-tie, bowtie',\n",
      " 458: 'brass, memorial tablet, plaque',\n",
      " 459: 'brassiere, bra, bandeau',\n",
      " 460: 'breakwater, groin, groyne, mole, bulwark, seawall, jetty',\n",
      " 461: 'breastplate, aegis, egis',\n",
      " 462: 'broom',\n",
      " 463: 'bucket, pail',\n",
      " 464: 'buckle',\n",
      " 465: 'bulletproof vest',\n",
      " 466: 'bullet train, bullet',\n",
      " 467: 'butcher shop, meat market',\n",
      " 468: 'cab, hack, taxi, taxicab',\n",
      " 469: 'caldron, cauldron',\n",
      " 470: 'candle, taper, wax light',\n",
      " 471: 'cannon',\n",
      " 472: 'canoe',\n",
      " 473: 'can opener, tin opener',\n",
      " 474: 'cardigan',\n",
      " 475: 'car mirror',\n",
      " 476: 'carousel, carrousel, merry-go-round, roundabout, whirligig',\n",
      " 477: \"carpenter's kit, tool kit\",\n",
      " 478: 'carton',\n",
      " 479: 'car wheel',\n",
      " 480: 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM',\n",
      " 481: 'cassette',\n",
      " 482: 'cassette player',\n",
      " 483: 'castle',\n",
      " 484: 'catamaran',\n",
      " 485: 'CD player',\n",
      " 486: 'cello, violoncello',\n",
      " 487: 'cellular telephone, cellular phone, cellphone, cell, mobile phone',\n",
      " 488: 'chain',\n",
      " 489: 'chainlink fence',\n",
      " 490: 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour',\n",
      " 491: 'chain saw, chainsaw',\n",
      " 492: 'chest',\n",
      " 493: 'chiffonier, commode',\n",
      " 494: 'chime, bell, gong',\n",
      " 495: 'china cabinet, china closet',\n",
      " 496: 'Christmas stocking',\n",
      " 497: 'church, church building',\n",
      " 498: 'cinema, movie theater, movie theatre, movie house, picture palace',\n",
      " 499: 'cleaver, meat cleaver, chopper',\n",
      " 500: 'cliff dwelling',\n",
      " 501: 'cloak',\n",
      " 502: 'clog, geta, patten, sabot',\n",
      " 503: 'cocktail shaker',\n",
      " 504: 'coffee mug',\n",
      " 505: 'coffeepot',\n",
      " 506: 'coil, spiral, volute, whorl, helix',\n",
      " 507: 'combination lock',\n",
      " 508: 'computer keyboard, keypad',\n",
      " 509: 'confectionery, confectionary, candy store',\n",
      " 510: 'container ship, containership, container vessel',\n",
      " 511: 'convertible',\n",
      " 512: 'corkscrew, bottle screw',\n",
      " 513: 'cornet, horn, trumpet, trump',\n",
      " 514: 'cowboy boot',\n",
      " 515: 'cowboy hat, ten-gallon hat',\n",
      " 516: 'cradle',\n",
      " 517: 'crane',\n",
      " 518: 'crash helmet',\n",
      " 519: 'crate',\n",
      " 520: 'crib, cot',\n",
      " 521: 'Crock Pot',\n",
      " 522: 'croquet ball',\n",
      " 523: 'crutch',\n",
      " 524: 'cuirass',\n",
      " 525: 'dam, dike, dyke',\n",
      " 526: 'desk',\n",
      " 527: 'desktop computer',\n",
      " 528: 'dial telephone, dial phone',\n",
      " 529: 'diaper, nappy, napkin',\n",
      " 530: 'digital clock',\n",
      " 531: 'digital watch',\n",
      " 532: 'dining table, board',\n",
      " 533: 'dishrag, dishcloth',\n",
      " 534: 'dishwasher, dish washer, dishwashing machine',\n",
      " 535: 'disk brake, disc brake',\n",
      " 536: 'dock, dockage, docking facility',\n",
      " 537: 'dogsled, dog sled, dog sleigh',\n",
      " 538: 'dome',\n",
      " 539: 'doormat, welcome mat',\n",
      " 540: 'drilling platform, offshore rig',\n",
      " 541: 'drum, membranophone, tympan',\n",
      " 542: 'drumstick',\n",
      " 543: 'dumbbell',\n",
      " 544: 'Dutch oven',\n",
      " 545: 'electric fan, blower',\n",
      " 546: 'electric guitar',\n",
      " 547: 'electric locomotive',\n",
      " 548: 'entertainment center',\n",
      " 549: 'envelope',\n",
      " 550: 'espresso maker',\n",
      " 551: 'face powder',\n",
      " 552: 'feather boa, boa',\n",
      " 553: 'file, file cabinet, filing cabinet',\n",
      " 554: 'fireboat',\n",
      " 555: 'fire engine, fire truck',\n",
      " 556: 'fire screen, fireguard',\n",
      " 557: 'flagpole, flagstaff',\n",
      " 558: 'flute, transverse flute',\n",
      " 559: 'folding chair',\n",
      " 560: 'football helmet',\n",
      " 561: 'forklift',\n",
      " 562: 'fountain',\n",
      " 563: 'fountain pen',\n",
      " 564: 'four-poster',\n",
      " 565: 'freight car',\n",
      " 566: 'French horn, horn',\n",
      " 567: 'frying pan, frypan, skillet',\n",
      " 568: 'fur coat',\n",
      " 569: 'garbage truck, dustcart',\n",
      " 570: 'gasmask, respirator, gas helmet',\n",
      " 571: 'gas pump, gasoline pump, petrol pump, island dispenser',\n",
      " 572: 'goblet',\n",
      " 573: 'go-kart',\n",
      " 574: 'golf ball',\n",
      " 575: 'golfcart, golf cart',\n",
      " 576: 'gondola',\n",
      " 577: 'gong, tam-tam',\n",
      " 578: 'gown',\n",
      " 579: 'grand piano, grand',\n",
      " 580: 'greenhouse, nursery, glasshouse',\n",
      " 581: 'grille, radiator grille',\n",
      " 582: 'grocery store, grocery, food market, market',\n",
      " 583: 'guillotine',\n",
      " 584: 'hair slide',\n",
      " 585: 'hair spray',\n",
      " 586: 'half track',\n",
      " 587: 'hammer',\n",
      " 588: 'hamper',\n",
      " 589: 'hand blower, blow dryer, blow drier, hair dryer, hair drier',\n",
      " 590: 'hand-held computer, hand-held microcomputer',\n",
      " 591: 'handkerchief, hankie, hanky, hankey',\n",
      " 592: 'hard disc, hard disk, fixed disk',\n",
      " 593: 'harmonica, mouth organ, harp, mouth harp',\n",
      " 594: 'harp',\n",
      " 595: 'harvester, reaper',\n",
      " 596: 'hatchet',\n",
      " 597: 'holster',\n",
      " 598: 'home theater, home theatre',\n",
      " 599: 'honeycomb',\n",
      " 600: 'hook, claw',\n",
      " 601: 'hoopskirt, crinoline',\n",
      " 602: 'horizontal bar, high bar',\n",
      " 603: 'horse cart, horse-cart',\n",
      " 604: 'hourglass',\n",
      " 605: 'iPod',\n",
      " 606: 'iron, smoothing iron',\n",
      " 607: \"jack-o'-lantern\",\n",
      " 608: 'jean, blue jean, denim',\n",
      " 609: 'jeep, landrover',\n",
      " 610: 'jersey, T-shirt, tee shirt',\n",
      " 611: 'jigsaw puzzle',\n",
      " 612: 'jinrikisha, ricksha, rickshaw',\n",
      " 613: 'joystick',\n",
      " 614: 'kimono',\n",
      " 615: 'knee pad',\n",
      " 616: 'knot',\n",
      " 617: 'lab coat, laboratory coat',\n",
      " 618: 'ladle',\n",
      " 619: 'lampshade, lamp shade',\n",
      " 620: 'laptop, laptop computer',\n",
      " 621: 'lawn mower, mower',\n",
      " 622: 'lens cap, lens cover',\n",
      " 623: 'letter opener, paper knife, paperknife',\n",
      " 624: 'library',\n",
      " 625: 'lifeboat',\n",
      " 626: 'lighter, light, igniter, ignitor',\n",
      " 627: 'limousine, limo',\n",
      " 628: 'liner, ocean liner',\n",
      " 629: 'lipstick, lip rouge',\n",
      " 630: 'Loafer',\n",
      " 631: 'lotion',\n",
      " 632: 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system',\n",
      " 633: \"loupe, jeweler's loupe\",\n",
      " 634: 'lumbermill, sawmill',\n",
      " 635: 'magnetic compass',\n",
      " 636: 'mailbag, postbag',\n",
      " 637: 'mailbox, letter box',\n",
      " 638: 'maillot',\n",
      " 639: 'maillot, tank suit',\n",
      " 640: 'manhole cover',\n",
      " 641: 'maraca',\n",
      " 642: 'marimba, xylophone',\n",
      " 643: 'mask',\n",
      " 644: 'matchstick',\n",
      " 645: 'maypole',\n",
      " 646: 'maze, labyrinth',\n",
      " 647: 'measuring cup',\n",
      " 648: 'medicine chest, medicine cabinet',\n",
      " 649: 'megalith, megalithic structure',\n",
      " 650: 'microphone, mike',\n",
      " 651: 'microwave, microwave oven',\n",
      " 652: 'military uniform',\n",
      " 653: 'milk can',\n",
      " 654: 'minibus',\n",
      " 655: 'miniskirt, mini',\n",
      " 656: 'minivan',\n",
      " 657: 'missile',\n",
      " 658: 'mitten',\n",
      " 659: 'mixing bowl',\n",
      " 660: 'mobile home, manufactured home',\n",
      " 661: 'Model T',\n",
      " 662: 'modem',\n",
      " 663: 'monastery',\n",
      " 664: 'monitor',\n",
      " 665: 'moped',\n",
      " 666: 'mortar',\n",
      " 667: 'mortarboard',\n",
      " 668: 'mosque',\n",
      " 669: 'mosquito net',\n",
      " 670: 'motor scooter, scooter',\n",
      " 671: 'mountain bike, all-terrain bike, off-roader',\n",
      " 672: 'mountain tent',\n",
      " 673: 'mouse, computer mouse',\n",
      " 674: 'mousetrap',\n",
      " 675: 'moving van',\n",
      " 676: 'muzzle',\n",
      " 677: 'nail',\n",
      " 678: 'neck brace',\n",
      " 679: 'necklace',\n",
      " 680: 'nipple',\n",
      " 681: 'notebook, notebook computer',\n",
      " 682: 'obelisk',\n",
      " 683: 'oboe, hautboy, hautbois',\n",
      " 684: 'ocarina, sweet potato',\n",
      " 685: 'odometer, hodometer, mileometer, milometer',\n",
      " 686: 'oil filter',\n",
      " 687: 'organ, pipe organ',\n",
      " 688: 'oscilloscope, scope, cathode-ray oscilloscope, CRO',\n",
      " 689: 'overskirt',\n",
      " 690: 'oxcart',\n",
      " 691: 'oxygen mask',\n",
      " 692: 'packet',\n",
      " 693: 'paddle, boat paddle',\n",
      " 694: 'paddlewheel, paddle wheel',\n",
      " 695: 'padlock',\n",
      " 696: 'paintbrush',\n",
      " 697: \"pajama, pyjama, pj's, jammies\",\n",
      " 698: 'palace',\n",
      " 699: 'panpipe, pandean pipe, syrinx',\n",
      " 700: 'paper towel',\n",
      " 701: 'parachute, chute',\n",
      " 702: 'parallel bars, bars',\n",
      " 703: 'park bench',\n",
      " 704: 'parking meter',\n",
      " 705: 'passenger car, coach, carriage',\n",
      " 706: 'patio, terrace',\n",
      " 707: 'pay-phone, pay-station',\n",
      " 708: 'pedestal, plinth, footstall',\n",
      " 709: 'pencil box, pencil case',\n",
      " 710: 'pencil sharpener',\n",
      " 711: 'perfume, essence',\n",
      " 712: 'Petri dish',\n",
      " 713: 'photocopier',\n",
      " 714: 'pick, plectrum, plectron',\n",
      " 715: 'pickelhaube',\n",
      " 716: 'picket fence, paling',\n",
      " 717: 'pickup, pickup truck',\n",
      " 718: 'pier',\n",
      " 719: 'piggy bank, penny bank',\n",
      " 720: 'pill bottle',\n",
      " 721: 'pillow',\n",
      " 722: 'ping-pong ball',\n",
      " 723: 'pinwheel',\n",
      " 724: 'pirate, pirate ship',\n",
      " 725: 'pitcher, ewer',\n",
      " 726: \"plane, carpenter's plane, woodworking plane\",\n",
      " 727: 'planetarium',\n",
      " 728: 'plastic bag',\n",
      " 729: 'plate rack',\n",
      " 730: 'plow, plough',\n",
      " 731: \"plunger, plumber's helper\",\n",
      " 732: 'Polaroid camera, Polaroid Land camera',\n",
      " 733: 'pole',\n",
      " 734: 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria',\n",
      " 735: 'poncho',\n",
      " 736: 'pool table, billiard table, snooker table',\n",
      " 737: 'pop bottle, soda bottle',\n",
      " 738: 'pot, flowerpot',\n",
      " 739: \"potter's wheel\",\n",
      " 740: 'power drill',\n",
      " 741: 'prayer rug, prayer mat',\n",
      " 742: 'printer',\n",
      " 743: 'prison, prison house',\n",
      " 744: 'projectile, missile',\n",
      " 745: 'projector',\n",
      " 746: 'puck, hockey puck',\n",
      " 747: 'punching bag, punch bag, punching ball, punchball',\n",
      " 748: 'purse',\n",
      " 749: 'quill, quill pen',\n",
      " 750: 'quilt, comforter, comfort, puff',\n",
      " 751: 'racer, race car, racing car',\n",
      " 752: 'racket, racquet',\n",
      " 753: 'radiator',\n",
      " 754: 'radio, wireless',\n",
      " 755: 'radio telescope, radio reflector',\n",
      " 756: 'rain barrel',\n",
      " 757: 'recreational vehicle, RV, R.V.',\n",
      " 758: 'reel',\n",
      " 759: 'reflex camera',\n",
      " 760: 'refrigerator, icebox',\n",
      " 761: 'remote control, remote',\n",
      " 762: 'restaurant, eating house, eating place, eatery',\n",
      " 763: 'revolver, six-gun, six-shooter',\n",
      " 764: 'rifle',\n",
      " 765: 'rocking chair, rocker',\n",
      " 766: 'rotisserie',\n",
      " 767: 'rubber eraser, rubber, pencil eraser',\n",
      " 768: 'rugby ball',\n",
      " 769: 'rule, ruler',\n",
      " 770: 'running shoe',\n",
      " 771: 'safe',\n",
      " 772: 'safety pin',\n",
      " 773: 'saltshaker, salt shaker',\n",
      " 774: 'sandal',\n",
      " 775: 'sarong',\n",
      " 776: 'sax, saxophone',\n",
      " 777: 'scabbard',\n",
      " 778: 'scale, weighing machine',\n",
      " 779: 'school bus',\n",
      " 780: 'schooner',\n",
      " 781: 'scoreboard',\n",
      " 782: 'screen, CRT screen',\n",
      " 783: 'screw',\n",
      " 784: 'screwdriver',\n",
      " 785: 'seat belt, seatbelt',\n",
      " 786: 'sewing machine',\n",
      " 787: 'shield, buckler',\n",
      " 788: 'shoe shop, shoe-shop, shoe store',\n",
      " 789: 'shoji',\n",
      " 790: 'shopping basket',\n",
      " 791: 'shopping cart',\n",
      " 792: 'shovel',\n",
      " 793: 'shower cap',\n",
      " 794: 'shower curtain',\n",
      " 795: 'ski',\n",
      " 796: 'ski mask',\n",
      " 797: 'sleeping bag',\n",
      " 798: 'slide rule, slipstick',\n",
      " 799: 'sliding door',\n",
      " 800: 'slot, one-armed bandit',\n",
      " 801: 'snorkel',\n",
      " 802: 'snowmobile',\n",
      " 803: 'snowplow, snowplough',\n",
      " 804: 'soap dispenser',\n",
      " 805: 'soccer ball',\n",
      " 806: 'sock',\n",
      " 807: 'solar dish, solar collector, solar furnace',\n",
      " 808: 'sombrero',\n",
      " 809: 'soup bowl',\n",
      " 810: 'space bar',\n",
      " 811: 'space heater',\n",
      " 812: 'space shuttle',\n",
      " 813: 'spatula',\n",
      " 814: 'speedboat',\n",
      " 815: \"spider web, spider's web\",\n",
      " 816: 'spindle',\n",
      " 817: 'sports car, sport car',\n",
      " 818: 'spotlight, spot',\n",
      " 819: 'stage',\n",
      " 820: 'steam locomotive',\n",
      " 821: 'steel arch bridge',\n",
      " 822: 'steel drum',\n",
      " 823: 'stethoscope',\n",
      " 824: 'stole',\n",
      " 825: 'stone wall',\n",
      " 826: 'stopwatch, stop watch',\n",
      " 827: 'stove',\n",
      " 828: 'strainer',\n",
      " 829: 'streetcar, tram, tramcar, trolley, trolley car',\n",
      " 830: 'stretcher',\n",
      " 831: 'studio couch, day bed',\n",
      " 832: 'stupa, tope',\n",
      " 833: 'submarine, pigboat, sub, U-boat',\n",
      " 834: 'suit, suit of clothes',\n",
      " 835: 'sundial',\n",
      " 836: 'sunglass',\n",
      " 837: 'sunglasses, dark glasses, shades',\n",
      " 838: 'sunscreen, sunblock, sun blocker',\n",
      " 839: 'suspension bridge',\n",
      " 840: 'swab, swob, mop',\n",
      " 841: 'sweatshirt',\n",
      " 842: 'swimming trunks, bathing trunks',\n",
      " 843: 'swing',\n",
      " 844: 'switch, electric switch, electrical switch',\n",
      " 845: 'syringe',\n",
      " 846: 'table lamp',\n",
      " 847: 'tank, army tank, armored combat vehicle, armoured combat vehicle',\n",
      " 848: 'tape player',\n",
      " 849: 'teapot',\n",
      " 850: 'teddy, teddy bear',\n",
      " 851: 'television, television system',\n",
      " 852: 'tennis ball',\n",
      " 853: 'thatch, thatched roof',\n",
      " 854: 'theater curtain, theatre curtain',\n",
      " 855: 'thimble',\n",
      " 856: 'thresher, thrasher, threshing machine',\n",
      " 857: 'throne',\n",
      " 858: 'tile roof',\n",
      " 859: 'toaster',\n",
      " 860: 'tobacco shop, tobacconist shop, tobacconist',\n",
      " 861: 'toilet seat',\n",
      " 862: 'torch',\n",
      " 863: 'totem pole',\n",
      " 864: 'tow truck, tow car, wrecker',\n",
      " 865: 'toyshop',\n",
      " 866: 'tractor',\n",
      " 867: 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi',\n",
      " 868: 'tray',\n",
      " 869: 'trench coat',\n",
      " 870: 'tricycle, trike, velocipede',\n",
      " 871: 'trimaran',\n",
      " 872: 'tripod',\n",
      " 873: 'triumphal arch',\n",
      " 874: 'trolleybus, trolley coach, trackless trolley',\n",
      " 875: 'trombone',\n",
      " 876: 'tub, vat',\n",
      " 877: 'turnstile',\n",
      " 878: 'typewriter keyboard',\n",
      " 879: 'umbrella',\n",
      " 880: 'unicycle, monocycle',\n",
      " 881: 'upright, upright piano',\n",
      " 882: 'vacuum, vacuum cleaner',\n",
      " 883: 'vase',\n",
      " 884: 'vault',\n",
      " 885: 'velvet',\n",
      " 886: 'vending machine',\n",
      " 887: 'vestment',\n",
      " 888: 'viaduct',\n",
      " 889: 'violin, fiddle',\n",
      " 890: 'volleyball',\n",
      " 891: 'waffle iron',\n",
      " 892: 'wall clock',\n",
      " 893: 'wallet, billfold, notecase, pocketbook',\n",
      " 894: 'wardrobe, closet, press',\n",
      " 895: 'warplane, military plane',\n",
      " 896: 'washbasin, handbasin, washbowl, lavabo, wash-hand basin',\n",
      " 897: 'washer, automatic washer, washing machine',\n",
      " 898: 'water bottle',\n",
      " 899: 'water jug',\n",
      " 900: 'water tower',\n",
      " 901: 'whiskey jug',\n",
      " 902: 'whistle',\n",
      " 903: 'wig',\n",
      " 904: 'window screen',\n",
      " 905: 'window shade',\n",
      " 906: 'Windsor tie',\n",
      " 907: 'wine bottle',\n",
      " 908: 'wing',\n",
      " 909: 'wok',\n",
      " 910: 'wooden spoon',\n",
      " 911: 'wool, woolen, woollen',\n",
      " 912: 'worm fence, snake fence, snake-rail fence, Virginia fence',\n",
      " 913: 'wreck',\n",
      " 914: 'yawl',\n",
      " 915: 'yurt',\n",
      " 916: 'web site, website, internet site, site',\n",
      " 917: 'comic book',\n",
      " 918: 'crossword puzzle, crossword',\n",
      " 919: 'street sign',\n",
      " 920: 'traffic light, traffic signal, stoplight',\n",
      " 921: 'book jacket, dust cover, dust jacket, dust wrapper',\n",
      " 922: 'menu',\n",
      " 923: 'plate',\n",
      " 924: 'guacamole',\n",
      " 925: 'consomme',\n",
      " 926: 'hot pot, hotpot',\n",
      " 927: 'trifle',\n",
      " 928: 'ice cream, icecream',\n",
      " 929: 'ice lolly, lolly, lollipop, popsicle',\n",
      " 930: 'French loaf',\n",
      " 931: 'bagel, beigel',\n",
      " 932: 'pretzel',\n",
      " 933: 'cheeseburger',\n",
      " 934: 'hotdog, hot dog, red hot',\n",
      " 935: 'mashed potato',\n",
      " 936: 'head cabbage',\n",
      " 937: 'broccoli',\n",
      " 938: 'cauliflower',\n",
      " 939: 'zucchini, courgette',\n",
      " 940: 'spaghetti squash',\n",
      " 941: 'acorn squash',\n",
      " 942: 'butternut squash',\n",
      " 943: 'cucumber, cuke',\n",
      " 944: 'artichoke, globe artichoke',\n",
      " 945: 'bell pepper',\n",
      " 946: 'cardoon',\n",
      " 947: 'mushroom',\n",
      " 948: 'Granny Smith',\n",
      " 949: 'strawberry',\n",
      " 950: 'orange',\n",
      " 951: 'lemon',\n",
      " 952: 'fig',\n",
      " 953: 'pineapple, ananas',\n",
      " 954: 'banana',\n",
      " 955: 'jackfruit, jak, jack',\n",
      " 956: 'custard apple',\n",
      " 957: 'pomegranate',\n",
      " 958: 'hay',\n",
      " 959: 'carbonara',\n",
      " 960: 'chocolate sauce, chocolate syrup',\n",
      " 961: 'dough',\n",
      " 962: 'meat loaf, meatloaf',\n",
      " 963: 'pizza, pizza pie',\n",
      " 964: 'potpie',\n",
      " 965: 'burrito',\n",
      " 966: 'red wine',\n",
      " 967: 'espresso',\n",
      " 968: 'cup',\n",
      " 969: 'eggnog',\n",
      " 970: 'alp',\n",
      " 971: 'bubble',\n",
      " 972: 'cliff, drop, drop-off',\n",
      " 973: 'coral reef',\n",
      " 974: 'geyser',\n",
      " 975: 'lakeside, lakeshore',\n",
      " 976: 'promontory, headland, head, foreland',\n",
      " 977: 'sandbar, sand bar',\n",
      " 978: 'seashore, coast, seacoast, sea-coast',\n",
      " 979: 'valley, vale',\n",
      " 980: 'volcano',\n",
      " 981: 'ballplayer, baseball player',\n",
      " 982: 'groom, bridegroom',\n",
      " 983: 'scuba diver',\n",
      " 984: 'rapeseed',\n",
      " 985: 'daisy',\n",
      " 986: \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\",\n",
      " 987: 'corn',\n",
      " 988: 'acorn',\n",
      " 989: 'hip, rose hip, rosehip',\n",
      " 990: 'buckeye, horse chestnut, conker',\n",
      " 991: 'coral fungus',\n",
      " 992: 'agaric',\n",
      " 993: 'gyromitra',\n",
      " 994: 'stinkhorn, carrion fungus',\n",
      " 995: 'earthstar',\n",
      " 996: 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa',\n",
      " 997: 'bolete',\n",
      " 998: 'ear, spike, capitulum',\n",
      " 999: 'toilet tissue, toilet paper, bathroom tissue'}\n",
      "20/51:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 224\n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "image = cv2.resize(image,(IMG_SIZE,IMG_SIZE))     # resize image to match model's expected sizing\n",
      "image = image.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "plt.imshow(image[0])\n",
      "print(np.shape(image))\n",
      "print(class_dict[np.argmax(model.predict(image), axis=1)])\n",
      "20/52:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 224\n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "image = cv2.resize(image,(IMG_SIZE,IMG_SIZE))     # resize image to match model's expected sizing\n",
      "image = image.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "plt.imshow(image[0])\n",
      "print(np.shape(image))\n",
      "print(class_dict[np.argmax(model.predict(image), axis=1)[0]])\n",
      "20/53:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 224\n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "image = cv2.resize(image,(IMG_SIZE,IMG_SIZE))     # resize image to match model's expected sizing\n",
      "image = image.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "plt.imshow(image[0])\n",
      "print(class_dict[np.argmax(model.predict(image), axis=1)[0]])\n",
      "20/54:\n",
      "result = {}\n",
      "for file in os.listdir(\"../data/images/\"):\n",
      "    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "        image = cv2.imread(\"../data/images/\" + file)\n",
      "        image = cv2.resize(image, (224,224), interpolation = cv2.INTER_AREA)\n",
      "        result[file] = model.predict(image)\n",
      "20/55:\n",
      "result = {}\n",
      "for file in os.listdir(\"../data/images/\"):\n",
      "    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "        print(file)\n",
      "        image = cv2.imread(\"../data/images/\" + file)\n",
      "        image = cv2.resize(image,(IMG_SIZE,IMG_SIZE))     # resize image to match model's expected sizing\n",
      "        image = image.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "        result[file] = model.predict(image)\n",
      "20/56:\n",
      "result = {}\n",
      "for file in os.listdir(\"../data/images/\"):\n",
      "    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "        print(file)\n",
      "        image = cv2.imread(\"../data/images/\" + file)\n",
      "        image = cv2.resize(image,(IMG_SIZE,IMG_SIZE))     # resize image to match model's expected sizing\n",
      "        image = image.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "        result[file] = model.predict(image)\n",
      "        \n",
      "pickle.dump(result, \"image_classes\")\n",
      "20/57:\n",
      "result = {}\n",
      "for file, index in os.listdir(\"../data/images/\"):\n",
      "    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "        print(index)\n",
      "        image = cv2.imread(\"../data/images/\" + file)\n",
      "        image = cv2.resize(image,(IMG_SIZE,IMG_SIZE))     # resize image to match model's expected sizing\n",
      "        image = image.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "        result[file] = model.predict(image)\n",
      "        \n",
      "pickle.dump(result, \"image_classes\")\n",
      "20/58:\n",
      "result = {}\n",
      "for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "        print(index)\n",
      "        image = cv2.imread(\"../data/images/\" + file)\n",
      "        image = cv2.resize(image,(IMG_SIZE,IMG_SIZE))     # resize image to match model's expected sizing\n",
      "        image = image.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "        result[file] = model.predict(image)\n",
      "        \n",
      "pickle.dump(result, \"image_classes\")\n",
      "20/59:\n",
      "result = {}\n",
      "for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "        print(index/len(os.listdir(\"../data/images/\")))\n",
      "        image = cv2.imread(\"../data/images/\" + file)\n",
      "        image = cv2.resize(image,(IMG_SIZE,IMG_SIZE))     # resize image to match model's expected sizing\n",
      "        image = image.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "        result[file] = model.predict(image)\n",
      "        \n",
      "pickle.dump(result, \"image_classes\")\n",
      "20/60:\n",
      "result = {}\n",
      "for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "        print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "        image = cv2.imread(\"../data/images/\" + file)\n",
      "        image = cv2.resize(image,(IMG_SIZE,IMG_SIZE))     # resize image to match model's expected sizing\n",
      "        image = image.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "        result[file] = model.predict(image)\n",
      "        \n",
      "pickle.dump(result, \"image_classes\")\n",
      "20/61:\n",
      "result = {}\n",
      "for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "        print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "        image = cv2.imread(\"../data/images/\" + file)\n",
      "        image = cv2.resize(image,(IMG_SIZE,IMG_SIZE))     # resize image to match model's expected sizing\n",
      "        image = image.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "        result[file] = model.predict(image)\n",
      "        pickle.dump(result, \"image_classes\")\n",
      "20/62:\n",
      "import picle\n",
      "\n",
      "result = {}\n",
      "for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "        print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "        image = cv2.imread(\"../data/images/\" + file)\n",
      "        image = cv2.resize(image,(IMG_SIZE,IMG_SIZE))     # resize image to match model's expected sizing\n",
      "        image = image.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "        result[file] = model.predict(image)\n",
      "        pickle.dump(result, \"image_classes\")\n",
      "20/63:\n",
      "import pickle\n",
      "\n",
      "result = {}\n",
      "for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "        print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "        image = cv2.imread(\"../data/images/\" + file)\n",
      "        image = cv2.resize(image,(IMG_SIZE,IMG_SIZE))     # resize image to match model's expected sizing\n",
      "        image = image.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "        result[file] = model.predict(image)\n",
      "        pickle.dump(result, \"image_classes\")\n",
      "20/64:\n",
      "import pickle\n",
      "\n",
      "result = {}\n",
      "for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "        print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "        image = cv2.imread(\"../data/images/\" + file)\n",
      "        image = cv2.resize(image,(IMG_SIZE,IMG_SIZE))     # resize image to match model's expected sizing\n",
      "        image = image.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "        result[file] = model.predict(image)\n",
      "        pickle.dump(result, open( \"image_classes\", \"wb\" ))\n",
      "20/65:\n",
      "import pickle\n",
      "\n",
      "result = {}\n",
      "for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "        print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "        image = cv2.imread(\"../data/images/\" + file)\n",
      "        image = cv2.resize(image,(IMG_SIZE,IMG_SIZE))     # resize image to match model's expected sizing\n",
      "        image = image.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "        result[file] = np.argmax(model.predict(image), axis=1)[0]\n",
      "        \n",
      "pickle.dump(result, open( \"image_classes\", \"wb\" ))\n",
      "20/66:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 224\n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "image = center_crop(image)\n",
      "image = cv2.resize(image,(IMG_SIZE,IMG_SIZE))     # resize image to match model's expected sizing\n",
      "image = image.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "plt.imshow(image[0])\n",
      "print(class_dict[np.argmax(model.predict(image), axis=1)[0]])\n",
      "20/67:\n",
      "from tensorflow.keras.applications import EfficientNetB0\n",
      "from tensorflow.image import resize\n",
      "model = EfficientNetB0(weights='imagenet')\n",
      "\n",
      "def center_crop(img, new_width=None, new_height=None):        \n",
      "\n",
      "    width = img.shape[1]\n",
      "    height = img.shape[0]\n",
      "\n",
      "    if new_width is None:\n",
      "        new_width = min(width, height)\n",
      "\n",
      "    if new_height is None:\n",
      "        new_height = min(width, height)\n",
      "\n",
      "    left = int(np.ceil((width - new_width) / 2))\n",
      "    right = width - int(np.floor((width - new_width) / 2))\n",
      "\n",
      "    top = int(np.ceil((height - new_height) / 2))\n",
      "    bottom = height - int(np.floor((height - new_height) / 2))\n",
      "\n",
      "    if len(img.shape) == 2:\n",
      "        center_cropped_img = img[top:bottom, left:right]\n",
      "    else:\n",
      "        center_cropped_img = img[top:bottom, left:right, ...]\n",
      "\n",
      "    return center_cropped_img\n",
      "\n",
      "class_dict = {0: 'tench, Tinca tinca',\n",
      " 1: 'goldfish, Carassius auratus',\n",
      " 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n",
      " 3: 'tiger shark, Galeocerdo cuvieri',\n",
      " 4: 'hammerhead, hammerhead shark',\n",
      " 5: 'electric ray, crampfish, numbfish, torpedo',\n",
      " 6: 'stingray',\n",
      " 7: 'cock',\n",
      " 8: 'hen',\n",
      " 9: 'ostrich, Struthio camelus',\n",
      " 10: 'brambling, Fringilla montifringilla',\n",
      " 11: 'goldfinch, Carduelis carduelis',\n",
      " 12: 'house finch, linnet, Carpodacus mexicanus',\n",
      " 13: 'junco, snowbird',\n",
      " 14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea',\n",
      " 15: 'robin, American robin, Turdus migratorius',\n",
      " 16: 'bulbul',\n",
      " 17: 'jay',\n",
      " 18: 'magpie',\n",
      " 19: 'chickadee',\n",
      " 20: 'water ouzel, dipper',\n",
      " 21: 'kite',\n",
      " 22: 'bald eagle, American eagle, Haliaeetus leucocephalus',\n",
      " 23: 'vulture',\n",
      " 24: 'great grey owl, great gray owl, Strix nebulosa',\n",
      " 25: 'European fire salamander, Salamandra salamandra',\n",
      " 26: 'common newt, Triturus vulgaris',\n",
      " 27: 'eft',\n",
      " 28: 'spotted salamander, Ambystoma maculatum',\n",
      " 29: 'axolotl, mud puppy, Ambystoma mexicanum',\n",
      " 30: 'bullfrog, Rana catesbeiana',\n",
      " 31: 'tree frog, tree-frog',\n",
      " 32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui',\n",
      " 33: 'loggerhead, loggerhead turtle, Caretta caretta',\n",
      " 34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea',\n",
      " 35: 'mud turtle',\n",
      " 36: 'terrapin',\n",
      " 37: 'box turtle, box tortoise',\n",
      " 38: 'banded gecko',\n",
      " 39: 'common iguana, iguana, Iguana iguana',\n",
      " 40: 'American chameleon, anole, Anolis carolinensis',\n",
      " 41: 'whiptail, whiptail lizard',\n",
      " 42: 'agama',\n",
      " 43: 'frilled lizard, Chlamydosaurus kingi',\n",
      " 44: 'alligator lizard',\n",
      " 45: 'Gila monster, Heloderma suspectum',\n",
      " 46: 'green lizard, Lacerta viridis',\n",
      " 47: 'African chameleon, Chamaeleo chamaeleon',\n",
      " 48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis',\n",
      " 49: 'African crocodile, Nile crocodile, Crocodylus niloticus',\n",
      " 50: 'American alligator, Alligator mississipiensis',\n",
      " 51: 'triceratops',\n",
      " 52: 'thunder snake, worm snake, Carphophis amoenus',\n",
      " 53: 'ringneck snake, ring-necked snake, ring snake',\n",
      " 54: 'hognose snake, puff adder, sand viper',\n",
      " 55: 'green snake, grass snake',\n",
      " 56: 'king snake, kingsnake',\n",
      " 57: 'garter snake, grass snake',\n",
      " 58: 'water snake',\n",
      " 59: 'vine snake',\n",
      " 60: 'night snake, Hypsiglena torquata',\n",
      " 61: 'boa constrictor, Constrictor constrictor',\n",
      " 62: 'rock python, rock snake, Python sebae',\n",
      " 63: 'Indian cobra, Naja naja',\n",
      " 64: 'green mamba',\n",
      " 65: 'sea snake',\n",
      " 66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus',\n",
      " 67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus',\n",
      " 68: 'sidewinder, horned rattlesnake, Crotalus cerastes',\n",
      " 69: 'trilobite',\n",
      " 70: 'harvestman, daddy longlegs, Phalangium opilio',\n",
      " 71: 'scorpion',\n",
      " 72: 'black and gold garden spider, Argiope aurantia',\n",
      " 73: 'barn spider, Araneus cavaticus',\n",
      " 74: 'garden spider, Aranea diademata',\n",
      " 75: 'black widow, Latrodectus mactans',\n",
      " 76: 'tarantula',\n",
      " 77: 'wolf spider, hunting spider',\n",
      " 78: 'tick',\n",
      " 79: 'centipede',\n",
      " 80: 'black grouse',\n",
      " 81: 'ptarmigan',\n",
      " 82: 'ruffed grouse, partridge, Bonasa umbellus',\n",
      " 83: 'prairie chicken, prairie grouse, prairie fowl',\n",
      " 84: 'peacock',\n",
      " 85: 'quail',\n",
      " 86: 'partridge',\n",
      " 87: 'African grey, African gray, Psittacus erithacus',\n",
      " 88: 'macaw',\n",
      " 89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n",
      " 90: 'lorikeet',\n",
      " 91: 'coucal',\n",
      " 92: 'bee eater',\n",
      " 93: 'hornbill',\n",
      " 94: 'hummingbird',\n",
      " 95: 'jacamar',\n",
      " 96: 'toucan',\n",
      " 97: 'drake',\n",
      " 98: 'red-breasted merganser, Mergus serrator',\n",
      " 99: 'goose',\n",
      " 100: 'black swan, Cygnus atratus',\n",
      " 101: 'tusker',\n",
      " 102: 'echidna, spiny anteater, anteater',\n",
      " 103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n",
      " 104: 'wallaby, brush kangaroo',\n",
      " 105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n",
      " 106: 'wombat',\n",
      " 107: 'jellyfish',\n",
      " 108: 'sea anemone, anemone',\n",
      " 109: 'brain coral',\n",
      " 110: 'flatworm, platyhelminth',\n",
      " 111: 'nematode, nematode worm, roundworm',\n",
      " 112: 'conch',\n",
      " 113: 'snail',\n",
      " 114: 'slug',\n",
      " 115: 'sea slug, nudibranch',\n",
      " 116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore',\n",
      " 117: 'chambered nautilus, pearly nautilus, nautilus',\n",
      " 118: 'Dungeness crab, Cancer magister',\n",
      " 119: 'rock crab, Cancer irroratus',\n",
      " 120: 'fiddler crab',\n",
      " 121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica',\n",
      " 122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus',\n",
      " 123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish',\n",
      " 124: 'crayfish, crawfish, crawdad, crawdaddy',\n",
      " 125: 'hermit crab',\n",
      " 126: 'isopod',\n",
      " 127: 'white stork, Ciconia ciconia',\n",
      " 128: 'black stork, Ciconia nigra',\n",
      " 129: 'spoonbill',\n",
      " 130: 'flamingo',\n",
      " 131: 'little blue heron, Egretta caerulea',\n",
      " 132: 'American egret, great white heron, Egretta albus',\n",
      " 133: 'bittern',\n",
      " 134: 'crane',\n",
      " 135: 'limpkin, Aramus pictus',\n",
      " 136: 'European gallinule, Porphyrio porphyrio',\n",
      " 137: 'American coot, marsh hen, mud hen, water hen, Fulica americana',\n",
      " 138: 'bustard',\n",
      " 139: 'ruddy turnstone, Arenaria interpres',\n",
      " 140: 'red-backed sandpiper, dunlin, Erolia alpina',\n",
      " 141: 'redshank, Tringa totanus',\n",
      " 142: 'dowitcher',\n",
      " 143: 'oystercatcher, oyster catcher',\n",
      " 144: 'pelican',\n",
      " 145: 'king penguin, Aptenodytes patagonica',\n",
      " 146: 'albatross, mollymawk',\n",
      " 147: 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus',\n",
      " 148: 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca',\n",
      " 149: 'dugong, Dugong dugon',\n",
      " 150: 'sea lion',\n",
      " 151: 'Chihuahua',\n",
      " 152: 'Japanese spaniel',\n",
      " 153: 'Maltese dog, Maltese terrier, Maltese',\n",
      " 154: 'Pekinese, Pekingese, Peke',\n",
      " 155: 'Shih-Tzu',\n",
      " 156: 'Blenheim spaniel',\n",
      " 157: 'papillon',\n",
      " 158: 'toy terrier',\n",
      " 159: 'Rhodesian ridgeback',\n",
      " 160: 'Afghan hound, Afghan',\n",
      " 161: 'basset, basset hound',\n",
      " 162: 'beagle',\n",
      " 163: 'bloodhound, sleuthhound',\n",
      " 164: 'bluetick',\n",
      " 165: 'black-and-tan coonhound',\n",
      " 166: 'Walker hound, Walker foxhound',\n",
      " 167: 'English foxhound',\n",
      " 168: 'redbone',\n",
      " 169: 'borzoi, Russian wolfhound',\n",
      " 170: 'Irish wolfhound',\n",
      " 171: 'Italian greyhound',\n",
      " 172: 'whippet',\n",
      " 173: 'Ibizan hound, Ibizan Podenco',\n",
      " 174: 'Norwegian elkhound, elkhound',\n",
      " 175: 'otterhound, otter hound',\n",
      " 176: 'Saluki, gazelle hound',\n",
      " 177: 'Scottish deerhound, deerhound',\n",
      " 178: 'Weimaraner',\n",
      " 179: 'Staffordshire bullterrier, Staffordshire bull terrier',\n",
      " 180: 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier',\n",
      " 181: 'Bedlington terrier',\n",
      " 182: 'Border terrier',\n",
      " 183: 'Kerry blue terrier',\n",
      " 184: 'Irish terrier',\n",
      " 185: 'Norfolk terrier',\n",
      " 186: 'Norwich terrier',\n",
      " 187: 'Yorkshire terrier',\n",
      " 188: 'wire-haired fox terrier',\n",
      " 189: 'Lakeland terrier',\n",
      " 190: 'Sealyham terrier, Sealyham',\n",
      " 191: 'Airedale, Airedale terrier',\n",
      " 192: 'cairn, cairn terrier',\n",
      " 193: 'Australian terrier',\n",
      " 194: 'Dandie Dinmont, Dandie Dinmont terrier',\n",
      " 195: 'Boston bull, Boston terrier',\n",
      " 196: 'miniature schnauzer',\n",
      " 197: 'giant schnauzer',\n",
      " 198: 'standard schnauzer',\n",
      " 199: 'Scotch terrier, Scottish terrier, Scottie',\n",
      " 200: 'Tibetan terrier, chrysanthemum dog',\n",
      " 201: 'silky terrier, Sydney silky',\n",
      " 202: 'soft-coated wheaten terrier',\n",
      " 203: 'West Highland white terrier',\n",
      " 204: 'Lhasa, Lhasa apso',\n",
      " 205: 'flat-coated retriever',\n",
      " 206: 'curly-coated retriever',\n",
      " 207: 'golden retriever',\n",
      " 208: 'Labrador retriever',\n",
      " 209: 'Chesapeake Bay retriever',\n",
      " 210: 'German short-haired pointer',\n",
      " 211: 'vizsla, Hungarian pointer',\n",
      " 212: 'English setter',\n",
      " 213: 'Irish setter, red setter',\n",
      " 214: 'Gordon setter',\n",
      " 215: 'Brittany spaniel',\n",
      " 216: 'clumber, clumber spaniel',\n",
      " 217: 'English springer, English springer spaniel',\n",
      " 218: 'Welsh springer spaniel',\n",
      " 219: 'cocker spaniel, English cocker spaniel, cocker',\n",
      " 220: 'Sussex spaniel',\n",
      " 221: 'Irish water spaniel',\n",
      " 222: 'kuvasz',\n",
      " 223: 'schipperke',\n",
      " 224: 'groenendael',\n",
      " 225: 'malinois',\n",
      " 226: 'briard',\n",
      " 227: 'kelpie',\n",
      " 228: 'komondor',\n",
      " 229: 'Old English sheepdog, bobtail',\n",
      " 230: 'Shetland sheepdog, Shetland sheep dog, Shetland',\n",
      " 231: 'collie',\n",
      " 232: 'Border collie',\n",
      " 233: 'Bouvier des Flandres, Bouviers des Flandres',\n",
      " 234: 'Rottweiler',\n",
      " 235: 'German shepherd, German shepherd dog, German police dog, alsatian',\n",
      " 236: 'Doberman, Doberman pinscher',\n",
      " 237: 'miniature pinscher',\n",
      " 238: 'Greater Swiss Mountain dog',\n",
      " 239: 'Bernese mountain dog',\n",
      " 240: 'Appenzeller',\n",
      " 241: 'EntleBucher',\n",
      " 242: 'boxer',\n",
      " 243: 'bull mastiff',\n",
      " 244: 'Tibetan mastiff',\n",
      " 245: 'French bulldog',\n",
      " 246: 'Great Dane',\n",
      " 247: 'Saint Bernard, St Bernard',\n",
      " 248: 'Eskimo dog, husky',\n",
      " 249: 'malamute, malemute, Alaskan malamute',\n",
      " 250: 'Siberian husky',\n",
      " 251: 'dalmatian, coach dog, carriage dog',\n",
      " 252: 'affenpinscher, monkey pinscher, monkey dog',\n",
      " 253: 'basenji',\n",
      " 254: 'pug, pug-dog',\n",
      " 255: 'Leonberg',\n",
      " 256: 'Newfoundland, Newfoundland dog',\n",
      " 257: 'Great Pyrenees',\n",
      " 258: 'Samoyed, Samoyede',\n",
      " 259: 'Pomeranian',\n",
      " 260: 'chow, chow chow',\n",
      " 261: 'keeshond',\n",
      " 262: 'Brabancon griffon',\n",
      " 263: 'Pembroke, Pembroke Welsh corgi',\n",
      " 264: 'Cardigan, Cardigan Welsh corgi',\n",
      " 265: 'toy poodle',\n",
      " 266: 'miniature poodle',\n",
      " 267: 'standard poodle',\n",
      " 268: 'Mexican hairless',\n",
      " 269: 'timber wolf, grey wolf, gray wolf, Canis lupus',\n",
      " 270: 'white wolf, Arctic wolf, Canis lupus tundrarum',\n",
      " 271: 'red wolf, maned wolf, Canis rufus, Canis niger',\n",
      " 272: 'coyote, prairie wolf, brush wolf, Canis latrans',\n",
      " 273: 'dingo, warrigal, warragal, Canis dingo',\n",
      " 274: 'dhole, Cuon alpinus',\n",
      " 275: 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus',\n",
      " 276: 'hyena, hyaena',\n",
      " 277: 'red fox, Vulpes vulpes',\n",
      " 278: 'kit fox, Vulpes macrotis',\n",
      " 279: 'Arctic fox, white fox, Alopex lagopus',\n",
      " 280: 'grey fox, gray fox, Urocyon cinereoargenteus',\n",
      " 281: 'tabby, tabby cat',\n",
      " 282: 'tiger cat',\n",
      " 283: 'Persian cat',\n",
      " 284: 'Siamese cat, Siamese',\n",
      " 285: 'Egyptian cat',\n",
      " 286: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n",
      " 287: 'lynx, catamount',\n",
      " 288: 'leopard, Panthera pardus',\n",
      " 289: 'snow leopard, ounce, Panthera uncia',\n",
      " 290: 'jaguar, panther, Panthera onca, Felis onca',\n",
      " 291: 'lion, king of beasts, Panthera leo',\n",
      " 292: 'tiger, Panthera tigris',\n",
      " 293: 'cheetah, chetah, Acinonyx jubatus',\n",
      " 294: 'brown bear, bruin, Ursus arctos',\n",
      " 295: 'American black bear, black bear, Ursus americanus, Euarctos americanus',\n",
      " 296: 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus',\n",
      " 297: 'sloth bear, Melursus ursinus, Ursus ursinus',\n",
      " 298: 'mongoose',\n",
      " 299: 'meerkat, mierkat',\n",
      " 300: 'tiger beetle',\n",
      " 301: 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle',\n",
      " 302: 'ground beetle, carabid beetle',\n",
      " 303: 'long-horned beetle, longicorn, longicorn beetle',\n",
      " 304: 'leaf beetle, chrysomelid',\n",
      " 305: 'dung beetle',\n",
      " 306: 'rhinoceros beetle',\n",
      " 307: 'weevil',\n",
      " 308: 'fly',\n",
      " 309: 'bee',\n",
      " 310: 'ant, emmet, pismire',\n",
      " 311: 'grasshopper, hopper',\n",
      " 312: 'cricket',\n",
      " 313: 'walking stick, walkingstick, stick insect',\n",
      " 314: 'cockroach, roach',\n",
      " 315: 'mantis, mantid',\n",
      " 316: 'cicada, cicala',\n",
      " 317: 'leafhopper',\n",
      " 318: 'lacewing, lacewing fly',\n",
      " 319: \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\",\n",
      " 320: 'damselfly',\n",
      " 321: 'admiral',\n",
      " 322: 'ringlet, ringlet butterfly',\n",
      " 323: 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus',\n",
      " 324: 'cabbage butterfly',\n",
      " 325: 'sulphur butterfly, sulfur butterfly',\n",
      " 326: 'lycaenid, lycaenid butterfly',\n",
      " 327: 'starfish, sea star',\n",
      " 328: 'sea urchin',\n",
      " 329: 'sea cucumber, holothurian',\n",
      " 330: 'wood rabbit, cottontail, cottontail rabbit',\n",
      " 331: 'hare',\n",
      " 332: 'Angora, Angora rabbit',\n",
      " 333: 'hamster',\n",
      " 334: 'porcupine, hedgehog',\n",
      " 335: 'fox squirrel, eastern fox squirrel, Sciurus niger',\n",
      " 336: 'marmot',\n",
      " 337: 'beaver',\n",
      " 338: 'guinea pig, Cavia cobaya',\n",
      " 339: 'sorrel',\n",
      " 340: 'zebra',\n",
      " 341: 'hog, pig, grunter, squealer, Sus scrofa',\n",
      " 342: 'wild boar, boar, Sus scrofa',\n",
      " 343: 'warthog',\n",
      " 344: 'hippopotamus, hippo, river horse, Hippopotamus amphibius',\n",
      " 345: 'ox',\n",
      " 346: 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis',\n",
      " 347: 'bison',\n",
      " 348: 'ram, tup',\n",
      " 349: 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis',\n",
      " 350: 'ibex, Capra ibex',\n",
      " 351: 'hartebeest',\n",
      " 352: 'impala, Aepyceros melampus',\n",
      " 353: 'gazelle',\n",
      " 354: 'Arabian camel, dromedary, Camelus dromedarius',\n",
      " 355: 'llama',\n",
      " 356: 'weasel',\n",
      " 357: 'mink',\n",
      " 358: 'polecat, fitch, foulmart, foumart, Mustela putorius',\n",
      " 359: 'black-footed ferret, ferret, Mustela nigripes',\n",
      " 360: 'otter',\n",
      " 361: 'skunk, polecat, wood pussy',\n",
      " 362: 'badger',\n",
      " 363: 'armadillo',\n",
      " 364: 'three-toed sloth, ai, Bradypus tridactylus',\n",
      " 365: 'orangutan, orang, orangutang, Pongo pygmaeus',\n",
      " 366: 'gorilla, Gorilla gorilla',\n",
      " 367: 'chimpanzee, chimp, Pan troglodytes',\n",
      " 368: 'gibbon, Hylobates lar',\n",
      " 369: 'siamang, Hylobates syndactylus, Symphalangus syndactylus',\n",
      " 370: 'guenon, guenon monkey',\n",
      " 371: 'patas, hussar monkey, Erythrocebus patas',\n",
      " 372: 'baboon',\n",
      " 373: 'macaque',\n",
      " 374: 'langur',\n",
      " 375: 'colobus, colobus monkey',\n",
      " 376: 'proboscis monkey, Nasalis larvatus',\n",
      " 377: 'marmoset',\n",
      " 378: 'capuchin, ringtail, Cebus capucinus',\n",
      " 379: 'howler monkey, howler',\n",
      " 380: 'titi, titi monkey',\n",
      " 381: 'spider monkey, Ateles geoffroyi',\n",
      " 382: 'squirrel monkey, Saimiri sciureus',\n",
      " 383: 'Madagascar cat, ring-tailed lemur, Lemur catta',\n",
      " 384: 'indri, indris, Indri indri, Indri brevicaudatus',\n",
      " 385: 'Indian elephant, Elephas maximus',\n",
      " 386: 'African elephant, Loxodonta africana',\n",
      " 387: 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens',\n",
      " 388: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca',\n",
      " 389: 'barracouta, snoek',\n",
      " 390: 'eel',\n",
      " 391: 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch',\n",
      " 392: 'rock beauty, Holocanthus tricolor',\n",
      " 393: 'anemone fish',\n",
      " 394: 'sturgeon',\n",
      " 395: 'gar, garfish, garpike, billfish, Lepisosteus osseus',\n",
      " 396: 'lionfish',\n",
      " 397: 'puffer, pufferfish, blowfish, globefish',\n",
      " 398: 'abacus',\n",
      " 399: 'abaya',\n",
      " 400: \"academic gown, academic robe, judge's robe\",\n",
      " 401: 'accordion, piano accordion, squeeze box',\n",
      " 402: 'acoustic guitar',\n",
      " 403: 'aircraft carrier, carrier, flattop, attack aircraft carrier',\n",
      " 404: 'airliner',\n",
      " 405: 'airship, dirigible',\n",
      " 406: 'altar',\n",
      " 407: 'ambulance',\n",
      " 408: 'amphibian, amphibious vehicle',\n",
      " 409: 'analog clock',\n",
      " 410: 'apiary, bee house',\n",
      " 411: 'apron',\n",
      " 412: 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin',\n",
      " 413: 'assault rifle, assault gun',\n",
      " 414: 'backpack, back pack, knapsack, packsack, rucksack, haversack',\n",
      " 415: 'bakery, bakeshop, bakehouse',\n",
      " 416: 'balance beam, beam',\n",
      " 417: 'balloon',\n",
      " 418: 'ballpoint, ballpoint pen, ballpen, Biro',\n",
      " 419: 'Band Aid',\n",
      " 420: 'banjo',\n",
      " 421: 'bannister, banister, balustrade, balusters, handrail',\n",
      " 422: 'barbell',\n",
      " 423: 'barber chair',\n",
      " 424: 'barbershop',\n",
      " 425: 'barn',\n",
      " 426: 'barometer',\n",
      " 427: 'barrel, cask',\n",
      " 428: 'barrow, garden cart, lawn cart, wheelbarrow',\n",
      " 429: 'baseball',\n",
      " 430: 'basketball',\n",
      " 431: 'bassinet',\n",
      " 432: 'bassoon',\n",
      " 433: 'bathing cap, swimming cap',\n",
      " 434: 'bath towel',\n",
      " 435: 'bathtub, bathing tub, bath, tub',\n",
      " 436: 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon',\n",
      " 437: 'beacon, lighthouse, beacon light, pharos',\n",
      " 438: 'beaker',\n",
      " 439: 'bearskin, busby, shako',\n",
      " 440: 'beer bottle',\n",
      " 441: 'beer glass',\n",
      " 442: 'bell cote, bell cot',\n",
      " 443: 'bib',\n",
      " 444: 'bicycle-built-for-two, tandem bicycle, tandem',\n",
      " 445: 'bikini, two-piece',\n",
      " 446: 'binder, ring-binder',\n",
      " 447: 'binoculars, field glasses, opera glasses',\n",
      " 448: 'birdhouse',\n",
      " 449: 'boathouse',\n",
      " 450: 'bobsled, bobsleigh, bob',\n",
      " 451: 'bolo tie, bolo, bola tie, bola',\n",
      " 452: 'bonnet, poke bonnet',\n",
      " 453: 'bookcase',\n",
      " 454: 'bookshop, bookstore, bookstall',\n",
      " 455: 'bottlecap',\n",
      " 456: 'bow',\n",
      " 457: 'bow tie, bow-tie, bowtie',\n",
      " 458: 'brass, memorial tablet, plaque',\n",
      " 459: 'brassiere, bra, bandeau',\n",
      " 460: 'breakwater, groin, groyne, mole, bulwark, seawall, jetty',\n",
      " 461: 'breastplate, aegis, egis',\n",
      " 462: 'broom',\n",
      " 463: 'bucket, pail',\n",
      " 464: 'buckle',\n",
      " 465: 'bulletproof vest',\n",
      " 466: 'bullet train, bullet',\n",
      " 467: 'butcher shop, meat market',\n",
      " 468: 'cab, hack, taxi, taxicab',\n",
      " 469: 'caldron, cauldron',\n",
      " 470: 'candle, taper, wax light',\n",
      " 471: 'cannon',\n",
      " 472: 'canoe',\n",
      " 473: 'can opener, tin opener',\n",
      " 474: 'cardigan',\n",
      " 475: 'car mirror',\n",
      " 476: 'carousel, carrousel, merry-go-round, roundabout, whirligig',\n",
      " 477: \"carpenter's kit, tool kit\",\n",
      " 478: 'carton',\n",
      " 479: 'car wheel',\n",
      " 480: 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM',\n",
      " 481: 'cassette',\n",
      " 482: 'cassette player',\n",
      " 483: 'castle',\n",
      " 484: 'catamaran',\n",
      " 485: 'CD player',\n",
      " 486: 'cello, violoncello',\n",
      " 487: 'cellular telephone, cellular phone, cellphone, cell, mobile phone',\n",
      " 488: 'chain',\n",
      " 489: 'chainlink fence',\n",
      " 490: 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour',\n",
      " 491: 'chain saw, chainsaw',\n",
      " 492: 'chest',\n",
      " 493: 'chiffonier, commode',\n",
      " 494: 'chime, bell, gong',\n",
      " 495: 'china cabinet, china closet',\n",
      " 496: 'Christmas stocking',\n",
      " 497: 'church, church building',\n",
      " 498: 'cinema, movie theater, movie theatre, movie house, picture palace',\n",
      " 499: 'cleaver, meat cleaver, chopper',\n",
      " 500: 'cliff dwelling',\n",
      " 501: 'cloak',\n",
      " 502: 'clog, geta, patten, sabot',\n",
      " 503: 'cocktail shaker',\n",
      " 504: 'coffee mug',\n",
      " 505: 'coffeepot',\n",
      " 506: 'coil, spiral, volute, whorl, helix',\n",
      " 507: 'combination lock',\n",
      " 508: 'computer keyboard, keypad',\n",
      " 509: 'confectionery, confectionary, candy store',\n",
      " 510: 'container ship, containership, container vessel',\n",
      " 511: 'convertible',\n",
      " 512: 'corkscrew, bottle screw',\n",
      " 513: 'cornet, horn, trumpet, trump',\n",
      " 514: 'cowboy boot',\n",
      " 515: 'cowboy hat, ten-gallon hat',\n",
      " 516: 'cradle',\n",
      " 517: 'crane',\n",
      " 518: 'crash helmet',\n",
      " 519: 'crate',\n",
      " 520: 'crib, cot',\n",
      " 521: 'Crock Pot',\n",
      " 522: 'croquet ball',\n",
      " 523: 'crutch',\n",
      " 524: 'cuirass',\n",
      " 525: 'dam, dike, dyke',\n",
      " 526: 'desk',\n",
      " 527: 'desktop computer',\n",
      " 528: 'dial telephone, dial phone',\n",
      " 529: 'diaper, nappy, napkin',\n",
      " 530: 'digital clock',\n",
      " 531: 'digital watch',\n",
      " 532: 'dining table, board',\n",
      " 533: 'dishrag, dishcloth',\n",
      " 534: 'dishwasher, dish washer, dishwashing machine',\n",
      " 535: 'disk brake, disc brake',\n",
      " 536: 'dock, dockage, docking facility',\n",
      " 537: 'dogsled, dog sled, dog sleigh',\n",
      " 538: 'dome',\n",
      " 539: 'doormat, welcome mat',\n",
      " 540: 'drilling platform, offshore rig',\n",
      " 541: 'drum, membranophone, tympan',\n",
      " 542: 'drumstick',\n",
      " 543: 'dumbbell',\n",
      " 544: 'Dutch oven',\n",
      " 545: 'electric fan, blower',\n",
      " 546: 'electric guitar',\n",
      " 547: 'electric locomotive',\n",
      " 548: 'entertainment center',\n",
      " 549: 'envelope',\n",
      " 550: 'espresso maker',\n",
      " 551: 'face powder',\n",
      " 552: 'feather boa, boa',\n",
      " 553: 'file, file cabinet, filing cabinet',\n",
      " 554: 'fireboat',\n",
      " 555: 'fire engine, fire truck',\n",
      " 556: 'fire screen, fireguard',\n",
      " 557: 'flagpole, flagstaff',\n",
      " 558: 'flute, transverse flute',\n",
      " 559: 'folding chair',\n",
      " 560: 'football helmet',\n",
      " 561: 'forklift',\n",
      " 562: 'fountain',\n",
      " 563: 'fountain pen',\n",
      " 564: 'four-poster',\n",
      " 565: 'freight car',\n",
      " 566: 'French horn, horn',\n",
      " 567: 'frying pan, frypan, skillet',\n",
      " 568: 'fur coat',\n",
      " 569: 'garbage truck, dustcart',\n",
      " 570: 'gasmask, respirator, gas helmet',\n",
      " 571: 'gas pump, gasoline pump, petrol pump, island dispenser',\n",
      " 572: 'goblet',\n",
      " 573: 'go-kart',\n",
      " 574: 'golf ball',\n",
      " 575: 'golfcart, golf cart',\n",
      " 576: 'gondola',\n",
      " 577: 'gong, tam-tam',\n",
      " 578: 'gown',\n",
      " 579: 'grand piano, grand',\n",
      " 580: 'greenhouse, nursery, glasshouse',\n",
      " 581: 'grille, radiator grille',\n",
      " 582: 'grocery store, grocery, food market, market',\n",
      " 583: 'guillotine',\n",
      " 584: 'hair slide',\n",
      " 585: 'hair spray',\n",
      " 586: 'half track',\n",
      " 587: 'hammer',\n",
      " 588: 'hamper',\n",
      " 589: 'hand blower, blow dryer, blow drier, hair dryer, hair drier',\n",
      " 590: 'hand-held computer, hand-held microcomputer',\n",
      " 591: 'handkerchief, hankie, hanky, hankey',\n",
      " 592: 'hard disc, hard disk, fixed disk',\n",
      " 593: 'harmonica, mouth organ, harp, mouth harp',\n",
      " 594: 'harp',\n",
      " 595: 'harvester, reaper',\n",
      " 596: 'hatchet',\n",
      " 597: 'holster',\n",
      " 598: 'home theater, home theatre',\n",
      " 599: 'honeycomb',\n",
      " 600: 'hook, claw',\n",
      " 601: 'hoopskirt, crinoline',\n",
      " 602: 'horizontal bar, high bar',\n",
      " 603: 'horse cart, horse-cart',\n",
      " 604: 'hourglass',\n",
      " 605: 'iPod',\n",
      " 606: 'iron, smoothing iron',\n",
      " 607: \"jack-o'-lantern\",\n",
      " 608: 'jean, blue jean, denim',\n",
      " 609: 'jeep, landrover',\n",
      " 610: 'jersey, T-shirt, tee shirt',\n",
      " 611: 'jigsaw puzzle',\n",
      " 612: 'jinrikisha, ricksha, rickshaw',\n",
      " 613: 'joystick',\n",
      " 614: 'kimono',\n",
      " 615: 'knee pad',\n",
      " 616: 'knot',\n",
      " 617: 'lab coat, laboratory coat',\n",
      " 618: 'ladle',\n",
      " 619: 'lampshade, lamp shade',\n",
      " 620: 'laptop, laptop computer',\n",
      " 621: 'lawn mower, mower',\n",
      " 622: 'lens cap, lens cover',\n",
      " 623: 'letter opener, paper knife, paperknife',\n",
      " 624: 'library',\n",
      " 625: 'lifeboat',\n",
      " 626: 'lighter, light, igniter, ignitor',\n",
      " 627: 'limousine, limo',\n",
      " 628: 'liner, ocean liner',\n",
      " 629: 'lipstick, lip rouge',\n",
      " 630: 'Loafer',\n",
      " 631: 'lotion',\n",
      " 632: 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system',\n",
      " 633: \"loupe, jeweler's loupe\",\n",
      " 634: 'lumbermill, sawmill',\n",
      " 635: 'magnetic compass',\n",
      " 636: 'mailbag, postbag',\n",
      " 637: 'mailbox, letter box',\n",
      " 638: 'maillot',\n",
      " 639: 'maillot, tank suit',\n",
      " 640: 'manhole cover',\n",
      " 641: 'maraca',\n",
      " 642: 'marimba, xylophone',\n",
      " 643: 'mask',\n",
      " 644: 'matchstick',\n",
      " 645: 'maypole',\n",
      " 646: 'maze, labyrinth',\n",
      " 647: 'measuring cup',\n",
      " 648: 'medicine chest, medicine cabinet',\n",
      " 649: 'megalith, megalithic structure',\n",
      " 650: 'microphone, mike',\n",
      " 651: 'microwave, microwave oven',\n",
      " 652: 'military uniform',\n",
      " 653: 'milk can',\n",
      " 654: 'minibus',\n",
      " 655: 'miniskirt, mini',\n",
      " 656: 'minivan',\n",
      " 657: 'missile',\n",
      " 658: 'mitten',\n",
      " 659: 'mixing bowl',\n",
      " 660: 'mobile home, manufactured home',\n",
      " 661: 'Model T',\n",
      " 662: 'modem',\n",
      " 663: 'monastery',\n",
      " 664: 'monitor',\n",
      " 665: 'moped',\n",
      " 666: 'mortar',\n",
      " 667: 'mortarboard',\n",
      " 668: 'mosque',\n",
      " 669: 'mosquito net',\n",
      " 670: 'motor scooter, scooter',\n",
      " 671: 'mountain bike, all-terrain bike, off-roader',\n",
      " 672: 'mountain tent',\n",
      " 673: 'mouse, computer mouse',\n",
      " 674: 'mousetrap',\n",
      " 675: 'moving van',\n",
      " 676: 'muzzle',\n",
      " 677: 'nail',\n",
      " 678: 'neck brace',\n",
      " 679: 'necklace',\n",
      " 680: 'nipple',\n",
      " 681: 'notebook, notebook computer',\n",
      " 682: 'obelisk',\n",
      " 683: 'oboe, hautboy, hautbois',\n",
      " 684: 'ocarina, sweet potato',\n",
      " 685: 'odometer, hodometer, mileometer, milometer',\n",
      " 686: 'oil filter',\n",
      " 687: 'organ, pipe organ',\n",
      " 688: 'oscilloscope, scope, cathode-ray oscilloscope, CRO',\n",
      " 689: 'overskirt',\n",
      " 690: 'oxcart',\n",
      " 691: 'oxygen mask',\n",
      " 692: 'packet',\n",
      " 693: 'paddle, boat paddle',\n",
      " 694: 'paddlewheel, paddle wheel',\n",
      " 695: 'padlock',\n",
      " 696: 'paintbrush',\n",
      " 697: \"pajama, pyjama, pj's, jammies\",\n",
      " 698: 'palace',\n",
      " 699: 'panpipe, pandean pipe, syrinx',\n",
      " 700: 'paper towel',\n",
      " 701: 'parachute, chute',\n",
      " 702: 'parallel bars, bars',\n",
      " 703: 'park bench',\n",
      " 704: 'parking meter',\n",
      " 705: 'passenger car, coach, carriage',\n",
      " 706: 'patio, terrace',\n",
      " 707: 'pay-phone, pay-station',\n",
      " 708: 'pedestal, plinth, footstall',\n",
      " 709: 'pencil box, pencil case',\n",
      " 710: 'pencil sharpener',\n",
      " 711: 'perfume, essence',\n",
      " 712: 'Petri dish',\n",
      " 713: 'photocopier',\n",
      " 714: 'pick, plectrum, plectron',\n",
      " 715: 'pickelhaube',\n",
      " 716: 'picket fence, paling',\n",
      " 717: 'pickup, pickup truck',\n",
      " 718: 'pier',\n",
      " 719: 'piggy bank, penny bank',\n",
      " 720: 'pill bottle',\n",
      " 721: 'pillow',\n",
      " 722: 'ping-pong ball',\n",
      " 723: 'pinwheel',\n",
      " 724: 'pirate, pirate ship',\n",
      " 725: 'pitcher, ewer',\n",
      " 726: \"plane, carpenter's plane, woodworking plane\",\n",
      " 727: 'planetarium',\n",
      " 728: 'plastic bag',\n",
      " 729: 'plate rack',\n",
      " 730: 'plow, plough',\n",
      " 731: \"plunger, plumber's helper\",\n",
      " 732: 'Polaroid camera, Polaroid Land camera',\n",
      " 733: 'pole',\n",
      " 734: 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria',\n",
      " 735: 'poncho',\n",
      " 736: 'pool table, billiard table, snooker table',\n",
      " 737: 'pop bottle, soda bottle',\n",
      " 738: 'pot, flowerpot',\n",
      " 739: \"potter's wheel\",\n",
      " 740: 'power drill',\n",
      " 741: 'prayer rug, prayer mat',\n",
      " 742: 'printer',\n",
      " 743: 'prison, prison house',\n",
      " 744: 'projectile, missile',\n",
      " 745: 'projector',\n",
      " 746: 'puck, hockey puck',\n",
      " 747: 'punching bag, punch bag, punching ball, punchball',\n",
      " 748: 'purse',\n",
      " 749: 'quill, quill pen',\n",
      " 750: 'quilt, comforter, comfort, puff',\n",
      " 751: 'racer, race car, racing car',\n",
      " 752: 'racket, racquet',\n",
      " 753: 'radiator',\n",
      " 754: 'radio, wireless',\n",
      " 755: 'radio telescope, radio reflector',\n",
      " 756: 'rain barrel',\n",
      " 757: 'recreational vehicle, RV, R.V.',\n",
      " 758: 'reel',\n",
      " 759: 'reflex camera',\n",
      " 760: 'refrigerator, icebox',\n",
      " 761: 'remote control, remote',\n",
      " 762: 'restaurant, eating house, eating place, eatery',\n",
      " 763: 'revolver, six-gun, six-shooter',\n",
      " 764: 'rifle',\n",
      " 765: 'rocking chair, rocker',\n",
      " 766: 'rotisserie',\n",
      " 767: 'rubber eraser, rubber, pencil eraser',\n",
      " 768: 'rugby ball',\n",
      " 769: 'rule, ruler',\n",
      " 770: 'running shoe',\n",
      " 771: 'safe',\n",
      " 772: 'safety pin',\n",
      " 773: 'saltshaker, salt shaker',\n",
      " 774: 'sandal',\n",
      " 775: 'sarong',\n",
      " 776: 'sax, saxophone',\n",
      " 777: 'scabbard',\n",
      " 778: 'scale, weighing machine',\n",
      " 779: 'school bus',\n",
      " 780: 'schooner',\n",
      " 781: 'scoreboard',\n",
      " 782: 'screen, CRT screen',\n",
      " 783: 'screw',\n",
      " 784: 'screwdriver',\n",
      " 785: 'seat belt, seatbelt',\n",
      " 786: 'sewing machine',\n",
      " 787: 'shield, buckler',\n",
      " 788: 'shoe shop, shoe-shop, shoe store',\n",
      " 789: 'shoji',\n",
      " 790: 'shopping basket',\n",
      " 791: 'shopping cart',\n",
      " 792: 'shovel',\n",
      " 793: 'shower cap',\n",
      " 794: 'shower curtain',\n",
      " 795: 'ski',\n",
      " 796: 'ski mask',\n",
      " 797: 'sleeping bag',\n",
      " 798: 'slide rule, slipstick',\n",
      " 799: 'sliding door',\n",
      " 800: 'slot, one-armed bandit',\n",
      " 801: 'snorkel',\n",
      " 802: 'snowmobile',\n",
      " 803: 'snowplow, snowplough',\n",
      " 804: 'soap dispenser',\n",
      " 805: 'soccer ball',\n",
      " 806: 'sock',\n",
      " 807: 'solar dish, solar collector, solar furnace',\n",
      " 808: 'sombrero',\n",
      " 809: 'soup bowl',\n",
      " 810: 'space bar',\n",
      " 811: 'space heater',\n",
      " 812: 'space shuttle',\n",
      " 813: 'spatula',\n",
      " 814: 'speedboat',\n",
      " 815: \"spider web, spider's web\",\n",
      " 816: 'spindle',\n",
      " 817: 'sports car, sport car',\n",
      " 818: 'spotlight, spot',\n",
      " 819: 'stage',\n",
      " 820: 'steam locomotive',\n",
      " 821: 'steel arch bridge',\n",
      " 822: 'steel drum',\n",
      " 823: 'stethoscope',\n",
      " 824: 'stole',\n",
      " 825: 'stone wall',\n",
      " 826: 'stopwatch, stop watch',\n",
      " 827: 'stove',\n",
      " 828: 'strainer',\n",
      " 829: 'streetcar, tram, tramcar, trolley, trolley car',\n",
      " 830: 'stretcher',\n",
      " 831: 'studio couch, day bed',\n",
      " 832: 'stupa, tope',\n",
      " 833: 'submarine, pigboat, sub, U-boat',\n",
      " 834: 'suit, suit of clothes',\n",
      " 835: 'sundial',\n",
      " 836: 'sunglass',\n",
      " 837: 'sunglasses, dark glasses, shades',\n",
      " 838: 'sunscreen, sunblock, sun blocker',\n",
      " 839: 'suspension bridge',\n",
      " 840: 'swab, swob, mop',\n",
      " 841: 'sweatshirt',\n",
      " 842: 'swimming trunks, bathing trunks',\n",
      " 843: 'swing',\n",
      " 844: 'switch, electric switch, electrical switch',\n",
      " 845: 'syringe',\n",
      " 846: 'table lamp',\n",
      " 847: 'tank, army tank, armored combat vehicle, armoured combat vehicle',\n",
      " 848: 'tape player',\n",
      " 849: 'teapot',\n",
      " 850: 'teddy, teddy bear',\n",
      " 851: 'television, television system',\n",
      " 852: 'tennis ball',\n",
      " 853: 'thatch, thatched roof',\n",
      " 854: 'theater curtain, theatre curtain',\n",
      " 855: 'thimble',\n",
      " 856: 'thresher, thrasher, threshing machine',\n",
      " 857: 'throne',\n",
      " 858: 'tile roof',\n",
      " 859: 'toaster',\n",
      " 860: 'tobacco shop, tobacconist shop, tobacconist',\n",
      " 861: 'toilet seat',\n",
      " 862: 'torch',\n",
      " 863: 'totem pole',\n",
      " 864: 'tow truck, tow car, wrecker',\n",
      " 865: 'toyshop',\n",
      " 866: 'tractor',\n",
      " 867: 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi',\n",
      " 868: 'tray',\n",
      " 869: 'trench coat',\n",
      " 870: 'tricycle, trike, velocipede',\n",
      " 871: 'trimaran',\n",
      " 872: 'tripod',\n",
      " 873: 'triumphal arch',\n",
      " 874: 'trolleybus, trolley coach, trackless trolley',\n",
      " 875: 'trombone',\n",
      " 876: 'tub, vat',\n",
      " 877: 'turnstile',\n",
      " 878: 'typewriter keyboard',\n",
      " 879: 'umbrella',\n",
      " 880: 'unicycle, monocycle',\n",
      " 881: 'upright, upright piano',\n",
      " 882: 'vacuum, vacuum cleaner',\n",
      " 883: 'vase',\n",
      " 884: 'vault',\n",
      " 885: 'velvet',\n",
      " 886: 'vending machine',\n",
      " 887: 'vestment',\n",
      " 888: 'viaduct',\n",
      " 889: 'violin, fiddle',\n",
      " 890: 'volleyball',\n",
      " 891: 'waffle iron',\n",
      " 892: 'wall clock',\n",
      " 893: 'wallet, billfold, notecase, pocketbook',\n",
      " 894: 'wardrobe, closet, press',\n",
      " 895: 'warplane, military plane',\n",
      " 896: 'washbasin, handbasin, washbowl, lavabo, wash-hand basin',\n",
      " 897: 'washer, automatic washer, washing machine',\n",
      " 898: 'water bottle',\n",
      " 899: 'water jug',\n",
      " 900: 'water tower',\n",
      " 901: 'whiskey jug',\n",
      " 902: 'whistle',\n",
      " 903: 'wig',\n",
      " 904: 'window screen',\n",
      " 905: 'window shade',\n",
      " 906: 'Windsor tie',\n",
      " 907: 'wine bottle',\n",
      " 908: 'wing',\n",
      " 909: 'wok',\n",
      " 910: 'wooden spoon',\n",
      " 911: 'wool, woolen, woollen',\n",
      " 912: 'worm fence, snake fence, snake-rail fence, Virginia fence',\n",
      " 913: 'wreck',\n",
      " 914: 'yawl',\n",
      " 915: 'yurt',\n",
      " 916: 'web site, website, internet site, site',\n",
      " 917: 'comic book',\n",
      " 918: 'crossword puzzle, crossword',\n",
      " 919: 'street sign',\n",
      " 920: 'traffic light, traffic signal, stoplight',\n",
      " 921: 'book jacket, dust cover, dust jacket, dust wrapper',\n",
      " 922: 'menu',\n",
      " 923: 'plate',\n",
      " 924: 'guacamole',\n",
      " 925: 'consomme',\n",
      " 926: 'hot pot, hotpot',\n",
      " 927: 'trifle',\n",
      " 928: 'ice cream, icecream',\n",
      " 929: 'ice lolly, lolly, lollipop, popsicle',\n",
      " 930: 'French loaf',\n",
      " 931: 'bagel, beigel',\n",
      " 932: 'pretzel',\n",
      " 933: 'cheeseburger',\n",
      " 934: 'hotdog, hot dog, red hot',\n",
      " 935: 'mashed potato',\n",
      " 936: 'head cabbage',\n",
      " 937: 'broccoli',\n",
      " 938: 'cauliflower',\n",
      " 939: 'zucchini, courgette',\n",
      " 940: 'spaghetti squash',\n",
      " 941: 'acorn squash',\n",
      " 942: 'butternut squash',\n",
      " 943: 'cucumber, cuke',\n",
      " 944: 'artichoke, globe artichoke',\n",
      " 945: 'bell pepper',\n",
      " 946: 'cardoon',\n",
      " 947: 'mushroom',\n",
      " 948: 'Granny Smith',\n",
      " 949: 'strawberry',\n",
      " 950: 'orange',\n",
      " 951: 'lemon',\n",
      " 952: 'fig',\n",
      " 953: 'pineapple, ananas',\n",
      " 954: 'banana',\n",
      " 955: 'jackfruit, jak, jack',\n",
      " 956: 'custard apple',\n",
      " 957: 'pomegranate',\n",
      " 958: 'hay',\n",
      " 959: 'carbonara',\n",
      " 960: 'chocolate sauce, chocolate syrup',\n",
      " 961: 'dough',\n",
      " 962: 'meat loaf, meatloaf',\n",
      " 963: 'pizza, pizza pie',\n",
      " 964: 'potpie',\n",
      " 965: 'burrito',\n",
      " 966: 'red wine',\n",
      " 967: 'espresso',\n",
      " 968: 'cup',\n",
      " 969: 'eggnog',\n",
      " 970: 'alp',\n",
      " 971: 'bubble',\n",
      " 972: 'cliff, drop, drop-off',\n",
      " 973: 'coral reef',\n",
      " 974: 'geyser',\n",
      " 975: 'lakeside, lakeshore',\n",
      " 976: 'promontory, headland, head, foreland',\n",
      " 977: 'sandbar, sand bar',\n",
      " 978: 'seashore, coast, seacoast, sea-coast',\n",
      " 979: 'valley, vale',\n",
      " 980: 'volcano',\n",
      " 981: 'ballplayer, baseball player',\n",
      " 982: 'groom, bridegroom',\n",
      " 983: 'scuba diver',\n",
      " 984: 'rapeseed',\n",
      " 985: 'daisy',\n",
      " 986: \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\",\n",
      " 987: 'corn',\n",
      " 988: 'acorn',\n",
      " 989: 'hip, rose hip, rosehip',\n",
      " 990: 'buckeye, horse chestnut, conker',\n",
      " 991: 'coral fungus',\n",
      " 992: 'agaric',\n",
      " 993: 'gyromitra',\n",
      " 994: 'stinkhorn, carrion fungus',\n",
      " 995: 'earthstar',\n",
      " 996: 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa',\n",
      " 997: 'bolete',\n",
      " 998: 'ear, spike, capitulum',\n",
      " 999: 'toilet tissue, toilet paper, bathroom tissue'}\n",
      "20/68:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 224\n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "image = center_crop(image)\n",
      "image = cv2.resize(image,(IMG_SIZE,IMG_SIZE))     # resize image to match model's expected sizing\n",
      "image = image.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "plt.imshow(image[0])\n",
      "print(class_dict[np.argmax(model.predict(image), axis=1)[0]])\n",
      "20/69:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 224\n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "#image = center_crop(image)\n",
      "image = cv2.resize(image,(IMG_SIZE,IMG_SIZE))     # resize image to match model's expected sizing\n",
      "image = image.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "plt.imshow(image[0])\n",
      "print(class_dict[np.argmax(model.predict(image), axis=1)[0]])\n",
      "20/70:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 224\n",
      "\n",
      "def prep_image(image):\n",
      "    res = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "    res = res.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "    return res\n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "cropped_image = center_crop(image)\n",
      "\n",
      "plt.imshow(image[0])\n",
      "print(class_dict[np.argmax(model.predict(prep_image(image)), axis=1)[0]])\n",
      "20/71:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 224\n",
      "\n",
      "def prep_image(image):\n",
      "    res = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "    res = res.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "    return res\n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "cropped_image = center_crop(image)\n",
      "\n",
      "plt.imshow(image)\n",
      "print(class_dict[np.argmax(model.predict(prep_image(image)), axis=1)[0]])\n",
      "20/72:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 224\n",
      "\n",
      "def prep_image(image):\n",
      "    res = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "    res = res.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "    return res\n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "cropped_image = center_crop(image)\n",
      "\n",
      "plt.imshow(image)\n",
      "prediction = model.predict([prep_image(image),prep_image(cropped_image)])\n",
      "if ()\n",
      "print(class_dict[np.argmax(model.predict(prep_image(image)), axis=1)[0]])\n",
      "20/73:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 224\n",
      "\n",
      "def prep_image(image):\n",
      "    res = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "    res = res.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "    return res\n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "cropped_image = center_crop(image)\n",
      "\n",
      "plt.imshow(image)\n",
      "prediction = model.predict([prep_image(image),prep_image(cropped_image)])\n",
      "if np.max(prediction[0]>=prediction[1]):\n",
      "    print(class_dict[np.argmax(prediction[0], axis=1)[0]])\n",
      "else:\n",
      "    print(class_dict[np.argmax(prediction[1], axis=1)[1]])\n",
      "20/74:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 224\n",
      "\n",
      "def prep_image(image):\n",
      "    res = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "    return res\n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "cropped_image = center_crop(image)\n",
      "\n",
      "plt.imshow(image)\n",
      "prediction = model.predict([prep_image(image),prep_image(cropped_image)])\n",
      "if np.max(prediction[0]>=prediction[1]):\n",
      "    print(class_dict[np.argmax(prediction[0], axis=1)[0]])\n",
      "else:\n",
      "    print(class_dict[np.argmax(prediction[1], axis=1)[1]])\n",
      "20/75:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 224\n",
      "\n",
      "def prep_image(image):\n",
      "    res = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "    res = res.reshape(IMG_SIZE,IMG_SIZE,3)\n",
      "    return res\n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "cropped_image = center_crop(image)\n",
      "\n",
      "plt.imshow(image)\n",
      "prediction = model.predict([prep_image(image),prep_image(cropped_image)])\n",
      "if np.max(prediction[0]>=prediction[1]):\n",
      "    print(class_dict[np.argmax(prediction[0], axis=1)[0]])\n",
      "else:\n",
      "    print(class_dict[np.argmax(prediction[1], axis=1)[1]])\n",
      "20/76:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 224\n",
      "\n",
      "def prep_image(image):\n",
      "    res = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "    res = res.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "    return res\n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "cropped_image = center_crop(image)\n",
      "\n",
      "plt.imshow(image)\n",
      "prediction = [model.predict(prep_image(image))model.predict(prep_image(cropped_image))]\n",
      "if np.max(prediction[0]>=prediction[1]):\n",
      "    print(class_dict[np.argmax(prediction[0], axis=1)[0]])\n",
      "else:\n",
      "    print(class_dict[np.argmax(prediction[1], axis=1)[1]])\n",
      "20/77:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 224\n",
      "\n",
      "def prep_image(image):\n",
      "    res = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "    res = res.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "    return res\n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "cropped_image = center_crop(image)\n",
      "\n",
      "plt.imshow(image)\n",
      "prediction = ([)model.predict(prep_image(image)), model.predict(prep_image(cropped_image)))\n",
      "if np.max(prediction[0]>=prediction[1]):\n",
      "    print(class_dict[np.argmax(prediction[0], axis=1)[0]])\n",
      "else:\n",
      "    print(class_dict[np.argmax(prediction[1], axis=1)[1]])\n",
      "20/78:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 224\n",
      "\n",
      "def prep_image(image):\n",
      "    res = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "    res = res.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "    return res\n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "cropped_image = center_crop(image)\n",
      "\n",
      "plt.imshow(image)\n",
      "prediction = (model.predict(prep_image(image)), model.predict(prep_image(cropped_image)))\n",
      "if np.max(prediction[0]>=prediction[1]):\n",
      "    print(class_dict[np.argmax(prediction[0], axis=1)[0]])\n",
      "else:\n",
      "    print(class_dict[np.argmax(prediction[1], axis=1)[1]])\n",
      "20/79:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 224\n",
      "\n",
      "def prep_image(image):\n",
      "    res = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "    res = res.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "    return res\n",
      "\n",
      "def predict(image):\n",
      "    cropped_image = center_crop(image)\n",
      "    prediction = (model.predict(prep_image(image)), model.predict(prep_image(cropped_image)))\n",
      "    if np.max(prediction[0]>=prediction[1]):\n",
      "        return np.argmax(prediction[0], axis=1)[0]\n",
      "    else:\n",
      "        return np.argmax(prediction[1], axis=1)[1]\n",
      "    \n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "plt.imshow(image)\n",
      "\n",
      "print(predict(image))\n",
      "20/80:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 224\n",
      "\n",
      "def prep_image(image):\n",
      "    res = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "    res = res.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "    return res\n",
      "\n",
      "def predict(image):\n",
      "    cropped_image = center_crop(image)\n",
      "    prediction = (model.predict(prep_image(image)), model.predict(prep_image(cropped_image)))\n",
      "    if np.max(prediction[0]>=prediction[1]):\n",
      "        return np.argmax(prediction[0], axis=1)[0]\n",
      "    else:\n",
      "        return np.argmax(prediction[1], axis=1)[1]\n",
      "    \n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "plt.imshow(image)\n",
      "\n",
      "print(class_dict[predict(image)])\n",
      "20/81:\n",
      "import pickle\n",
      "\n",
      "result = {}\n",
      "for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "        print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "        image = cv2.imread(\"../data/images/\" + file)\n",
      "\n",
      "        result[file] = predict(image)\n",
      "        \n",
      "pickle.dump(result, open( \"image_classes\", \"wb\" ))\n",
      "20/82:\n",
      "# import pickle\n",
      "\n",
      "# result = {}\n",
      "# for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "#     if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "#         print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "#         image = cv2.imread(\"../data/images/\" + file)\n",
      "\n",
      "#         result[file] = predict(image)\n",
      "        \n",
      "# pickle.dump(result, open( \"image_classes\", \"wb\" ))\n",
      "\n",
      "result = pickle.load(open( \"image_classes\" )\n",
      "20/83:\n",
      "# import pickle\n",
      "\n",
      "# result = {}\n",
      "# for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "#     if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "#         print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "#         image = cv2.imread(\"../data/images/\" + file)\n",
      "\n",
      "#         result[file] = predict(image)\n",
      "        \n",
      "# pickle.dump(result, open( \"image_classes\", \"wb\" ))\n",
      "\n",
      "result = pickle.load(open( \"image_classes\" ))\n",
      "20/84:\n",
      "# import pickle\n",
      "\n",
      "# result = {}\n",
      "# for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "#     if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "#         print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "#         image = cv2.imread(\"../data/images/\" + file)\n",
      "\n",
      "#         result[file] = predict(image)\n",
      "        \n",
      "# pickle.dump(result, open( \"image_classes\", \"wb\" ))\n",
      "\n",
      "result = pickle.load( open( \"image_classes\", \"rb\" ) )\n",
      "20/85: print(result)\n",
      "19/30: text_attributes.describe()\n",
      "19/31:\n",
      "text_attributes['link'] = df['body'].apply(lambda s: len(re.findall(r'(https?://[^\\s]+)', s)))\n",
      "text_attributes['reddit_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?redd\\.?it[^\\s]+)', s)))\n",
      "text_attributes['yt_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?youtu\\.?be[^\\s]+)', s)))\n",
      "text_attributes['tweet_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?twitter[^\\s]+)', s)))\n",
      "text_attributes['facebook_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?facebook[^\\s]+)', s)))\n",
      "\n",
      "text_attributes = pandas.DataFrame(text_attributes)\n",
      "text_attributes[yt_link]>1\n",
      "19/32:\n",
      "text_attributes['link'] = df['body'].apply(lambda s: len(re.findall(r'(https?://[^\\s]+)', s)))\n",
      "text_attributes['reddit_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?redd\\.?it[^\\s]+)', s)))\n",
      "text_attributes['yt_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?youtu\\.?be[^\\s]+)', s)))\n",
      "text_attributes['tweet_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?twitter[^\\s]+)', s)))\n",
      "text_attributes['facebook_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?facebook[^\\s]+)', s)))\n",
      "\n",
      "text_attributes = pandas.DataFrame(text_attributes)\n",
      "text_attributes['yt_link']>1\n",
      "19/33:\n",
      "text_attributes['link'] = df['body'].apply(lambda s: len(re.findall(r'(https?://[^\\s]+)', s)))\n",
      "text_attributes['reddit_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?redd\\.?it[^\\s]+)', s)))\n",
      "text_attributes['yt_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?youtu\\.?be[^\\s]+)', s)))\n",
      "text_attributes['tweet_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?twitter[^\\s]+)', s)))\n",
      "text_attributes['facebook_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?facebook[^\\s]+)', s)))\n",
      "\n",
      "text_attributes = pandas.DataFrame(text_attributes)\n",
      "text_attributes[text_attributes['yt_link']>1]\n",
      "19/34:\n",
      "text_attributes['link'] = df['body'].apply(lambda s: len(re.findall(r'(https?://[^\\s]+)', s)))\n",
      "text_attributes['reddit_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?redd\\.?it[^\\s]+)', s)))\n",
      "text_attributes['yt_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?youtu\\.?be[^\\s]+)', s)))\n",
      "text_attributes['tweet_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?twitter[^\\s]+)', s)))\n",
      "text_attributes['facebook_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?facebook[^\\s]+)', s)))\n",
      "\n",
      "text_attributes = pandas.DataFrame(text_attributes)\n",
      "df[text_attributes['yt_link']>1]\n",
      "19/35:\n",
      "text_attributes['link'] = df['body'].apply(lambda s: len(re.findall(r'(https?://[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(https?://[^\\s]+)', s)))\n",
      "text_attributes['reddit_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?redd\\.?it[^\\s]+)', s)))\n",
      "text_attributes['yt_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?youtu\\.?be[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?youtu\\.?be[^\\s]+)', s)))\n",
      "text_attributes['tweet_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?twitter[^\\s]+)', s)))\n",
      "text_attributes['facebook_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?facebook[^\\s]+)', s)))\n",
      "\n",
      "text_attributes = pandas.DataFrame(text_attributes)\n",
      "df[text_attributes['yt_link']>1]\n",
      "19/36:\n",
      "text_attributes['link'] = df['body'].apply(lambda s: len(re.findall(r'(https?://[^\\s]+)', s)))\n",
      "text_attributes['reddit_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?redd\\.?it[^\\s]+)', s)))\n",
      "text_attributes['yt_link'] = df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?youtu\\.?be[^\\s]+)', s)))\n",
      "text_attributes['tweet_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?twitter[^\\s]+)', s)))\n",
      "text_attributes['facebook_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?facebook[^\\s]+)', s)))\n",
      "\n",
      "text_attributes = pandas.DataFrame(text_attributes)\n",
      "df[text_attributes['yt_link']>1]\n",
      "19/37:\n",
      "text_attributes['link'] = df['body'].apply(lambda s: len(re.findall(r'(https?://[^\\s]+)', s)))\n",
      "text_attributes['reddit_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?redd\\.?it[^\\s]+)', s)))\n",
      "text_attributes['yt_link'] = df['url'].apply(lambda s: len(re.findall(r'(https?://[^\\s]+', s)))\n",
      "text_attributes['tweet_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?twitter[^\\s]+)', s)))\n",
      "text_attributes['facebook_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?facebook[^\\s]+)', s)))\n",
      "\n",
      "text_attributes = pandas.DataFrame(text_attributes)\n",
      "df[text_attributes['yt_link']>1]\n",
      "19/38:\n",
      "text_attributes['link'] = df['body'].apply(lambda s: len(re.findall(r'(https?://[^\\s]+)', s)))\n",
      "text_attributes['reddit_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?redd\\.?it[^\\s]+)', s)))\n",
      "text_attributes['yt_link'] = df['url'].apply(lambda s: len(re.findall(r'(https?://[^\\s]+)', s)))\n",
      "text_attributes['tweet_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?twitter[^\\s]+)', s)))\n",
      "text_attributes['facebook_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?facebook[^\\s]+)', s)))\n",
      "\n",
      "text_attributes = pandas.DataFrame(text_attributes)\n",
      "df[text_attributes['yt_link']>1]\n",
      "19/39:\n",
      "text_attributes['link'] = df['body'].apply(lambda s: len(re.findall(r'(https?://[^\\s]+)', s)))\n",
      "text_attributes['reddit_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?redd\\.?it[^\\s]+)', s)))\n",
      "text_attributes['yt_link'] = df['url'].apply(lambda s: len(re.findall(r'(https)', s)))\n",
      "text_attributes['tweet_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?twitter[^\\s]+)', s)))\n",
      "text_attributes['facebook_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?facebook[^\\s]+)', s)))\n",
      "\n",
      "text_attributes = pandas.DataFrame(text_attributes)\n",
      "df[text_attributes['yt_link']>1]\n",
      "19/40:\n",
      "text_attributes['link'] = df['body'].apply(lambda s: len(re.findall(r'(https?://[^\\s]+)', s)))\n",
      "text_attributes['reddit_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?redd\\.?it[^\\s]+)', s)))\n",
      "text_attributes['yt_link'] = df['url'].apply(lambda s: len(re.findall(r'(https)', s)))\n",
      "text_attributes['tweet_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?twitter[^\\s]+)', s)))\n",
      "text_attributes['facebook_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?facebook[^\\s]+)', s)))\n",
      "\n",
      "text_attributes = pandas.DataFrame(text_attributes)\n",
      "df[text_attributes['yt_link']>=1]\n",
      "19/41:\n",
      "text_attributes['link'] = df['body'].apply(lambda s: len(re.findall(r'(https?://[^\\s]+)', s)))\n",
      "text_attributes['reddit_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?redd\\.?it[^\\s]+)', s)))\n",
      "text_attributes['yt_link'] = df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?youtu\\.?be[^\\s]+)', s)))\n",
      "text_attributes['tweet_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?twitter[^\\s]+)', s)))\n",
      "text_attributes['facebook_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?facebook[^\\s]+)', s)))\n",
      "\n",
      "text_attributes = pandas.DataFrame(text_attributes)\n",
      "df[text_attributes['yt_link']>=1]\n",
      "19/42:\n",
      "text_attributes['link'] = df['body'].apply(lambda s: len(re.findall(r'(https?://[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(https?://[^\\s]+)', s)))\n",
      "text_attributes['reddit_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?redd\\.?it[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?redd\\.?it[^\\s]+)', s)))\n",
      "text_attributes['yt_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?youtu\\.?be[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?youtu\\.?be[^\\s]+)', s)))\n",
      "text_attributes['tweet_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?twitter[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?twitter[^\\s]+)', s)))\n",
      "text_attributes['facebook_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?facebook[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?facebook[^\\s]+)', s)))\n",
      "\n",
      "text_attributes = pandas.DataFrame(text_attributes)\n",
      "df[text_attributes['yt_link']>=1]\n",
      "19/43:\n",
      "text_attributes['link'] = df['body'].apply(lambda s: len(re.findall(r'(https?://[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(https?://[^\\s]+)', s)))\n",
      "text_attributes['reddit_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?redd\\.?it[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?redd\\.?it[^\\s]+)', s)))\n",
      "text_attributes['yt_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?youtu\\.?be[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?youtu\\.?be[^\\s]+)', s)))\n",
      "text_attributes['tweet_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?twitter[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?twitter[^\\s]+)', s)))\n",
      "text_attributes['facebook_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?facebook[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?facebook[^\\s]+)', s)))\n",
      "\n",
      "text_attributes = pandas.DataFrame(text_attributes)\n",
      "df[text_attributes['facebook_link']>=1]\n",
      "19/44:\n",
      "text_attributes['link'] = df['body'].apply(lambda s: len(re.findall(r'(https?://[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(https?://[^\\s]+)', s)))\n",
      "text_attributes['reddit_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?redd\\.?it[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?redd\\.?it[^\\s]+)', s)))\n",
      "text_attributes['yt_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?youtu\\.?be[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?youtu\\.?be[^\\s]+)', s)))\n",
      "text_attributes['tweet_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?twitter[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?twitter[^\\s]+)', s)))\n",
      "text_attributes['facebook_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?facebook[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?facebook[^\\s]+)', s)))\n",
      "\n",
      "text_attributes = pandas.DataFrame(text_attributes)\n",
      "df[text_attributes['tweet_link']>=1]\n",
      "19/45:\n",
      "text_attributes['link'] = df['body'].apply(lambda s: len(re.findall(r'(https?://[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(https?://[^\\s]+)', s)))\n",
      "text_attributes['reddit_link'] = df['body'].apply(lambda s: len(re.findall(r'(?:(?:http|https):\\/\\/)?(?:www.)?redd\\.?it[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(?:(?:http|https):\\/\\/)?(?:www.)?redd\\.?it[^\\s]+)', s)))\n",
      "text_attributes['yt_link'] = df['body'].apply(lambda s: len(re.findall(r'(?:(?:http|https):\\/\\/)?(?:www.)?youtu\\.?be[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(?:(?:http|https):\\/\\/)?(?:www.)?youtu\\.?be[^\\s]+)', s)))\n",
      "text_attributes['tweet_link'] = df['body'].apply(lambda s: len(re.findall(r'(?:(?:http|https):\\/\\/)?(?:www.)?twitter[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(?:(?:http|https):\\/\\/)?(?:www.)?twitter[^\\s]+)', s)))\n",
      "text_attributes['facebook_link'] = df['body'].apply(lambda s: len(re.findall(r'(?:(?:http|https):\\/\\/)?(?:www.)?facebook[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(?:(?:http|https):\\/\\/)?(?:www.)?facebook[^\\s]+)', s)))\n",
      "\n",
      "text_attributes = pandas.DataFrame(text_attributes)\n",
      "df[text_attributes['tweet_link']>=1]\n",
      "19/46:\n",
      "text_attributes['link'] = df['body'].apply(lambda s: len(re.findall(r'(https?://[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(https?://[^\\s]+)', s)))\n",
      "text_attributes['reddit_link'] = df['body'].apply(lambda s: len(re.findall(r'(?:(?:http|https):\\/\\/)?(?:www.)?redd\\.?it[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?redd\\.?it[^\\s]+)', s)))\n",
      "text_attributes['yt_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?youtu\\.?be[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?youtu\\.?be[^\\s]+)', s)))\n",
      "text_attributes['tweet_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?twitter[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?twitter[^\\s]+)', s)))\n",
      "text_attributes['facebook_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?facebook[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?facebook[^\\s]+)', s)))\n",
      "\n",
      "text_attributes = pandas.DataFrame(text_attributes)\n",
      "df[text_attributes['tweet_link']>=1]\n",
      "19/47:\n",
      "text_attributes['link'] = df['body'].apply(lambda s: len(re.findall(r'(https?://[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(https?://[^\\s]+)', s)))\n",
      "text_attributes['reddit_link'] = df['body'].apply(lambda s: len(re.findall(r'((?:http:\\/\\/)?(?:www\\.)?redd\\.?it[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?redd\\.?it[^\\s]+)', s)))\n",
      "text_attributes['yt_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?youtu\\.?be[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?youtu\\.?be[^\\s]+)', s)))\n",
      "text_attributes['tweet_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?twitter[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?twitter[^\\s]+)', s)))\n",
      "text_attributes['facebook_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?facebook[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?facebook[^\\s]+)', s)))\n",
      "\n",
      "text_attributes = pandas.DataFrame(text_attributes)\n",
      "df[text_attributes['tweet_link']>=1]\n",
      "19/48:\n",
      "text_attributes['link'] = df['body'].apply(lambda s: len(re.findall(r'(https?://[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(https?://[^\\s]+)', s)))\n",
      "text_attributes['reddit_link'] = df['body'].apply(lambda s: len(re.findall(r'((?:http:\\/\\/)?(?:www\\.)?redd\\.?it[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?redd\\.?it[^\\s]+)', s)))\n",
      "text_attributes['yt_link'] = df['body'].apply(lambda s: len(re.findall(r'((?:http:\\/\\/)?(?:www\\.)?youtu\\.?be[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?youtu\\.?be[^\\s]+)', s)))\n",
      "text_attributes['tweet_link'] = df['body'].apply(lambda s: len(re.findall(r'((?:http:\\/\\/)?(?:www\\.)?twitter[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?twitter[^\\s]+)', s)))\n",
      "text_attributes['facebook_link'] = df['body'].apply(lambda s: len(re.findall(r'((?:http:\\/\\/)?(?:www\\.)?facebook[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?facebook[^\\s]+)', s)))\n",
      "\n",
      "text_attributes = pandas.DataFrame(text_attributes)\n",
      "df[text_attributes['tweet_link']>=1]\n",
      "19/49:\n",
      "text_attributes['link'] = df['body'].apply(lambda s: len(re.findall(r'(https?://[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(https?://[^\\s]+)', s)))\n",
      "text_attributes['reddit_link'] = df['body'].apply(lambda s: len(re.findall(r'((?:http:\\/\\/)?(?:www\\.)?redd\\.?it[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'((?:http:\\/\\/)?(?:www\\.)?redd\\.?it[^\\s]+)', s)))\n",
      "text_attributes['yt_link'] = df['body'].apply(lambda s: len(re.findall(r'((?:http:\\/\\/)?(?:www\\.)?youtu\\.?be[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'((?:http:\\/\\/)?(?:www\\.)?youtu\\.?be[^\\s]+)', s)))\n",
      "text_attributes['tweet_link'] = df['body'].apply(lambda s: len(re.findall(r'((?:http:\\/\\/)?(?:www\\.)?twitter[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'((?:http:\\/\\/)?(?:www\\.)?twitter[^\\s]+)', s)))\n",
      "text_attributes['facebook_link'] = df['body'].apply(lambda s: len(re.findall(r'((?:http:\\/\\/)?(?:www\\.)?facebook[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'((?:http:\\/\\/)?(?:www\\.)?facebook[^\\s]+)', s)))\n",
      "\n",
      "text_attributes = pandas.DataFrame(text_attributes)\n",
      "df[text_attributes['tweet_link']>=1]\n",
      "19/50:\n",
      "text_attributes['link'] = df['body'].apply(lambda s: len(re.findall(r'(https?://[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(https?://[^\\s]+)', s)))\n",
      "text_attributes['reddit_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?redd\\.?it[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?redd\\.?it[^\\s]+)', s)))\n",
      "text_attributes['yt_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?youtu\\.?be[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?youtu\\.?be[^\\s]+)', s)))\n",
      "text_attributes['tweet_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?twitter[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?twitter[^\\s]+)', s)))\n",
      "text_attributes['facebook_link'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?facebook[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?facebook[^\\s]+)', s)))\n",
      "\n",
      "text_attributes = pandas.DataFrame(text_attributes)\n",
      "df[text_attributes['tweet_link']>=1]\n",
      "19/51:\n",
      "text_attributes['gif'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?giphy[^\\s]+)', s)))\n",
      "+ df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?giphy[^\\s]+)', s)))\n",
      "+ df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?gfycat[^\\s]+)', s)))\n",
      "+ df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?gfycat[^\\s]+)', s)))\n",
      "+ df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?gifyu[^\\s]+)', s)))\n",
      "+ df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?gifyu[^\\s]+)', s)))\n",
      "\n",
      "df[text_attributes['gif']>=1]\n",
      "19/52:\n",
      "text_attributes['gif'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?giphy[^\\s]+)', s)))\n",
      "+ df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?giphy[^\\s]+)', s)))\n",
      "+ df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?gfycat[^\\s]+)', s)))\n",
      "+ df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?gfycat[^\\s]+)', s)))\n",
      "+ df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?gifyu[^\\s]+)', s)))\n",
      "+ df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?gifyu[^\\s]+)', s)))\n",
      "+ df['url'].apply(lambda s: len(re.findall(r'(\\.gif)', s)))\n",
      "\n",
      "df[text_attributes['gif']>=1]\n",
      "19/53:\n",
      "text_attributes['gif'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?giphy[^\\s]+)', s)))\n",
      "+ df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?giphy[^\\s]+)', s)))\n",
      "+ df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?gfycat[^\\s]+)', s)))\n",
      "+ df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?gfycat[^\\s]+)', s)))\n",
      "+ df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?gifyu[^\\s]+)', s)))\n",
      "+ df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?gifyu[^\\s]+)', s)))\n",
      "+ df['url'].apply(lambda s: len(re.findall(r'(\\.gif)', s)))\n",
      "\n",
      "df[text_attributes['gif']>=1]\n",
      "19/54:\n",
      "text_attributes['gif'] = df['url'].apply(lambda s: len(re.findall(r'(\\.gif)', s)))\n",
      "\n",
      "df[text_attributes['gif']>=1]\n",
      "19/55:\n",
      "text_attributes['gif'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?giphy[^\\s]+)', s)))\n",
      "+ df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?giphy[^\\s]+)', s)))\n",
      "+ df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?gfycat[^\\s]+)', s)))\n",
      "+ df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?gfycat[^\\s]+)', s)))\n",
      "+ df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?gifyu[^\\s]+)', s)))\n",
      "+ df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?gifyu[^\\s]+)', s)))\n",
      "+ df['url'].apply(lambda s: len(re.findall(r'(\\.gif)', s)))\n",
      "\n",
      "df[text_attributes['gif']>=1]\n",
      "19/56:\n",
      "text_attributes['gif'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?giphy[^\\s]+)', s))) + \n",
      "df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?giphy[^\\s]+)', s)))\n",
      "+ df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?gfycat[^\\s]+)', s)))\n",
      "+ df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?gfycat[^\\s]+)', s)))\n",
      "+ df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?gifyu[^\\s]+)', s)))\n",
      "+ df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?gifyu[^\\s]+)', s)))\n",
      "+ df['url'].apply(lambda s: len(re.findall(r'(\\.gif)', s)))\n",
      "\n",
      "df[text_attributes['gif']>=1]\n",
      "19/57:\n",
      "text_attributes['gif'] = df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?giphy[^\\s]+)', s))) + df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?giphy[^\\s]+)', s)))+ df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?gfycat[^\\s]+)', s)))+ df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?gfycat[^\\s]+)', s)))+ df['body'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?gifyu[^\\s]+)', s)))+ df['url'].apply(lambda s: len(re.findall(r'(https?:\\/\\/w?w?w?\\.?gifyu[^\\s]+)', s)))+ df['url'].apply(lambda s: len(re.findall(r'(\\.gif)', s)))\n",
      "\n",
      "df[text_attributes['gif']>=1]\n",
      "19/58:\n",
      "def extract_from_url_and_body(regex):\n",
      "    return df['body'].apply(lambda s: len(re.findall(regex, s))) + df['url'].apply(lambda s: len(re.findall(regex, s)))\n",
      "    \n",
      "\n",
      "text_attributes['link'] = extract_from_url_and_body(r'(https?://[^\\s]+)')\n",
      "text_attributes['reddit_link'] = extract_from_url_and_body(r'(https?:\\/\\/w?w?w?\\.?redd\\.?it[^\\s]+)')\n",
      "text_attributes['yt_link'] = extract_from_url_and_body(r'(https?:\\/\\/w?w?w?\\.?youtu\\.?be[^\\s]+)')\n",
      "text_attributes['tweet_link'] = extract_from_url_and_body(r'(https?:\\/\\/w?w?w?\\.?twitter[^\\s]+)')\n",
      "text_attributes['facebook_link'] = extract_from_url_and_body(r'(https?:\\/\\/w?w?w?\\.?facebook[^\\s]+)')\n",
      "\n",
      "text_attributes = pandas.DataFrame(text_attributes)\n",
      "df[text_attributes['tweet_link']>=1]\n",
      "19/59:\n",
      "def extract_from_url_and_body(regex):\n",
      "    return df['body'].apply(lambda s: len(re.findall(regex, s))) + df['url'].apply(lambda s: len(re.findall(regex, s)))\n",
      "    \n",
      "\n",
      "text_attributes['link'] = extract_from_url_and_body(r'(https?://[^\\s]+)')\n",
      "text_attributes['reddit_link'] = extract_from_url_and_body(r'(https?:\\/\\/w?w?w?\\.?redd\\.?it[^\\s]+)')\n",
      "text_attributes['yt_link'] = extract_from_url_and_body(r'(https?:\\/\\/w?w?w?\\.?youtu\\.?be[^\\s]+)')\n",
      "text_attributes['tweet_link'] = extract_from_url_and_body(r'(https?:\\/\\/w?w?w?\\.?twitter[^\\s]+)')\n",
      "text_attributes['facebook_link'] = extract_from_url_and_body(r'(https?:\\/\\/w?w?w?\\.?facebook[^\\s]+)')\n",
      "\n",
      "text_attributes = pandas.DataFrame(text_attributes)\n",
      "df[text_attributes['yt_link']>=1]\n",
      "19/60:\n",
      "def extract_from_url_and_body(regex):\n",
      "    return df['body'].apply(lambda s: len(re.findall(regex, s))) + df['url'].apply(lambda s: len(re.findall(regex, s)))\n",
      "    \n",
      "\n",
      "text_attributes['link'] = extract_from_url_and_body(r'(https?://[^\\s]+)')\n",
      "text_attributes['reddit_link'] = extract_from_url_and_body(r'(https?:\\/\\/w?w?w?\\.?redd\\.?it[^\\s]+)')\n",
      "text_attributes['yt_link'] = extract_from_url_and_body(r'(https?:\\/\\/w?w?w?\\.?youtu\\.?be[^\\s]+)')\n",
      "text_attributes['tweet_link'] = extract_from_url_and_body(r'(https?:\\/\\/w?w?w?\\.?twitter[^\\s]+)')\n",
      "text_attributes['facebook_link'] = extract_from_url_and_body(r'(https?:\\/\\/w?w?w?\\.?facebook[^\\s]+)')\n",
      "\n",
      "text_attributes = pandas.DataFrame(text_attributes)\n",
      "df[text_attributes['tweet_link']>=1]\n",
      "19/61:\n",
      "text_attributes['gif'] = extract_from_url_and_body(r'(https?:\\/\\/w?w?w?\\.?giphy[^\\s]+)')\n",
      "text_attributes['gif'] += extract_from_url_and_body(r'(https?:\\/\\/w?w?w?\\.?gfycat[^\\s]+)')\n",
      "text_attributes['gif'] += extract_from_url_and_body(r'(https?:\\/\\/w?w?w?\\.?gifyu[^\\s]+)') \n",
      "text_attributes['gif'] += df['url'].apply(lambda s: len(re.findall(r'(\\.gif)', s)))\n",
      "\n",
      "df[text_attributes['gif']>=1]\n",
      "20/86:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "color_attribute = {}\n",
      "for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "        print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "        image = cv2.imread(\"../data/images/\" + file)\n",
      "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "        reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "        cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "        print(clusters)\n",
      "20/87:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "color_attribute = {}\n",
      "for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "        print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "        image = cv2.imread(\"../data/images/\" + file)\n",
      "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "        reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "        cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "        print(cluster)\n",
      "20/88: print(result)\n",
      "20/89:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "color_attribute = {}\n",
      "for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "        print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "        image = cv2.imread(\"../data/images/\" + file)\n",
      "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "        reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "        cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "        print(cluster.cluster_centers_)\n",
      "20/90:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "color_attribute = {}\n",
      "for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "        print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "        image = cv2.imread(\"../data/images/\" + file)\n",
      "        image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
      "        print(cv2.mean(image))\n",
      "20/91:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "color_attribute = {}\n",
      "for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "        print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "        image = cv2.imread(\"../data/images/\" + file)\n",
      "\n",
      "        print(cv2.mean(image))\n",
      "22/1:\n",
      "from tensorflow.keras.applications import EfficientNetB0\n",
      "from tensorflow.image import resize\n",
      "model = EfficientNetB0(weights='imagenet')\n",
      "\n",
      "def center_crop(img, new_width=None, new_height=None):        \n",
      "\n",
      "    width = img.shape[1]\n",
      "    height = img.shape[0]\n",
      "\n",
      "    if new_width is None:\n",
      "        new_width = min(width, height)\n",
      "\n",
      "    if new_height is None:\n",
      "        new_height = min(width, height)\n",
      "\n",
      "    left = int(np.ceil((width - new_width) / 2))\n",
      "    right = width - int(np.floor((width - new_width) / 2))\n",
      "\n",
      "    top = int(np.ceil((height - new_height) / 2))\n",
      "    bottom = height - int(np.floor((height - new_height) / 2))\n",
      "\n",
      "    if len(img.shape) == 2:\n",
      "        center_cropped_img = img[top:bottom, left:right]\n",
      "    else:\n",
      "        center_cropped_img = img[top:bottom, left:right, ...]\n",
      "\n",
      "    return center_cropped_img\n",
      "\n",
      "class_dict = {0: 'tench, Tinca tinca',\n",
      " 1: 'goldfish, Carassius auratus',\n",
      " 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n",
      " 3: 'tiger shark, Galeocerdo cuvieri',\n",
      " 4: 'hammerhead, hammerhead shark',\n",
      " 5: 'electric ray, crampfish, numbfish, torpedo',\n",
      " 6: 'stingray',\n",
      " 7: 'cock',\n",
      " 8: 'hen',\n",
      " 9: 'ostrich, Struthio camelus',\n",
      " 10: 'brambling, Fringilla montifringilla',\n",
      " 11: 'goldfinch, Carduelis carduelis',\n",
      " 12: 'house finch, linnet, Carpodacus mexicanus',\n",
      " 13: 'junco, snowbird',\n",
      " 14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea',\n",
      " 15: 'robin, American robin, Turdus migratorius',\n",
      " 16: 'bulbul',\n",
      " 17: 'jay',\n",
      " 18: 'magpie',\n",
      " 19: 'chickadee',\n",
      " 20: 'water ouzel, dipper',\n",
      " 21: 'kite',\n",
      " 22: 'bald eagle, American eagle, Haliaeetus leucocephalus',\n",
      " 23: 'vulture',\n",
      " 24: 'great grey owl, great gray owl, Strix nebulosa',\n",
      " 25: 'European fire salamander, Salamandra salamandra',\n",
      " 26: 'common newt, Triturus vulgaris',\n",
      " 27: 'eft',\n",
      " 28: 'spotted salamander, Ambystoma maculatum',\n",
      " 29: 'axolotl, mud puppy, Ambystoma mexicanum',\n",
      " 30: 'bullfrog, Rana catesbeiana',\n",
      " 31: 'tree frog, tree-frog',\n",
      " 32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui',\n",
      " 33: 'loggerhead, loggerhead turtle, Caretta caretta',\n",
      " 34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea',\n",
      " 35: 'mud turtle',\n",
      " 36: 'terrapin',\n",
      " 37: 'box turtle, box tortoise',\n",
      " 38: 'banded gecko',\n",
      " 39: 'common iguana, iguana, Iguana iguana',\n",
      " 40: 'American chameleon, anole, Anolis carolinensis',\n",
      " 41: 'whiptail, whiptail lizard',\n",
      " 42: 'agama',\n",
      " 43: 'frilled lizard, Chlamydosaurus kingi',\n",
      " 44: 'alligator lizard',\n",
      " 45: 'Gila monster, Heloderma suspectum',\n",
      " 46: 'green lizard, Lacerta viridis',\n",
      " 47: 'African chameleon, Chamaeleo chamaeleon',\n",
      " 48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis',\n",
      " 49: 'African crocodile, Nile crocodile, Crocodylus niloticus',\n",
      " 50: 'American alligator, Alligator mississipiensis',\n",
      " 51: 'triceratops',\n",
      " 52: 'thunder snake, worm snake, Carphophis amoenus',\n",
      " 53: 'ringneck snake, ring-necked snake, ring snake',\n",
      " 54: 'hognose snake, puff adder, sand viper',\n",
      " 55: 'green snake, grass snake',\n",
      " 56: 'king snake, kingsnake',\n",
      " 57: 'garter snake, grass snake',\n",
      " 58: 'water snake',\n",
      " 59: 'vine snake',\n",
      " 60: 'night snake, Hypsiglena torquata',\n",
      " 61: 'boa constrictor, Constrictor constrictor',\n",
      " 62: 'rock python, rock snake, Python sebae',\n",
      " 63: 'Indian cobra, Naja naja',\n",
      " 64: 'green mamba',\n",
      " 65: 'sea snake',\n",
      " 66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus',\n",
      " 67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus',\n",
      " 68: 'sidewinder, horned rattlesnake, Crotalus cerastes',\n",
      " 69: 'trilobite',\n",
      " 70: 'harvestman, daddy longlegs, Phalangium opilio',\n",
      " 71: 'scorpion',\n",
      " 72: 'black and gold garden spider, Argiope aurantia',\n",
      " 73: 'barn spider, Araneus cavaticus',\n",
      " 74: 'garden spider, Aranea diademata',\n",
      " 75: 'black widow, Latrodectus mactans',\n",
      " 76: 'tarantula',\n",
      " 77: 'wolf spider, hunting spider',\n",
      " 78: 'tick',\n",
      " 79: 'centipede',\n",
      " 80: 'black grouse',\n",
      " 81: 'ptarmigan',\n",
      " 82: 'ruffed grouse, partridge, Bonasa umbellus',\n",
      " 83: 'prairie chicken, prairie grouse, prairie fowl',\n",
      " 84: 'peacock',\n",
      " 85: 'quail',\n",
      " 86: 'partridge',\n",
      " 87: 'African grey, African gray, Psittacus erithacus',\n",
      " 88: 'macaw',\n",
      " 89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n",
      " 90: 'lorikeet',\n",
      " 91: 'coucal',\n",
      " 92: 'bee eater',\n",
      " 93: 'hornbill',\n",
      " 94: 'hummingbird',\n",
      " 95: 'jacamar',\n",
      " 96: 'toucan',\n",
      " 97: 'drake',\n",
      " 98: 'red-breasted merganser, Mergus serrator',\n",
      " 99: 'goose',\n",
      " 100: 'black swan, Cygnus atratus',\n",
      " 101: 'tusker',\n",
      " 102: 'echidna, spiny anteater, anteater',\n",
      " 103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n",
      " 104: 'wallaby, brush kangaroo',\n",
      " 105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n",
      " 106: 'wombat',\n",
      " 107: 'jellyfish',\n",
      " 108: 'sea anemone, anemone',\n",
      " 109: 'brain coral',\n",
      " 110: 'flatworm, platyhelminth',\n",
      " 111: 'nematode, nematode worm, roundworm',\n",
      " 112: 'conch',\n",
      " 113: 'snail',\n",
      " 114: 'slug',\n",
      " 115: 'sea slug, nudibranch',\n",
      " 116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore',\n",
      " 117: 'chambered nautilus, pearly nautilus, nautilus',\n",
      " 118: 'Dungeness crab, Cancer magister',\n",
      " 119: 'rock crab, Cancer irroratus',\n",
      " 120: 'fiddler crab',\n",
      " 121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica',\n",
      " 122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus',\n",
      " 123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish',\n",
      " 124: 'crayfish, crawfish, crawdad, crawdaddy',\n",
      " 125: 'hermit crab',\n",
      " 126: 'isopod',\n",
      " 127: 'white stork, Ciconia ciconia',\n",
      " 128: 'black stork, Ciconia nigra',\n",
      " 129: 'spoonbill',\n",
      " 130: 'flamingo',\n",
      " 131: 'little blue heron, Egretta caerulea',\n",
      " 132: 'American egret, great white heron, Egretta albus',\n",
      " 133: 'bittern',\n",
      " 134: 'crane',\n",
      " 135: 'limpkin, Aramus pictus',\n",
      " 136: 'European gallinule, Porphyrio porphyrio',\n",
      " 137: 'American coot, marsh hen, mud hen, water hen, Fulica americana',\n",
      " 138: 'bustard',\n",
      " 139: 'ruddy turnstone, Arenaria interpres',\n",
      " 140: 'red-backed sandpiper, dunlin, Erolia alpina',\n",
      " 141: 'redshank, Tringa totanus',\n",
      " 142: 'dowitcher',\n",
      " 143: 'oystercatcher, oyster catcher',\n",
      " 144: 'pelican',\n",
      " 145: 'king penguin, Aptenodytes patagonica',\n",
      " 146: 'albatross, mollymawk',\n",
      " 147: 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus',\n",
      " 148: 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca',\n",
      " 149: 'dugong, Dugong dugon',\n",
      " 150: 'sea lion',\n",
      " 151: 'Chihuahua',\n",
      " 152: 'Japanese spaniel',\n",
      " 153: 'Maltese dog, Maltese terrier, Maltese',\n",
      " 154: 'Pekinese, Pekingese, Peke',\n",
      " 155: 'Shih-Tzu',\n",
      " 156: 'Blenheim spaniel',\n",
      " 157: 'papillon',\n",
      " 158: 'toy terrier',\n",
      " 159: 'Rhodesian ridgeback',\n",
      " 160: 'Afghan hound, Afghan',\n",
      " 161: 'basset, basset hound',\n",
      " 162: 'beagle',\n",
      " 163: 'bloodhound, sleuthhound',\n",
      " 164: 'bluetick',\n",
      " 165: 'black-and-tan coonhound',\n",
      " 166: 'Walker hound, Walker foxhound',\n",
      " 167: 'English foxhound',\n",
      " 168: 'redbone',\n",
      " 169: 'borzoi, Russian wolfhound',\n",
      " 170: 'Irish wolfhound',\n",
      " 171: 'Italian greyhound',\n",
      " 172: 'whippet',\n",
      " 173: 'Ibizan hound, Ibizan Podenco',\n",
      " 174: 'Norwegian elkhound, elkhound',\n",
      " 175: 'otterhound, otter hound',\n",
      " 176: 'Saluki, gazelle hound',\n",
      " 177: 'Scottish deerhound, deerhound',\n",
      " 178: 'Weimaraner',\n",
      " 179: 'Staffordshire bullterrier, Staffordshire bull terrier',\n",
      " 180: 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier',\n",
      " 181: 'Bedlington terrier',\n",
      " 182: 'Border terrier',\n",
      " 183: 'Kerry blue terrier',\n",
      " 184: 'Irish terrier',\n",
      " 185: 'Norfolk terrier',\n",
      " 186: 'Norwich terrier',\n",
      " 187: 'Yorkshire terrier',\n",
      " 188: 'wire-haired fox terrier',\n",
      " 189: 'Lakeland terrier',\n",
      " 190: 'Sealyham terrier, Sealyham',\n",
      " 191: 'Airedale, Airedale terrier',\n",
      " 192: 'cairn, cairn terrier',\n",
      " 193: 'Australian terrier',\n",
      " 194: 'Dandie Dinmont, Dandie Dinmont terrier',\n",
      " 195: 'Boston bull, Boston terrier',\n",
      " 196: 'miniature schnauzer',\n",
      " 197: 'giant schnauzer',\n",
      " 198: 'standard schnauzer',\n",
      " 199: 'Scotch terrier, Scottish terrier, Scottie',\n",
      " 200: 'Tibetan terrier, chrysanthemum dog',\n",
      " 201: 'silky terrier, Sydney silky',\n",
      " 202: 'soft-coated wheaten terrier',\n",
      " 203: 'West Highland white terrier',\n",
      " 204: 'Lhasa, Lhasa apso',\n",
      " 205: 'flat-coated retriever',\n",
      " 206: 'curly-coated retriever',\n",
      " 207: 'golden retriever',\n",
      " 208: 'Labrador retriever',\n",
      " 209: 'Chesapeake Bay retriever',\n",
      " 210: 'German short-haired pointer',\n",
      " 211: 'vizsla, Hungarian pointer',\n",
      " 212: 'English setter',\n",
      " 213: 'Irish setter, red setter',\n",
      " 214: 'Gordon setter',\n",
      " 215: 'Brittany spaniel',\n",
      " 216: 'clumber, clumber spaniel',\n",
      " 217: 'English springer, English springer spaniel',\n",
      " 218: 'Welsh springer spaniel',\n",
      " 219: 'cocker spaniel, English cocker spaniel, cocker',\n",
      " 220: 'Sussex spaniel',\n",
      " 221: 'Irish water spaniel',\n",
      " 222: 'kuvasz',\n",
      " 223: 'schipperke',\n",
      " 224: 'groenendael',\n",
      " 225: 'malinois',\n",
      " 226: 'briard',\n",
      " 227: 'kelpie',\n",
      " 228: 'komondor',\n",
      " 229: 'Old English sheepdog, bobtail',\n",
      " 230: 'Shetland sheepdog, Shetland sheep dog, Shetland',\n",
      " 231: 'collie',\n",
      " 232: 'Border collie',\n",
      " 233: 'Bouvier des Flandres, Bouviers des Flandres',\n",
      " 234: 'Rottweiler',\n",
      " 235: 'German shepherd, German shepherd dog, German police dog, alsatian',\n",
      " 236: 'Doberman, Doberman pinscher',\n",
      " 237: 'miniature pinscher',\n",
      " 238: 'Greater Swiss Mountain dog',\n",
      " 239: 'Bernese mountain dog',\n",
      " 240: 'Appenzeller',\n",
      " 241: 'EntleBucher',\n",
      " 242: 'boxer',\n",
      " 243: 'bull mastiff',\n",
      " 244: 'Tibetan mastiff',\n",
      " 245: 'French bulldog',\n",
      " 246: 'Great Dane',\n",
      " 247: 'Saint Bernard, St Bernard',\n",
      " 248: 'Eskimo dog, husky',\n",
      " 249: 'malamute, malemute, Alaskan malamute',\n",
      " 250: 'Siberian husky',\n",
      " 251: 'dalmatian, coach dog, carriage dog',\n",
      " 252: 'affenpinscher, monkey pinscher, monkey dog',\n",
      " 253: 'basenji',\n",
      " 254: 'pug, pug-dog',\n",
      " 255: 'Leonberg',\n",
      " 256: 'Newfoundland, Newfoundland dog',\n",
      " 257: 'Great Pyrenees',\n",
      " 258: 'Samoyed, Samoyede',\n",
      " 259: 'Pomeranian',\n",
      " 260: 'chow, chow chow',\n",
      " 261: 'keeshond',\n",
      " 262: 'Brabancon griffon',\n",
      " 263: 'Pembroke, Pembroke Welsh corgi',\n",
      " 264: 'Cardigan, Cardigan Welsh corgi',\n",
      " 265: 'toy poodle',\n",
      " 266: 'miniature poodle',\n",
      " 267: 'standard poodle',\n",
      " 268: 'Mexican hairless',\n",
      " 269: 'timber wolf, grey wolf, gray wolf, Canis lupus',\n",
      " 270: 'white wolf, Arctic wolf, Canis lupus tundrarum',\n",
      " 271: 'red wolf, maned wolf, Canis rufus, Canis niger',\n",
      " 272: 'coyote, prairie wolf, brush wolf, Canis latrans',\n",
      " 273: 'dingo, warrigal, warragal, Canis dingo',\n",
      " 274: 'dhole, Cuon alpinus',\n",
      " 275: 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus',\n",
      " 276: 'hyena, hyaena',\n",
      " 277: 'red fox, Vulpes vulpes',\n",
      " 278: 'kit fox, Vulpes macrotis',\n",
      " 279: 'Arctic fox, white fox, Alopex lagopus',\n",
      " 280: 'grey fox, gray fox, Urocyon cinereoargenteus',\n",
      " 281: 'tabby, tabby cat',\n",
      " 282: 'tiger cat',\n",
      " 283: 'Persian cat',\n",
      " 284: 'Siamese cat, Siamese',\n",
      " 285: 'Egyptian cat',\n",
      " 286: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n",
      " 287: 'lynx, catamount',\n",
      " 288: 'leopard, Panthera pardus',\n",
      " 289: 'snow leopard, ounce, Panthera uncia',\n",
      " 290: 'jaguar, panther, Panthera onca, Felis onca',\n",
      " 291: 'lion, king of beasts, Panthera leo',\n",
      " 292: 'tiger, Panthera tigris',\n",
      " 293: 'cheetah, chetah, Acinonyx jubatus',\n",
      " 294: 'brown bear, bruin, Ursus arctos',\n",
      " 295: 'American black bear, black bear, Ursus americanus, Euarctos americanus',\n",
      " 296: 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus',\n",
      " 297: 'sloth bear, Melursus ursinus, Ursus ursinus',\n",
      " 298: 'mongoose',\n",
      " 299: 'meerkat, mierkat',\n",
      " 300: 'tiger beetle',\n",
      " 301: 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle',\n",
      " 302: 'ground beetle, carabid beetle',\n",
      " 303: 'long-horned beetle, longicorn, longicorn beetle',\n",
      " 304: 'leaf beetle, chrysomelid',\n",
      " 305: 'dung beetle',\n",
      " 306: 'rhinoceros beetle',\n",
      " 307: 'weevil',\n",
      " 308: 'fly',\n",
      " 309: 'bee',\n",
      " 310: 'ant, emmet, pismire',\n",
      " 311: 'grasshopper, hopper',\n",
      " 312: 'cricket',\n",
      " 313: 'walking stick, walkingstick, stick insect',\n",
      " 314: 'cockroach, roach',\n",
      " 315: 'mantis, mantid',\n",
      " 316: 'cicada, cicala',\n",
      " 317: 'leafhopper',\n",
      " 318: 'lacewing, lacewing fly',\n",
      " 319: \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\",\n",
      " 320: 'damselfly',\n",
      " 321: 'admiral',\n",
      " 322: 'ringlet, ringlet butterfly',\n",
      " 323: 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus',\n",
      " 324: 'cabbage butterfly',\n",
      " 325: 'sulphur butterfly, sulfur butterfly',\n",
      " 326: 'lycaenid, lycaenid butterfly',\n",
      " 327: 'starfish, sea star',\n",
      " 328: 'sea urchin',\n",
      " 329: 'sea cucumber, holothurian',\n",
      " 330: 'wood rabbit, cottontail, cottontail rabbit',\n",
      " 331: 'hare',\n",
      " 332: 'Angora, Angora rabbit',\n",
      " 333: 'hamster',\n",
      " 334: 'porcupine, hedgehog',\n",
      " 335: 'fox squirrel, eastern fox squirrel, Sciurus niger',\n",
      " 336: 'marmot',\n",
      " 337: 'beaver',\n",
      " 338: 'guinea pig, Cavia cobaya',\n",
      " 339: 'sorrel',\n",
      " 340: 'zebra',\n",
      " 341: 'hog, pig, grunter, squealer, Sus scrofa',\n",
      " 342: 'wild boar, boar, Sus scrofa',\n",
      " 343: 'warthog',\n",
      " 344: 'hippopotamus, hippo, river horse, Hippopotamus amphibius',\n",
      " 345: 'ox',\n",
      " 346: 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis',\n",
      " 347: 'bison',\n",
      " 348: 'ram, tup',\n",
      " 349: 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis',\n",
      " 350: 'ibex, Capra ibex',\n",
      " 351: 'hartebeest',\n",
      " 352: 'impala, Aepyceros melampus',\n",
      " 353: 'gazelle',\n",
      " 354: 'Arabian camel, dromedary, Camelus dromedarius',\n",
      " 355: 'llama',\n",
      " 356: 'weasel',\n",
      " 357: 'mink',\n",
      " 358: 'polecat, fitch, foulmart, foumart, Mustela putorius',\n",
      " 359: 'black-footed ferret, ferret, Mustela nigripes',\n",
      " 360: 'otter',\n",
      " 361: 'skunk, polecat, wood pussy',\n",
      " 362: 'badger',\n",
      " 363: 'armadillo',\n",
      " 364: 'three-toed sloth, ai, Bradypus tridactylus',\n",
      " 365: 'orangutan, orang, orangutang, Pongo pygmaeus',\n",
      " 366: 'gorilla, Gorilla gorilla',\n",
      " 367: 'chimpanzee, chimp, Pan troglodytes',\n",
      " 368: 'gibbon, Hylobates lar',\n",
      " 369: 'siamang, Hylobates syndactylus, Symphalangus syndactylus',\n",
      " 370: 'guenon, guenon monkey',\n",
      " 371: 'patas, hussar monkey, Erythrocebus patas',\n",
      " 372: 'baboon',\n",
      " 373: 'macaque',\n",
      " 374: 'langur',\n",
      " 375: 'colobus, colobus monkey',\n",
      " 376: 'proboscis monkey, Nasalis larvatus',\n",
      " 377: 'marmoset',\n",
      " 378: 'capuchin, ringtail, Cebus capucinus',\n",
      " 379: 'howler monkey, howler',\n",
      " 380: 'titi, titi monkey',\n",
      " 381: 'spider monkey, Ateles geoffroyi',\n",
      " 382: 'squirrel monkey, Saimiri sciureus',\n",
      " 383: 'Madagascar cat, ring-tailed lemur, Lemur catta',\n",
      " 384: 'indri, indris, Indri indri, Indri brevicaudatus',\n",
      " 385: 'Indian elephant, Elephas maximus',\n",
      " 386: 'African elephant, Loxodonta africana',\n",
      " 387: 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens',\n",
      " 388: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca',\n",
      " 389: 'barracouta, snoek',\n",
      " 390: 'eel',\n",
      " 391: 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch',\n",
      " 392: 'rock beauty, Holocanthus tricolor',\n",
      " 393: 'anemone fish',\n",
      " 394: 'sturgeon',\n",
      " 395: 'gar, garfish, garpike, billfish, Lepisosteus osseus',\n",
      " 396: 'lionfish',\n",
      " 397: 'puffer, pufferfish, blowfish, globefish',\n",
      " 398: 'abacus',\n",
      " 399: 'abaya',\n",
      " 400: \"academic gown, academic robe, judge's robe\",\n",
      " 401: 'accordion, piano accordion, squeeze box',\n",
      " 402: 'acoustic guitar',\n",
      " 403: 'aircraft carrier, carrier, flattop, attack aircraft carrier',\n",
      " 404: 'airliner',\n",
      " 405: 'airship, dirigible',\n",
      " 406: 'altar',\n",
      " 407: 'ambulance',\n",
      " 408: 'amphibian, amphibious vehicle',\n",
      " 409: 'analog clock',\n",
      " 410: 'apiary, bee house',\n",
      " 411: 'apron',\n",
      " 412: 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin',\n",
      " 413: 'assault rifle, assault gun',\n",
      " 414: 'backpack, back pack, knapsack, packsack, rucksack, haversack',\n",
      " 415: 'bakery, bakeshop, bakehouse',\n",
      " 416: 'balance beam, beam',\n",
      " 417: 'balloon',\n",
      " 418: 'ballpoint, ballpoint pen, ballpen, Biro',\n",
      " 419: 'Band Aid',\n",
      " 420: 'banjo',\n",
      " 421: 'bannister, banister, balustrade, balusters, handrail',\n",
      " 422: 'barbell',\n",
      " 423: 'barber chair',\n",
      " 424: 'barbershop',\n",
      " 425: 'barn',\n",
      " 426: 'barometer',\n",
      " 427: 'barrel, cask',\n",
      " 428: 'barrow, garden cart, lawn cart, wheelbarrow',\n",
      " 429: 'baseball',\n",
      " 430: 'basketball',\n",
      " 431: 'bassinet',\n",
      " 432: 'bassoon',\n",
      " 433: 'bathing cap, swimming cap',\n",
      " 434: 'bath towel',\n",
      " 435: 'bathtub, bathing tub, bath, tub',\n",
      " 436: 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon',\n",
      " 437: 'beacon, lighthouse, beacon light, pharos',\n",
      " 438: 'beaker',\n",
      " 439: 'bearskin, busby, shako',\n",
      " 440: 'beer bottle',\n",
      " 441: 'beer glass',\n",
      " 442: 'bell cote, bell cot',\n",
      " 443: 'bib',\n",
      " 444: 'bicycle-built-for-two, tandem bicycle, tandem',\n",
      " 445: 'bikini, two-piece',\n",
      " 446: 'binder, ring-binder',\n",
      " 447: 'binoculars, field glasses, opera glasses',\n",
      " 448: 'birdhouse',\n",
      " 449: 'boathouse',\n",
      " 450: 'bobsled, bobsleigh, bob',\n",
      " 451: 'bolo tie, bolo, bola tie, bola',\n",
      " 452: 'bonnet, poke bonnet',\n",
      " 453: 'bookcase',\n",
      " 454: 'bookshop, bookstore, bookstall',\n",
      " 455: 'bottlecap',\n",
      " 456: 'bow',\n",
      " 457: 'bow tie, bow-tie, bowtie',\n",
      " 458: 'brass, memorial tablet, plaque',\n",
      " 459: 'brassiere, bra, bandeau',\n",
      " 460: 'breakwater, groin, groyne, mole, bulwark, seawall, jetty',\n",
      " 461: 'breastplate, aegis, egis',\n",
      " 462: 'broom',\n",
      " 463: 'bucket, pail',\n",
      " 464: 'buckle',\n",
      " 465: 'bulletproof vest',\n",
      " 466: 'bullet train, bullet',\n",
      " 467: 'butcher shop, meat market',\n",
      " 468: 'cab, hack, taxi, taxicab',\n",
      " 469: 'caldron, cauldron',\n",
      " 470: 'candle, taper, wax light',\n",
      " 471: 'cannon',\n",
      " 472: 'canoe',\n",
      " 473: 'can opener, tin opener',\n",
      " 474: 'cardigan',\n",
      " 475: 'car mirror',\n",
      " 476: 'carousel, carrousel, merry-go-round, roundabout, whirligig',\n",
      " 477: \"carpenter's kit, tool kit\",\n",
      " 478: 'carton',\n",
      " 479: 'car wheel',\n",
      " 480: 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM',\n",
      " 481: 'cassette',\n",
      " 482: 'cassette player',\n",
      " 483: 'castle',\n",
      " 484: 'catamaran',\n",
      " 485: 'CD player',\n",
      " 486: 'cello, violoncello',\n",
      " 487: 'cellular telephone, cellular phone, cellphone, cell, mobile phone',\n",
      " 488: 'chain',\n",
      " 489: 'chainlink fence',\n",
      " 490: 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour',\n",
      " 491: 'chain saw, chainsaw',\n",
      " 492: 'chest',\n",
      " 493: 'chiffonier, commode',\n",
      " 494: 'chime, bell, gong',\n",
      " 495: 'china cabinet, china closet',\n",
      " 496: 'Christmas stocking',\n",
      " 497: 'church, church building',\n",
      " 498: 'cinema, movie theater, movie theatre, movie house, picture palace',\n",
      " 499: 'cleaver, meat cleaver, chopper',\n",
      " 500: 'cliff dwelling',\n",
      " 501: 'cloak',\n",
      " 502: 'clog, geta, patten, sabot',\n",
      " 503: 'cocktail shaker',\n",
      " 504: 'coffee mug',\n",
      " 505: 'coffeepot',\n",
      " 506: 'coil, spiral, volute, whorl, helix',\n",
      " 507: 'combination lock',\n",
      " 508: 'computer keyboard, keypad',\n",
      " 509: 'confectionery, confectionary, candy store',\n",
      " 510: 'container ship, containership, container vessel',\n",
      " 511: 'convertible',\n",
      " 512: 'corkscrew, bottle screw',\n",
      " 513: 'cornet, horn, trumpet, trump',\n",
      " 514: 'cowboy boot',\n",
      " 515: 'cowboy hat, ten-gallon hat',\n",
      " 516: 'cradle',\n",
      " 517: 'crane',\n",
      " 518: 'crash helmet',\n",
      " 519: 'crate',\n",
      " 520: 'crib, cot',\n",
      " 521: 'Crock Pot',\n",
      " 522: 'croquet ball',\n",
      " 523: 'crutch',\n",
      " 524: 'cuirass',\n",
      " 525: 'dam, dike, dyke',\n",
      " 526: 'desk',\n",
      " 527: 'desktop computer',\n",
      " 528: 'dial telephone, dial phone',\n",
      " 529: 'diaper, nappy, napkin',\n",
      " 530: 'digital clock',\n",
      " 531: 'digital watch',\n",
      " 532: 'dining table, board',\n",
      " 533: 'dishrag, dishcloth',\n",
      " 534: 'dishwasher, dish washer, dishwashing machine',\n",
      " 535: 'disk brake, disc brake',\n",
      " 536: 'dock, dockage, docking facility',\n",
      " 537: 'dogsled, dog sled, dog sleigh',\n",
      " 538: 'dome',\n",
      " 539: 'doormat, welcome mat',\n",
      " 540: 'drilling platform, offshore rig',\n",
      " 541: 'drum, membranophone, tympan',\n",
      " 542: 'drumstick',\n",
      " 543: 'dumbbell',\n",
      " 544: 'Dutch oven',\n",
      " 545: 'electric fan, blower',\n",
      " 546: 'electric guitar',\n",
      " 547: 'electric locomotive',\n",
      " 548: 'entertainment center',\n",
      " 549: 'envelope',\n",
      " 550: 'espresso maker',\n",
      " 551: 'face powder',\n",
      " 552: 'feather boa, boa',\n",
      " 553: 'file, file cabinet, filing cabinet',\n",
      " 554: 'fireboat',\n",
      " 555: 'fire engine, fire truck',\n",
      " 556: 'fire screen, fireguard',\n",
      " 557: 'flagpole, flagstaff',\n",
      " 558: 'flute, transverse flute',\n",
      " 559: 'folding chair',\n",
      " 560: 'football helmet',\n",
      " 561: 'forklift',\n",
      " 562: 'fountain',\n",
      " 563: 'fountain pen',\n",
      " 564: 'four-poster',\n",
      " 565: 'freight car',\n",
      " 566: 'French horn, horn',\n",
      " 567: 'frying pan, frypan, skillet',\n",
      " 568: 'fur coat',\n",
      " 569: 'garbage truck, dustcart',\n",
      " 570: 'gasmask, respirator, gas helmet',\n",
      " 571: 'gas pump, gasoline pump, petrol pump, island dispenser',\n",
      " 572: 'goblet',\n",
      " 573: 'go-kart',\n",
      " 574: 'golf ball',\n",
      " 575: 'golfcart, golf cart',\n",
      " 576: 'gondola',\n",
      " 577: 'gong, tam-tam',\n",
      " 578: 'gown',\n",
      " 579: 'grand piano, grand',\n",
      " 580: 'greenhouse, nursery, glasshouse',\n",
      " 581: 'grille, radiator grille',\n",
      " 582: 'grocery store, grocery, food market, market',\n",
      " 583: 'guillotine',\n",
      " 584: 'hair slide',\n",
      " 585: 'hair spray',\n",
      " 586: 'half track',\n",
      " 587: 'hammer',\n",
      " 588: 'hamper',\n",
      " 589: 'hand blower, blow dryer, blow drier, hair dryer, hair drier',\n",
      " 590: 'hand-held computer, hand-held microcomputer',\n",
      " 591: 'handkerchief, hankie, hanky, hankey',\n",
      " 592: 'hard disc, hard disk, fixed disk',\n",
      " 593: 'harmonica, mouth organ, harp, mouth harp',\n",
      " 594: 'harp',\n",
      " 595: 'harvester, reaper',\n",
      " 596: 'hatchet',\n",
      " 597: 'holster',\n",
      " 598: 'home theater, home theatre',\n",
      " 599: 'honeycomb',\n",
      " 600: 'hook, claw',\n",
      " 601: 'hoopskirt, crinoline',\n",
      " 602: 'horizontal bar, high bar',\n",
      " 603: 'horse cart, horse-cart',\n",
      " 604: 'hourglass',\n",
      " 605: 'iPod',\n",
      " 606: 'iron, smoothing iron',\n",
      " 607: \"jack-o'-lantern\",\n",
      " 608: 'jean, blue jean, denim',\n",
      " 609: 'jeep, landrover',\n",
      " 610: 'jersey, T-shirt, tee shirt',\n",
      " 611: 'jigsaw puzzle',\n",
      " 612: 'jinrikisha, ricksha, rickshaw',\n",
      " 613: 'joystick',\n",
      " 614: 'kimono',\n",
      " 615: 'knee pad',\n",
      " 616: 'knot',\n",
      " 617: 'lab coat, laboratory coat',\n",
      " 618: 'ladle',\n",
      " 619: 'lampshade, lamp shade',\n",
      " 620: 'laptop, laptop computer',\n",
      " 621: 'lawn mower, mower',\n",
      " 622: 'lens cap, lens cover',\n",
      " 623: 'letter opener, paper knife, paperknife',\n",
      " 624: 'library',\n",
      " 625: 'lifeboat',\n",
      " 626: 'lighter, light, igniter, ignitor',\n",
      " 627: 'limousine, limo',\n",
      " 628: 'liner, ocean liner',\n",
      " 629: 'lipstick, lip rouge',\n",
      " 630: 'Loafer',\n",
      " 631: 'lotion',\n",
      " 632: 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system',\n",
      " 633: \"loupe, jeweler's loupe\",\n",
      " 634: 'lumbermill, sawmill',\n",
      " 635: 'magnetic compass',\n",
      " 636: 'mailbag, postbag',\n",
      " 637: 'mailbox, letter box',\n",
      " 638: 'maillot',\n",
      " 639: 'maillot, tank suit',\n",
      " 640: 'manhole cover',\n",
      " 641: 'maraca',\n",
      " 642: 'marimba, xylophone',\n",
      " 643: 'mask',\n",
      " 644: 'matchstick',\n",
      " 645: 'maypole',\n",
      " 646: 'maze, labyrinth',\n",
      " 647: 'measuring cup',\n",
      " 648: 'medicine chest, medicine cabinet',\n",
      " 649: 'megalith, megalithic structure',\n",
      " 650: 'microphone, mike',\n",
      " 651: 'microwave, microwave oven',\n",
      " 652: 'military uniform',\n",
      " 653: 'milk can',\n",
      " 654: 'minibus',\n",
      " 655: 'miniskirt, mini',\n",
      " 656: 'minivan',\n",
      " 657: 'missile',\n",
      " 658: 'mitten',\n",
      " 659: 'mixing bowl',\n",
      " 660: 'mobile home, manufactured home',\n",
      " 661: 'Model T',\n",
      " 662: 'modem',\n",
      " 663: 'monastery',\n",
      " 664: 'monitor',\n",
      " 665: 'moped',\n",
      " 666: 'mortar',\n",
      " 667: 'mortarboard',\n",
      " 668: 'mosque',\n",
      " 669: 'mosquito net',\n",
      " 670: 'motor scooter, scooter',\n",
      " 671: 'mountain bike, all-terrain bike, off-roader',\n",
      " 672: 'mountain tent',\n",
      " 673: 'mouse, computer mouse',\n",
      " 674: 'mousetrap',\n",
      " 675: 'moving van',\n",
      " 676: 'muzzle',\n",
      " 677: 'nail',\n",
      " 678: 'neck brace',\n",
      " 679: 'necklace',\n",
      " 680: 'nipple',\n",
      " 681: 'notebook, notebook computer',\n",
      " 682: 'obelisk',\n",
      " 683: 'oboe, hautboy, hautbois',\n",
      " 684: 'ocarina, sweet potato',\n",
      " 685: 'odometer, hodometer, mileometer, milometer',\n",
      " 686: 'oil filter',\n",
      " 687: 'organ, pipe organ',\n",
      " 688: 'oscilloscope, scope, cathode-ray oscilloscope, CRO',\n",
      " 689: 'overskirt',\n",
      " 690: 'oxcart',\n",
      " 691: 'oxygen mask',\n",
      " 692: 'packet',\n",
      " 693: 'paddle, boat paddle',\n",
      " 694: 'paddlewheel, paddle wheel',\n",
      " 695: 'padlock',\n",
      " 696: 'paintbrush',\n",
      " 697: \"pajama, pyjama, pj's, jammies\",\n",
      " 698: 'palace',\n",
      " 699: 'panpipe, pandean pipe, syrinx',\n",
      " 700: 'paper towel',\n",
      " 701: 'parachute, chute',\n",
      " 702: 'parallel bars, bars',\n",
      " 703: 'park bench',\n",
      " 704: 'parking meter',\n",
      " 705: 'passenger car, coach, carriage',\n",
      " 706: 'patio, terrace',\n",
      " 707: 'pay-phone, pay-station',\n",
      " 708: 'pedestal, plinth, footstall',\n",
      " 709: 'pencil box, pencil case',\n",
      " 710: 'pencil sharpener',\n",
      " 711: 'perfume, essence',\n",
      " 712: 'Petri dish',\n",
      " 713: 'photocopier',\n",
      " 714: 'pick, plectrum, plectron',\n",
      " 715: 'pickelhaube',\n",
      " 716: 'picket fence, paling',\n",
      " 717: 'pickup, pickup truck',\n",
      " 718: 'pier',\n",
      " 719: 'piggy bank, penny bank',\n",
      " 720: 'pill bottle',\n",
      " 721: 'pillow',\n",
      " 722: 'ping-pong ball',\n",
      " 723: 'pinwheel',\n",
      " 724: 'pirate, pirate ship',\n",
      " 725: 'pitcher, ewer',\n",
      " 726: \"plane, carpenter's plane, woodworking plane\",\n",
      " 727: 'planetarium',\n",
      " 728: 'plastic bag',\n",
      " 729: 'plate rack',\n",
      " 730: 'plow, plough',\n",
      " 731: \"plunger, plumber's helper\",\n",
      " 732: 'Polaroid camera, Polaroid Land camera',\n",
      " 733: 'pole',\n",
      " 734: 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria',\n",
      " 735: 'poncho',\n",
      " 736: 'pool table, billiard table, snooker table',\n",
      " 737: 'pop bottle, soda bottle',\n",
      " 738: 'pot, flowerpot',\n",
      " 739: \"potter's wheel\",\n",
      " 740: 'power drill',\n",
      " 741: 'prayer rug, prayer mat',\n",
      " 742: 'printer',\n",
      " 743: 'prison, prison house',\n",
      " 744: 'projectile, missile',\n",
      " 745: 'projector',\n",
      " 746: 'puck, hockey puck',\n",
      " 747: 'punching bag, punch bag, punching ball, punchball',\n",
      " 748: 'purse',\n",
      " 749: 'quill, quill pen',\n",
      " 750: 'quilt, comforter, comfort, puff',\n",
      " 751: 'racer, race car, racing car',\n",
      " 752: 'racket, racquet',\n",
      " 753: 'radiator',\n",
      " 754: 'radio, wireless',\n",
      " 755: 'radio telescope, radio reflector',\n",
      " 756: 'rain barrel',\n",
      " 757: 'recreational vehicle, RV, R.V.',\n",
      " 758: 'reel',\n",
      " 759: 'reflex camera',\n",
      " 760: 'refrigerator, icebox',\n",
      " 761: 'remote control, remote',\n",
      " 762: 'restaurant, eating house, eating place, eatery',\n",
      " 763: 'revolver, six-gun, six-shooter',\n",
      " 764: 'rifle',\n",
      " 765: 'rocking chair, rocker',\n",
      " 766: 'rotisserie',\n",
      " 767: 'rubber eraser, rubber, pencil eraser',\n",
      " 768: 'rugby ball',\n",
      " 769: 'rule, ruler',\n",
      " 770: 'running shoe',\n",
      " 771: 'safe',\n",
      " 772: 'safety pin',\n",
      " 773: 'saltshaker, salt shaker',\n",
      " 774: 'sandal',\n",
      " 775: 'sarong',\n",
      " 776: 'sax, saxophone',\n",
      " 777: 'scabbard',\n",
      " 778: 'scale, weighing machine',\n",
      " 779: 'school bus',\n",
      " 780: 'schooner',\n",
      " 781: 'scoreboard',\n",
      " 782: 'screen, CRT screen',\n",
      " 783: 'screw',\n",
      " 784: 'screwdriver',\n",
      " 785: 'seat belt, seatbelt',\n",
      " 786: 'sewing machine',\n",
      " 787: 'shield, buckler',\n",
      " 788: 'shoe shop, shoe-shop, shoe store',\n",
      " 789: 'shoji',\n",
      " 790: 'shopping basket',\n",
      " 791: 'shopping cart',\n",
      " 792: 'shovel',\n",
      " 793: 'shower cap',\n",
      " 794: 'shower curtain',\n",
      " 795: 'ski',\n",
      " 796: 'ski mask',\n",
      " 797: 'sleeping bag',\n",
      " 798: 'slide rule, slipstick',\n",
      " 799: 'sliding door',\n",
      " 800: 'slot, one-armed bandit',\n",
      " 801: 'snorkel',\n",
      " 802: 'snowmobile',\n",
      " 803: 'snowplow, snowplough',\n",
      " 804: 'soap dispenser',\n",
      " 805: 'soccer ball',\n",
      " 806: 'sock',\n",
      " 807: 'solar dish, solar collector, solar furnace',\n",
      " 808: 'sombrero',\n",
      " 809: 'soup bowl',\n",
      " 810: 'space bar',\n",
      " 811: 'space heater',\n",
      " 812: 'space shuttle',\n",
      " 813: 'spatula',\n",
      " 814: 'speedboat',\n",
      " 815: \"spider web, spider's web\",\n",
      " 816: 'spindle',\n",
      " 817: 'sports car, sport car',\n",
      " 818: 'spotlight, spot',\n",
      " 819: 'stage',\n",
      " 820: 'steam locomotive',\n",
      " 821: 'steel arch bridge',\n",
      " 822: 'steel drum',\n",
      " 823: 'stethoscope',\n",
      " 824: 'stole',\n",
      " 825: 'stone wall',\n",
      " 826: 'stopwatch, stop watch',\n",
      " 827: 'stove',\n",
      " 828: 'strainer',\n",
      " 829: 'streetcar, tram, tramcar, trolley, trolley car',\n",
      " 830: 'stretcher',\n",
      " 831: 'studio couch, day bed',\n",
      " 832: 'stupa, tope',\n",
      " 833: 'submarine, pigboat, sub, U-boat',\n",
      " 834: 'suit, suit of clothes',\n",
      " 835: 'sundial',\n",
      " 836: 'sunglass',\n",
      " 837: 'sunglasses, dark glasses, shades',\n",
      " 838: 'sunscreen, sunblock, sun blocker',\n",
      " 839: 'suspension bridge',\n",
      " 840: 'swab, swob, mop',\n",
      " 841: 'sweatshirt',\n",
      " 842: 'swimming trunks, bathing trunks',\n",
      " 843: 'swing',\n",
      " 844: 'switch, electric switch, electrical switch',\n",
      " 845: 'syringe',\n",
      " 846: 'table lamp',\n",
      " 847: 'tank, army tank, armored combat vehicle, armoured combat vehicle',\n",
      " 848: 'tape player',\n",
      " 849: 'teapot',\n",
      " 850: 'teddy, teddy bear',\n",
      " 851: 'television, television system',\n",
      " 852: 'tennis ball',\n",
      " 853: 'thatch, thatched roof',\n",
      " 854: 'theater curtain, theatre curtain',\n",
      " 855: 'thimble',\n",
      " 856: 'thresher, thrasher, threshing machine',\n",
      " 857: 'throne',\n",
      " 858: 'tile roof',\n",
      " 859: 'toaster',\n",
      " 860: 'tobacco shop, tobacconist shop, tobacconist',\n",
      " 861: 'toilet seat',\n",
      " 862: 'torch',\n",
      " 863: 'totem pole',\n",
      " 864: 'tow truck, tow car, wrecker',\n",
      " 865: 'toyshop',\n",
      " 866: 'tractor',\n",
      " 867: 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi',\n",
      " 868: 'tray',\n",
      " 869: 'trench coat',\n",
      " 870: 'tricycle, trike, velocipede',\n",
      " 871: 'trimaran',\n",
      " 872: 'tripod',\n",
      " 873: 'triumphal arch',\n",
      " 874: 'trolleybus, trolley coach, trackless trolley',\n",
      " 875: 'trombone',\n",
      " 876: 'tub, vat',\n",
      " 877: 'turnstile',\n",
      " 878: 'typewriter keyboard',\n",
      " 879: 'umbrella',\n",
      " 880: 'unicycle, monocycle',\n",
      " 881: 'upright, upright piano',\n",
      " 882: 'vacuum, vacuum cleaner',\n",
      " 883: 'vase',\n",
      " 884: 'vault',\n",
      " 885: 'velvet',\n",
      " 886: 'vending machine',\n",
      " 887: 'vestment',\n",
      " 888: 'viaduct',\n",
      " 889: 'violin, fiddle',\n",
      " 890: 'volleyball',\n",
      " 891: 'waffle iron',\n",
      " 892: 'wall clock',\n",
      " 893: 'wallet, billfold, notecase, pocketbook',\n",
      " 894: 'wardrobe, closet, press',\n",
      " 895: 'warplane, military plane',\n",
      " 896: 'washbasin, handbasin, washbowl, lavabo, wash-hand basin',\n",
      " 897: 'washer, automatic washer, washing machine',\n",
      " 898: 'water bottle',\n",
      " 899: 'water jug',\n",
      " 900: 'water tower',\n",
      " 901: 'whiskey jug',\n",
      " 902: 'whistle',\n",
      " 903: 'wig',\n",
      " 904: 'window screen',\n",
      " 905: 'window shade',\n",
      " 906: 'Windsor tie',\n",
      " 907: 'wine bottle',\n",
      " 908: 'wing',\n",
      " 909: 'wok',\n",
      " 910: 'wooden spoon',\n",
      " 911: 'wool, woolen, woollen',\n",
      " 912: 'worm fence, snake fence, snake-rail fence, Virginia fence',\n",
      " 913: 'wreck',\n",
      " 914: 'yawl',\n",
      " 915: 'yurt',\n",
      " 916: 'web site, website, internet site, site',\n",
      " 917: 'comic book',\n",
      " 918: 'crossword puzzle, crossword',\n",
      " 919: 'street sign',\n",
      " 920: 'traffic light, traffic signal, stoplight',\n",
      " 921: 'book jacket, dust cover, dust jacket, dust wrapper',\n",
      " 922: 'menu',\n",
      " 923: 'plate',\n",
      " 924: 'guacamole',\n",
      " 925: 'consomme',\n",
      " 926: 'hot pot, hotpot',\n",
      " 927: 'trifle',\n",
      " 928: 'ice cream, icecream',\n",
      " 929: 'ice lolly, lolly, lollipop, popsicle',\n",
      " 930: 'French loaf',\n",
      " 931: 'bagel, beigel',\n",
      " 932: 'pretzel',\n",
      " 933: 'cheeseburger',\n",
      " 934: 'hotdog, hot dog, red hot',\n",
      " 935: 'mashed potato',\n",
      " 936: 'head cabbage',\n",
      " 937: 'broccoli',\n",
      " 938: 'cauliflower',\n",
      " 939: 'zucchini, courgette',\n",
      " 940: 'spaghetti squash',\n",
      " 941: 'acorn squash',\n",
      " 942: 'butternut squash',\n",
      " 943: 'cucumber, cuke',\n",
      " 944: 'artichoke, globe artichoke',\n",
      " 945: 'bell pepper',\n",
      " 946: 'cardoon',\n",
      " 947: 'mushroom',\n",
      " 948: 'Granny Smith',\n",
      " 949: 'strawberry',\n",
      " 950: 'orange',\n",
      " 951: 'lemon',\n",
      " 952: 'fig',\n",
      " 953: 'pineapple, ananas',\n",
      " 954: 'banana',\n",
      " 955: 'jackfruit, jak, jack',\n",
      " 956: 'custard apple',\n",
      " 957: 'pomegranate',\n",
      " 958: 'hay',\n",
      " 959: 'carbonara',\n",
      " 960: 'chocolate sauce, chocolate syrup',\n",
      " 961: 'dough',\n",
      " 962: 'meat loaf, meatloaf',\n",
      " 963: 'pizza, pizza pie',\n",
      " 964: 'potpie',\n",
      " 965: 'burrito',\n",
      " 966: 'red wine',\n",
      " 967: 'espresso',\n",
      " 968: 'cup',\n",
      " 969: 'eggnog',\n",
      " 970: 'alp',\n",
      " 971: 'bubble',\n",
      " 972: 'cliff, drop, drop-off',\n",
      " 973: 'coral reef',\n",
      " 974: 'geyser',\n",
      " 975: 'lakeside, lakeshore',\n",
      " 976: 'promontory, headland, head, foreland',\n",
      " 977: 'sandbar, sand bar',\n",
      " 978: 'seashore, coast, seacoast, sea-coast',\n",
      " 979: 'valley, vale',\n",
      " 980: 'volcano',\n",
      " 981: 'ballplayer, baseball player',\n",
      " 982: 'groom, bridegroom',\n",
      " 983: 'scuba diver',\n",
      " 984: 'rapeseed',\n",
      " 985: 'daisy',\n",
      " 986: \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\",\n",
      " 987: 'corn',\n",
      " 988: 'acorn',\n",
      " 989: 'hip, rose hip, rosehip',\n",
      " 990: 'buckeye, horse chestnut, conker',\n",
      " 991: 'coral fungus',\n",
      " 992: 'agaric',\n",
      " 993: 'gyromitra',\n",
      " 994: 'stinkhorn, carrion fungus',\n",
      " 995: 'earthstar',\n",
      " 996: 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa',\n",
      " 997: 'bolete',\n",
      " 998: 'ear, spike, capitulum',\n",
      " 999: 'toilet tissue, toilet paper, bathroom tissue'}\n",
      "22/2:\n",
      "import pickle\n",
      "\n",
      "# result = {}\n",
      "# for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "#     if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "#         print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "#         image = cv2.imread(\"../data/images/\" + file)\n",
      "\n",
      "#         result[file] = predict(image)\n",
      "        \n",
      "# pickle.dump(result, open( \"image_classes\", \"wb\" ))\n",
      "\n",
      "image_classes = pickle.load(result, open( \"image_classes\", \"rb\" ))\n",
      "image_classes\n",
      "22/3:\n",
      "import pickle\n",
      "\n",
      "# result = {}\n",
      "# for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "#     if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "#         print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "#         image = cv2.imread(\"../data/images/\" + file)\n",
      "\n",
      "#         result[file] = predict(image)\n",
      "        \n",
      "# pickle.dump(result, open( \"image_classes\", \"wb\" ))\n",
      "\n",
      "image_classes = pickle.load(open( \"image_classes\", \"rb\" ))\n",
      "image_classes\n",
      "22/4:\n",
      "result = {}\n",
      "for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "        print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "        image = cv2.imread(\"../data/images/\" + file)\n",
      "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "        reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "        cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "        result[file] = cluster.cluster_centers_\n",
      "        \n",
      "pickle.dump(result, open( \"image_classes\", \"wb\" ))\n",
      "22/5:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 224\n",
      "\n",
      "def prep_image(image):\n",
      "    res = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "    res = res.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "    return res\n",
      "\n",
      "def predict(image):\n",
      "    cropped_image = center_crop(image)\n",
      "    prediction = (model.predict(prep_image(image)), model.predict(prep_image(cropped_image)))\n",
      "    if np.max(prediction[0]>=prediction[1]):\n",
      "        return np.argmax(prediction[0], axis=1)[0]\n",
      "    else:\n",
      "        return np.argmax(prediction[1], axis=1)[1]\n",
      "    \n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "plt.imshow(image)\n",
      "plt.imshow(prep_imageimage)\n",
      "\n",
      "print(class_dict[predict(image)])\n",
      "22/6:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 224\n",
      "\n",
      "def prep_image(image):\n",
      "    res = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "    res = res.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "    return res\n",
      "\n",
      "def predict(image):\n",
      "    cropped_image = center_crop(image)\n",
      "    prediction = (model.predict(prep_image(image)), model.predict(prep_image(cropped_image)))\n",
      "    if np.max(prediction[0]>=prediction[1]):\n",
      "        return np.argmax(prediction[0], axis=1)[0]\n",
      "    else:\n",
      "        return np.argmax(prediction[1], axis=1)[1]\n",
      "    \n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "plt.imshow(image)\n",
      "plt.imshow(prep_image)\n",
      "\n",
      "print(class_dict[predict(image)])\n",
      "22/7:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 224\n",
      "\n",
      "def prep_image(image):\n",
      "    res = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "    res = res.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "    return res\n",
      "\n",
      "def predict(image):\n",
      "    cropped_image = center_crop(image)\n",
      "    prediction = (model.predict(prep_image(image)), model.predict(prep_image(cropped_image)))\n",
      "    if np.max(prediction[0]>=prediction[1]):\n",
      "        return np.argmax(prediction[0], axis=1)[0]\n",
      "    else:\n",
      "        return np.argmax(prediction[1], axis=1)[1]\n",
      "    \n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "plt.imshow(image)\n",
      "\n",
      "print(class_dict[predict(image)])\n",
      "22/8:\n",
      "result = {}\n",
      "for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "        print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "        image = cv2.imread(\"../data/images/\" + file)\n",
      "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "        reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "        cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "        result[file] = cluster.cluster_centers_\n",
      "        \n",
      "pickle.dump(result, open( \"image_classes\", \"wb\" ))\n",
      "22/9:\n",
      "from sklearn.cluster import KMeans\n",
      "result = {}\n",
      "for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "        print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "        image = cv2.imread(\"../data/images/\" + file)\n",
      "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "        reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "        cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "        result[file] = cluster.cluster_centers_\n",
      "        \n",
      "pickle.dump(result, open( \"image_classes\", \"wb\" ))\n",
      "22/10:\n",
      "from sklearn.cluster import KMeans\n",
      "result = {}\n",
      "for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "        print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "        image = cv2.imread(\"../data/images/\" + file)\n",
      "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "        reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "        cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "        print(cluster.cluster_centers_)\n",
      "        result[file] = cluster.cluster_centers_\n",
      "        \n",
      "pickle.dump(result, open( \"image_classes\", \"wb\" ))\n",
      "22/11:\n",
      "from sklearn.cluster import KMeans\n",
      "result = {}\n",
      "for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "        print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "        image = cv2.imread(\"../data/images/\" + file)\n",
      "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "        reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "        cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "        print(cluster.cluster_centers_[0])\n",
      "        result[file] = cluster.cluster_centers_[0]\n",
      "        \n",
      "pickle.dump(result, open( \"image_classes\", \"wb\" ))\n",
      "22/12:\n",
      "from sklearn.cluster import KMeans\n",
      "result = {}\n",
      "for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "        print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "        image = cv2.imread(\"../data/images/\" + file)\n",
      "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "        reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "        cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "        print(cluster.cluster_centers_)\n",
      "        result[file] = cluster.cluster_centers_\n",
      "        \n",
      "pickle.dump(result, open( \"image_color_clusters\", \"wb\" ))\n",
      "23/1:\n",
      "# from google.colab import drive\n",
      "\n",
      "# drive.mount('/content/gdrive')\n",
      "23/2:\n",
      "# !ls gdrive/MyDrive/PED\n",
      "# df = pandas.read_csv('gdrive/MyDrive/PED/reddit_wsb.csv')\n",
      "23/3:\n",
      "import pandas\n",
      "from collections import Counter\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import nltk\n",
      "nltk.download('punkt')\n",
      "%matplotlib inline\n",
      "\n",
      "df = pandas.read_csv('../data/reddit_wsb.csv')\n",
      "df[\"body\"] = df[\"body\"].fillna('')\n",
      "df[\"title\"] = df[\"title\"].fillna('')\n",
      "\n",
      "df\n",
      "23/4:\n",
      "seconds_in_day = 60 * 60 * 24.\n",
      "df['sin_time'] = np.sin(2*np.pi*(df.created % seconds_in_day)/seconds_in_day)\n",
      "df['cos_time'] = np.cos(2*np.pi*(df.created % seconds_in_day)/seconds_in_day)\n",
      "23/5: df.describe()\n",
      "23/6: df.sample(5000).plot.scatter('sin_time','cos_time').set_aspect('equal')\n",
      "23/7: df.columns\n",
      "23/8:\n",
      "plt.hist(df['score'], bins=30)\n",
      "plt.ylabel('Count')\n",
      "plt.yscale('log')\n",
      "plt.xlabel('Data');\n",
      "23/9:\n",
      "plt.hist(df['created'], bins=30)\n",
      "plt.ylabel('Count')\n",
      "plt.yscale('log')\n",
      "plt.xlabel('Data');\n",
      "23/10:\n",
      "#!pip install emoji\n",
      "from collections import Counter\n",
      "from nltk import word_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
      "\n",
      "nltk.download('stopwords')\n",
      "# tokenizer = RegexpTokenizer(r'\\w+')\n",
      "from nltk.corpus import stopwords\n",
      "from collections import Counter\n",
      "23/11:\n",
      "body_most_common = Counter(word_tokenize(\" \".join(df[\"body\"])))\n",
      "for key, cnts in list(body_most_common.items()):\n",
      "  if not key in emoji.UNICODE_EMOJI and (key in stopwords.words('english') or not re.match(r'\\w+',key)):\n",
      "    del body_most_common[key]\n",
      "body_most_common = body_most_common.most_common(100)\n",
      "body_most_common\n",
      "23/12:\n",
      "title_most_common = Counter(word_tokenize(\" \".join(df[\"title\"])))\n",
      "for key, cnts in list(title_most_common.items()):\n",
      "  if not key in emoji.UNICODE_EMOJI and (key in stopwords.words('english') or not re.match(r'\\w+',key)):\n",
      "    del title_most_common[key]\n",
      "title_most_common = title_most_common.most_common(100)\n",
      "title_most_common\n",
      "23/13:\n",
      "additional_keywords = ['gme', 'hold', 'buy', 'retard', 'wife', 'hedgies', '', 'moon','','', '', '','','', '', '']\n",
      "count = lambda l1,l2: sum([1 for x in l1 if x in l2])\n",
      "text_attributes = {}\n",
      "for key in additional_keywords: \n",
      "  text_attributes['title_' + key] = df[\"title\"].str.lower().str.count(key)\n",
      "  text_attributes['body_' + key] = df[\"body\"].str.lower().str.count(key)\n",
      "23/14:\n",
      "for key, cnts in body_most_common: \n",
      "  text_attributes['body_' + key.lower()] = df[\"body\"].str.lower().str.count(key.lower())\n",
      "\n",
      "\n",
      "for key, cnts in title_most_common: \n",
      "  text_attributes['title_' + key.lower()] = df[\"title\"].str.lower().str.count(key.lower())\n",
      "23/15:\n",
      "\n",
      "text_attributes['body_punctuation'] = df['body'].apply(lambda s: count(s, string.punctuation))\n",
      "text_attributes['title_punctuation'] = df['title'].apply(lambda s: count(s, string.punctuation))\n",
      "\n",
      "text_attributes['body_cap_ratio'] = df['body'].str.count(r'[A-Z]')/df['body'].str.count(r'[a-zA-Z]')\n",
      "text_attributes['title_cap_ratio'] = df['title'].str.count(r'[A-Z]')/df['title'].str.count(r'[a-zA-Z]')\n",
      "\n",
      "text_attributes['time_of_day'] = pandas.to_datetime(df['timestamp']).dt.hour + pandas.to_datetime(df['timestamp']).dt.minute/60\n",
      "\n",
      "text_attributes['title_length'] = df['title'].apply(len)\n",
      "text_attributes['title_non_alphanumeric_ratio'] = df['title'].str.count(r'[^A-Za-z0-9]')/df['title'].apply(len)\n",
      "23/16:\n",
      "def regex_count_in_url_and_body(regex):\n",
      "    return df['body'].apply(lambda s: len(re.findall(regex, s))) + df['url'].apply(lambda s: len(re.findall(regex, s)))\n",
      "23/17:\n",
      "text_attributes['link'] = regex_count_in_url_and_body(r'(https?://[^\\s]+)')\n",
      "text_attributes['reddit_link'] = regex_count_in_url_and_body(r'(https?:\\/\\/w?w?w?\\.?redd\\.?it[^\\s]+)')\n",
      "text_attributes['yt_link'] = regex_count_in_url_and_body(r'(https?:\\/\\/w?w?w?\\.?youtu\\.?be[^\\s]+)')\n",
      "text_attributes['tweet_link'] = regex_count_in_url_and_body(r'(https?:\\/\\/w?w?w?\\.?twitter[^\\s]+)')\n",
      "text_attributes['facebook_link'] = regex_count_in_url_and_body(r'(https?:\\/\\/w?w?w?\\.?facebook[^\\s]+)')\n",
      "\n",
      "text_attributes = pandas.DataFrame(text_attributes)\n",
      "text_attributes\n",
      "23/18: text_attributes.describe()\n",
      "23/19:\n",
      "# print(len(d['link']))\n",
      "# print(len([x for x in d['link'] if x is not None]))\n",
      "\n",
      "for i,x in df.head(100).iterrows():\n",
      "  if x['title'].startswith('Grand'):\n",
      "    print(x['body'])\n",
      "    print(re.findall(\"https?://[^\\s()|]+\", x['body']))\n",
      "23/20:\n",
      "print(re.findall('http','ahttpjkshdoifjdshttp'))\n",
      "print()\n",
      "print(re.findall(r\"https?://(?:(?:[^\\s()])|(?:\\(\\S*\\)]*\\)))+\", 'https://www.benzinga.com/government/21/01/19337399/something-systemically-wrong-with-gamestop-options-trading-says-massachusetts-securities-regulator(dupa=dupa()))'))\n",
      "23/21:\n",
      "from urllib.parse import urlparse\n",
      "urlparse('https://www.benzinga.com/government/21/01/19337399/something-systemically-wrong-with-gamestop-options-trading-says-massachusetts-securities-regulator)')\n",
      "23/22:\n",
      "count = 0\n",
      "for i,x in df.iterrows():\n",
      "  if not x['is_self']:\n",
      "    count+=1\n",
      "    print(x['body'])\n",
      "    print(x['score'],x['url'], 'https://reddit.com' + x['permalink'])\n",
      "print(count)\n",
      "23/23:\n",
      "text_attributes['link'] = regex_count_in_url_and_body(r'(https?://[^\\s]+)')\n",
      "text_attributes['reddit_link'] = regex_count_in_url_and_body(r'(https?:\\/\\/w?w?w?\\.?redd\\.?it[^\\s]+)')\n",
      "text_attributes['yt_link'] = regex_count_in_url_and_body(r'(https?:\\/\\/w?w?w?\\.?youtu\\.?be[^\\s]+)')\n",
      "text_attributes['tweet_link'] = regex_count_in_url_and_body(r'(https?:\\/\\/w?w?w?\\.?twitter[^\\s]+)')\n",
      "text_attributes['facebook_link'] = regex_count_in_url_and_body(r'(https?:\\/\\/w?w?w?\\.?facebook[^\\s]+)')\n",
      "\n",
      "text_attributes['gif'] = regex_count_in_url_and_body(r'(https?:\\/\\/w?w?w?\\.?giphy[^\\s]+)') + regex_count_in_url_and_body(r'(https?:\\/\\/w?w?w?\\.?gifyu[^\\s]+)') + regex_count_in_url_and_body(r'(https?:\\/\\/w?w?w?\\.?gfycat[^\\s]+)') + df['url'].apply(lambda s: len(re.findall(r'(\\.gif)', s)))\n",
      "\n",
      "text_attributes = pandas.DataFrame(text_attributes)\n",
      "text_attributes\n",
      "23/24:\n",
      "text_attributes['link'] = regex_count_in_url_and_body(r'(https?://[^\\s]+)')\n",
      "text_attributes['reddit_link'] = regex_count_in_url_and_body(r'(https?:\\/\\/w?w?w?\\.?redd\\.?it[^\\s]+)')\n",
      "text_attributes['yt_link'] = regex_count_in_url_and_body(r'(https?:\\/\\/w?w?w?\\.?youtu\\.?be[^\\s]+)')\n",
      "text_attributes['tweet_link'] = regex_count_in_url_and_body(r'(https?:\\/\\/w?w?w?\\.?twitter[^\\s]+)')\n",
      "text_attributes['facebook_link'] = regex_count_in_url_and_body(r'(https?:\\/\\/w?w?w?\\.?facebook[^\\s]+)')\n",
      "\n",
      "text_attributes['gif'] = regex_count_in_url_and_body(r'(https?:\\/\\/w?w?w?\\.?giphy[^\\s]+)') + regex_count_in_url_and_body(r'(https?:\\/\\/w?w?w?\\.?gifyu[^\\s]+)') + regex_count_in_url_and_body(r'(https?:\\/\\/w?w?w?\\.?gfycat[^\\s]+)') + df['url'].apply(lambda s: len(re.findall(r'(\\.gif)', s)))\n",
      "\n",
      "text_attributes = pandas.DataFrame(text_attributes)\n",
      "text_attributes\n",
      "df[text_attributes['gif']>=1]\n",
      "22/13:\n",
      "\n",
      "result = {}\n",
      "for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "        print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "        image = cv2.imread(\"../data/images/\" + file)\n",
      "        image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
      "\n",
      "        result[file] = cv2.mean(image)\n",
      "        \n",
      "pickle.dump(result, open( \"image_hsv_means\", \"wb\" ))\n",
      "22/14:\n",
      "\n",
      "# result = {}\n",
      "# for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "#     if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "#         print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "#         image = cv2.imread(\"../data/images/\" + file)\n",
      "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
      "\n",
      "#         result[file] = cv2.mean(image)\n",
      "        \n",
      "# pickle.dump(result, open( \"image_hsv_means\", \"wb\" ))\n",
      "\n",
      "image_hsv_means = pickle.load(open( \"image_color_clusters\", \"rb\" ))\n",
      "22/15:\n",
      "\n",
      "# result = {}\n",
      "# for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "#     if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "#         print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "#         image = cv2.imread(\"../data/images/\" + file)\n",
      "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
      "\n",
      "#         result[file] = cv2.mean(image)\n",
      "        \n",
      "# pickle.dump(result, open( \"image_hsv_means\", \"wb\" ))\n",
      "\n",
      "image_hsv_means = pickle.load(open( \"image_hsv_means\", \"rb\" ))\n",
      "image_hsv_means\n",
      "22/16:\n",
      "\n",
      "# result = {}\n",
      "# for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "#     if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "#         print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "#         image = cv2.imread(\"../data/images/\" + file)\n",
      "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
      "\n",
      "#         result[file] = cv2.mean(image)\n",
      "        \n",
      "# pickle.dump(result, open( \"../pickle/image_hsv_means.pkl\", \"wb\" ))\n",
      "\n",
      "image_hsv_means = pickle.load(open( \"../pickle/image_hsv_means.pkl\", \"rb\" ))\n",
      "image_hsv_means\n",
      "22/17:\n",
      "import pickle\n",
      "\n",
      "# result = {}\n",
      "# for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "#     if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "#         print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "#         image = cv2.imread(\"../data/images/\" + file)\n",
      "\n",
      "#         result[file] = predict(image)\n",
      "        \n",
      "# pickle.dump(result, open( \"../pickle/image_classes.pkl\", \"wb\" ))\n",
      "\n",
      "image_classes = pickle.load(open( \"../pickle/image_classes.pkl\", \"rb\" ))\n",
      "image_classes\n",
      "22/18:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "def visualize_colors(cluster, centroids):\n",
      "    # Get the number of different clusters, create histogram, and normalize\n",
      "    labels = np.arange(0, len(np.unique(cluster.labels_)) + 1)\n",
      "    (hist, _) = np.histogram(cluster.labels_, bins = labels)\n",
      "    hist = hist.astype(\"float\")\n",
      "    hist /= hist.sum()\n",
      "\n",
      "    # Create frequency rect and iterate through each cluster's color and percentage\n",
      "    rect = np.zeros((50, 300, 3), dtype=np.uint8)\n",
      "    colors = sorted([(percent, color) for (percent, color) in zip(hist, centroids)])\n",
      "    start = 0\n",
      "    for (percent, color) in colors:\n",
      "        print(color, \"{:0.2f}%\".format(percent * 100))\n",
      "        end = start + (percent * 300)\n",
      "        cv2.rectangle(rect, (int(start), 0), (int(end), 50), \\\n",
      "                      color.astype(\"uint8\").tolist(), -1)\n",
      "        start = end\n",
      "    return rect\n",
      "\n",
      "image = cv2.imread('../data/images/l31llr.png')\n",
      "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "\n",
      "# Find and display most dominant colors\n",
      "cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "visualize = visualize_colors(image_hsv_means[], image_hsv_means[].cluster_centers_)\n",
      "visualize = cv2.cvtColor(visualize, cv2.COLOR_RGB2BGR)\n",
      "cv2.imshow('visualize', visualize)\n",
      "\n",
      "# result = {}\n",
      "# for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "#     if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "#         print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "#         image = cv2.imread(\"../data/images/\" + file)\n",
      "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "#         reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "#         cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "#         print(cluster.cluster_centers_)\n",
      "#         result[file] = cluster.cluster_centers_\n",
      "        \n",
      "# pickle.dump(result, open( \"../pickle/image_color_clusters.pkl\", \"wb\" ))\n",
      "22/19:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "def visualize_colors(cluster, centroids):\n",
      "    # Get the number of different clusters, create histogram, and normalize\n",
      "    labels = np.arange(0, len(np.unique(cluster.labels_)) + 1)\n",
      "    (hist, _) = np.histogram(cluster.labels_, bins = labels)\n",
      "    hist = hist.astype(\"float\")\n",
      "    hist /= hist.sum()\n",
      "\n",
      "    # Create frequency rect and iterate through each cluster's color and percentage\n",
      "    rect = np.zeros((50, 300, 3), dtype=np.uint8)\n",
      "    colors = sorted([(percent, color) for (percent, color) in zip(hist, centroids)])\n",
      "    start = 0\n",
      "    for (percent, color) in colors:\n",
      "        print(color, \"{:0.2f}%\".format(percent * 100))\n",
      "        end = start + (percent * 300)\n",
      "        cv2.rectangle(rect, (int(start), 0), (int(end), 50), \\\n",
      "                      color.astype(\"uint8\").tolist(), -1)\n",
      "        start = end\n",
      "    return rect\n",
      "\n",
      "image = cv2.imread('../data/images/l31llr.png')\n",
      "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "\n",
      "# Find and display most dominant colors\n",
      "cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "visualize = visualize_colors(cluster, cluster.cluster_centers_)\n",
      "visualize = cv2.cvtColor(visualize, cv2.COLOR_RGB2BGR)\n",
      "cv2.imshow('visualize', visualize)\n",
      "\n",
      "# result = {}\n",
      "# for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "#     if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "#         print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "#         image = cv2.imread(\"../data/images/\" + file)\n",
      "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "#         reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "#         cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "#         print(cluster.cluster_centers_)\n",
      "#         result[file] = cluster.cluster_centers_\n",
      "        \n",
      "# pickle.dump(result, open( \"../pickle/image_color_clusters.pkl\", \"wb\" ))\n",
      "24/1:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "def visualize_colors(cluster, centroids):\n",
      "    # Get the number of different clusters, create histogram, and normalize\n",
      "    labels = np.arange(0, len(np.unique(cluster.labels_)) + 1)\n",
      "    (hist, _) = np.histogram(cluster.labels_, bins = labels)\n",
      "    hist = hist.astype(\"float\")\n",
      "    hist /= hist.sum()\n",
      "\n",
      "    # Create frequency rect and iterate through each cluster's color and percentage\n",
      "    rect = np.zeros((50, 300, 3), dtype=np.uint8)\n",
      "    colors = sorted([(percent, color) for (percent, color) in zip(hist, centroids)])\n",
      "    start = 0\n",
      "    for (percent, color) in colors:\n",
      "        print(color, \"{:0.2f}%\".format(percent * 100))\n",
      "        end = start + (percent * 300)\n",
      "        cv2.rectangle(rect, (int(start), 0), (int(end), 50), \\\n",
      "                      color.astype(\"uint8\").tolist(), -1)\n",
      "        start = end\n",
      "    return rect\n",
      "\n",
      "image = cv2.imread('../data/images/l31llr.png')\n",
      "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "\n",
      "# Find and display most dominant colors\n",
      "cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "visualize = visualize_colors(cluster, cluster.cluster_centers_)\n",
      "plt.imshow(visualize)\n",
      "\n",
      "# result = {}\n",
      "# for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "#     if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "#         print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "#         image = cv2.imread(\"../data/images/\" + file)\n",
      "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "#         reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "#         cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "#         print(cluster.cluster_centers_)\n",
      "#         result[file] = cluster.cluster_centers_\n",
      "        \n",
      "# pickle.dump(result, open( \"../pickle/image_color_clusters.pkl\", \"wb\" ))\n",
      "24/2:\n",
      "from tensorflow.keras.applications import EfficientNetB0\n",
      "from tensorflow.image import resize\n",
      "model = EfficientNetB0(weights='imagenet')\n",
      "\n",
      "def center_crop(img, new_width=None, new_height=None):        \n",
      "\n",
      "    width = img.shape[1]\n",
      "    height = img.shape[0]\n",
      "\n",
      "    if new_width is None:\n",
      "        new_width = min(width, height)\n",
      "\n",
      "    if new_height is None:\n",
      "        new_height = min(width, height)\n",
      "\n",
      "    left = int(np.ceil((width - new_width) / 2))\n",
      "    right = width - int(np.floor((width - new_width) / 2))\n",
      "\n",
      "    top = int(np.ceil((height - new_height) / 2))\n",
      "    bottom = height - int(np.floor((height - new_height) / 2))\n",
      "\n",
      "    if len(img.shape) == 2:\n",
      "        center_cropped_img = img[top:bottom, left:right]\n",
      "    else:\n",
      "        center_cropped_img = img[top:bottom, left:right, ...]\n",
      "\n",
      "    return center_cropped_img\n",
      "\n",
      "class_dict = {0: 'tench, Tinca tinca',\n",
      " 1: 'goldfish, Carassius auratus',\n",
      " 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n",
      " 3: 'tiger shark, Galeocerdo cuvieri',\n",
      " 4: 'hammerhead, hammerhead shark',\n",
      " 5: 'electric ray, crampfish, numbfish, torpedo',\n",
      " 6: 'stingray',\n",
      " 7: 'cock',\n",
      " 8: 'hen',\n",
      " 9: 'ostrich, Struthio camelus',\n",
      " 10: 'brambling, Fringilla montifringilla',\n",
      " 11: 'goldfinch, Carduelis carduelis',\n",
      " 12: 'house finch, linnet, Carpodacus mexicanus',\n",
      " 13: 'junco, snowbird',\n",
      " 14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea',\n",
      " 15: 'robin, American robin, Turdus migratorius',\n",
      " 16: 'bulbul',\n",
      " 17: 'jay',\n",
      " 18: 'magpie',\n",
      " 19: 'chickadee',\n",
      " 20: 'water ouzel, dipper',\n",
      " 21: 'kite',\n",
      " 22: 'bald eagle, American eagle, Haliaeetus leucocephalus',\n",
      " 23: 'vulture',\n",
      " 24: 'great grey owl, great gray owl, Strix nebulosa',\n",
      " 25: 'European fire salamander, Salamandra salamandra',\n",
      " 26: 'common newt, Triturus vulgaris',\n",
      " 27: 'eft',\n",
      " 28: 'spotted salamander, Ambystoma maculatum',\n",
      " 29: 'axolotl, mud puppy, Ambystoma mexicanum',\n",
      " 30: 'bullfrog, Rana catesbeiana',\n",
      " 31: 'tree frog, tree-frog',\n",
      " 32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui',\n",
      " 33: 'loggerhead, loggerhead turtle, Caretta caretta',\n",
      " 34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea',\n",
      " 35: 'mud turtle',\n",
      " 36: 'terrapin',\n",
      " 37: 'box turtle, box tortoise',\n",
      " 38: 'banded gecko',\n",
      " 39: 'common iguana, iguana, Iguana iguana',\n",
      " 40: 'American chameleon, anole, Anolis carolinensis',\n",
      " 41: 'whiptail, whiptail lizard',\n",
      " 42: 'agama',\n",
      " 43: 'frilled lizard, Chlamydosaurus kingi',\n",
      " 44: 'alligator lizard',\n",
      " 45: 'Gila monster, Heloderma suspectum',\n",
      " 46: 'green lizard, Lacerta viridis',\n",
      " 47: 'African chameleon, Chamaeleo chamaeleon',\n",
      " 48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis',\n",
      " 49: 'African crocodile, Nile crocodile, Crocodylus niloticus',\n",
      " 50: 'American alligator, Alligator mississipiensis',\n",
      " 51: 'triceratops',\n",
      " 52: 'thunder snake, worm snake, Carphophis amoenus',\n",
      " 53: 'ringneck snake, ring-necked snake, ring snake',\n",
      " 54: 'hognose snake, puff adder, sand viper',\n",
      " 55: 'green snake, grass snake',\n",
      " 56: 'king snake, kingsnake',\n",
      " 57: 'garter snake, grass snake',\n",
      " 58: 'water snake',\n",
      " 59: 'vine snake',\n",
      " 60: 'night snake, Hypsiglena torquata',\n",
      " 61: 'boa constrictor, Constrictor constrictor',\n",
      " 62: 'rock python, rock snake, Python sebae',\n",
      " 63: 'Indian cobra, Naja naja',\n",
      " 64: 'green mamba',\n",
      " 65: 'sea snake',\n",
      " 66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus',\n",
      " 67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus',\n",
      " 68: 'sidewinder, horned rattlesnake, Crotalus cerastes',\n",
      " 69: 'trilobite',\n",
      " 70: 'harvestman, daddy longlegs, Phalangium opilio',\n",
      " 71: 'scorpion',\n",
      " 72: 'black and gold garden spider, Argiope aurantia',\n",
      " 73: 'barn spider, Araneus cavaticus',\n",
      " 74: 'garden spider, Aranea diademata',\n",
      " 75: 'black widow, Latrodectus mactans',\n",
      " 76: 'tarantula',\n",
      " 77: 'wolf spider, hunting spider',\n",
      " 78: 'tick',\n",
      " 79: 'centipede',\n",
      " 80: 'black grouse',\n",
      " 81: 'ptarmigan',\n",
      " 82: 'ruffed grouse, partridge, Bonasa umbellus',\n",
      " 83: 'prairie chicken, prairie grouse, prairie fowl',\n",
      " 84: 'peacock',\n",
      " 85: 'quail',\n",
      " 86: 'partridge',\n",
      " 87: 'African grey, African gray, Psittacus erithacus',\n",
      " 88: 'macaw',\n",
      " 89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n",
      " 90: 'lorikeet',\n",
      " 91: 'coucal',\n",
      " 92: 'bee eater',\n",
      " 93: 'hornbill',\n",
      " 94: 'hummingbird',\n",
      " 95: 'jacamar',\n",
      " 96: 'toucan',\n",
      " 97: 'drake',\n",
      " 98: 'red-breasted merganser, Mergus serrator',\n",
      " 99: 'goose',\n",
      " 100: 'black swan, Cygnus atratus',\n",
      " 101: 'tusker',\n",
      " 102: 'echidna, spiny anteater, anteater',\n",
      " 103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n",
      " 104: 'wallaby, brush kangaroo',\n",
      " 105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n",
      " 106: 'wombat',\n",
      " 107: 'jellyfish',\n",
      " 108: 'sea anemone, anemone',\n",
      " 109: 'brain coral',\n",
      " 110: 'flatworm, platyhelminth',\n",
      " 111: 'nematode, nematode worm, roundworm',\n",
      " 112: 'conch',\n",
      " 113: 'snail',\n",
      " 114: 'slug',\n",
      " 115: 'sea slug, nudibranch',\n",
      " 116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore',\n",
      " 117: 'chambered nautilus, pearly nautilus, nautilus',\n",
      " 118: 'Dungeness crab, Cancer magister',\n",
      " 119: 'rock crab, Cancer irroratus',\n",
      " 120: 'fiddler crab',\n",
      " 121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica',\n",
      " 122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus',\n",
      " 123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish',\n",
      " 124: 'crayfish, crawfish, crawdad, crawdaddy',\n",
      " 125: 'hermit crab',\n",
      " 126: 'isopod',\n",
      " 127: 'white stork, Ciconia ciconia',\n",
      " 128: 'black stork, Ciconia nigra',\n",
      " 129: 'spoonbill',\n",
      " 130: 'flamingo',\n",
      " 131: 'little blue heron, Egretta caerulea',\n",
      " 132: 'American egret, great white heron, Egretta albus',\n",
      " 133: 'bittern',\n",
      " 134: 'crane',\n",
      " 135: 'limpkin, Aramus pictus',\n",
      " 136: 'European gallinule, Porphyrio porphyrio',\n",
      " 137: 'American coot, marsh hen, mud hen, water hen, Fulica americana',\n",
      " 138: 'bustard',\n",
      " 139: 'ruddy turnstone, Arenaria interpres',\n",
      " 140: 'red-backed sandpiper, dunlin, Erolia alpina',\n",
      " 141: 'redshank, Tringa totanus',\n",
      " 142: 'dowitcher',\n",
      " 143: 'oystercatcher, oyster catcher',\n",
      " 144: 'pelican',\n",
      " 145: 'king penguin, Aptenodytes patagonica',\n",
      " 146: 'albatross, mollymawk',\n",
      " 147: 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus',\n",
      " 148: 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca',\n",
      " 149: 'dugong, Dugong dugon',\n",
      " 150: 'sea lion',\n",
      " 151: 'Chihuahua',\n",
      " 152: 'Japanese spaniel',\n",
      " 153: 'Maltese dog, Maltese terrier, Maltese',\n",
      " 154: 'Pekinese, Pekingese, Peke',\n",
      " 155: 'Shih-Tzu',\n",
      " 156: 'Blenheim spaniel',\n",
      " 157: 'papillon',\n",
      " 158: 'toy terrier',\n",
      " 159: 'Rhodesian ridgeback',\n",
      " 160: 'Afghan hound, Afghan',\n",
      " 161: 'basset, basset hound',\n",
      " 162: 'beagle',\n",
      " 163: 'bloodhound, sleuthhound',\n",
      " 164: 'bluetick',\n",
      " 165: 'black-and-tan coonhound',\n",
      " 166: 'Walker hound, Walker foxhound',\n",
      " 167: 'English foxhound',\n",
      " 168: 'redbone',\n",
      " 169: 'borzoi, Russian wolfhound',\n",
      " 170: 'Irish wolfhound',\n",
      " 171: 'Italian greyhound',\n",
      " 172: 'whippet',\n",
      " 173: 'Ibizan hound, Ibizan Podenco',\n",
      " 174: 'Norwegian elkhound, elkhound',\n",
      " 175: 'otterhound, otter hound',\n",
      " 176: 'Saluki, gazelle hound',\n",
      " 177: 'Scottish deerhound, deerhound',\n",
      " 178: 'Weimaraner',\n",
      " 179: 'Staffordshire bullterrier, Staffordshire bull terrier',\n",
      " 180: 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier',\n",
      " 181: 'Bedlington terrier',\n",
      " 182: 'Border terrier',\n",
      " 183: 'Kerry blue terrier',\n",
      " 184: 'Irish terrier',\n",
      " 185: 'Norfolk terrier',\n",
      " 186: 'Norwich terrier',\n",
      " 187: 'Yorkshire terrier',\n",
      " 188: 'wire-haired fox terrier',\n",
      " 189: 'Lakeland terrier',\n",
      " 190: 'Sealyham terrier, Sealyham',\n",
      " 191: 'Airedale, Airedale terrier',\n",
      " 192: 'cairn, cairn terrier',\n",
      " 193: 'Australian terrier',\n",
      " 194: 'Dandie Dinmont, Dandie Dinmont terrier',\n",
      " 195: 'Boston bull, Boston terrier',\n",
      " 196: 'miniature schnauzer',\n",
      " 197: 'giant schnauzer',\n",
      " 198: 'standard schnauzer',\n",
      " 199: 'Scotch terrier, Scottish terrier, Scottie',\n",
      " 200: 'Tibetan terrier, chrysanthemum dog',\n",
      " 201: 'silky terrier, Sydney silky',\n",
      " 202: 'soft-coated wheaten terrier',\n",
      " 203: 'West Highland white terrier',\n",
      " 204: 'Lhasa, Lhasa apso',\n",
      " 205: 'flat-coated retriever',\n",
      " 206: 'curly-coated retriever',\n",
      " 207: 'golden retriever',\n",
      " 208: 'Labrador retriever',\n",
      " 209: 'Chesapeake Bay retriever',\n",
      " 210: 'German short-haired pointer',\n",
      " 211: 'vizsla, Hungarian pointer',\n",
      " 212: 'English setter',\n",
      " 213: 'Irish setter, red setter',\n",
      " 214: 'Gordon setter',\n",
      " 215: 'Brittany spaniel',\n",
      " 216: 'clumber, clumber spaniel',\n",
      " 217: 'English springer, English springer spaniel',\n",
      " 218: 'Welsh springer spaniel',\n",
      " 219: 'cocker spaniel, English cocker spaniel, cocker',\n",
      " 220: 'Sussex spaniel',\n",
      " 221: 'Irish water spaniel',\n",
      " 222: 'kuvasz',\n",
      " 223: 'schipperke',\n",
      " 224: 'groenendael',\n",
      " 225: 'malinois',\n",
      " 226: 'briard',\n",
      " 227: 'kelpie',\n",
      " 228: 'komondor',\n",
      " 229: 'Old English sheepdog, bobtail',\n",
      " 230: 'Shetland sheepdog, Shetland sheep dog, Shetland',\n",
      " 231: 'collie',\n",
      " 232: 'Border collie',\n",
      " 233: 'Bouvier des Flandres, Bouviers des Flandres',\n",
      " 234: 'Rottweiler',\n",
      " 235: 'German shepherd, German shepherd dog, German police dog, alsatian',\n",
      " 236: 'Doberman, Doberman pinscher',\n",
      " 237: 'miniature pinscher',\n",
      " 238: 'Greater Swiss Mountain dog',\n",
      " 239: 'Bernese mountain dog',\n",
      " 240: 'Appenzeller',\n",
      " 241: 'EntleBucher',\n",
      " 242: 'boxer',\n",
      " 243: 'bull mastiff',\n",
      " 244: 'Tibetan mastiff',\n",
      " 245: 'French bulldog',\n",
      " 246: 'Great Dane',\n",
      " 247: 'Saint Bernard, St Bernard',\n",
      " 248: 'Eskimo dog, husky',\n",
      " 249: 'malamute, malemute, Alaskan malamute',\n",
      " 250: 'Siberian husky',\n",
      " 251: 'dalmatian, coach dog, carriage dog',\n",
      " 252: 'affenpinscher, monkey pinscher, monkey dog',\n",
      " 253: 'basenji',\n",
      " 254: 'pug, pug-dog',\n",
      " 255: 'Leonberg',\n",
      " 256: 'Newfoundland, Newfoundland dog',\n",
      " 257: 'Great Pyrenees',\n",
      " 258: 'Samoyed, Samoyede',\n",
      " 259: 'Pomeranian',\n",
      " 260: 'chow, chow chow',\n",
      " 261: 'keeshond',\n",
      " 262: 'Brabancon griffon',\n",
      " 263: 'Pembroke, Pembroke Welsh corgi',\n",
      " 264: 'Cardigan, Cardigan Welsh corgi',\n",
      " 265: 'toy poodle',\n",
      " 266: 'miniature poodle',\n",
      " 267: 'standard poodle',\n",
      " 268: 'Mexican hairless',\n",
      " 269: 'timber wolf, grey wolf, gray wolf, Canis lupus',\n",
      " 270: 'white wolf, Arctic wolf, Canis lupus tundrarum',\n",
      " 271: 'red wolf, maned wolf, Canis rufus, Canis niger',\n",
      " 272: 'coyote, prairie wolf, brush wolf, Canis latrans',\n",
      " 273: 'dingo, warrigal, warragal, Canis dingo',\n",
      " 274: 'dhole, Cuon alpinus',\n",
      " 275: 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus',\n",
      " 276: 'hyena, hyaena',\n",
      " 277: 'red fox, Vulpes vulpes',\n",
      " 278: 'kit fox, Vulpes macrotis',\n",
      " 279: 'Arctic fox, white fox, Alopex lagopus',\n",
      " 280: 'grey fox, gray fox, Urocyon cinereoargenteus',\n",
      " 281: 'tabby, tabby cat',\n",
      " 282: 'tiger cat',\n",
      " 283: 'Persian cat',\n",
      " 284: 'Siamese cat, Siamese',\n",
      " 285: 'Egyptian cat',\n",
      " 286: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n",
      " 287: 'lynx, catamount',\n",
      " 288: 'leopard, Panthera pardus',\n",
      " 289: 'snow leopard, ounce, Panthera uncia',\n",
      " 290: 'jaguar, panther, Panthera onca, Felis onca',\n",
      " 291: 'lion, king of beasts, Panthera leo',\n",
      " 292: 'tiger, Panthera tigris',\n",
      " 293: 'cheetah, chetah, Acinonyx jubatus',\n",
      " 294: 'brown bear, bruin, Ursus arctos',\n",
      " 295: 'American black bear, black bear, Ursus americanus, Euarctos americanus',\n",
      " 296: 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus',\n",
      " 297: 'sloth bear, Melursus ursinus, Ursus ursinus',\n",
      " 298: 'mongoose',\n",
      " 299: 'meerkat, mierkat',\n",
      " 300: 'tiger beetle',\n",
      " 301: 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle',\n",
      " 302: 'ground beetle, carabid beetle',\n",
      " 303: 'long-horned beetle, longicorn, longicorn beetle',\n",
      " 304: 'leaf beetle, chrysomelid',\n",
      " 305: 'dung beetle',\n",
      " 306: 'rhinoceros beetle',\n",
      " 307: 'weevil',\n",
      " 308: 'fly',\n",
      " 309: 'bee',\n",
      " 310: 'ant, emmet, pismire',\n",
      " 311: 'grasshopper, hopper',\n",
      " 312: 'cricket',\n",
      " 313: 'walking stick, walkingstick, stick insect',\n",
      " 314: 'cockroach, roach',\n",
      " 315: 'mantis, mantid',\n",
      " 316: 'cicada, cicala',\n",
      " 317: 'leafhopper',\n",
      " 318: 'lacewing, lacewing fly',\n",
      " 319: \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\",\n",
      " 320: 'damselfly',\n",
      " 321: 'admiral',\n",
      " 322: 'ringlet, ringlet butterfly',\n",
      " 323: 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus',\n",
      " 324: 'cabbage butterfly',\n",
      " 325: 'sulphur butterfly, sulfur butterfly',\n",
      " 326: 'lycaenid, lycaenid butterfly',\n",
      " 327: 'starfish, sea star',\n",
      " 328: 'sea urchin',\n",
      " 329: 'sea cucumber, holothurian',\n",
      " 330: 'wood rabbit, cottontail, cottontail rabbit',\n",
      " 331: 'hare',\n",
      " 332: 'Angora, Angora rabbit',\n",
      " 333: 'hamster',\n",
      " 334: 'porcupine, hedgehog',\n",
      " 335: 'fox squirrel, eastern fox squirrel, Sciurus niger',\n",
      " 336: 'marmot',\n",
      " 337: 'beaver',\n",
      " 338: 'guinea pig, Cavia cobaya',\n",
      " 339: 'sorrel',\n",
      " 340: 'zebra',\n",
      " 341: 'hog, pig, grunter, squealer, Sus scrofa',\n",
      " 342: 'wild boar, boar, Sus scrofa',\n",
      " 343: 'warthog',\n",
      " 344: 'hippopotamus, hippo, river horse, Hippopotamus amphibius',\n",
      " 345: 'ox',\n",
      " 346: 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis',\n",
      " 347: 'bison',\n",
      " 348: 'ram, tup',\n",
      " 349: 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis',\n",
      " 350: 'ibex, Capra ibex',\n",
      " 351: 'hartebeest',\n",
      " 352: 'impala, Aepyceros melampus',\n",
      " 353: 'gazelle',\n",
      " 354: 'Arabian camel, dromedary, Camelus dromedarius',\n",
      " 355: 'llama',\n",
      " 356: 'weasel',\n",
      " 357: 'mink',\n",
      " 358: 'polecat, fitch, foulmart, foumart, Mustela putorius',\n",
      " 359: 'black-footed ferret, ferret, Mustela nigripes',\n",
      " 360: 'otter',\n",
      " 361: 'skunk, polecat, wood pussy',\n",
      " 362: 'badger',\n",
      " 363: 'armadillo',\n",
      " 364: 'three-toed sloth, ai, Bradypus tridactylus',\n",
      " 365: 'orangutan, orang, orangutang, Pongo pygmaeus',\n",
      " 366: 'gorilla, Gorilla gorilla',\n",
      " 367: 'chimpanzee, chimp, Pan troglodytes',\n",
      " 368: 'gibbon, Hylobates lar',\n",
      " 369: 'siamang, Hylobates syndactylus, Symphalangus syndactylus',\n",
      " 370: 'guenon, guenon monkey',\n",
      " 371: 'patas, hussar monkey, Erythrocebus patas',\n",
      " 372: 'baboon',\n",
      " 373: 'macaque',\n",
      " 374: 'langur',\n",
      " 375: 'colobus, colobus monkey',\n",
      " 376: 'proboscis monkey, Nasalis larvatus',\n",
      " 377: 'marmoset',\n",
      " 378: 'capuchin, ringtail, Cebus capucinus',\n",
      " 379: 'howler monkey, howler',\n",
      " 380: 'titi, titi monkey',\n",
      " 381: 'spider monkey, Ateles geoffroyi',\n",
      " 382: 'squirrel monkey, Saimiri sciureus',\n",
      " 383: 'Madagascar cat, ring-tailed lemur, Lemur catta',\n",
      " 384: 'indri, indris, Indri indri, Indri brevicaudatus',\n",
      " 385: 'Indian elephant, Elephas maximus',\n",
      " 386: 'African elephant, Loxodonta africana',\n",
      " 387: 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens',\n",
      " 388: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca',\n",
      " 389: 'barracouta, snoek',\n",
      " 390: 'eel',\n",
      " 391: 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch',\n",
      " 392: 'rock beauty, Holocanthus tricolor',\n",
      " 393: 'anemone fish',\n",
      " 394: 'sturgeon',\n",
      " 395: 'gar, garfish, garpike, billfish, Lepisosteus osseus',\n",
      " 396: 'lionfish',\n",
      " 397: 'puffer, pufferfish, blowfish, globefish',\n",
      " 398: 'abacus',\n",
      " 399: 'abaya',\n",
      " 400: \"academic gown, academic robe, judge's robe\",\n",
      " 401: 'accordion, piano accordion, squeeze box',\n",
      " 402: 'acoustic guitar',\n",
      " 403: 'aircraft carrier, carrier, flattop, attack aircraft carrier',\n",
      " 404: 'airliner',\n",
      " 405: 'airship, dirigible',\n",
      " 406: 'altar',\n",
      " 407: 'ambulance',\n",
      " 408: 'amphibian, amphibious vehicle',\n",
      " 409: 'analog clock',\n",
      " 410: 'apiary, bee house',\n",
      " 411: 'apron',\n",
      " 412: 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin',\n",
      " 413: 'assault rifle, assault gun',\n",
      " 414: 'backpack, back pack, knapsack, packsack, rucksack, haversack',\n",
      " 415: 'bakery, bakeshop, bakehouse',\n",
      " 416: 'balance beam, beam',\n",
      " 417: 'balloon',\n",
      " 418: 'ballpoint, ballpoint pen, ballpen, Biro',\n",
      " 419: 'Band Aid',\n",
      " 420: 'banjo',\n",
      " 421: 'bannister, banister, balustrade, balusters, handrail',\n",
      " 422: 'barbell',\n",
      " 423: 'barber chair',\n",
      " 424: 'barbershop',\n",
      " 425: 'barn',\n",
      " 426: 'barometer',\n",
      " 427: 'barrel, cask',\n",
      " 428: 'barrow, garden cart, lawn cart, wheelbarrow',\n",
      " 429: 'baseball',\n",
      " 430: 'basketball',\n",
      " 431: 'bassinet',\n",
      " 432: 'bassoon',\n",
      " 433: 'bathing cap, swimming cap',\n",
      " 434: 'bath towel',\n",
      " 435: 'bathtub, bathing tub, bath, tub',\n",
      " 436: 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon',\n",
      " 437: 'beacon, lighthouse, beacon light, pharos',\n",
      " 438: 'beaker',\n",
      " 439: 'bearskin, busby, shako',\n",
      " 440: 'beer bottle',\n",
      " 441: 'beer glass',\n",
      " 442: 'bell cote, bell cot',\n",
      " 443: 'bib',\n",
      " 444: 'bicycle-built-for-two, tandem bicycle, tandem',\n",
      " 445: 'bikini, two-piece',\n",
      " 446: 'binder, ring-binder',\n",
      " 447: 'binoculars, field glasses, opera glasses',\n",
      " 448: 'birdhouse',\n",
      " 449: 'boathouse',\n",
      " 450: 'bobsled, bobsleigh, bob',\n",
      " 451: 'bolo tie, bolo, bola tie, bola',\n",
      " 452: 'bonnet, poke bonnet',\n",
      " 453: 'bookcase',\n",
      " 454: 'bookshop, bookstore, bookstall',\n",
      " 455: 'bottlecap',\n",
      " 456: 'bow',\n",
      " 457: 'bow tie, bow-tie, bowtie',\n",
      " 458: 'brass, memorial tablet, plaque',\n",
      " 459: 'brassiere, bra, bandeau',\n",
      " 460: 'breakwater, groin, groyne, mole, bulwark, seawall, jetty',\n",
      " 461: 'breastplate, aegis, egis',\n",
      " 462: 'broom',\n",
      " 463: 'bucket, pail',\n",
      " 464: 'buckle',\n",
      " 465: 'bulletproof vest',\n",
      " 466: 'bullet train, bullet',\n",
      " 467: 'butcher shop, meat market',\n",
      " 468: 'cab, hack, taxi, taxicab',\n",
      " 469: 'caldron, cauldron',\n",
      " 470: 'candle, taper, wax light',\n",
      " 471: 'cannon',\n",
      " 472: 'canoe',\n",
      " 473: 'can opener, tin opener',\n",
      " 474: 'cardigan',\n",
      " 475: 'car mirror',\n",
      " 476: 'carousel, carrousel, merry-go-round, roundabout, whirligig',\n",
      " 477: \"carpenter's kit, tool kit\",\n",
      " 478: 'carton',\n",
      " 479: 'car wheel',\n",
      " 480: 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM',\n",
      " 481: 'cassette',\n",
      " 482: 'cassette player',\n",
      " 483: 'castle',\n",
      " 484: 'catamaran',\n",
      " 485: 'CD player',\n",
      " 486: 'cello, violoncello',\n",
      " 487: 'cellular telephone, cellular phone, cellphone, cell, mobile phone',\n",
      " 488: 'chain',\n",
      " 489: 'chainlink fence',\n",
      " 490: 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour',\n",
      " 491: 'chain saw, chainsaw',\n",
      " 492: 'chest',\n",
      " 493: 'chiffonier, commode',\n",
      " 494: 'chime, bell, gong',\n",
      " 495: 'china cabinet, china closet',\n",
      " 496: 'Christmas stocking',\n",
      " 497: 'church, church building',\n",
      " 498: 'cinema, movie theater, movie theatre, movie house, picture palace',\n",
      " 499: 'cleaver, meat cleaver, chopper',\n",
      " 500: 'cliff dwelling',\n",
      " 501: 'cloak',\n",
      " 502: 'clog, geta, patten, sabot',\n",
      " 503: 'cocktail shaker',\n",
      " 504: 'coffee mug',\n",
      " 505: 'coffeepot',\n",
      " 506: 'coil, spiral, volute, whorl, helix',\n",
      " 507: 'combination lock',\n",
      " 508: 'computer keyboard, keypad',\n",
      " 509: 'confectionery, confectionary, candy store',\n",
      " 510: 'container ship, containership, container vessel',\n",
      " 511: 'convertible',\n",
      " 512: 'corkscrew, bottle screw',\n",
      " 513: 'cornet, horn, trumpet, trump',\n",
      " 514: 'cowboy boot',\n",
      " 515: 'cowboy hat, ten-gallon hat',\n",
      " 516: 'cradle',\n",
      " 517: 'crane',\n",
      " 518: 'crash helmet',\n",
      " 519: 'crate',\n",
      " 520: 'crib, cot',\n",
      " 521: 'Crock Pot',\n",
      " 522: 'croquet ball',\n",
      " 523: 'crutch',\n",
      " 524: 'cuirass',\n",
      " 525: 'dam, dike, dyke',\n",
      " 526: 'desk',\n",
      " 527: 'desktop computer',\n",
      " 528: 'dial telephone, dial phone',\n",
      " 529: 'diaper, nappy, napkin',\n",
      " 530: 'digital clock',\n",
      " 531: 'digital watch',\n",
      " 532: 'dining table, board',\n",
      " 533: 'dishrag, dishcloth',\n",
      " 534: 'dishwasher, dish washer, dishwashing machine',\n",
      " 535: 'disk brake, disc brake',\n",
      " 536: 'dock, dockage, docking facility',\n",
      " 537: 'dogsled, dog sled, dog sleigh',\n",
      " 538: 'dome',\n",
      " 539: 'doormat, welcome mat',\n",
      " 540: 'drilling platform, offshore rig',\n",
      " 541: 'drum, membranophone, tympan',\n",
      " 542: 'drumstick',\n",
      " 543: 'dumbbell',\n",
      " 544: 'Dutch oven',\n",
      " 545: 'electric fan, blower',\n",
      " 546: 'electric guitar',\n",
      " 547: 'electric locomotive',\n",
      " 548: 'entertainment center',\n",
      " 549: 'envelope',\n",
      " 550: 'espresso maker',\n",
      " 551: 'face powder',\n",
      " 552: 'feather boa, boa',\n",
      " 553: 'file, file cabinet, filing cabinet',\n",
      " 554: 'fireboat',\n",
      " 555: 'fire engine, fire truck',\n",
      " 556: 'fire screen, fireguard',\n",
      " 557: 'flagpole, flagstaff',\n",
      " 558: 'flute, transverse flute',\n",
      " 559: 'folding chair',\n",
      " 560: 'football helmet',\n",
      " 561: 'forklift',\n",
      " 562: 'fountain',\n",
      " 563: 'fountain pen',\n",
      " 564: 'four-poster',\n",
      " 565: 'freight car',\n",
      " 566: 'French horn, horn',\n",
      " 567: 'frying pan, frypan, skillet',\n",
      " 568: 'fur coat',\n",
      " 569: 'garbage truck, dustcart',\n",
      " 570: 'gasmask, respirator, gas helmet',\n",
      " 571: 'gas pump, gasoline pump, petrol pump, island dispenser',\n",
      " 572: 'goblet',\n",
      " 573: 'go-kart',\n",
      " 574: 'golf ball',\n",
      " 575: 'golfcart, golf cart',\n",
      " 576: 'gondola',\n",
      " 577: 'gong, tam-tam',\n",
      " 578: 'gown',\n",
      " 579: 'grand piano, grand',\n",
      " 580: 'greenhouse, nursery, glasshouse',\n",
      " 581: 'grille, radiator grille',\n",
      " 582: 'grocery store, grocery, food market, market',\n",
      " 583: 'guillotine',\n",
      " 584: 'hair slide',\n",
      " 585: 'hair spray',\n",
      " 586: 'half track',\n",
      " 587: 'hammer',\n",
      " 588: 'hamper',\n",
      " 589: 'hand blower, blow dryer, blow drier, hair dryer, hair drier',\n",
      " 590: 'hand-held computer, hand-held microcomputer',\n",
      " 591: 'handkerchief, hankie, hanky, hankey',\n",
      " 592: 'hard disc, hard disk, fixed disk',\n",
      " 593: 'harmonica, mouth organ, harp, mouth harp',\n",
      " 594: 'harp',\n",
      " 595: 'harvester, reaper',\n",
      " 596: 'hatchet',\n",
      " 597: 'holster',\n",
      " 598: 'home theater, home theatre',\n",
      " 599: 'honeycomb',\n",
      " 600: 'hook, claw',\n",
      " 601: 'hoopskirt, crinoline',\n",
      " 602: 'horizontal bar, high bar',\n",
      " 603: 'horse cart, horse-cart',\n",
      " 604: 'hourglass',\n",
      " 605: 'iPod',\n",
      " 606: 'iron, smoothing iron',\n",
      " 607: \"jack-o'-lantern\",\n",
      " 608: 'jean, blue jean, denim',\n",
      " 609: 'jeep, landrover',\n",
      " 610: 'jersey, T-shirt, tee shirt',\n",
      " 611: 'jigsaw puzzle',\n",
      " 612: 'jinrikisha, ricksha, rickshaw',\n",
      " 613: 'joystick',\n",
      " 614: 'kimono',\n",
      " 615: 'knee pad',\n",
      " 616: 'knot',\n",
      " 617: 'lab coat, laboratory coat',\n",
      " 618: 'ladle',\n",
      " 619: 'lampshade, lamp shade',\n",
      " 620: 'laptop, laptop computer',\n",
      " 621: 'lawn mower, mower',\n",
      " 622: 'lens cap, lens cover',\n",
      " 623: 'letter opener, paper knife, paperknife',\n",
      " 624: 'library',\n",
      " 625: 'lifeboat',\n",
      " 626: 'lighter, light, igniter, ignitor',\n",
      " 627: 'limousine, limo',\n",
      " 628: 'liner, ocean liner',\n",
      " 629: 'lipstick, lip rouge',\n",
      " 630: 'Loafer',\n",
      " 631: 'lotion',\n",
      " 632: 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system',\n",
      " 633: \"loupe, jeweler's loupe\",\n",
      " 634: 'lumbermill, sawmill',\n",
      " 635: 'magnetic compass',\n",
      " 636: 'mailbag, postbag',\n",
      " 637: 'mailbox, letter box',\n",
      " 638: 'maillot',\n",
      " 639: 'maillot, tank suit',\n",
      " 640: 'manhole cover',\n",
      " 641: 'maraca',\n",
      " 642: 'marimba, xylophone',\n",
      " 643: 'mask',\n",
      " 644: 'matchstick',\n",
      " 645: 'maypole',\n",
      " 646: 'maze, labyrinth',\n",
      " 647: 'measuring cup',\n",
      " 648: 'medicine chest, medicine cabinet',\n",
      " 649: 'megalith, megalithic structure',\n",
      " 650: 'microphone, mike',\n",
      " 651: 'microwave, microwave oven',\n",
      " 652: 'military uniform',\n",
      " 653: 'milk can',\n",
      " 654: 'minibus',\n",
      " 655: 'miniskirt, mini',\n",
      " 656: 'minivan',\n",
      " 657: 'missile',\n",
      " 658: 'mitten',\n",
      " 659: 'mixing bowl',\n",
      " 660: 'mobile home, manufactured home',\n",
      " 661: 'Model T',\n",
      " 662: 'modem',\n",
      " 663: 'monastery',\n",
      " 664: 'monitor',\n",
      " 665: 'moped',\n",
      " 666: 'mortar',\n",
      " 667: 'mortarboard',\n",
      " 668: 'mosque',\n",
      " 669: 'mosquito net',\n",
      " 670: 'motor scooter, scooter',\n",
      " 671: 'mountain bike, all-terrain bike, off-roader',\n",
      " 672: 'mountain tent',\n",
      " 673: 'mouse, computer mouse',\n",
      " 674: 'mousetrap',\n",
      " 675: 'moving van',\n",
      " 676: 'muzzle',\n",
      " 677: 'nail',\n",
      " 678: 'neck brace',\n",
      " 679: 'necklace',\n",
      " 680: 'nipple',\n",
      " 681: 'notebook, notebook computer',\n",
      " 682: 'obelisk',\n",
      " 683: 'oboe, hautboy, hautbois',\n",
      " 684: 'ocarina, sweet potato',\n",
      " 685: 'odometer, hodometer, mileometer, milometer',\n",
      " 686: 'oil filter',\n",
      " 687: 'organ, pipe organ',\n",
      " 688: 'oscilloscope, scope, cathode-ray oscilloscope, CRO',\n",
      " 689: 'overskirt',\n",
      " 690: 'oxcart',\n",
      " 691: 'oxygen mask',\n",
      " 692: 'packet',\n",
      " 693: 'paddle, boat paddle',\n",
      " 694: 'paddlewheel, paddle wheel',\n",
      " 695: 'padlock',\n",
      " 696: 'paintbrush',\n",
      " 697: \"pajama, pyjama, pj's, jammies\",\n",
      " 698: 'palace',\n",
      " 699: 'panpipe, pandean pipe, syrinx',\n",
      " 700: 'paper towel',\n",
      " 701: 'parachute, chute',\n",
      " 702: 'parallel bars, bars',\n",
      " 703: 'park bench',\n",
      " 704: 'parking meter',\n",
      " 705: 'passenger car, coach, carriage',\n",
      " 706: 'patio, terrace',\n",
      " 707: 'pay-phone, pay-station',\n",
      " 708: 'pedestal, plinth, footstall',\n",
      " 709: 'pencil box, pencil case',\n",
      " 710: 'pencil sharpener',\n",
      " 711: 'perfume, essence',\n",
      " 712: 'Petri dish',\n",
      " 713: 'photocopier',\n",
      " 714: 'pick, plectrum, plectron',\n",
      " 715: 'pickelhaube',\n",
      " 716: 'picket fence, paling',\n",
      " 717: 'pickup, pickup truck',\n",
      " 718: 'pier',\n",
      " 719: 'piggy bank, penny bank',\n",
      " 720: 'pill bottle',\n",
      " 721: 'pillow',\n",
      " 722: 'ping-pong ball',\n",
      " 723: 'pinwheel',\n",
      " 724: 'pirate, pirate ship',\n",
      " 725: 'pitcher, ewer',\n",
      " 726: \"plane, carpenter's plane, woodworking plane\",\n",
      " 727: 'planetarium',\n",
      " 728: 'plastic bag',\n",
      " 729: 'plate rack',\n",
      " 730: 'plow, plough',\n",
      " 731: \"plunger, plumber's helper\",\n",
      " 732: 'Polaroid camera, Polaroid Land camera',\n",
      " 733: 'pole',\n",
      " 734: 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria',\n",
      " 735: 'poncho',\n",
      " 736: 'pool table, billiard table, snooker table',\n",
      " 737: 'pop bottle, soda bottle',\n",
      " 738: 'pot, flowerpot',\n",
      " 739: \"potter's wheel\",\n",
      " 740: 'power drill',\n",
      " 741: 'prayer rug, prayer mat',\n",
      " 742: 'printer',\n",
      " 743: 'prison, prison house',\n",
      " 744: 'projectile, missile',\n",
      " 745: 'projector',\n",
      " 746: 'puck, hockey puck',\n",
      " 747: 'punching bag, punch bag, punching ball, punchball',\n",
      " 748: 'purse',\n",
      " 749: 'quill, quill pen',\n",
      " 750: 'quilt, comforter, comfort, puff',\n",
      " 751: 'racer, race car, racing car',\n",
      " 752: 'racket, racquet',\n",
      " 753: 'radiator',\n",
      " 754: 'radio, wireless',\n",
      " 755: 'radio telescope, radio reflector',\n",
      " 756: 'rain barrel',\n",
      " 757: 'recreational vehicle, RV, R.V.',\n",
      " 758: 'reel',\n",
      " 759: 'reflex camera',\n",
      " 760: 'refrigerator, icebox',\n",
      " 761: 'remote control, remote',\n",
      " 762: 'restaurant, eating house, eating place, eatery',\n",
      " 763: 'revolver, six-gun, six-shooter',\n",
      " 764: 'rifle',\n",
      " 765: 'rocking chair, rocker',\n",
      " 766: 'rotisserie',\n",
      " 767: 'rubber eraser, rubber, pencil eraser',\n",
      " 768: 'rugby ball',\n",
      " 769: 'rule, ruler',\n",
      " 770: 'running shoe',\n",
      " 771: 'safe',\n",
      " 772: 'safety pin',\n",
      " 773: 'saltshaker, salt shaker',\n",
      " 774: 'sandal',\n",
      " 775: 'sarong',\n",
      " 776: 'sax, saxophone',\n",
      " 777: 'scabbard',\n",
      " 778: 'scale, weighing machine',\n",
      " 779: 'school bus',\n",
      " 780: 'schooner',\n",
      " 781: 'scoreboard',\n",
      " 782: 'screen, CRT screen',\n",
      " 783: 'screw',\n",
      " 784: 'screwdriver',\n",
      " 785: 'seat belt, seatbelt',\n",
      " 786: 'sewing machine',\n",
      " 787: 'shield, buckler',\n",
      " 788: 'shoe shop, shoe-shop, shoe store',\n",
      " 789: 'shoji',\n",
      " 790: 'shopping basket',\n",
      " 791: 'shopping cart',\n",
      " 792: 'shovel',\n",
      " 793: 'shower cap',\n",
      " 794: 'shower curtain',\n",
      " 795: 'ski',\n",
      " 796: 'ski mask',\n",
      " 797: 'sleeping bag',\n",
      " 798: 'slide rule, slipstick',\n",
      " 799: 'sliding door',\n",
      " 800: 'slot, one-armed bandit',\n",
      " 801: 'snorkel',\n",
      " 802: 'snowmobile',\n",
      " 803: 'snowplow, snowplough',\n",
      " 804: 'soap dispenser',\n",
      " 805: 'soccer ball',\n",
      " 806: 'sock',\n",
      " 807: 'solar dish, solar collector, solar furnace',\n",
      " 808: 'sombrero',\n",
      " 809: 'soup bowl',\n",
      " 810: 'space bar',\n",
      " 811: 'space heater',\n",
      " 812: 'space shuttle',\n",
      " 813: 'spatula',\n",
      " 814: 'speedboat',\n",
      " 815: \"spider web, spider's web\",\n",
      " 816: 'spindle',\n",
      " 817: 'sports car, sport car',\n",
      " 818: 'spotlight, spot',\n",
      " 819: 'stage',\n",
      " 820: 'steam locomotive',\n",
      " 821: 'steel arch bridge',\n",
      " 822: 'steel drum',\n",
      " 823: 'stethoscope',\n",
      " 824: 'stole',\n",
      " 825: 'stone wall',\n",
      " 826: 'stopwatch, stop watch',\n",
      " 827: 'stove',\n",
      " 828: 'strainer',\n",
      " 829: 'streetcar, tram, tramcar, trolley, trolley car',\n",
      " 830: 'stretcher',\n",
      " 831: 'studio couch, day bed',\n",
      " 832: 'stupa, tope',\n",
      " 833: 'submarine, pigboat, sub, U-boat',\n",
      " 834: 'suit, suit of clothes',\n",
      " 835: 'sundial',\n",
      " 836: 'sunglass',\n",
      " 837: 'sunglasses, dark glasses, shades',\n",
      " 838: 'sunscreen, sunblock, sun blocker',\n",
      " 839: 'suspension bridge',\n",
      " 840: 'swab, swob, mop',\n",
      " 841: 'sweatshirt',\n",
      " 842: 'swimming trunks, bathing trunks',\n",
      " 843: 'swing',\n",
      " 844: 'switch, electric switch, electrical switch',\n",
      " 845: 'syringe',\n",
      " 846: 'table lamp',\n",
      " 847: 'tank, army tank, armored combat vehicle, armoured combat vehicle',\n",
      " 848: 'tape player',\n",
      " 849: 'teapot',\n",
      " 850: 'teddy, teddy bear',\n",
      " 851: 'television, television system',\n",
      " 852: 'tennis ball',\n",
      " 853: 'thatch, thatched roof',\n",
      " 854: 'theater curtain, theatre curtain',\n",
      " 855: 'thimble',\n",
      " 856: 'thresher, thrasher, threshing machine',\n",
      " 857: 'throne',\n",
      " 858: 'tile roof',\n",
      " 859: 'toaster',\n",
      " 860: 'tobacco shop, tobacconist shop, tobacconist',\n",
      " 861: 'toilet seat',\n",
      " 862: 'torch',\n",
      " 863: 'totem pole',\n",
      " 864: 'tow truck, tow car, wrecker',\n",
      " 865: 'toyshop',\n",
      " 866: 'tractor',\n",
      " 867: 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi',\n",
      " 868: 'tray',\n",
      " 869: 'trench coat',\n",
      " 870: 'tricycle, trike, velocipede',\n",
      " 871: 'trimaran',\n",
      " 872: 'tripod',\n",
      " 873: 'triumphal arch',\n",
      " 874: 'trolleybus, trolley coach, trackless trolley',\n",
      " 875: 'trombone',\n",
      " 876: 'tub, vat',\n",
      " 877: 'turnstile',\n",
      " 878: 'typewriter keyboard',\n",
      " 879: 'umbrella',\n",
      " 880: 'unicycle, monocycle',\n",
      " 881: 'upright, upright piano',\n",
      " 882: 'vacuum, vacuum cleaner',\n",
      " 883: 'vase',\n",
      " 884: 'vault',\n",
      " 885: 'velvet',\n",
      " 886: 'vending machine',\n",
      " 887: 'vestment',\n",
      " 888: 'viaduct',\n",
      " 889: 'violin, fiddle',\n",
      " 890: 'volleyball',\n",
      " 891: 'waffle iron',\n",
      " 892: 'wall clock',\n",
      " 893: 'wallet, billfold, notecase, pocketbook',\n",
      " 894: 'wardrobe, closet, press',\n",
      " 895: 'warplane, military plane',\n",
      " 896: 'washbasin, handbasin, washbowl, lavabo, wash-hand basin',\n",
      " 897: 'washer, automatic washer, washing machine',\n",
      " 898: 'water bottle',\n",
      " 899: 'water jug',\n",
      " 900: 'water tower',\n",
      " 901: 'whiskey jug',\n",
      " 902: 'whistle',\n",
      " 903: 'wig',\n",
      " 904: 'window screen',\n",
      " 905: 'window shade',\n",
      " 906: 'Windsor tie',\n",
      " 907: 'wine bottle',\n",
      " 908: 'wing',\n",
      " 909: 'wok',\n",
      " 910: 'wooden spoon',\n",
      " 911: 'wool, woolen, woollen',\n",
      " 912: 'worm fence, snake fence, snake-rail fence, Virginia fence',\n",
      " 913: 'wreck',\n",
      " 914: 'yawl',\n",
      " 915: 'yurt',\n",
      " 916: 'web site, website, internet site, site',\n",
      " 917: 'comic book',\n",
      " 918: 'crossword puzzle, crossword',\n",
      " 919: 'street sign',\n",
      " 920: 'traffic light, traffic signal, stoplight',\n",
      " 921: 'book jacket, dust cover, dust jacket, dust wrapper',\n",
      " 922: 'menu',\n",
      " 923: 'plate',\n",
      " 924: 'guacamole',\n",
      " 925: 'consomme',\n",
      " 926: 'hot pot, hotpot',\n",
      " 927: 'trifle',\n",
      " 928: 'ice cream, icecream',\n",
      " 929: 'ice lolly, lolly, lollipop, popsicle',\n",
      " 930: 'French loaf',\n",
      " 931: 'bagel, beigel',\n",
      " 932: 'pretzel',\n",
      " 933: 'cheeseburger',\n",
      " 934: 'hotdog, hot dog, red hot',\n",
      " 935: 'mashed potato',\n",
      " 936: 'head cabbage',\n",
      " 937: 'broccoli',\n",
      " 938: 'cauliflower',\n",
      " 939: 'zucchini, courgette',\n",
      " 940: 'spaghetti squash',\n",
      " 941: 'acorn squash',\n",
      " 942: 'butternut squash',\n",
      " 943: 'cucumber, cuke',\n",
      " 944: 'artichoke, globe artichoke',\n",
      " 945: 'bell pepper',\n",
      " 946: 'cardoon',\n",
      " 947: 'mushroom',\n",
      " 948: 'Granny Smith',\n",
      " 949: 'strawberry',\n",
      " 950: 'orange',\n",
      " 951: 'lemon',\n",
      " 952: 'fig',\n",
      " 953: 'pineapple, ananas',\n",
      " 954: 'banana',\n",
      " 955: 'jackfruit, jak, jack',\n",
      " 956: 'custard apple',\n",
      " 957: 'pomegranate',\n",
      " 958: 'hay',\n",
      " 959: 'carbonara',\n",
      " 960: 'chocolate sauce, chocolate syrup',\n",
      " 961: 'dough',\n",
      " 962: 'meat loaf, meatloaf',\n",
      " 963: 'pizza, pizza pie',\n",
      " 964: 'potpie',\n",
      " 965: 'burrito',\n",
      " 966: 'red wine',\n",
      " 967: 'espresso',\n",
      " 968: 'cup',\n",
      " 969: 'eggnog',\n",
      " 970: 'alp',\n",
      " 971: 'bubble',\n",
      " 972: 'cliff, drop, drop-off',\n",
      " 973: 'coral reef',\n",
      " 974: 'geyser',\n",
      " 975: 'lakeside, lakeshore',\n",
      " 976: 'promontory, headland, head, foreland',\n",
      " 977: 'sandbar, sand bar',\n",
      " 978: 'seashore, coast, seacoast, sea-coast',\n",
      " 979: 'valley, vale',\n",
      " 980: 'volcano',\n",
      " 981: 'ballplayer, baseball player',\n",
      " 982: 'groom, bridegroom',\n",
      " 983: 'scuba diver',\n",
      " 984: 'rapeseed',\n",
      " 985: 'daisy',\n",
      " 986: \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\",\n",
      " 987: 'corn',\n",
      " 988: 'acorn',\n",
      " 989: 'hip, rose hip, rosehip',\n",
      " 990: 'buckeye, horse chestnut, conker',\n",
      " 991: 'coral fungus',\n",
      " 992: 'agaric',\n",
      " 993: 'gyromitra',\n",
      " 994: 'stinkhorn, carrion fungus',\n",
      " 995: 'earthstar',\n",
      " 996: 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa',\n",
      " 997: 'bolete',\n",
      " 998: 'ear, spike, capitulum',\n",
      " 999: 'toilet tissue, toilet paper, bathroom tissue'}\n",
      "24/3:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 224\n",
      "\n",
      "def prep_image(image):\n",
      "    res = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "    res = res.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "    return res\n",
      "\n",
      "def predict(image):\n",
      "    cropped_image = center_crop(image)\n",
      "    prediction = (model.predict(prep_image(image)), model.predict(prep_image(cropped_image)))\n",
      "    if np.max(prediction[0]>=prediction[1]):\n",
      "        return np.argmax(prediction[0], axis=1)[0]\n",
      "    else:\n",
      "        return np.argmax(prediction[1], axis=1)[1]\n",
      "    \n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "plt.imshow(image)\n",
      "\n",
      "print(class_dict[predict(image)])\n",
      "24/4:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "def visualize_colors(cluster, centroids):\n",
      "    # Get the number of different clusters, create histogram, and normalize\n",
      "    labels = np.arange(0, len(np.unique(cluster.labels_)) + 1)\n",
      "    (hist, _) = np.histogram(cluster.labels_, bins = labels)\n",
      "    hist = hist.astype(\"float\")\n",
      "    hist /= hist.sum()\n",
      "\n",
      "    # Create frequency rect and iterate through each cluster's color and percentage\n",
      "    rect = np.zeros((50, 300, 3), dtype=np.uint8)\n",
      "    colors = sorted([(percent, color) for (percent, color) in zip(hist, centroids)])\n",
      "    start = 0\n",
      "    for (percent, color) in colors:\n",
      "        print(color, \"{:0.2f}%\".format(percent * 100))\n",
      "        end = start + (percent * 300)\n",
      "        cv2.rectangle(rect, (int(start), 0), (int(end), 50), \\\n",
      "                      color.astype(\"uint8\").tolist(), -1)\n",
      "        start = end\n",
      "    return rect\n",
      "\n",
      "image = cv2.imread('../data/images/l31llr.png')\n",
      "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "\n",
      "# Find and display most dominant colors\n",
      "cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "visualize = visualize_colors(cluster, cluster.cluster_centers_)\n",
      "plt.imshow(visualize)\n",
      "\n",
      "# result = {}\n",
      "# for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "#     if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "#         print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "#         image = cv2.imread(\"../data/images/\" + file)\n",
      "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "#         reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "#         cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "#         print(cluster.cluster_centers_)\n",
      "#         result[file] = cluster.cluster_centers_\n",
      "        \n",
      "# pickle.dump(result, open( \"../pickle/image_color_clusters.pkl\", \"wb\" ))\n",
      "24/5:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "def visualize_colors(cluster, centroids):\n",
      "    # Get the number of different clusters, create histogram, and normalize\n",
      "    labels = np.arange(0, len(np.unique(cluster.labels_)) + 1)\n",
      "    (hist, _) = np.histogram(cluster.labels_, bins = labels)\n",
      "    hist = hist.astype(\"float\")\n",
      "    hist /= hist.sum()\n",
      "\n",
      "    # Create frequency rect and iterate through each cluster's color and percentage\n",
      "    rect = np.zeros((50, 300, 3), dtype=np.uint8)\n",
      "    colors = sorted([(percent, color) for (percent, color) in zip(hist, centroids)])\n",
      "    start = 0\n",
      "    for (percent, color) in colors:\n",
      "        print(color, \"{:0.2f}%\".format(percent * 100))\n",
      "        end = start + (percent * 300)\n",
      "        cv2.rectangle(rect, (int(start), 0), (int(end), 50), \\\n",
      "                      color.astype(\"uint8\").tolist(), -1)\n",
      "        start = end\n",
      "    return rect\n",
      "\n",
      "image = cv2.imread('../data/images/l31llr.png')\n",
      "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "\n",
      "# Find and display most dominant colors\n",
      "cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "visualize = visualize_colors(cluster, cluster.cluster_centers_)\n",
      "\n",
      "plt.imshow(image)\n",
      "plt.imshow(visualize)\n",
      "\n",
      "# result = {}\n",
      "# for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "#     if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "#         print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "#         image = cv2.imread(\"../data/images/\" + file)\n",
      "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "#         reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "#         cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "#         print(cluster.cluster_centers_)\n",
      "#         result[file] = cluster.cluster_centers_\n",
      "        \n",
      "# pickle.dump(result, open( \"../pickle/image_color_clusters.pkl\", \"wb\" ))\n",
      "24/6:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "def visualize_colors(cluster, centroids):\n",
      "    # Get the number of different clusters, create histogram, and normalize\n",
      "    labels = np.arange(0, len(np.unique(cluster.labels_)) + 1)\n",
      "    (hist, _) = np.histogram(cluster.labels_, bins = labels)\n",
      "    hist = hist.astype(\"float\")\n",
      "    hist /= hist.sum()\n",
      "\n",
      "    # Create frequency rect and iterate through each cluster's color and percentage\n",
      "    rect = np.zeros((50, 300, 3), dtype=np.uint8)\n",
      "    colors = sorted([(percent, color) for (percent, color) in zip(hist, centroids)])\n",
      "    start = 0\n",
      "    for (percent, color) in colors:\n",
      "        print(color, \"{:0.2f}%\".format(percent * 100))\n",
      "        end = start + (percent * 300)\n",
      "        cv2.rectangle(rect, (int(start), 0), (int(end), 50), \\\n",
      "                      color.astype(\"uint8\").tolist(), -1)\n",
      "        start = end\n",
      "    return rect\n",
      "\n",
      "image = cv2.imread('../data/images/l31llr.png')\n",
      "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "\n",
      "# Find and display most dominant colors\n",
      "cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "visualize = visualize_colors(cluster, cluster.cluster_centers_)\n",
      "\n",
      "plt.imshow(image)\n",
      "plt.imshow(visualize)\n",
      "\n",
      "# result = {}\n",
      "# for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "#     if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "#         print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "#         image = cv2.imread(\"../data/images/\" + file)\n",
      "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "#         reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "#         cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "#         print(cluster.cluster_centers_)\n",
      "#         result[file] = cluster.cluster_centers_\n",
      "        \n",
      "# pickle.dump(result, open( \"../pickle/image_color_clusters.pkl\", \"wb\" ))\n",
      "24/7:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "def visualize_colors(cluster, centroids):\n",
      "    # Get the number of different clusters, create histogram, and normalize\n",
      "    labels = np.arange(0, len(np.unique(cluster.labels_)) + 1)\n",
      "    (hist, _) = np.histogram(cluster.labels_, bins = labels)\n",
      "    hist = hist.astype(\"float\")\n",
      "    hist /= hist.sum()\n",
      "\n",
      "    # Create frequency rect and iterate through each cluster's color and percentage\n",
      "    rect = np.zeros((50, 300, 3), dtype=np.uint8)\n",
      "    colors = sorted([(percent, color) for (percent, color) in zip(hist, centroids)])\n",
      "    start = 0\n",
      "    for (percent, color) in colors:\n",
      "        print(color, \"{:0.2f}%\".format(percent * 100))\n",
      "        end = start + (percent * 300)\n",
      "        cv2.rectangle(rect, (int(start), 0), (int(end), 50), \\\n",
      "                      color.astype(\"uint8\").tolist(), -1)\n",
      "        start = end\n",
      "    return rect\n",
      "\n",
      "image = cv2.imread('../data/images/l31llr.png')\n",
      "plt.imshow(image)\n",
      "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "\n",
      "# Find and display most dominant colors\n",
      "cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "visualize = visualize_colors(cluster, cluster.cluster_centers_)\n",
      "\n",
      "plt.imshow(visualize)\n",
      "\n",
      "# result = {}\n",
      "# for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "#     if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "#         print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "#         image = cv2.imread(\"../data/images/\" + file)\n",
      "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "#         reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "#         cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "#         print(cluster.cluster_centers_)\n",
      "#         result[file] = cluster.cluster_centers_\n",
      "        \n",
      "# pickle.dump(result, open( \"../pickle/image_color_clusters.pkl\", \"wb\" ))\n",
      "24/8:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "def visualize_colors(cluster, centroids):\n",
      "    # Get the number of different clusters, create histogram, and normalize\n",
      "    labels = np.arange(0, len(np.unique(cluster.labels_)) + 1)\n",
      "    (hist, _) = np.histogram(cluster.labels_, bins = labels)\n",
      "    hist = hist.astype(\"float\")\n",
      "    hist /= hist.sum()\n",
      "\n",
      "    # Create frequency rect and iterate through each cluster's color and percentage\n",
      "    rect = np.zeros((50, 300, 3), dtype=np.uint8)\n",
      "    colors = sorted([(percent, color) for (percent, color) in zip(hist, centroids)])\n",
      "    start = 0\n",
      "    for (percent, color) in colors:\n",
      "        print(color, \"{:0.2f}%\".format(percent * 100))\n",
      "        end = start + (percent * 300)\n",
      "        cv2.rectangle(rect, (int(start), 0), (int(end), 50), \\\n",
      "                      color.astype(\"uint8\").tolist(), -1)\n",
      "        start = end\n",
      "    return rect\n",
      "\n",
      "image = cv2.imread('../data/images/l31llr.png')\n",
      "plt.imshow(image)\n",
      "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "\n",
      "# Find and display most dominant colors\n",
      "cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "visualize = visualize_colors(cluster, cluster.cluster_centers_)\n",
      "\n",
      "#plt.imshow(visualize)\n",
      "\n",
      "# result = {}\n",
      "# for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "#     if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "#         print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "#         image = cv2.imread(\"../data/images/\" + file)\n",
      "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "#         reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "#         cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "#         print(cluster.cluster_centers_)\n",
      "#         result[file] = cluster.cluster_centers_\n",
      "        \n",
      "# pickle.dump(result, open( \"../pickle/image_color_clusters.pkl\", \"wb\" ))\n",
      "24/9:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "def visualize_colors(cluster, centroids):\n",
      "    # Get the number of different clusters, create histogram, and normalize\n",
      "    labels = np.arange(0, len(np.unique(cluster.labels_)) + 1)\n",
      "    (hist, _) = np.histogram(cluster.labels_, bins = labels)\n",
      "    hist = hist.astype(\"float\")\n",
      "    hist /= hist.sum()\n",
      "\n",
      "    # Create frequency rect and iterate through each cluster's color and percentage\n",
      "    rect = np.zeros((50, 300, 3), dtype=np.uint8)\n",
      "    colors = sorted([(percent, color) for (percent, color) in zip(hist, centroids)])\n",
      "    start = 0\n",
      "    for (percent, color) in colors:\n",
      "        print(color, \"{:0.2f}%\".format(percent * 100))\n",
      "        end = start + (percent * 300)\n",
      "        cv2.rectangle(rect, (int(start), 0), (int(end), 50), \\\n",
      "                      color.astype(\"uint8\").tolist(), -1)\n",
      "        start = end\n",
      "    return rect\n",
      "\n",
      "image = cv2.imread('../data/images/l31llr.png')\n",
      "plt.imshow(image)\n",
      "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "\n",
      "# Find and display most dominant colors\n",
      "cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "visualize = visualize_colors(cluster, cluster.cluster_centers_)\n",
      "\n",
      "plt.imshow(visualize)\n",
      "\n",
      "# result = {}\n",
      "# for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "#     if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "#         print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "#         image = cv2.imread(\"../data/images/\" + file)\n",
      "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "#         reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "#         cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "#         print(cluster.cluster_centers_)\n",
      "#         result[file] = cluster.cluster_centers_\n",
      "        \n",
      "# pickle.dump(result, open( \"../pickle/image_color_clusters.pkl\", \"wb\" ))\n",
      "24/10:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "def visualize_colors(cluster, centroids):\n",
      "    # Get the number of different clusters, create histogram, and normalize\n",
      "    labels = np.arange(0, len(np.unique(cluster.labels_)) + 1)\n",
      "    (hist, _) = np.histogram(cluster.labels_, bins = labels)\n",
      "    hist = hist.astype(\"float\")\n",
      "    hist /= hist.sum()\n",
      "\n",
      "    # Create frequency rect and iterate through each cluster's color and percentage\n",
      "    rect = np.zeros((50, 300, 3), dtype=np.uint8)\n",
      "    colors = sorted([(percent, color) for (percent, color) in zip(hist, centroids)])\n",
      "    start = 0\n",
      "    for (percent, color) in colors:\n",
      "        print(color, \"{:0.2f}%\".format(percent * 100))\n",
      "        end = start + (percent * 300)\n",
      "        cv2.rectangle(rect, (int(start), 0), (int(end), 50), \\\n",
      "                      color.astype(\"uint8\").tolist(), -1)\n",
      "        start = end\n",
      "    return rect\n",
      "\n",
      "image = cv2.imread('../data/images/l31llr.png')\n",
      "plt.imshow(image)\n",
      "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "\n",
      "# Find and display most dominant colors\n",
      "cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "visualize = visualize_colors(cluster, cluster.cluster_centers_)\n",
      "\n",
      "plt.imshow(visualize)\n",
      "\n",
      "plt.show()\n",
      "\n",
      "# result = {}\n",
      "# for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "#     if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "#         print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "#         image = cv2.imread(\"../data/images/\" + file)\n",
      "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "#         reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "#         cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "#         print(cluster.cluster_centers_)\n",
      "#         result[file] = cluster.cluster_centers_\n",
      "        \n",
      "# pickle.dump(result, open( \"../pickle/image_color_clusters.pkl\", \"wb\" ))\n",
      "24/11:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "def visualize_colors(cluster, centroids):\n",
      "    # Get the number of different clusters, create histogram, and normalize\n",
      "    labels = np.arange(0, len(np.unique(cluster.labels_)) + 1)\n",
      "    (hist, _) = np.histogram(cluster.labels_, bins = labels)\n",
      "    hist = hist.astype(\"float\")\n",
      "    hist /= hist.sum()\n",
      "\n",
      "    # Create frequency rect and iterate through each cluster's color and percentage\n",
      "    rect = np.zeros((50, 300, 3), dtype=np.uint8)\n",
      "    colors = sorted([(percent, color) for (percent, color) in zip(hist, centroids)])\n",
      "    start = 0\n",
      "    for (percent, color) in colors:\n",
      "        print(color, \"{:0.2f}%\".format(percent * 100))\n",
      "        end = start + (percent * 300)\n",
      "        cv2.rectangle(rect, (int(start), 0), (int(end), 50), \\\n",
      "                      color.astype(\"uint8\").tolist(), -1)\n",
      "        start = end\n",
      "    return rect\n",
      "\n",
      "image = cv2.imread('../data/images/l31llr.png')\n",
      "\n",
      "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "\n",
      "# Find and display most dominant colors\n",
      "cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "visualize = visualize_colors(cluster, cluster.cluster_centers_)\n",
      "\n",
      "plt.imshow(visualize, image)\n",
      "\n",
      "# result = {}\n",
      "# for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "#     if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "#         print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "#         image = cv2.imread(\"../data/images/\" + file)\n",
      "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "#         reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "#         cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "#         print(cluster.cluster_centers_)\n",
      "#         result[file] = cluster.cluster_centers_\n",
      "        \n",
      "# pickle.dump(result, open( \"../pickle/image_color_clusters.pkl\", \"wb\" ))\n",
      "24/12:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "def visualize_colors(cluster, centroids):\n",
      "    # Get the number of different clusters, create histogram, and normalize\n",
      "    labels = np.arange(0, len(np.unique(cluster.labels_)) + 1)\n",
      "    (hist, _) = np.histogram(cluster.labels_, bins = labels)\n",
      "    hist = hist.astype(\"float\")\n",
      "    hist /= hist.sum()\n",
      "\n",
      "    # Create frequency rect and iterate through each cluster's color and percentage\n",
      "    rect = np.zeros((50, 300, 3), dtype=np.uint8)\n",
      "    colors = sorted([(percent, color) for (percent, color) in zip(hist, centroids)])\n",
      "    start = 0\n",
      "    for (percent, color) in colors:\n",
      "        print(color, \"{:0.2f}%\".format(percent * 100))\n",
      "        end = start + (percent * 300)\n",
      "        cv2.rectangle(rect, (int(start), 0), (int(end), 50), \\\n",
      "                      color.astype(\"uint8\").tolist(), -1)\n",
      "        start = end\n",
      "    return rect\n",
      "\n",
      "image = cv2.imread('../data/images/l31llr.png')\n",
      "plt.figure()\n",
      "plt.imshow(image)\n",
      "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "\n",
      "# Find and display most dominant colors\n",
      "cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "visualize = visualize_colors(cluster, cluster.cluster_centers_)\n",
      "plt.figure()\n",
      "plt.imshow(visualize)\n",
      "\n",
      "# result = {}\n",
      "# for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "#     if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "#         print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "#         image = cv2.imread(\"../data/images/\" + file)\n",
      "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "#         reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "#         cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "#         print(cluster.cluster_centers_)\n",
      "#         result[file] = cluster.cluster_centers_\n",
      "        \n",
      "# pickle.dump(result, open( \"../pickle/image_color_clusters.pkl\", \"wb\" ))\n",
      "24/13:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "def visualize_colors(cluster, centroids):\n",
      "    # Get the number of different clusters, create histogram, and normalize\n",
      "    labels = np.arange(0, len(np.unique(cluster.labels_)) + 1)\n",
      "    (hist, _) = np.histogram(cluster.labels_, bins = labels)\n",
      "    hist = hist.astype(\"float\")\n",
      "    hist /= hist.sum()\n",
      "\n",
      "    # Create frequency rect and iterate through each cluster's color and percentage\n",
      "    rect = np.zeros((50, 300, 3), dtype=np.uint8)\n",
      "    colors = sorted([(percent, color) for (percent, color) in zip(hist, centroids)])\n",
      "    start = 0\n",
      "    for (percent, color) in colors:\n",
      "        print(color, \"{:0.2f}%\".format(percent * 100))\n",
      "        end = start + (percent * 300)\n",
      "        cv2.rectangle(rect, (int(start), 0), (int(end), 50), \\\n",
      "                      color.astype(\"uint8\").tolist(), -1)\n",
      "        start = end\n",
      "    return rect\n",
      "\n",
      "image = cv2.imread('../data/images/l35vsy.png')\n",
      "plt.figure()\n",
      "plt.imshow(image)\n",
      "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "visualize = visualize_colors(cluster, cluster.cluster_centers_)\n",
      "plt.figure()\n",
      "plt.imshow(visualize)\n",
      "\n",
      "image = cv2.imread('../data/images/l35vsy.png')\n",
      "plt.figure()\n",
      "plt.imshow(image)\n",
      "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "visualize = visualize_colors(cluster, cluster.cluster_centers_)\n",
      "plt.figure()\n",
      "plt.imshow(visualize)\n",
      "\n",
      "# result = {}\n",
      "# for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "#     if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "#         print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "#         image = cv2.imread(\"../data/images/\" + file)\n",
      "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "#         reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "#         cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "#         print(cluster.cluster_centers_)\n",
      "#         result[file] = cluster.cluster_centers_\n",
      "        \n",
      "# pickle.dump(result, open( \"../pickle/image_color_clusters.pkl\", \"wb\" ))\n",
      "24/14:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "def visualize_colors(cluster, centroids):\n",
      "    # Get the number of different clusters, create histogram, and normalize\n",
      "    labels = np.arange(0, len(np.unique(cluster.labels_)) + 1)\n",
      "    (hist, _) = np.histogram(cluster.labels_, bins = labels)\n",
      "    hist = hist.astype(\"float\")\n",
      "    hist /= hist.sum()\n",
      "\n",
      "    # Create frequency rect and iterate through each cluster's color and percentage\n",
      "    rect = np.zeros((50, 300, 3), dtype=np.uint8)\n",
      "    colors = sorted([(percent, color) for (percent, color) in zip(hist, centroids)])\n",
      "    start = 0\n",
      "    for (percent, color) in colors:\n",
      "        print(color, \"{:0.2f}%\".format(percent * 100))\n",
      "        end = start + (percent * 300)\n",
      "        cv2.rectangle(rect, (int(start), 0), (int(end), 50), \\\n",
      "                      color.astype(\"uint8\").tolist(), -1)\n",
      "        start = end\n",
      "    return rect\n",
      "\n",
      "image = cv2.imread('../data/images/l35vsy.png')\n",
      "plt.figure()\n",
      "plt.imshow(image)\n",
      "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "visualize = visualize_colors(cluster, cluster.cluster_centers_)\n",
      "plt.figure()\n",
      "plt.imshow(visualize)\n",
      "\n",
      "image = cv2.imread('../data/images/l31llr.png')\n",
      "plt.figure()\n",
      "plt.imshow(image)\n",
      "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "visualize = visualize_colors(cluster, cluster.cluster_centers_)\n",
      "plt.figure()\n",
      "plt.imshow(visualize)\n",
      "\n",
      "# result = {}\n",
      "# for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "#     if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "#         print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "#         image = cv2.imread(\"../data/images/\" + file)\n",
      "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "#         reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "#         cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "#         print(cluster.cluster_centers_)\n",
      "#         result[file] = cluster.cluster_centers_\n",
      "        \n",
      "# pickle.dump(result, open( \"../pickle/image_color_clusters.pkl\", \"wb\" ))\n",
      "25/1:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 224\n",
      "\n",
      "def prep_image(image):\n",
      "    res = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "    res = res.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "    return res\n",
      "\n",
      "def predict(image):\n",
      "    cropped_image = center_crop(image)\n",
      "    prediction = (model.predict(prep_image(image)), model.predict(prep_image(cropped_image)))\n",
      "    if np.max(prediction[0]>=prediction[1]):\n",
      "        return np.argmax(prediction[0], axis=1)[0]\n",
      "    else:\n",
      "        return np.argmax(prediction[1], axis=1)[1]\n",
      "    \n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "plt.imshow(image)\n",
      "\n",
      "print(class_dict[predict(image)])\n",
      "25/2:\n",
      "from tensorflow.keras.applications import EfficientNetB0\n",
      "from tensorflow.image import resize\n",
      "model = EfficientNetB0(weights='imagenet')\n",
      "\n",
      "def center_crop(img, new_width=None, new_height=None):        \n",
      "\n",
      "    width = img.shape[1]\n",
      "    height = img.shape[0]\n",
      "\n",
      "    if new_width is None:\n",
      "        new_width = min(width, height)\n",
      "\n",
      "    if new_height is None:\n",
      "        new_height = min(width, height)\n",
      "\n",
      "    left = int(np.ceil((width - new_width) / 2))\n",
      "    right = width - int(np.floor((width - new_width) / 2))\n",
      "\n",
      "    top = int(np.ceil((height - new_height) / 2))\n",
      "    bottom = height - int(np.floor((height - new_height) / 2))\n",
      "\n",
      "    if len(img.shape) == 2:\n",
      "        center_cropped_img = img[top:bottom, left:right]\n",
      "    else:\n",
      "        center_cropped_img = img[top:bottom, left:right, ...]\n",
      "\n",
      "    return center_cropped_img\n",
      "\n",
      "class_dict = {0: 'tench, Tinca tinca',\n",
      " 1: 'goldfish, Carassius auratus',\n",
      " 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n",
      " 3: 'tiger shark, Galeocerdo cuvieri',\n",
      " 4: 'hammerhead, hammerhead shark',\n",
      " 5: 'electric ray, crampfish, numbfish, torpedo',\n",
      " 6: 'stingray',\n",
      " 7: 'cock',\n",
      " 8: 'hen',\n",
      " 9: 'ostrich, Struthio camelus',\n",
      " 10: 'brambling, Fringilla montifringilla',\n",
      " 11: 'goldfinch, Carduelis carduelis',\n",
      " 12: 'house finch, linnet, Carpodacus mexicanus',\n",
      " 13: 'junco, snowbird',\n",
      " 14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea',\n",
      " 15: 'robin, American robin, Turdus migratorius',\n",
      " 16: 'bulbul',\n",
      " 17: 'jay',\n",
      " 18: 'magpie',\n",
      " 19: 'chickadee',\n",
      " 20: 'water ouzel, dipper',\n",
      " 21: 'kite',\n",
      " 22: 'bald eagle, American eagle, Haliaeetus leucocephalus',\n",
      " 23: 'vulture',\n",
      " 24: 'great grey owl, great gray owl, Strix nebulosa',\n",
      " 25: 'European fire salamander, Salamandra salamandra',\n",
      " 26: 'common newt, Triturus vulgaris',\n",
      " 27: 'eft',\n",
      " 28: 'spotted salamander, Ambystoma maculatum',\n",
      " 29: 'axolotl, mud puppy, Ambystoma mexicanum',\n",
      " 30: 'bullfrog, Rana catesbeiana',\n",
      " 31: 'tree frog, tree-frog',\n",
      " 32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui',\n",
      " 33: 'loggerhead, loggerhead turtle, Caretta caretta',\n",
      " 34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea',\n",
      " 35: 'mud turtle',\n",
      " 36: 'terrapin',\n",
      " 37: 'box turtle, box tortoise',\n",
      " 38: 'banded gecko',\n",
      " 39: 'common iguana, iguana, Iguana iguana',\n",
      " 40: 'American chameleon, anole, Anolis carolinensis',\n",
      " 41: 'whiptail, whiptail lizard',\n",
      " 42: 'agama',\n",
      " 43: 'frilled lizard, Chlamydosaurus kingi',\n",
      " 44: 'alligator lizard',\n",
      " 45: 'Gila monster, Heloderma suspectum',\n",
      " 46: 'green lizard, Lacerta viridis',\n",
      " 47: 'African chameleon, Chamaeleo chamaeleon',\n",
      " 48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis',\n",
      " 49: 'African crocodile, Nile crocodile, Crocodylus niloticus',\n",
      " 50: 'American alligator, Alligator mississipiensis',\n",
      " 51: 'triceratops',\n",
      " 52: 'thunder snake, worm snake, Carphophis amoenus',\n",
      " 53: 'ringneck snake, ring-necked snake, ring snake',\n",
      " 54: 'hognose snake, puff adder, sand viper',\n",
      " 55: 'green snake, grass snake',\n",
      " 56: 'king snake, kingsnake',\n",
      " 57: 'garter snake, grass snake',\n",
      " 58: 'water snake',\n",
      " 59: 'vine snake',\n",
      " 60: 'night snake, Hypsiglena torquata',\n",
      " 61: 'boa constrictor, Constrictor constrictor',\n",
      " 62: 'rock python, rock snake, Python sebae',\n",
      " 63: 'Indian cobra, Naja naja',\n",
      " 64: 'green mamba',\n",
      " 65: 'sea snake',\n",
      " 66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus',\n",
      " 67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus',\n",
      " 68: 'sidewinder, horned rattlesnake, Crotalus cerastes',\n",
      " 69: 'trilobite',\n",
      " 70: 'harvestman, daddy longlegs, Phalangium opilio',\n",
      " 71: 'scorpion',\n",
      " 72: 'black and gold garden spider, Argiope aurantia',\n",
      " 73: 'barn spider, Araneus cavaticus',\n",
      " 74: 'garden spider, Aranea diademata',\n",
      " 75: 'black widow, Latrodectus mactans',\n",
      " 76: 'tarantula',\n",
      " 77: 'wolf spider, hunting spider',\n",
      " 78: 'tick',\n",
      " 79: 'centipede',\n",
      " 80: 'black grouse',\n",
      " 81: 'ptarmigan',\n",
      " 82: 'ruffed grouse, partridge, Bonasa umbellus',\n",
      " 83: 'prairie chicken, prairie grouse, prairie fowl',\n",
      " 84: 'peacock',\n",
      " 85: 'quail',\n",
      " 86: 'partridge',\n",
      " 87: 'African grey, African gray, Psittacus erithacus',\n",
      " 88: 'macaw',\n",
      " 89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n",
      " 90: 'lorikeet',\n",
      " 91: 'coucal',\n",
      " 92: 'bee eater',\n",
      " 93: 'hornbill',\n",
      " 94: 'hummingbird',\n",
      " 95: 'jacamar',\n",
      " 96: 'toucan',\n",
      " 97: 'drake',\n",
      " 98: 'red-breasted merganser, Mergus serrator',\n",
      " 99: 'goose',\n",
      " 100: 'black swan, Cygnus atratus',\n",
      " 101: 'tusker',\n",
      " 102: 'echidna, spiny anteater, anteater',\n",
      " 103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n",
      " 104: 'wallaby, brush kangaroo',\n",
      " 105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n",
      " 106: 'wombat',\n",
      " 107: 'jellyfish',\n",
      " 108: 'sea anemone, anemone',\n",
      " 109: 'brain coral',\n",
      " 110: 'flatworm, platyhelminth',\n",
      " 111: 'nematode, nematode worm, roundworm',\n",
      " 112: 'conch',\n",
      " 113: 'snail',\n",
      " 114: 'slug',\n",
      " 115: 'sea slug, nudibranch',\n",
      " 116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore',\n",
      " 117: 'chambered nautilus, pearly nautilus, nautilus',\n",
      " 118: 'Dungeness crab, Cancer magister',\n",
      " 119: 'rock crab, Cancer irroratus',\n",
      " 120: 'fiddler crab',\n",
      " 121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica',\n",
      " 122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus',\n",
      " 123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish',\n",
      " 124: 'crayfish, crawfish, crawdad, crawdaddy',\n",
      " 125: 'hermit crab',\n",
      " 126: 'isopod',\n",
      " 127: 'white stork, Ciconia ciconia',\n",
      " 128: 'black stork, Ciconia nigra',\n",
      " 129: 'spoonbill',\n",
      " 130: 'flamingo',\n",
      " 131: 'little blue heron, Egretta caerulea',\n",
      " 132: 'American egret, great white heron, Egretta albus',\n",
      " 133: 'bittern',\n",
      " 134: 'crane',\n",
      " 135: 'limpkin, Aramus pictus',\n",
      " 136: 'European gallinule, Porphyrio porphyrio',\n",
      " 137: 'American coot, marsh hen, mud hen, water hen, Fulica americana',\n",
      " 138: 'bustard',\n",
      " 139: 'ruddy turnstone, Arenaria interpres',\n",
      " 140: 'red-backed sandpiper, dunlin, Erolia alpina',\n",
      " 141: 'redshank, Tringa totanus',\n",
      " 142: 'dowitcher',\n",
      " 143: 'oystercatcher, oyster catcher',\n",
      " 144: 'pelican',\n",
      " 145: 'king penguin, Aptenodytes patagonica',\n",
      " 146: 'albatross, mollymawk',\n",
      " 147: 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus',\n",
      " 148: 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca',\n",
      " 149: 'dugong, Dugong dugon',\n",
      " 150: 'sea lion',\n",
      " 151: 'Chihuahua',\n",
      " 152: 'Japanese spaniel',\n",
      " 153: 'Maltese dog, Maltese terrier, Maltese',\n",
      " 154: 'Pekinese, Pekingese, Peke',\n",
      " 155: 'Shih-Tzu',\n",
      " 156: 'Blenheim spaniel',\n",
      " 157: 'papillon',\n",
      " 158: 'toy terrier',\n",
      " 159: 'Rhodesian ridgeback',\n",
      " 160: 'Afghan hound, Afghan',\n",
      " 161: 'basset, basset hound',\n",
      " 162: 'beagle',\n",
      " 163: 'bloodhound, sleuthhound',\n",
      " 164: 'bluetick',\n",
      " 165: 'black-and-tan coonhound',\n",
      " 166: 'Walker hound, Walker foxhound',\n",
      " 167: 'English foxhound',\n",
      " 168: 'redbone',\n",
      " 169: 'borzoi, Russian wolfhound',\n",
      " 170: 'Irish wolfhound',\n",
      " 171: 'Italian greyhound',\n",
      " 172: 'whippet',\n",
      " 173: 'Ibizan hound, Ibizan Podenco',\n",
      " 174: 'Norwegian elkhound, elkhound',\n",
      " 175: 'otterhound, otter hound',\n",
      " 176: 'Saluki, gazelle hound',\n",
      " 177: 'Scottish deerhound, deerhound',\n",
      " 178: 'Weimaraner',\n",
      " 179: 'Staffordshire bullterrier, Staffordshire bull terrier',\n",
      " 180: 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier',\n",
      " 181: 'Bedlington terrier',\n",
      " 182: 'Border terrier',\n",
      " 183: 'Kerry blue terrier',\n",
      " 184: 'Irish terrier',\n",
      " 185: 'Norfolk terrier',\n",
      " 186: 'Norwich terrier',\n",
      " 187: 'Yorkshire terrier',\n",
      " 188: 'wire-haired fox terrier',\n",
      " 189: 'Lakeland terrier',\n",
      " 190: 'Sealyham terrier, Sealyham',\n",
      " 191: 'Airedale, Airedale terrier',\n",
      " 192: 'cairn, cairn terrier',\n",
      " 193: 'Australian terrier',\n",
      " 194: 'Dandie Dinmont, Dandie Dinmont terrier',\n",
      " 195: 'Boston bull, Boston terrier',\n",
      " 196: 'miniature schnauzer',\n",
      " 197: 'giant schnauzer',\n",
      " 198: 'standard schnauzer',\n",
      " 199: 'Scotch terrier, Scottish terrier, Scottie',\n",
      " 200: 'Tibetan terrier, chrysanthemum dog',\n",
      " 201: 'silky terrier, Sydney silky',\n",
      " 202: 'soft-coated wheaten terrier',\n",
      " 203: 'West Highland white terrier',\n",
      " 204: 'Lhasa, Lhasa apso',\n",
      " 205: 'flat-coated retriever',\n",
      " 206: 'curly-coated retriever',\n",
      " 207: 'golden retriever',\n",
      " 208: 'Labrador retriever',\n",
      " 209: 'Chesapeake Bay retriever',\n",
      " 210: 'German short-haired pointer',\n",
      " 211: 'vizsla, Hungarian pointer',\n",
      " 212: 'English setter',\n",
      " 213: 'Irish setter, red setter',\n",
      " 214: 'Gordon setter',\n",
      " 215: 'Brittany spaniel',\n",
      " 216: 'clumber, clumber spaniel',\n",
      " 217: 'English springer, English springer spaniel',\n",
      " 218: 'Welsh springer spaniel',\n",
      " 219: 'cocker spaniel, English cocker spaniel, cocker',\n",
      " 220: 'Sussex spaniel',\n",
      " 221: 'Irish water spaniel',\n",
      " 222: 'kuvasz',\n",
      " 223: 'schipperke',\n",
      " 224: 'groenendael',\n",
      " 225: 'malinois',\n",
      " 226: 'briard',\n",
      " 227: 'kelpie',\n",
      " 228: 'komondor',\n",
      " 229: 'Old English sheepdog, bobtail',\n",
      " 230: 'Shetland sheepdog, Shetland sheep dog, Shetland',\n",
      " 231: 'collie',\n",
      " 232: 'Border collie',\n",
      " 233: 'Bouvier des Flandres, Bouviers des Flandres',\n",
      " 234: 'Rottweiler',\n",
      " 235: 'German shepherd, German shepherd dog, German police dog, alsatian',\n",
      " 236: 'Doberman, Doberman pinscher',\n",
      " 237: 'miniature pinscher',\n",
      " 238: 'Greater Swiss Mountain dog',\n",
      " 239: 'Bernese mountain dog',\n",
      " 240: 'Appenzeller',\n",
      " 241: 'EntleBucher',\n",
      " 242: 'boxer',\n",
      " 243: 'bull mastiff',\n",
      " 244: 'Tibetan mastiff',\n",
      " 245: 'French bulldog',\n",
      " 246: 'Great Dane',\n",
      " 247: 'Saint Bernard, St Bernard',\n",
      " 248: 'Eskimo dog, husky',\n",
      " 249: 'malamute, malemute, Alaskan malamute',\n",
      " 250: 'Siberian husky',\n",
      " 251: 'dalmatian, coach dog, carriage dog',\n",
      " 252: 'affenpinscher, monkey pinscher, monkey dog',\n",
      " 253: 'basenji',\n",
      " 254: 'pug, pug-dog',\n",
      " 255: 'Leonberg',\n",
      " 256: 'Newfoundland, Newfoundland dog',\n",
      " 257: 'Great Pyrenees',\n",
      " 258: 'Samoyed, Samoyede',\n",
      " 259: 'Pomeranian',\n",
      " 260: 'chow, chow chow',\n",
      " 261: 'keeshond',\n",
      " 262: 'Brabancon griffon',\n",
      " 263: 'Pembroke, Pembroke Welsh corgi',\n",
      " 264: 'Cardigan, Cardigan Welsh corgi',\n",
      " 265: 'toy poodle',\n",
      " 266: 'miniature poodle',\n",
      " 267: 'standard poodle',\n",
      " 268: 'Mexican hairless',\n",
      " 269: 'timber wolf, grey wolf, gray wolf, Canis lupus',\n",
      " 270: 'white wolf, Arctic wolf, Canis lupus tundrarum',\n",
      " 271: 'red wolf, maned wolf, Canis rufus, Canis niger',\n",
      " 272: 'coyote, prairie wolf, brush wolf, Canis latrans',\n",
      " 273: 'dingo, warrigal, warragal, Canis dingo',\n",
      " 274: 'dhole, Cuon alpinus',\n",
      " 275: 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus',\n",
      " 276: 'hyena, hyaena',\n",
      " 277: 'red fox, Vulpes vulpes',\n",
      " 278: 'kit fox, Vulpes macrotis',\n",
      " 279: 'Arctic fox, white fox, Alopex lagopus',\n",
      " 280: 'grey fox, gray fox, Urocyon cinereoargenteus',\n",
      " 281: 'tabby, tabby cat',\n",
      " 282: 'tiger cat',\n",
      " 283: 'Persian cat',\n",
      " 284: 'Siamese cat, Siamese',\n",
      " 285: 'Egyptian cat',\n",
      " 286: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n",
      " 287: 'lynx, catamount',\n",
      " 288: 'leopard, Panthera pardus',\n",
      " 289: 'snow leopard, ounce, Panthera uncia',\n",
      " 290: 'jaguar, panther, Panthera onca, Felis onca',\n",
      " 291: 'lion, king of beasts, Panthera leo',\n",
      " 292: 'tiger, Panthera tigris',\n",
      " 293: 'cheetah, chetah, Acinonyx jubatus',\n",
      " 294: 'brown bear, bruin, Ursus arctos',\n",
      " 295: 'American black bear, black bear, Ursus americanus, Euarctos americanus',\n",
      " 296: 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus',\n",
      " 297: 'sloth bear, Melursus ursinus, Ursus ursinus',\n",
      " 298: 'mongoose',\n",
      " 299: 'meerkat, mierkat',\n",
      " 300: 'tiger beetle',\n",
      " 301: 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle',\n",
      " 302: 'ground beetle, carabid beetle',\n",
      " 303: 'long-horned beetle, longicorn, longicorn beetle',\n",
      " 304: 'leaf beetle, chrysomelid',\n",
      " 305: 'dung beetle',\n",
      " 306: 'rhinoceros beetle',\n",
      " 307: 'weevil',\n",
      " 308: 'fly',\n",
      " 309: 'bee',\n",
      " 310: 'ant, emmet, pismire',\n",
      " 311: 'grasshopper, hopper',\n",
      " 312: 'cricket',\n",
      " 313: 'walking stick, walkingstick, stick insect',\n",
      " 314: 'cockroach, roach',\n",
      " 315: 'mantis, mantid',\n",
      " 316: 'cicada, cicala',\n",
      " 317: 'leafhopper',\n",
      " 318: 'lacewing, lacewing fly',\n",
      " 319: \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\",\n",
      " 320: 'damselfly',\n",
      " 321: 'admiral',\n",
      " 322: 'ringlet, ringlet butterfly',\n",
      " 323: 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus',\n",
      " 324: 'cabbage butterfly',\n",
      " 325: 'sulphur butterfly, sulfur butterfly',\n",
      " 326: 'lycaenid, lycaenid butterfly',\n",
      " 327: 'starfish, sea star',\n",
      " 328: 'sea urchin',\n",
      " 329: 'sea cucumber, holothurian',\n",
      " 330: 'wood rabbit, cottontail, cottontail rabbit',\n",
      " 331: 'hare',\n",
      " 332: 'Angora, Angora rabbit',\n",
      " 333: 'hamster',\n",
      " 334: 'porcupine, hedgehog',\n",
      " 335: 'fox squirrel, eastern fox squirrel, Sciurus niger',\n",
      " 336: 'marmot',\n",
      " 337: 'beaver',\n",
      " 338: 'guinea pig, Cavia cobaya',\n",
      " 339: 'sorrel',\n",
      " 340: 'zebra',\n",
      " 341: 'hog, pig, grunter, squealer, Sus scrofa',\n",
      " 342: 'wild boar, boar, Sus scrofa',\n",
      " 343: 'warthog',\n",
      " 344: 'hippopotamus, hippo, river horse, Hippopotamus amphibius',\n",
      " 345: 'ox',\n",
      " 346: 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis',\n",
      " 347: 'bison',\n",
      " 348: 'ram, tup',\n",
      " 349: 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis',\n",
      " 350: 'ibex, Capra ibex',\n",
      " 351: 'hartebeest',\n",
      " 352: 'impala, Aepyceros melampus',\n",
      " 353: 'gazelle',\n",
      " 354: 'Arabian camel, dromedary, Camelus dromedarius',\n",
      " 355: 'llama',\n",
      " 356: 'weasel',\n",
      " 357: 'mink',\n",
      " 358: 'polecat, fitch, foulmart, foumart, Mustela putorius',\n",
      " 359: 'black-footed ferret, ferret, Mustela nigripes',\n",
      " 360: 'otter',\n",
      " 361: 'skunk, polecat, wood pussy',\n",
      " 362: 'badger',\n",
      " 363: 'armadillo',\n",
      " 364: 'three-toed sloth, ai, Bradypus tridactylus',\n",
      " 365: 'orangutan, orang, orangutang, Pongo pygmaeus',\n",
      " 366: 'gorilla, Gorilla gorilla',\n",
      " 367: 'chimpanzee, chimp, Pan troglodytes',\n",
      " 368: 'gibbon, Hylobates lar',\n",
      " 369: 'siamang, Hylobates syndactylus, Symphalangus syndactylus',\n",
      " 370: 'guenon, guenon monkey',\n",
      " 371: 'patas, hussar monkey, Erythrocebus patas',\n",
      " 372: 'baboon',\n",
      " 373: 'macaque',\n",
      " 374: 'langur',\n",
      " 375: 'colobus, colobus monkey',\n",
      " 376: 'proboscis monkey, Nasalis larvatus',\n",
      " 377: 'marmoset',\n",
      " 378: 'capuchin, ringtail, Cebus capucinus',\n",
      " 379: 'howler monkey, howler',\n",
      " 380: 'titi, titi monkey',\n",
      " 381: 'spider monkey, Ateles geoffroyi',\n",
      " 382: 'squirrel monkey, Saimiri sciureus',\n",
      " 383: 'Madagascar cat, ring-tailed lemur, Lemur catta',\n",
      " 384: 'indri, indris, Indri indri, Indri brevicaudatus',\n",
      " 385: 'Indian elephant, Elephas maximus',\n",
      " 386: 'African elephant, Loxodonta africana',\n",
      " 387: 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens',\n",
      " 388: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca',\n",
      " 389: 'barracouta, snoek',\n",
      " 390: 'eel',\n",
      " 391: 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch',\n",
      " 392: 'rock beauty, Holocanthus tricolor',\n",
      " 393: 'anemone fish',\n",
      " 394: 'sturgeon',\n",
      " 395: 'gar, garfish, garpike, billfish, Lepisosteus osseus',\n",
      " 396: 'lionfish',\n",
      " 397: 'puffer, pufferfish, blowfish, globefish',\n",
      " 398: 'abacus',\n",
      " 399: 'abaya',\n",
      " 400: \"academic gown, academic robe, judge's robe\",\n",
      " 401: 'accordion, piano accordion, squeeze box',\n",
      " 402: 'acoustic guitar',\n",
      " 403: 'aircraft carrier, carrier, flattop, attack aircraft carrier',\n",
      " 404: 'airliner',\n",
      " 405: 'airship, dirigible',\n",
      " 406: 'altar',\n",
      " 407: 'ambulance',\n",
      " 408: 'amphibian, amphibious vehicle',\n",
      " 409: 'analog clock',\n",
      " 410: 'apiary, bee house',\n",
      " 411: 'apron',\n",
      " 412: 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin',\n",
      " 413: 'assault rifle, assault gun',\n",
      " 414: 'backpack, back pack, knapsack, packsack, rucksack, haversack',\n",
      " 415: 'bakery, bakeshop, bakehouse',\n",
      " 416: 'balance beam, beam',\n",
      " 417: 'balloon',\n",
      " 418: 'ballpoint, ballpoint pen, ballpen, Biro',\n",
      " 419: 'Band Aid',\n",
      " 420: 'banjo',\n",
      " 421: 'bannister, banister, balustrade, balusters, handrail',\n",
      " 422: 'barbell',\n",
      " 423: 'barber chair',\n",
      " 424: 'barbershop',\n",
      " 425: 'barn',\n",
      " 426: 'barometer',\n",
      " 427: 'barrel, cask',\n",
      " 428: 'barrow, garden cart, lawn cart, wheelbarrow',\n",
      " 429: 'baseball',\n",
      " 430: 'basketball',\n",
      " 431: 'bassinet',\n",
      " 432: 'bassoon',\n",
      " 433: 'bathing cap, swimming cap',\n",
      " 434: 'bath towel',\n",
      " 435: 'bathtub, bathing tub, bath, tub',\n",
      " 436: 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon',\n",
      " 437: 'beacon, lighthouse, beacon light, pharos',\n",
      " 438: 'beaker',\n",
      " 439: 'bearskin, busby, shako',\n",
      " 440: 'beer bottle',\n",
      " 441: 'beer glass',\n",
      " 442: 'bell cote, bell cot',\n",
      " 443: 'bib',\n",
      " 444: 'bicycle-built-for-two, tandem bicycle, tandem',\n",
      " 445: 'bikini, two-piece',\n",
      " 446: 'binder, ring-binder',\n",
      " 447: 'binoculars, field glasses, opera glasses',\n",
      " 448: 'birdhouse',\n",
      " 449: 'boathouse',\n",
      " 450: 'bobsled, bobsleigh, bob',\n",
      " 451: 'bolo tie, bolo, bola tie, bola',\n",
      " 452: 'bonnet, poke bonnet',\n",
      " 453: 'bookcase',\n",
      " 454: 'bookshop, bookstore, bookstall',\n",
      " 455: 'bottlecap',\n",
      " 456: 'bow',\n",
      " 457: 'bow tie, bow-tie, bowtie',\n",
      " 458: 'brass, memorial tablet, plaque',\n",
      " 459: 'brassiere, bra, bandeau',\n",
      " 460: 'breakwater, groin, groyne, mole, bulwark, seawall, jetty',\n",
      " 461: 'breastplate, aegis, egis',\n",
      " 462: 'broom',\n",
      " 463: 'bucket, pail',\n",
      " 464: 'buckle',\n",
      " 465: 'bulletproof vest',\n",
      " 466: 'bullet train, bullet',\n",
      " 467: 'butcher shop, meat market',\n",
      " 468: 'cab, hack, taxi, taxicab',\n",
      " 469: 'caldron, cauldron',\n",
      " 470: 'candle, taper, wax light',\n",
      " 471: 'cannon',\n",
      " 472: 'canoe',\n",
      " 473: 'can opener, tin opener',\n",
      " 474: 'cardigan',\n",
      " 475: 'car mirror',\n",
      " 476: 'carousel, carrousel, merry-go-round, roundabout, whirligig',\n",
      " 477: \"carpenter's kit, tool kit\",\n",
      " 478: 'carton',\n",
      " 479: 'car wheel',\n",
      " 480: 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM',\n",
      " 481: 'cassette',\n",
      " 482: 'cassette player',\n",
      " 483: 'castle',\n",
      " 484: 'catamaran',\n",
      " 485: 'CD player',\n",
      " 486: 'cello, violoncello',\n",
      " 487: 'cellular telephone, cellular phone, cellphone, cell, mobile phone',\n",
      " 488: 'chain',\n",
      " 489: 'chainlink fence',\n",
      " 490: 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour',\n",
      " 491: 'chain saw, chainsaw',\n",
      " 492: 'chest',\n",
      " 493: 'chiffonier, commode',\n",
      " 494: 'chime, bell, gong',\n",
      " 495: 'china cabinet, china closet',\n",
      " 496: 'Christmas stocking',\n",
      " 497: 'church, church building',\n",
      " 498: 'cinema, movie theater, movie theatre, movie house, picture palace',\n",
      " 499: 'cleaver, meat cleaver, chopper',\n",
      " 500: 'cliff dwelling',\n",
      " 501: 'cloak',\n",
      " 502: 'clog, geta, patten, sabot',\n",
      " 503: 'cocktail shaker',\n",
      " 504: 'coffee mug',\n",
      " 505: 'coffeepot',\n",
      " 506: 'coil, spiral, volute, whorl, helix',\n",
      " 507: 'combination lock',\n",
      " 508: 'computer keyboard, keypad',\n",
      " 509: 'confectionery, confectionary, candy store',\n",
      " 510: 'container ship, containership, container vessel',\n",
      " 511: 'convertible',\n",
      " 512: 'corkscrew, bottle screw',\n",
      " 513: 'cornet, horn, trumpet, trump',\n",
      " 514: 'cowboy boot',\n",
      " 515: 'cowboy hat, ten-gallon hat',\n",
      " 516: 'cradle',\n",
      " 517: 'crane',\n",
      " 518: 'crash helmet',\n",
      " 519: 'crate',\n",
      " 520: 'crib, cot',\n",
      " 521: 'Crock Pot',\n",
      " 522: 'croquet ball',\n",
      " 523: 'crutch',\n",
      " 524: 'cuirass',\n",
      " 525: 'dam, dike, dyke',\n",
      " 526: 'desk',\n",
      " 527: 'desktop computer',\n",
      " 528: 'dial telephone, dial phone',\n",
      " 529: 'diaper, nappy, napkin',\n",
      " 530: 'digital clock',\n",
      " 531: 'digital watch',\n",
      " 532: 'dining table, board',\n",
      " 533: 'dishrag, dishcloth',\n",
      " 534: 'dishwasher, dish washer, dishwashing machine',\n",
      " 535: 'disk brake, disc brake',\n",
      " 536: 'dock, dockage, docking facility',\n",
      " 537: 'dogsled, dog sled, dog sleigh',\n",
      " 538: 'dome',\n",
      " 539: 'doormat, welcome mat',\n",
      " 540: 'drilling platform, offshore rig',\n",
      " 541: 'drum, membranophone, tympan',\n",
      " 542: 'drumstick',\n",
      " 543: 'dumbbell',\n",
      " 544: 'Dutch oven',\n",
      " 545: 'electric fan, blower',\n",
      " 546: 'electric guitar',\n",
      " 547: 'electric locomotive',\n",
      " 548: 'entertainment center',\n",
      " 549: 'envelope',\n",
      " 550: 'espresso maker',\n",
      " 551: 'face powder',\n",
      " 552: 'feather boa, boa',\n",
      " 553: 'file, file cabinet, filing cabinet',\n",
      " 554: 'fireboat',\n",
      " 555: 'fire engine, fire truck',\n",
      " 556: 'fire screen, fireguard',\n",
      " 557: 'flagpole, flagstaff',\n",
      " 558: 'flute, transverse flute',\n",
      " 559: 'folding chair',\n",
      " 560: 'football helmet',\n",
      " 561: 'forklift',\n",
      " 562: 'fountain',\n",
      " 563: 'fountain pen',\n",
      " 564: 'four-poster',\n",
      " 565: 'freight car',\n",
      " 566: 'French horn, horn',\n",
      " 567: 'frying pan, frypan, skillet',\n",
      " 568: 'fur coat',\n",
      " 569: 'garbage truck, dustcart',\n",
      " 570: 'gasmask, respirator, gas helmet',\n",
      " 571: 'gas pump, gasoline pump, petrol pump, island dispenser',\n",
      " 572: 'goblet',\n",
      " 573: 'go-kart',\n",
      " 574: 'golf ball',\n",
      " 575: 'golfcart, golf cart',\n",
      " 576: 'gondola',\n",
      " 577: 'gong, tam-tam',\n",
      " 578: 'gown',\n",
      " 579: 'grand piano, grand',\n",
      " 580: 'greenhouse, nursery, glasshouse',\n",
      " 581: 'grille, radiator grille',\n",
      " 582: 'grocery store, grocery, food market, market',\n",
      " 583: 'guillotine',\n",
      " 584: 'hair slide',\n",
      " 585: 'hair spray',\n",
      " 586: 'half track',\n",
      " 587: 'hammer',\n",
      " 588: 'hamper',\n",
      " 589: 'hand blower, blow dryer, blow drier, hair dryer, hair drier',\n",
      " 590: 'hand-held computer, hand-held microcomputer',\n",
      " 591: 'handkerchief, hankie, hanky, hankey',\n",
      " 592: 'hard disc, hard disk, fixed disk',\n",
      " 593: 'harmonica, mouth organ, harp, mouth harp',\n",
      " 594: 'harp',\n",
      " 595: 'harvester, reaper',\n",
      " 596: 'hatchet',\n",
      " 597: 'holster',\n",
      " 598: 'home theater, home theatre',\n",
      " 599: 'honeycomb',\n",
      " 600: 'hook, claw',\n",
      " 601: 'hoopskirt, crinoline',\n",
      " 602: 'horizontal bar, high bar',\n",
      " 603: 'horse cart, horse-cart',\n",
      " 604: 'hourglass',\n",
      " 605: 'iPod',\n",
      " 606: 'iron, smoothing iron',\n",
      " 607: \"jack-o'-lantern\",\n",
      " 608: 'jean, blue jean, denim',\n",
      " 609: 'jeep, landrover',\n",
      " 610: 'jersey, T-shirt, tee shirt',\n",
      " 611: 'jigsaw puzzle',\n",
      " 612: 'jinrikisha, ricksha, rickshaw',\n",
      " 613: 'joystick',\n",
      " 614: 'kimono',\n",
      " 615: 'knee pad',\n",
      " 616: 'knot',\n",
      " 617: 'lab coat, laboratory coat',\n",
      " 618: 'ladle',\n",
      " 619: 'lampshade, lamp shade',\n",
      " 620: 'laptop, laptop computer',\n",
      " 621: 'lawn mower, mower',\n",
      " 622: 'lens cap, lens cover',\n",
      " 623: 'letter opener, paper knife, paperknife',\n",
      " 624: 'library',\n",
      " 625: 'lifeboat',\n",
      " 626: 'lighter, light, igniter, ignitor',\n",
      " 627: 'limousine, limo',\n",
      " 628: 'liner, ocean liner',\n",
      " 629: 'lipstick, lip rouge',\n",
      " 630: 'Loafer',\n",
      " 631: 'lotion',\n",
      " 632: 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system',\n",
      " 633: \"loupe, jeweler's loupe\",\n",
      " 634: 'lumbermill, sawmill',\n",
      " 635: 'magnetic compass',\n",
      " 636: 'mailbag, postbag',\n",
      " 637: 'mailbox, letter box',\n",
      " 638: 'maillot',\n",
      " 639: 'maillot, tank suit',\n",
      " 640: 'manhole cover',\n",
      " 641: 'maraca',\n",
      " 642: 'marimba, xylophone',\n",
      " 643: 'mask',\n",
      " 644: 'matchstick',\n",
      " 645: 'maypole',\n",
      " 646: 'maze, labyrinth',\n",
      " 647: 'measuring cup',\n",
      " 648: 'medicine chest, medicine cabinet',\n",
      " 649: 'megalith, megalithic structure',\n",
      " 650: 'microphone, mike',\n",
      " 651: 'microwave, microwave oven',\n",
      " 652: 'military uniform',\n",
      " 653: 'milk can',\n",
      " 654: 'minibus',\n",
      " 655: 'miniskirt, mini',\n",
      " 656: 'minivan',\n",
      " 657: 'missile',\n",
      " 658: 'mitten',\n",
      " 659: 'mixing bowl',\n",
      " 660: 'mobile home, manufactured home',\n",
      " 661: 'Model T',\n",
      " 662: 'modem',\n",
      " 663: 'monastery',\n",
      " 664: 'monitor',\n",
      " 665: 'moped',\n",
      " 666: 'mortar',\n",
      " 667: 'mortarboard',\n",
      " 668: 'mosque',\n",
      " 669: 'mosquito net',\n",
      " 670: 'motor scooter, scooter',\n",
      " 671: 'mountain bike, all-terrain bike, off-roader',\n",
      " 672: 'mountain tent',\n",
      " 673: 'mouse, computer mouse',\n",
      " 674: 'mousetrap',\n",
      " 675: 'moving van',\n",
      " 676: 'muzzle',\n",
      " 677: 'nail',\n",
      " 678: 'neck brace',\n",
      " 679: 'necklace',\n",
      " 680: 'nipple',\n",
      " 681: 'notebook, notebook computer',\n",
      " 682: 'obelisk',\n",
      " 683: 'oboe, hautboy, hautbois',\n",
      " 684: 'ocarina, sweet potato',\n",
      " 685: 'odometer, hodometer, mileometer, milometer',\n",
      " 686: 'oil filter',\n",
      " 687: 'organ, pipe organ',\n",
      " 688: 'oscilloscope, scope, cathode-ray oscilloscope, CRO',\n",
      " 689: 'overskirt',\n",
      " 690: 'oxcart',\n",
      " 691: 'oxygen mask',\n",
      " 692: 'packet',\n",
      " 693: 'paddle, boat paddle',\n",
      " 694: 'paddlewheel, paddle wheel',\n",
      " 695: 'padlock',\n",
      " 696: 'paintbrush',\n",
      " 697: \"pajama, pyjama, pj's, jammies\",\n",
      " 698: 'palace',\n",
      " 699: 'panpipe, pandean pipe, syrinx',\n",
      " 700: 'paper towel',\n",
      " 701: 'parachute, chute',\n",
      " 702: 'parallel bars, bars',\n",
      " 703: 'park bench',\n",
      " 704: 'parking meter',\n",
      " 705: 'passenger car, coach, carriage',\n",
      " 706: 'patio, terrace',\n",
      " 707: 'pay-phone, pay-station',\n",
      " 708: 'pedestal, plinth, footstall',\n",
      " 709: 'pencil box, pencil case',\n",
      " 710: 'pencil sharpener',\n",
      " 711: 'perfume, essence',\n",
      " 712: 'Petri dish',\n",
      " 713: 'photocopier',\n",
      " 714: 'pick, plectrum, plectron',\n",
      " 715: 'pickelhaube',\n",
      " 716: 'picket fence, paling',\n",
      " 717: 'pickup, pickup truck',\n",
      " 718: 'pier',\n",
      " 719: 'piggy bank, penny bank',\n",
      " 720: 'pill bottle',\n",
      " 721: 'pillow',\n",
      " 722: 'ping-pong ball',\n",
      " 723: 'pinwheel',\n",
      " 724: 'pirate, pirate ship',\n",
      " 725: 'pitcher, ewer',\n",
      " 726: \"plane, carpenter's plane, woodworking plane\",\n",
      " 727: 'planetarium',\n",
      " 728: 'plastic bag',\n",
      " 729: 'plate rack',\n",
      " 730: 'plow, plough',\n",
      " 731: \"plunger, plumber's helper\",\n",
      " 732: 'Polaroid camera, Polaroid Land camera',\n",
      " 733: 'pole',\n",
      " 734: 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria',\n",
      " 735: 'poncho',\n",
      " 736: 'pool table, billiard table, snooker table',\n",
      " 737: 'pop bottle, soda bottle',\n",
      " 738: 'pot, flowerpot',\n",
      " 739: \"potter's wheel\",\n",
      " 740: 'power drill',\n",
      " 741: 'prayer rug, prayer mat',\n",
      " 742: 'printer',\n",
      " 743: 'prison, prison house',\n",
      " 744: 'projectile, missile',\n",
      " 745: 'projector',\n",
      " 746: 'puck, hockey puck',\n",
      " 747: 'punching bag, punch bag, punching ball, punchball',\n",
      " 748: 'purse',\n",
      " 749: 'quill, quill pen',\n",
      " 750: 'quilt, comforter, comfort, puff',\n",
      " 751: 'racer, race car, racing car',\n",
      " 752: 'racket, racquet',\n",
      " 753: 'radiator',\n",
      " 754: 'radio, wireless',\n",
      " 755: 'radio telescope, radio reflector',\n",
      " 756: 'rain barrel',\n",
      " 757: 'recreational vehicle, RV, R.V.',\n",
      " 758: 'reel',\n",
      " 759: 'reflex camera',\n",
      " 760: 'refrigerator, icebox',\n",
      " 761: 'remote control, remote',\n",
      " 762: 'restaurant, eating house, eating place, eatery',\n",
      " 763: 'revolver, six-gun, six-shooter',\n",
      " 764: 'rifle',\n",
      " 765: 'rocking chair, rocker',\n",
      " 766: 'rotisserie',\n",
      " 767: 'rubber eraser, rubber, pencil eraser',\n",
      " 768: 'rugby ball',\n",
      " 769: 'rule, ruler',\n",
      " 770: 'running shoe',\n",
      " 771: 'safe',\n",
      " 772: 'safety pin',\n",
      " 773: 'saltshaker, salt shaker',\n",
      " 774: 'sandal',\n",
      " 775: 'sarong',\n",
      " 776: 'sax, saxophone',\n",
      " 777: 'scabbard',\n",
      " 778: 'scale, weighing machine',\n",
      " 779: 'school bus',\n",
      " 780: 'schooner',\n",
      " 781: 'scoreboard',\n",
      " 782: 'screen, CRT screen',\n",
      " 783: 'screw',\n",
      " 784: 'screwdriver',\n",
      " 785: 'seat belt, seatbelt',\n",
      " 786: 'sewing machine',\n",
      " 787: 'shield, buckler',\n",
      " 788: 'shoe shop, shoe-shop, shoe store',\n",
      " 789: 'shoji',\n",
      " 790: 'shopping basket',\n",
      " 791: 'shopping cart',\n",
      " 792: 'shovel',\n",
      " 793: 'shower cap',\n",
      " 794: 'shower curtain',\n",
      " 795: 'ski',\n",
      " 796: 'ski mask',\n",
      " 797: 'sleeping bag',\n",
      " 798: 'slide rule, slipstick',\n",
      " 799: 'sliding door',\n",
      " 800: 'slot, one-armed bandit',\n",
      " 801: 'snorkel',\n",
      " 802: 'snowmobile',\n",
      " 803: 'snowplow, snowplough',\n",
      " 804: 'soap dispenser',\n",
      " 805: 'soccer ball',\n",
      " 806: 'sock',\n",
      " 807: 'solar dish, solar collector, solar furnace',\n",
      " 808: 'sombrero',\n",
      " 809: 'soup bowl',\n",
      " 810: 'space bar',\n",
      " 811: 'space heater',\n",
      " 812: 'space shuttle',\n",
      " 813: 'spatula',\n",
      " 814: 'speedboat',\n",
      " 815: \"spider web, spider's web\",\n",
      " 816: 'spindle',\n",
      " 817: 'sports car, sport car',\n",
      " 818: 'spotlight, spot',\n",
      " 819: 'stage',\n",
      " 820: 'steam locomotive',\n",
      " 821: 'steel arch bridge',\n",
      " 822: 'steel drum',\n",
      " 823: 'stethoscope',\n",
      " 824: 'stole',\n",
      " 825: 'stone wall',\n",
      " 826: 'stopwatch, stop watch',\n",
      " 827: 'stove',\n",
      " 828: 'strainer',\n",
      " 829: 'streetcar, tram, tramcar, trolley, trolley car',\n",
      " 830: 'stretcher',\n",
      " 831: 'studio couch, day bed',\n",
      " 832: 'stupa, tope',\n",
      " 833: 'submarine, pigboat, sub, U-boat',\n",
      " 834: 'suit, suit of clothes',\n",
      " 835: 'sundial',\n",
      " 836: 'sunglass',\n",
      " 837: 'sunglasses, dark glasses, shades',\n",
      " 838: 'sunscreen, sunblock, sun blocker',\n",
      " 839: 'suspension bridge',\n",
      " 840: 'swab, swob, mop',\n",
      " 841: 'sweatshirt',\n",
      " 842: 'swimming trunks, bathing trunks',\n",
      " 843: 'swing',\n",
      " 844: 'switch, electric switch, electrical switch',\n",
      " 845: 'syringe',\n",
      " 846: 'table lamp',\n",
      " 847: 'tank, army tank, armored combat vehicle, armoured combat vehicle',\n",
      " 848: 'tape player',\n",
      " 849: 'teapot',\n",
      " 850: 'teddy, teddy bear',\n",
      " 851: 'television, television system',\n",
      " 852: 'tennis ball',\n",
      " 853: 'thatch, thatched roof',\n",
      " 854: 'theater curtain, theatre curtain',\n",
      " 855: 'thimble',\n",
      " 856: 'thresher, thrasher, threshing machine',\n",
      " 857: 'throne',\n",
      " 858: 'tile roof',\n",
      " 859: 'toaster',\n",
      " 860: 'tobacco shop, tobacconist shop, tobacconist',\n",
      " 861: 'toilet seat',\n",
      " 862: 'torch',\n",
      " 863: 'totem pole',\n",
      " 864: 'tow truck, tow car, wrecker',\n",
      " 865: 'toyshop',\n",
      " 866: 'tractor',\n",
      " 867: 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi',\n",
      " 868: 'tray',\n",
      " 869: 'trench coat',\n",
      " 870: 'tricycle, trike, velocipede',\n",
      " 871: 'trimaran',\n",
      " 872: 'tripod',\n",
      " 873: 'triumphal arch',\n",
      " 874: 'trolleybus, trolley coach, trackless trolley',\n",
      " 875: 'trombone',\n",
      " 876: 'tub, vat',\n",
      " 877: 'turnstile',\n",
      " 878: 'typewriter keyboard',\n",
      " 879: 'umbrella',\n",
      " 880: 'unicycle, monocycle',\n",
      " 881: 'upright, upright piano',\n",
      " 882: 'vacuum, vacuum cleaner',\n",
      " 883: 'vase',\n",
      " 884: 'vault',\n",
      " 885: 'velvet',\n",
      " 886: 'vending machine',\n",
      " 887: 'vestment',\n",
      " 888: 'viaduct',\n",
      " 889: 'violin, fiddle',\n",
      " 890: 'volleyball',\n",
      " 891: 'waffle iron',\n",
      " 892: 'wall clock',\n",
      " 893: 'wallet, billfold, notecase, pocketbook',\n",
      " 894: 'wardrobe, closet, press',\n",
      " 895: 'warplane, military plane',\n",
      " 896: 'washbasin, handbasin, washbowl, lavabo, wash-hand basin',\n",
      " 897: 'washer, automatic washer, washing machine',\n",
      " 898: 'water bottle',\n",
      " 899: 'water jug',\n",
      " 900: 'water tower',\n",
      " 901: 'whiskey jug',\n",
      " 902: 'whistle',\n",
      " 903: 'wig',\n",
      " 904: 'window screen',\n",
      " 905: 'window shade',\n",
      " 906: 'Windsor tie',\n",
      " 907: 'wine bottle',\n",
      " 908: 'wing',\n",
      " 909: 'wok',\n",
      " 910: 'wooden spoon',\n",
      " 911: 'wool, woolen, woollen',\n",
      " 912: 'worm fence, snake fence, snake-rail fence, Virginia fence',\n",
      " 913: 'wreck',\n",
      " 914: 'yawl',\n",
      " 915: 'yurt',\n",
      " 916: 'web site, website, internet site, site',\n",
      " 917: 'comic book',\n",
      " 918: 'crossword puzzle, crossword',\n",
      " 919: 'street sign',\n",
      " 920: 'traffic light, traffic signal, stoplight',\n",
      " 921: 'book jacket, dust cover, dust jacket, dust wrapper',\n",
      " 922: 'menu',\n",
      " 923: 'plate',\n",
      " 924: 'guacamole',\n",
      " 925: 'consomme',\n",
      " 926: 'hot pot, hotpot',\n",
      " 927: 'trifle',\n",
      " 928: 'ice cream, icecream',\n",
      " 929: 'ice lolly, lolly, lollipop, popsicle',\n",
      " 930: 'French loaf',\n",
      " 931: 'bagel, beigel',\n",
      " 932: 'pretzel',\n",
      " 933: 'cheeseburger',\n",
      " 934: 'hotdog, hot dog, red hot',\n",
      " 935: 'mashed potato',\n",
      " 936: 'head cabbage',\n",
      " 937: 'broccoli',\n",
      " 938: 'cauliflower',\n",
      " 939: 'zucchini, courgette',\n",
      " 940: 'spaghetti squash',\n",
      " 941: 'acorn squash',\n",
      " 942: 'butternut squash',\n",
      " 943: 'cucumber, cuke',\n",
      " 944: 'artichoke, globe artichoke',\n",
      " 945: 'bell pepper',\n",
      " 946: 'cardoon',\n",
      " 947: 'mushroom',\n",
      " 948: 'Granny Smith',\n",
      " 949: 'strawberry',\n",
      " 950: 'orange',\n",
      " 951: 'lemon',\n",
      " 952: 'fig',\n",
      " 953: 'pineapple, ananas',\n",
      " 954: 'banana',\n",
      " 955: 'jackfruit, jak, jack',\n",
      " 956: 'custard apple',\n",
      " 957: 'pomegranate',\n",
      " 958: 'hay',\n",
      " 959: 'carbonara',\n",
      " 960: 'chocolate sauce, chocolate syrup',\n",
      " 961: 'dough',\n",
      " 962: 'meat loaf, meatloaf',\n",
      " 963: 'pizza, pizza pie',\n",
      " 964: 'potpie',\n",
      " 965: 'burrito',\n",
      " 966: 'red wine',\n",
      " 967: 'espresso',\n",
      " 968: 'cup',\n",
      " 969: 'eggnog',\n",
      " 970: 'alp',\n",
      " 971: 'bubble',\n",
      " 972: 'cliff, drop, drop-off',\n",
      " 973: 'coral reef',\n",
      " 974: 'geyser',\n",
      " 975: 'lakeside, lakeshore',\n",
      " 976: 'promontory, headland, head, foreland',\n",
      " 977: 'sandbar, sand bar',\n",
      " 978: 'seashore, coast, seacoast, sea-coast',\n",
      " 979: 'valley, vale',\n",
      " 980: 'volcano',\n",
      " 981: 'ballplayer, baseball player',\n",
      " 982: 'groom, bridegroom',\n",
      " 983: 'scuba diver',\n",
      " 984: 'rapeseed',\n",
      " 985: 'daisy',\n",
      " 986: \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\",\n",
      " 987: 'corn',\n",
      " 988: 'acorn',\n",
      " 989: 'hip, rose hip, rosehip',\n",
      " 990: 'buckeye, horse chestnut, conker',\n",
      " 991: 'coral fungus',\n",
      " 992: 'agaric',\n",
      " 993: 'gyromitra',\n",
      " 994: 'stinkhorn, carrion fungus',\n",
      " 995: 'earthstar',\n",
      " 996: 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa',\n",
      " 997: 'bolete',\n",
      " 998: 'ear, spike, capitulum',\n",
      " 999: 'toilet tissue, toilet paper, bathroom tissue'}\n",
      "25/3:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 224\n",
      "\n",
      "def prep_image(image):\n",
      "    res = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "    res = res.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "    return res\n",
      "\n",
      "def predict(image):\n",
      "    cropped_image = center_crop(image)\n",
      "    prediction = (model.predict(prep_image(image)), model.predict(prep_image(cropped_image)))\n",
      "    if np.max(prediction[0]>=prediction[1]):\n",
      "        return np.argmax(prediction[0], axis=1)[0]\n",
      "    else:\n",
      "        return np.argmax(prediction[1], axis=1)[1]\n",
      "    \n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "plt.imshow(image)\n",
      "\n",
      "print(class_dict[predict(image)])\n",
      "25/4:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "def visualize_colors(cluster, centroids):\n",
      "    # Get the number of different clusters, create histogram, and normalize\n",
      "    labels = np.arange(0, len(np.unique(cluster.labels_)) + 1)\n",
      "    (hist, _) = np.histogram(cluster.labels_, bins = labels)\n",
      "    hist = hist.astype(\"float\")\n",
      "    hist /= hist.sum()\n",
      "\n",
      "    # Create frequency rect and iterate through each cluster's color and percentage\n",
      "    rect = np.zeros((50, 300, 3), dtype=np.uint8)\n",
      "    colors = sorted([(percent, color) for (percent, color) in zip(hist, centroids)])\n",
      "    start = 0\n",
      "    for (percent, color) in colors:\n",
      "        print(color, \"{:0.2f}%\".format(percent * 100))\n",
      "        end = start + (percent * 300)\n",
      "        cv2.rectangle(rect, (int(start), 0), (int(end), 50), \\\n",
      "                      color.astype(\"uint8\").tolist(), -1)\n",
      "        start = end\n",
      "    return rect\n",
      "\n",
      "image = cv2.imread('../data/images/l0n31j.png')\n",
      "plt.figure()\n",
      "plt.imshow(image)\n",
      "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "visualize = visualize_colors(cluster, cluster.cluster_centers_)\n",
      "plt.figure()\n",
      "plt.imshow(visualize)\n",
      "\n",
      "image = cv2.imread('../data/images/l31llr.png')\n",
      "plt.figure()\n",
      "plt.imshow(image)\n",
      "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "visualize = visualize_colors(cluster, cluster.cluster_centers_)\n",
      "plt.figure()\n",
      "plt.imshow(visualize)\n",
      "\n",
      "# result = {}\n",
      "# for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "#     if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "#         print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "#         image = cv2.imread(\"../data/images/\" + file)\n",
      "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "#         reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "#         cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "#         print(cluster.cluster_centers_)\n",
      "#         result[file] = cluster.cluster_centers_\n",
      "        \n",
      "# pickle.dump(result, open( \"../pickle/image_color_clusters.pkl\", \"wb\" ))\n",
      "25/5:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "def visualize_colors(cluster, centroids):\n",
      "    # Get the number of different clusters, create histogram, and normalize\n",
      "    labels = np.arange(0, len(np.unique(cluster.labels_)) + 1)\n",
      "    (hist, _) = np.histogram(cluster.labels_, bins = labels)\n",
      "    hist = hist.astype(\"float\")\n",
      "    hist /= hist.sum()\n",
      "\n",
      "    # Create frequency rect and iterate through each cluster's color and percentage\n",
      "    rect = np.zeros((50, 300, 3), dtype=np.uint8)\n",
      "    colors = sorted([(percent, color) for (percent, color) in zip(hist, centroids)])\n",
      "    start = 0\n",
      "    for (percent, color) in colors:\n",
      "        print(color, \"{:0.2f}%\".format(percent * 100))\n",
      "        end = start + (percent * 300)\n",
      "        cv2.rectangle(rect, (int(start), 0), (int(end), 50), \\\n",
      "                      color.astype(\"uint8\").tolist(), -1)\n",
      "        start = end\n",
      "    return rect\n",
      "\n",
      "image = cv2.imread('../data/images/l146p3.jpg')\n",
      "plt.figure()\n",
      "plt.imshow(image)\n",
      "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "visualize = visualize_colors(cluster, cluster.cluster_centers_)\n",
      "plt.figure()\n",
      "plt.imshow(visualize)\n",
      "\n",
      "image = cv2.imread('../data/images/l31llr.png')\n",
      "plt.figure()\n",
      "plt.imshow(image)\n",
      "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "visualize = visualize_colors(cluster, cluster.cluster_centers_)\n",
      "plt.figure()\n",
      "plt.imshow(visualize)\n",
      "\n",
      "# result = {}\n",
      "# for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "#     if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "#         print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "#         image = cv2.imread(\"../data/images/\" + file)\n",
      "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "#         reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "#         cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "#         print(cluster.cluster_centers_)\n",
      "#         result[file] = cluster.cluster_centers_\n",
      "        \n",
      "# pickle.dump(result, open( \"../pickle/image_color_clusters.pkl\", \"wb\" ))\n",
      "28/1:\n",
      "import h5py\n",
      "import logging\n",
      "import numpy as np\n",
      "import os\n",
      "import random\n",
      "import signal\n",
      "import sys\n",
      "import tensorflow as tf\n",
      "\n",
      "from datetime import datetime\n",
      "from glob import glob\n",
      "from scipy.misc import imread, imresize, imsave\n",
      "from tqdm import trange, tqdm\n",
      "28/2:\n",
      "import h5py\n",
      "import logging\n",
      "import numpy as np\n",
      "import os\n",
      "import random\n",
      "import signal\n",
      "import sys\n",
      "import tensorflow as tf\n",
      "\n",
      "from datetime import datetime\n",
      "from glob import glob\n",
      "from scipy.misc import imread, imresize, imsave\n",
      "from tqdm import trange, tqdm\n",
      "28/3:\n",
      "import h5py\n",
      "import logging\n",
      "import numpy as np\n",
      "import os\n",
      "import random\n",
      "import signal\n",
      "import sys\n",
      "import tensorflow as tf\n",
      "\n",
      "from datetime import datetime\n",
      "from glob import glob\n",
      "from scipy.misc import imread, imresize, imsave\n",
      "from tqdm import trange, tqdm\n",
      "28/4:\n",
      "import h5py\n",
      "import logging\n",
      "import numpy as np\n",
      "import os\n",
      "import random\n",
      "import signal\n",
      "import sys\n",
      "import tensorflow as tf\n",
      "\n",
      "from datetime import datetime\n",
      "from glob import glob\n",
      "from scipy.misc import imread, imresize, imsave\n",
      "from tqdm import trange, tqdm\n",
      "28/5:\n",
      "import h5py\n",
      "import logging\n",
      "import numpy as np\n",
      "import os\n",
      "import random\n",
      "import signal\n",
      "import sys\n",
      "import tensorflow as tf\n",
      "\n",
      "from datetime import datetime\n",
      "from glob import glob\n",
      "from scipy.misc import imread, imresize, imsave\n",
      "from tqdm import trange, tqdm\n",
      "28/6:\n",
      "import h5py\n",
      "import logging\n",
      "import numpy as np\n",
      "import os\n",
      "import random\n",
      "import signal\n",
      "import sys\n",
      "import tensorflow as tf\n",
      "\n",
      "from datetime import datetime\n",
      "from glob import glob\n",
      "from skimage import imread, imresize, imsave\n",
      "from tqdm import trange, tqdm\n",
      "28/7:\n",
      "import h5py\n",
      "import logging\n",
      "import numpy as np\n",
      "import os\n",
      "import random\n",
      "import signal\n",
      "import sys\n",
      "import tensorflow as tf\n",
      "\n",
      "from datetime import datetime\n",
      "from glob import glob\n",
      "from imageio import imread, imresize, imsave\n",
      "from tqdm import trange, tqdm\n",
      "28/8:\n",
      "import h5py\n",
      "import logging\n",
      "import numpy as np\n",
      "import os\n",
      "import random\n",
      "import signal\n",
      "import sys\n",
      "import tensorflow as tf\n",
      "\n",
      "from datetime import datetime\n",
      "from glob import glob\n",
      "from imageio import imread, imsave\n",
      "from tqdm import trange, tqdm\n",
      "28/9:\n",
      "datasets = ['maps', 'cityscapes', 'facades', 'edges2handbags', 'edges2shoes']\n",
      "\n",
      "def read_image(path):\n",
      "    ''' Seperates input image and ground truth image from a given image path and normalizes it '''\n",
      "\n",
      "    image = imread(path)\n",
      "    h, w, c = image.shape\n",
      "    image_a = image[:, :int(w/2), :].astype(np.float32) / 255.0\n",
      "    image_b = image[:, int(w/2):, :].astype(np.float32) / 255.0\n",
      "\n",
      "    # range of pixel values = [-1.0, 1.0]\n",
      "    image_a = image_a * 2.0 - 1.0\n",
      "    image_b = image_b * 2.0 - 1.0\n",
      "    return image_a, image_b\n",
      "\n",
      "def read_images(base_dir):\n",
      "    ret = []\n",
      "    for dir_name in ['train', 'val']:\n",
      "        data_dir = os.path.join(base_dir, dir_name)\n",
      "        paths = glob(os.path.join(data_dir, '*.jpg'))\n",
      "\n",
      "        images_A = []\n",
      "        images_B = []\n",
      "        for path in tqdm(paths):\n",
      "            image_A, image_B = read_image(path)\n",
      "            if image_A is not None:\n",
      "                images_A.append(image_A)\n",
      "                images_B.append(image_B)\n",
      "        ret.append((dir_name + 'A', images_A))\n",
      "        ret.append((dir_name + 'B', images_B))\n",
      "    return ret\n",
      "    \n",
      "def store_h5py(base_dir, dir_name, images, image_size):\n",
      "    f = h5py.File(os.path.join(base_dir, '{}_{}.hy'.format(dir_name, image_size)), 'w')\n",
      "    for i in range(len(images)):\n",
      "        grp = f.create_group(str(i))\n",
      "        if images[i].shape[0] != image_size:\n",
      "            image = imresize(images[i], (image_size, image_size, 3))\n",
      "            \n",
      "            # range of pixel values = [-1.0, 1.0]\n",
      "            image = image.astype(np.float32) / 255.0\n",
      "            image = image * 2.0 - 1.0\n",
      "            grp['image'] = image\n",
      "        else:\n",
      "            grp['image'] = images[i]\n",
      "    f.close()\n",
      "\n",
      "def convert_h5py(task_name):\n",
      "    print('Generating h5py file')\n",
      "    base_dir = os.path.join('datasets', task_name)\n",
      "    data = read_images(base_dir)\n",
      "    for dir_name, images in data:\n",
      "        if images[0].shape[0] == 256:\n",
      "            store_h5py(base_dir, dir_name, images, 256)\n",
      "        store_h5py(base_dir, dir_name, images, 128)\n",
      "\n",
      "def read_h5py(task_name, image_size):\n",
      "    base_dir = 'datasets/' + task_name\n",
      "    paths = glob(os.path.join(base_dir, '*_{}.hy'.format(image_size)))\n",
      "    if len(paths) != 4:\n",
      "        convert_h5py(task_name)\n",
      "    ret = []\n",
      "    for dir_name in ['trainA', 'trainB', 'valA', 'valB']:\n",
      "        try:\n",
      "            dataset = h5py.File(os.path.join(base_dir, '{}_{}.hy'.format(dir_name, image_size)), 'r')\n",
      "        except:\n",
      "            raise IOError('Dataset is not available. Please try it again')\n",
      "\n",
      "        images = []\n",
      "        for id in dataset:\n",
      "            images.append(dataset[id]['image'].value.astype(np.float32))\n",
      "        ret.append(images)\n",
      "    return ret\n",
      "\n",
      "def get_data(task_name, image_size):\n",
      "    base_dir = os.path.join('datasets', task_name)\n",
      "\n",
      "    print('Load data %s' % task_name)\n",
      "    train_A, train_B, test_A, test_B = \\\n",
      "        read_h5py(task_name, image_size)\n",
      "    return train_A, train_B, test_A, test_B\n",
      "28/10:\n",
      "\n",
      "class Discriminator(object):\n",
      "    def __init__(self, name, is_train, norm='instance', activation='leaky', image_size=128):\n",
      "        logger.info('Init Discriminator %s', name)\n",
      "        self.name = name\n",
      "        self._is_train = is_train\n",
      "        self._norm = norm\n",
      "        self._activation = activation\n",
      "        self._reuse = False\n",
      "        self._image_size = image_size\n",
      "\n",
      "    def __call__(self, input):\n",
      "        with tf.variable_scope(self.name, reuse=self._reuse):\n",
      "            D = conv_block(input, 64, 'C64', 4, 2, self._is_train,\n",
      "                               self._reuse, norm=None, activation=self._activation)\n",
      "            D = conv_block(D, 128, 'C128', 4, 2, self._is_train,\n",
      "                               self._reuse, self._norm, self._activation)\n",
      "            D = conv_block(D, 256, 'C256', 4, 2, self._is_train,\n",
      "                               self._reuse, self._norm, self._activation)\n",
      "            num_layers = 3 if self._image_size == 256 else 1\n",
      "            for i in range(num_layers):\n",
      "                D = conv_block(D, 512, 'C512_{}'.format(i), 4, 2, self._is_train,\n",
      "                                   self._reuse, self._norm, self._activation)\n",
      "            D = conv_block(D, 1, 'C1', 4, 1, self._is_train,\n",
      "                               self._reuse, norm=None, activation=None, bias=True)\n",
      "            D = tf.reduce_mean(D, axis=[1,2,3])\n",
      "\n",
      "            self._reuse = True\n",
      "            self.var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, self.name)\n",
      "            return D\n",
      "28/11:\n",
      "class Encoder(object):\n",
      "    def __init__(self, name, is_train, norm='instance', activation='leaky',\n",
      "                 image_size=128, latent_dim=8, use_resnet=True):\n",
      "        logger.info('Init Encoder %s', name)\n",
      "        self.name = name\n",
      "        self._is_train = is_train\n",
      "        self._norm = norm\n",
      "        self._activation = activation\n",
      "        self._reuse = False\n",
      "        self._image_size = image_size\n",
      "        self._latent_dim = latent_dim\n",
      "        self._use_resnet = use_resnet\n",
      "\n",
      "    def __call__(self, input):\n",
      "        if self._use_resnet:\n",
      "            return self._resnet(input)\n",
      "        else:\n",
      "            return self._convnet(input)\n",
      "\n",
      "    def _convnet(self, input):\n",
      "        with tf.variable_scope(self.name, reuse=self._reuse):\n",
      "            num_filters = [64, 128, 256, 512, 512, 512, 512]\n",
      "            if self._image_size == 256:\n",
      "                num_filters.append(512)\n",
      "\n",
      "            E = input\n",
      "            for i, n in enumerate(num_filters):\n",
      "                E = conv_block(E, n, 'C{}_{}'.format(n, i), 4, 2, self._is_train,\n",
      "                                   self._reuse, norm=self._norm if i else None, activation='leaky')\n",
      "            E = flatten(E)\n",
      "            mu = mlp(E, self._latent_dim, 'FC8_mu', self._is_train, self._reuse,\n",
      "                         norm=None, activation=None)\n",
      "            log_sigma = mlp(E, self._latent_dim, 'FC8_sigma', self._is_train, self._reuse,\n",
      "                                norm=None, activation=None)\n",
      "\n",
      "            z = mu + tf.random_normal(shape=tf.shape(self._latent_dim)) * tf.exp(log_sigma)\n",
      "\n",
      "            self._reuse = True\n",
      "            self.var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, self.name)\n",
      "            return z, mu, log_sigma\n",
      "\n",
      "    def _resnet(self, input):\n",
      "        with tf.variable_scope(self.name, reuse=self._reuse):\n",
      "            num_filters = [128, 256, 512, 512]\n",
      "            if self._image_size == 256:\n",
      "                num_filters.append(512)\n",
      "\n",
      "            E = input\n",
      "            E = conv_block(E, 64, 'C{}_{}'.format(64, 0), 4, 2, self._is_train,\n",
      "                               self._reuse, norm=None, activation='leaky', bias=True)\n",
      "            for i, n in enumerate(num_filters):\n",
      "                E = residual(E, n, 'res{}_{}'.format(n, i + 1), self._is_train,\n",
      "                                 self._reuse, norm=self._norm, bias=True)\n",
      "                E = tf.nn.avg_pool(E, [1, 2, 2, 1], [1, 2, 2, 1], 'SAME')\n",
      "            E = tf.nn.relu(E)\n",
      "            E = tf.nn.avg_pool(E, [1, 8, 8, 1], [1, 8, 8, 1], 'SAME')\n",
      "            E = flatten(E)\n",
      "            mu = mlp(E, self._latent_dim, 'FC8_mu', self._is_train, self._reuse,\n",
      "                         norm=None, activation=None)\n",
      "            log_sigma = mlp(E, self._latent_dim, 'FC8_sigma', self._is_train, self._reuse,\n",
      "                                norm=None, activation=None)\n",
      "\n",
      "            z = mu + tf.random_normal(shape=tf.shape(self._latent_dim)) * tf.exp(log_sigma)\n",
      "\n",
      "            self._reuse = True\n",
      "            self.var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, self.name)\n",
      "            return z, mu, log_sigma\n",
      "28/12:\n",
      "class Generator(object):\n",
      "    def __init__(self, name, is_train, norm='batch', image_size=128):\n",
      "        logger.info('Init Generator %s', name)\n",
      "        self.name = name\n",
      "        self._is_train = is_train\n",
      "        self._norm = norm\n",
      "        self._reuse = False\n",
      "        self._image_size = image_size\n",
      "\n",
      "    def __call__(self, input, z):\n",
      "        with tf.variable_scope(self.name, reuse=self._reuse):\n",
      "            batch_size = int(input.get_shape()[0])\n",
      "            latent_dim = int(z.get_shape()[-1])\n",
      "            num_filters = [64, 128, 256, 512, 512, 512, 512]\n",
      "            if self._image_size == 256:\n",
      "                num_filters.append(512)\n",
      "\n",
      "            layers = []\n",
      "            G = input\n",
      "            z = tf.reshape(z, [batch_size, 1, 1, latent_dim])\n",
      "            z = tf.tile(z, [1, self._image_size, self._image_size, 1])\n",
      "            G = tf.concat([G, z], axis=3)\n",
      "            for i, n in enumerate(num_filters):\n",
      "                G = conv_block(G, n, 'C{}_{}'.format(n, i), 4, 2, self._is_train,\n",
      "                                self._reuse, norm=self._norm if i else None, activation='leaky')\n",
      "                layers.append(G)\n",
      "\n",
      "            layers.pop()\n",
      "            num_filters.pop()\n",
      "            num_filters.reverse()\n",
      "\n",
      "            for i, n in enumerate(num_filters):\n",
      "                G = deconv_block(G, n, 'CD{}_{}'.format(n, i), 4, 2, self._is_train,\n",
      "                                self._reuse, norm=self._norm, activation='relu')\n",
      "                G = tf.concat([G, layers.pop()], axis=3)\n",
      "            G = deconv_block(G, 3, 'last_layer', 4, 2, self._is_train,\n",
      "                               self._reuse, norm=None, activation='tanh')\n",
      "\n",
      "            self._reuse = True\n",
      "            self.var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, self.name)\n",
      "            return G\n",
      "28/13:\n",
      "def _norm(input, is_train, reuse=True, norm=None):\n",
      "    assert norm in ['instance', 'batch', None]\n",
      "    if norm == 'instance':\n",
      "        with tf.variable_scope('instance_norm', reuse=reuse):\n",
      "            eps = 1e-5\n",
      "            mean, sigma = tf.nn.moments(input, [1, 2], keep_dims=True)\n",
      "            normalized = (input - mean) / (tf.sqrt(sigma) + eps)\n",
      "            out = normalized\n",
      "\n",
      "    elif norm == 'batch':\n",
      "        with tf.variable_scope('batch_norm', reuse=reuse):\n",
      "            out = tf.contrib.layers.batch_norm(input,\n",
      "                                               decay=0.99, center=True,\n",
      "                                               scale=True, is_training=True)\n",
      "    else:\n",
      "        out = input\n",
      "\n",
      "    return out\n",
      "\n",
      "def _activation(input, activation=None):\n",
      "    assert activation in ['relu', 'leaky', 'tanh', 'sigmoid', None]\n",
      "    if activation == 'relu':\n",
      "        return tf.nn.relu(input)\n",
      "    elif activation == 'leaky':\n",
      "        return tf.contrib.keras.layers.LeakyReLU(0.2)(input)\n",
      "    elif activation == 'tanh':\n",
      "        return tf.tanh(input)\n",
      "    elif activation == 'sigmoid':\n",
      "        return tf.sigmoid(input)\n",
      "    else:\n",
      "        return input\n",
      "\n",
      "def flatten(input):\n",
      "    return tf.reshape(input, [-1, np.prod(input.get_shape().as_list()[1:])])\n",
      "\n",
      "def conv2d(input, num_filters, filter_size, stride, reuse=False,\n",
      "           pad='SAME', dtype=tf.float32, bias=False):\n",
      "    stride_shape = [1, stride, stride, 1]\n",
      "    filter_shape = [filter_size, filter_size, input.get_shape()[3], num_filters]\n",
      "\n",
      "    w = tf.get_variable('w', filter_shape, dtype, tf.random_normal_initializer(0.0, 0.02))\n",
      "    if pad == 'REFLECT':\n",
      "        p = (filter_size - 1) // 2\n",
      "        x = tf.pad(input, [[0,0],[p,p],[p,p],[0,0]], 'REFLECT')\n",
      "        conv = tf.nn.conv2d(x, w, stride_shape, padding='VALID')\n",
      "    else:\n",
      "        assert pad in ['SAME', 'VALID']\n",
      "        conv = tf.nn.conv2d(input, w, stride_shape, padding=pad)\n",
      "\n",
      "    if bias:\n",
      "        b = tf.get_variable('b', [1,1,1,num_filters], initializer=tf.constant_initializer(0.0))\n",
      "        conv = conv + b\n",
      "    return conv\n",
      "\n",
      "def conv2d_transpose(input, num_filters, filter_size, stride, reuse,\n",
      "                     pad='SAME', dtype=tf.float32):\n",
      "    assert pad == 'SAME'\n",
      "    n, h, w, c = input.get_shape().as_list()\n",
      "    stride_shape = [1, stride, stride, 1]\n",
      "    filter_shape = [filter_size, filter_size, num_filters, c]\n",
      "    output_shape = [n, h * stride, w * stride, num_filters]\n",
      "\n",
      "    w = tf.get_variable('w', filter_shape, dtype, tf.random_normal_initializer(0.0, 0.02))\n",
      "    deconv = tf.nn.conv2d_transpose(input, w, output_shape, stride_shape, pad)\n",
      "    return deconv\n",
      "\n",
      "def mlp(input, out_dim, name, is_train, reuse, norm=None, activation=None,\n",
      "        dtype=tf.float32, bias=True):\n",
      "    with tf.variable_scope(name, reuse=reuse):\n",
      "        _, n = input.get_shape()\n",
      "        w = tf.get_variable('w', [n, out_dim], dtype, tf.random_normal_initializer(0.0, 0.02))\n",
      "        out = tf.matmul(input, w)\n",
      "        if bias:\n",
      "            b = tf.get_variable('b', [out_dim], initializer=tf.constant_initializer(0.0))\n",
      "            out = out + b\n",
      "        out = _activation(out, activation)\n",
      "        out = _norm(out, is_train, reuse, norm)\n",
      "        return out\n",
      "\n",
      "def conv_block(input, num_filters, name, k_size, stride, is_train, reuse, norm,\n",
      "          activation, pad='SAME', bias=False):\n",
      "    with tf.variable_scope(name, reuse=reuse):\n",
      "        out = conv2d(input, num_filters, k_size, stride, reuse, pad, bias=bias)\n",
      "        out = _norm(out, is_train, reuse, norm)\n",
      "        out = _activation(out, activation)\n",
      "        return out\n",
      "\n",
      "def residual(input, num_filters, name, is_train, reuse, norm, pad='REFLECT',\n",
      "             bias=False):\n",
      "    with tf.variable_scope(name, reuse=reuse):\n",
      "        with tf.variable_scope('res1', reuse=reuse):\n",
      "            out = conv2d(input, num_filters, 3, 1, reuse, pad, bias=bias)\n",
      "            out = _norm(out, is_train, reuse, norm)\n",
      "            out = tf.nn.relu(out)\n",
      "\n",
      "        with tf.variable_scope('res2', reuse=reuse):\n",
      "            out = conv2d(out, num_filters, 3, 1, reuse, pad, bias=bias)\n",
      "            out = _norm(out, is_train, reuse, norm)\n",
      "\n",
      "        with tf.variable_scope('shortcut', reuse=reuse):\n",
      "            shortcut = conv2d(input, num_filters, 1, 1, reuse, pad, bias=bias)\n",
      "\n",
      "        return tf.nn.relu(shortcut + out)\n",
      "\n",
      "def deconv_block(input, num_filters, name, k_size, stride, is_train, reuse,\n",
      "                 norm, activation):\n",
      "    with tf.variable_scope(name, reuse=reuse):\n",
      "        out = conv2d_transpose(input, num_filters, k_size, stride, reuse)\n",
      "        out = _norm(out, is_train, reuse, norm)\n",
      "        out = _activation(out, activation)\n",
      "        return out\n",
      "28/14:\n",
      "logging.info(\"Start BicycleGAN\")\n",
      "logger = logging.getLogger('Bicycle-gan')\n",
      "logger.setLevel(logging.INFO)\n",
      "\n",
      "def makedirs(path):\n",
      "    if not os.path.exists(path):\n",
      "        os.makedirs(path)\n",
      "28/15:\n",
      "from collections import defaultdict\n",
      "args = defaultdict(dict)\n",
      "args['train'] = True\n",
      "args['task'] = 'facades'\n",
      "args['coeff_gan'] = 1.0\n",
      "args['coeff_vae'] = 1.0\n",
      "args['coeff_kl'] = 0.01\n",
      "args['coeff_reconstruct'] = 10\n",
      "args['coeff_latent'] = 0.5\n",
      "args['instance_normalization'] = False\n",
      "args['log_step'] = 100\n",
      "args['batch_size'] = 1\n",
      "args['image_size'] = 256\n",
      "args['latent_dim'] = 8\n",
      "args['use_resnet'] = True\n",
      "args['load_model'] = ''\n",
      "28/16:\n",
      "class BicycleGAN(object):\n",
      "    def __init__(self, args):\n",
      "        self._log_step = args['log_step']\n",
      "        self._batch_size = args['batch_size']\n",
      "        self._image_size = args['image_size']\n",
      "        self._latent_dim = args['latent_dim']\n",
      "        self._coeff_gan = args['coeff_gan']\n",
      "        self._coeff_vae = args['coeff_vae']\n",
      "        self._coeff_reconstruct = args['coeff_reconstruct']\n",
      "        self._coeff_latent = args['coeff_latent']\n",
      "        self._coeff_kl = args['coeff_kl']\n",
      "        self._norm = 'instance' if args['instance_normalization'] else 'batch'\n",
      "        self._use_resnet = args['use_resnet']\n",
      "\n",
      "        self._augment_size = self._image_size + (30 if self._image_size == 256 else 15)\n",
      "        self._image_shape = [self._image_size, self._image_size, 3]\n",
      "\n",
      "        self.is_train = tf.placeholder(tf.bool, name='is_train')\n",
      "        self.lr = tf.placeholder(tf.float32, name='lr')\n",
      "        self.global_step = tf.train.get_or_create_global_step(graph=None)\n",
      "\n",
      "        image_a = self.image_a = \\\n",
      "            tf.placeholder(tf.float32, [self._batch_size] + self._image_shape, name='image_a')\n",
      "        image_b = self.image_b = \\\n",
      "            tf.placeholder(tf.float32, [self._batch_size] + self._image_shape, name='image_b')\n",
      "        z = self.z = \\\n",
      "            tf.placeholder(tf.float32, [self._batch_size, self._latent_dim], name='z')\n",
      "\n",
      "        # Data augmentation\n",
      "        seed = random.randint(0, 2**31 - 1)\n",
      "        def augment_image(image):\n",
      "            image = tf.image.resize_images(image, [self._augment_size, self._augment_size])\n",
      "            image = tf.random_crop(image, [self._batch_size] + self._image_shape, seed=seed)\n",
      "            image = tf.map_fn(lambda x: tf.image.random_flip_left_right(x, seed), image)\n",
      "            return image\n",
      "\n",
      "        image_a = tf.cond(self.is_train,\n",
      "                          lambda: augment_image(image_a),\n",
      "                          lambda: image_a)\n",
      "        image_b = tf.cond(self.is_train,\n",
      "                          lambda: augment_image(image_b),\n",
      "                          lambda: image_b)\n",
      "\n",
      "        # Generator\n",
      "        G = Generator('G', is_train=self.is_train,\n",
      "                      norm=self._norm, image_size=self._image_size)\n",
      "\n",
      "        # Discriminator\n",
      "        D = Discriminator('D', is_train=self.is_train,\n",
      "                          norm=self._norm, activation='leaky',\n",
      "                          image_size=self._image_size)\n",
      "\n",
      "        # Encoder\n",
      "        E = Encoder('E', is_train=self.is_train,\n",
      "                    norm=self._norm, activation='relu',\n",
      "                    image_size=self._image_size, latent_dim=self._latent_dim,\n",
      "                    use_resnet=self._use_resnet)\n",
      "\n",
      "        # conditional VAE-GAN: B -> z -> B'\n",
      "        z_encoded, z_encoded_mu, z_encoded_log_sigma = E(image_b)\n",
      "        image_ab_encoded = G(image_a, z_encoded)\n",
      "\n",
      "        # conditional Latent Regressor-GAN: z -> B' -> z'\n",
      "        image_ab = self.image_ab = G(image_a, z)\n",
      "        z_recon, z_recon_mu, z_recon_log_sigma = E(image_ab)\n",
      "\n",
      "\n",
      "        # Discriminate real/fake images\n",
      "        D_real = D(image_b)\n",
      "        D_fake = D(image_ab)\n",
      "        D_fake_encoded = D(image_ab_encoded)\n",
      "\n",
      "        loss_vae_gan = (tf.reduce_mean(tf.squared_difference(D_real, 0.9)) +\n",
      "            tf.reduce_mean(tf.square(D_fake_encoded)))\n",
      "\n",
      "        loss_image_cycle = tf.reduce_mean(tf.abs(image_b - image_ab_encoded))\n",
      "\n",
      "        loss_gan = (tf.reduce_mean(tf.squared_difference(D_real, 0.9)) +\n",
      "            tf.reduce_mean(tf.square(D_fake)))\n",
      "\n",
      "        loss_latent_cycle = tf.reduce_mean(tf.abs(z - z_recon))\n",
      "\n",
      "        loss_kl = -0.5 * tf.reduce_mean(1 + 2 * z_encoded_log_sigma - z_encoded_mu ** 2 -\n",
      "                                       tf.exp(2 * z_encoded_log_sigma))\n",
      "\n",
      "        loss = self._coeff_vae * loss_vae_gan - self._coeff_reconstruct * loss_image_cycle + \\\n",
      "            self._coeff_gan * loss_gan - self._coeff_latent * loss_latent_cycle - \\\n",
      "            self._coeff_kl * loss_kl\n",
      "\n",
      "        # Optimizer\n",
      "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
      "        with tf.control_dependencies(update_ops):\n",
      "            self.optimizer_D = tf.train.AdamOptimizer(learning_rate=self.lr, beta1=0.5) \\\n",
      "                                .minimize(loss, var_list=D.var_list, global_step=self.global_step)\n",
      "            self.optimizer_G = tf.train.AdamOptimizer(learning_rate=self.lr, beta1=0.5) \\\n",
      "                                .minimize(-loss, var_list=G.var_list)\n",
      "            self.optimizer_E = tf.train.AdamOptimizer(learning_rate=self.lr, beta1=0.5) \\\n",
      "                                .minimize(-loss, var_list=E.var_list)\n",
      "\n",
      "        # Summaries\n",
      "        self.loss_vae_gan = loss_vae_gan\n",
      "        self.loss_image_cycle = loss_image_cycle\n",
      "        self.loss_latent_cycle = loss_latent_cycle\n",
      "        self.loss_gan = loss_gan\n",
      "        self.loss_kl = loss_kl\n",
      "        self.loss = loss\n",
      "\n",
      "        tf.summary.scalar('loss/vae_gan', loss_vae_gan)\n",
      "        tf.summary.scalar('loss/image_cycle', loss_image_cycle)\n",
      "        tf.summary.scalar('loss/latent_cycle', loss_latent_cycle)\n",
      "        tf.summary.scalar('loss/gan', loss_gan)\n",
      "        tf.summary.scalar('loss/kl', loss_kl)\n",
      "        tf.summary.scalar('loss/total', loss)\n",
      "        tf.summary.scalar('model/D_real', tf.reduce_mean(D_real))\n",
      "        tf.summary.scalar('model/D_fake', tf.reduce_mean(D_fake))\n",
      "        tf.summary.scalar('model/D_fake_encoded', tf.reduce_mean(D_fake_encoded))\n",
      "        tf.summary.scalar('model/lr', self.lr)\n",
      "        tf.summary.image('image/A', image_a[0:1])\n",
      "        tf.summary.image('image/B', image_b[0:1])\n",
      "        tf.summary.image('image/A-B', image_ab[0:1])\n",
      "        tf.summary.image('image/A-B_encoded', image_ab_encoded[0:1])\n",
      "        self.summary_op = tf.summary.merge_all()\n",
      "\n",
      "                \n",
      "    def train(self, sess, summary_writer, data_A, data_B):\n",
      "        logger.info('Start training.')\n",
      "        logger.info('  {} images from A'.format(len(data_A)))\n",
      "        logger.info('  {} images from B'.format(len(data_B)))\n",
      "\n",
      "        assert len(data_A) == len(data_B), \\\n",
      "            'Data size mismatch dataA(%d) dataB(%d)' % (len(data_A), len(data_B))\n",
      "        data_size = len(data_A)\n",
      "        num_batch = data_size // self._batch_size\n",
      "        epoch_length = 310\n",
      "        \n",
      "        num_initial_iter = 8\n",
      "        num_decay_iter = 2\n",
      "        lr = lr_initial = 0.0002\n",
      "        lr_decay = lr_initial / num_decay_iter\n",
      "  \n",
      "        initial_step = sess.run(self.global_step)\n",
      "        num_global_step = (num_initial_iter + num_decay_iter) * epoch_length\n",
      "        t = trange(initial_step, num_global_step,\n",
      "                   total=num_global_step, initial=initial_step)\n",
      "\n",
      "        for step in t:\n",
      "            epoch = step // epoch_length\n",
      "            iter = step % epoch_length\n",
      "\n",
      "            if epoch > num_initial_iter:\n",
      "                lr = max(0.0, lr_initial - (epoch - num_initial_iter) * lr_decay)\n",
      "\n",
      "            if iter == 0:\n",
      "                data = list(zip(data_A, data_B))\n",
      "                random.shuffle(data)\n",
      "                data_A, data_B = zip(*data)\n",
      "\n",
      "            image_a = np.stack(data_A[iter*self._batch_size:(iter+1)*self._batch_size])\n",
      "            image_b = np.stack(data_B[iter*self._batch_size:(iter+1)*self._batch_size])\n",
      "            sample_z = np.random.normal(size=(self._batch_size, self._latent_dim))\n",
      "\n",
      "            fetches = [self.loss, self.optimizer_D,\n",
      "                       self.optimizer_G, self.optimizer_E]\n",
      "            if step % self._log_step == 0:\n",
      "                fetches += [self.summary_op]\n",
      "\n",
      "            fetched = sess.run(fetches, feed_dict={self.image_a: image_a,\n",
      "                                                   self.image_b: image_b,\n",
      "                                                   self.is_train: True,\n",
      "                                                   self.lr: lr,\n",
      "                                                   self.z: sample_z})\n",
      "\n",
      "            if step % self._log_step == 0:\n",
      "                z = np.random.normal(size=(1, self._latent_dim))\n",
      "                image_ab = sess.run(self.image_ab, feed_dict={self.image_a: image_a,\n",
      "                                                            self.z: z,\n",
      "                                                            self.is_train: False})\n",
      "                imsave('results/r_{}.jpg'.format(step), np.squeeze(image_ab, axis=0))\n",
      "\n",
      "                summary_writer.add_summary(fetched[-1], step)\n",
      "                summary_writer.flush()\n",
      "                t.set_description('Loss({:.3f})'.format(fetched[0]))\n",
      "\n",
      "    def test(self, sess, data_A, data_B, base_dir):\n",
      "        step = 0\n",
      "        for (dataA, dataB) in tqdm(zip(data_A, data_B)):\n",
      "            step += 1\n",
      "            image_a = np.expand_dims(dataA, axis=0)\n",
      "            image_b = np.expand_dims(dataB, axis=0)\n",
      "            images_random = []\n",
      "            images_random.append(image_a)\n",
      "            images_random.append(image_b)\n",
      "            images_linear = []\n",
      "            images_linear.append(image_a)\n",
      "            images_linear.append(image_b)\n",
      "\n",
      "            z = np.random.normal(size=(1, self._latent_dim))\n",
      "            image_ab = sess.run(self.image_ab, feed_dict={self.image_a: image_a,\n",
      "                                                         self.z: z,\n",
      "                                                         self.is_train: False})\n",
      "            images_random.append(image_ab)\n",
      "            \n",
      "            z = np.zeros((1, self._latent_dim))\n",
      "            image_ab = sess.run(self.image_ab, feed_dict={self.image_a: image_a,\n",
      "                                                         self.z: z,\n",
      "                                                         self.is_train: False})\n",
      "            images_linear.append(image_ab)\n",
      "            \n",
      "            images = np.concatenate(images_random, axis=1)\n",
      "            images = np.squeeze(images, axis=0)        \n",
      "            imsave(os.path.join(base_dir, 'random_{}.jpg'.format(step)), images)\n",
      "            \n",
      "            images = np.concatenate(images_linear, axis=1)\n",
      "            images = np.squeeze(images, axis=0)\n",
      "            imsave(os.path.join(base_dir, 'linear_{}.jpg'.format(step)), images)\n",
      "28/17:\n",
      "def str2bool(v):\n",
      "    return v.lower() == 'true'\n",
      "\n",
      "class FastSaver(tf.train.Saver):\n",
      "    def save(self, sess, save_path, global_step=None, latest_filename=None,\n",
      "             meta_graph_suffix=\"meta\", write_meta_graph=True):\n",
      "        super(FastSaver, self).save(sess, save_path, global_step, latest_filename,\n",
      "                                    meta_graph_suffix, False)\n",
      "\n",
      "\n",
      "def run(args):\n",
      "    logger.info('Read data:')\n",
      "    train_A, train_B, test_A, test_B = get_data(args['task'], args['image_size'])\n",
      "    \n",
      "    tempA = train_A\n",
      "    train_A = train_B\n",
      "    train_B = tempA\n",
      "    \n",
      "    tempB = test_A\n",
      "    test_A = test_B\n",
      "    test_B = tempB\n",
      "    \n",
      "    logger.info('Build graph:')\n",
      "    model = BicycleGAN(args)\n",
      "\n",
      "    variables_to_save = tf.global_variables()\n",
      "    init_op = tf.variables_initializer(variables_to_save)\n",
      "    init_all_op = tf.global_variables_initializer()\n",
      "    saver = FastSaver(variables_to_save)\n",
      "\n",
      "    logger.info('Trainable vars:')\n",
      "    var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
      "                                 tf.get_variable_scope().name)\n",
      "    for v in var_list:\n",
      "        logger.info('  %s %s', v.name, v.get_shape())\n",
      "\n",
      "    if args['load_model'] != '':\n",
      "        model_name = args['load_model']\n",
      "    else:\n",
      "        model_name = '{}_{}'.format(args['task'], datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
      "    logdir = './logs'\n",
      "    makedirs(logdir)\n",
      "    logdir = os.path.join(logdir, model_name)\n",
      "    logger.info('Events directory: %s', logdir)\n",
      "    summary_writer = tf.summary.FileWriter(logdir)\n",
      "\n",
      "    makedirs('./results')\n",
      "\n",
      "    def init_fn(sess):\n",
      "        logger.info('Initializing all parameters.')\n",
      "        sess.run(init_all_op)\n",
      "\n",
      "    sv = tf.train.Supervisor(is_chief=True,\n",
      "                             logdir=logdir,\n",
      "                             saver=saver,\n",
      "                             summary_op=None,\n",
      "                             init_op=init_op,\n",
      "                             init_fn=init_fn,\n",
      "                             summary_writer=summary_writer,\n",
      "                             ready_op=tf.report_uninitialized_variables(variables_to_save),\n",
      "                             global_step=model.global_step,\n",
      "                             save_model_secs=300,\n",
      "                             save_summaries_secs=30)\n",
      "\n",
      "    if args['train']:\n",
      "        logger.info(\"Starting training session.\")\n",
      "        with sv.managed_session() as sess:\n",
      "            model.train(sess, summary_writer, train_A, train_B)\n",
      "\n",
      "    logger.info(\"Starting testing session.\")\n",
      "    with sv.managed_session() as sess:\n",
      "        base_dir = os.path.join('results', model_name)\n",
      "        makedirs(base_dir)\n",
      "        model.test(sess, test_A, test_B, base_dir)\n",
      "\n",
      "def main():\n",
      "    \n",
      "    def shutdown(signal, frame):\n",
      "        tf.logging.warn('Received signal %s: exiting', signal)\n",
      "        sys.exit(128+signal)\n",
      "    signal.signal(signal.SIGINT, shutdown)\n",
      "    signal.signal(signal.SIGTERM, shutdown)\n",
      "\n",
      "    run(args)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "29/1:\n",
      "print('PyDev console: using IPython 7.19.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['C:\\\\Users\\\\luker\\\\Documents\\\\Github\\\\magisterka', 'C:/Users/luker/Documents/Github/magisterka'])\n",
      "31/1:\n",
      "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "# https://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "31/2:\n",
      "import tensorflow as tf\n",
      "\n",
      "import os\n",
      "import time\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython import display\n",
      "31/3: !pip install -q -U tensorboard\n",
      "31/4:\n",
      "_URL = 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz'\n",
      "\n",
      "path_to_zip = tf.keras.utils.get_file('facades.tar.gz',\n",
      "                                      origin=_URL,\n",
      "                                      extract=True)\n",
      "\n",
      "PATH = os.path.join(os.path.dirname(path_to_zip), 'facades/')\n",
      "31/5:\n",
      "BUFFER_SIZE = 400\n",
      "BATCH_SIZE = 1\n",
      "IMG_WIDTH = 256\n",
      "IMG_HEIGHT = 256\n",
      "31/6:\n",
      "def load(image_file):\n",
      "  image = tf.io.read_file(image_file)\n",
      "  image = tf.image.decode_jpeg(image)\n",
      "\n",
      "  w = tf.shape(image)[1]\n",
      "\n",
      "  w = w // 2\n",
      "  real_image = image[:, :w, :]\n",
      "  input_image = image[:, w:, :]\n",
      "\n",
      "  input_image = tf.cast(input_image, tf.float32)\n",
      "  real_image = tf.cast(real_image, tf.float32)\n",
      "\n",
      "  return input_image, real_image\n",
      "31/7:\n",
      "inp, re = load(PATH+'train/100.jpg')\n",
      "# casting to int for matplotlib to show the image\n",
      "plt.figure()\n",
      "plt.imshow(inp/255.0)\n",
      "plt.figure()\n",
      "plt.imshow(re/255.0)\n",
      "31/8:\n",
      "def resize(input_image, real_image, height, width):\n",
      "  input_image = tf.image.resize(input_image, [height, width],\n",
      "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "  real_image = tf.image.resize(real_image, [height, width],\n",
      "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "\n",
      "  return input_image, real_image\n",
      "31/9:\n",
      "def random_crop(input_image, real_image):\n",
      "  stacked_image = tf.stack([input_image, real_image], axis=0)\n",
      "  cropped_image = tf.image.random_crop(\n",
      "      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
      "\n",
      "  return cropped_image[0], cropped_image[1]\n",
      "31/10:\n",
      "# normalizing the images to [-1, 1]\n",
      "\n",
      "def normalize(input_image, real_image):\n",
      "  input_image = (input_image / 127.5) - 1\n",
      "  real_image = (real_image / 127.5) - 1\n",
      "\n",
      "  return input_image, real_image\n",
      "31/11:\n",
      "@tf.function()\n",
      "def random_jitter(input_image, real_image):\n",
      "  # resizing to 286 x 286 x 3\n",
      "  input_image, real_image = resize(input_image, real_image, 286, 286)\n",
      "\n",
      "  # randomly cropping to 256 x 256 x 3\n",
      "  input_image, real_image = random_crop(input_image, real_image)\n",
      "\n",
      "  if tf.random.uniform(()) > 0.5:\n",
      "    # random mirroring\n",
      "    input_image = tf.image.flip_left_right(input_image)\n",
      "    real_image = tf.image.flip_left_right(real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "31/12:\n",
      "plt.figure(figsize=(6, 6))\n",
      "for i in range(4):\n",
      "  rj_inp, rj_re = random_jitter(inp, re)\n",
      "  plt.subplot(2, 2, i+1)\n",
      "  plt.imshow(rj_inp/255.0)\n",
      "  plt.axis('off')\n",
      "plt.show()\n",
      "31/13:\n",
      "def load_image_train(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = random_jitter(input_image, real_image)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "31/14:\n",
      "def load_image_test(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = resize(input_image, real_image,\n",
      "                                   IMG_HEIGHT, IMG_WIDTH)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "31/15:\n",
      "train_dataset = tf.data.Dataset.list_files(PATH+'train/*.jpg')\n",
      "train_dataset = train_dataset.map(load_image_train,\n",
      "                                  num_parallel_calls=tf.data.AUTOTUNE)\n",
      "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
      "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
      "31/16:\n",
      "test_dataset = tf.data.Dataset.list_files(PATH+'test/*.jpg')\n",
      "test_dataset = test_dataset.map(load_image_test)\n",
      "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
      "31/17: OUTPUT_CHANNELS = 3\n",
      "31/18:\n",
      "def downsample(filters, size, apply_batchnorm=True):\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "\n",
      "  result = tf.keras.Sequential()\n",
      "  result.add(\n",
      "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
      "                             kernel_initializer=initializer, use_bias=False))\n",
      "\n",
      "  if apply_batchnorm:\n",
      "    result.add(tf.keras.layers.BatchNormalization())\n",
      "\n",
      "  result.add(tf.keras.layers.LeakyReLU())\n",
      "\n",
      "  return result\n",
      "31/19:\n",
      "down_model = downsample(3, 4)\n",
      "down_result = down_model(tf.expand_dims(inp, 0))\n",
      "print (down_result.shape)\n",
      "31/20:\n",
      "def upsample(filters, size, apply_dropout=False):\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "\n",
      "  result = tf.keras.Sequential()\n",
      "  result.add(\n",
      "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
      "                                    padding='same',\n",
      "                                    kernel_initializer=initializer,\n",
      "                                    use_bias=False))\n",
      "\n",
      "  result.add(tf.keras.layers.BatchNormalization())\n",
      "\n",
      "  if apply_dropout:\n",
      "      result.add(tf.keras.layers.Dropout(0.5))\n",
      "\n",
      "  result.add(tf.keras.layers.ReLU())\n",
      "\n",
      "  return result\n",
      "31/21:\n",
      "up_model = upsample(3, 4)\n",
      "up_result = up_model(down_result)\n",
      "print (up_result.shape)\n",
      "31/22:\n",
      "def Generator():\n",
      "  inputs = tf.keras.layers.Input(shape=[256, 256, 3])\n",
      "\n",
      "  down_stack = [\n",
      "    downsample(64, 4, apply_batchnorm=False),  # (bs, 128, 128, 64)\n",
      "    downsample(128, 4),  # (bs, 64, 64, 128)\n",
      "    downsample(256, 4),  # (bs, 32, 32, 256)\n",
      "    downsample(512, 4),  # (bs, 16, 16, 512)\n",
      "    downsample(512, 4),  # (bs, 8, 8, 512)\n",
      "    downsample(512, 4),  # (bs, 4, 4, 512)\n",
      "    downsample(512, 4),  # (bs, 2, 2, 512)\n",
      "    downsample(512, 4),  # (bs, 1, 1, 512)\n",
      "  ]\n",
      "\n",
      "  up_stack = [\n",
      "    upsample(512, 4, apply_dropout=True),  # (bs, 2, 2, 1024)\n",
      "    upsample(512, 4, apply_dropout=True),  # (bs, 4, 4, 1024)\n",
      "    upsample(512, 4, apply_dropout=True),  # (bs, 8, 8, 1024)\n",
      "    upsample(512, 4),  # (bs, 16, 16, 1024)\n",
      "    upsample(256, 4),  # (bs, 32, 32, 512)\n",
      "    upsample(128, 4),  # (bs, 64, 64, 256)\n",
      "    upsample(64, 4),  # (bs, 128, 128, 128)\n",
      "  ]\n",
      "\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "  last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
      "                                         strides=2,\n",
      "                                         padding='same',\n",
      "                                         kernel_initializer=initializer,\n",
      "                                         activation='tanh')  # (bs, 256, 256, 3)\n",
      "\n",
      "  x = inputs\n",
      "\n",
      "  # Downsampling through the model\n",
      "  skips = []\n",
      "  for down in down_stack:\n",
      "    x = down(x)\n",
      "    skips.append(x)\n",
      "\n",
      "  skips = reversed(skips[:-1])\n",
      "\n",
      "  # Upsampling and establishing the skip connections\n",
      "  for up, skip in zip(up_stack, skips):\n",
      "    x = up(x)\n",
      "    x = tf.keras.layers.Concatenate()([x, skip])\n",
      "\n",
      "  x = last(x)\n",
      "\n",
      "  return tf.keras.Model(inputs=inputs, outputs=x)\n",
      "31/23:\n",
      "generator = Generator()\n",
      "tf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)\n",
      "31/24:\n",
      "gen_output = generator(inp[tf.newaxis, ...], training=False)\n",
      "plt.imshow(gen_output[0, ...])\n",
      "31/25: LAMBDA = 100\n",
      "31/26: loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
      "31/27:\n",
      "def generator_loss(disc_generated_output, gen_output, target):\n",
      "  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
      "\n",
      "  # mean absolute error\n",
      "  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
      "\n",
      "  total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
      "\n",
      "  return total_gen_loss, gan_loss, l1_loss\n",
      "31/28:\n",
      "def Discriminator():\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "\n",
      "  inp = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image')\n",
      "  tar = tf.keras.layers.Input(shape=[256, 256, 3], name='target_image')\n",
      "\n",
      "  x = tf.keras.layers.concatenate([inp, tar])  # (bs, 256, 256, channels*2)\n",
      "\n",
      "  down1 = downsample(64, 4, False)(x)  # (bs, 128, 128, 64)\n",
      "  down2 = downsample(128, 4)(down1)  # (bs, 64, 64, 128)\n",
      "  down3 = downsample(256, 4)(down2)  # (bs, 32, 32, 256)\n",
      "\n",
      "  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)  # (bs, 34, 34, 256)\n",
      "  conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n",
      "                                kernel_initializer=initializer,\n",
      "                                use_bias=False)(zero_pad1)  # (bs, 31, 31, 512)\n",
      "\n",
      "  batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
      "\n",
      "  leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
      "\n",
      "  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)  # (bs, 33, 33, 512)\n",
      "\n",
      "  last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
      "                                kernel_initializer=initializer)(zero_pad2)  # (bs, 30, 30, 1)\n",
      "\n",
      "  return tf.keras.Model(inputs=[inp, tar], outputs=last)\n",
      "31/29:\n",
      "discriminator = Discriminator()\n",
      "tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)\n",
      "31/30:\n",
      "disc_out = discriminator([inp[tf.newaxis, ...], gen_output], training=False)\n",
      "plt.imshow(disc_out[0, ..., -1], vmin=-20, vmax=20, cmap='RdBu_r')\n",
      "plt.colorbar()\n",
      "31/31:\n",
      "def discriminator_loss(disc_real_output, disc_generated_output):\n",
      "  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
      "\n",
      "  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
      "\n",
      "  total_disc_loss = real_loss + generated_loss\n",
      "\n",
      "  return total_disc_loss\n",
      "31/32:\n",
      "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
      "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
      "31/33:\n",
      "checkpoint_dir = './training_checkpoints'\n",
      "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
      "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
      "                                 discriminator_optimizer=discriminator_optimizer,\n",
      "                                 generator=generator,\n",
      "                                 discriminator=discriminator)\n",
      "31/34:\n",
      "def generate_images(model, test_input, tar):\n",
      "  prediction = model(test_input, training=True)\n",
      "  plt.figure(figsize=(15, 15))\n",
      "\n",
      "  display_list = [test_input[0], tar[0], prediction[0]]\n",
      "  title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
      "\n",
      "  for i in range(3):\n",
      "    plt.subplot(1, 3, i+1)\n",
      "    plt.title(title[i])\n",
      "    # getting the pixel values between [0, 1] to plot it.\n",
      "    plt.imshow(display_list[i] * 0.5 + 0.5)\n",
      "    plt.axis('off')\n",
      "  plt.show()\n",
      "31/35:\n",
      "for example_input, example_target in test_dataset.take(1):\n",
      "  generate_images(generator, example_input, example_target)\n",
      "31/36: EPOCHS = 150\n",
      "31/37:\n",
      "import datetime\n",
      "log_dir=\"logs/\"\n",
      "\n",
      "summary_writer = tf.summary.create_file_writer(\n",
      "  log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
      "31/38:\n",
      "@tf.function\n",
      "def train_step(input_image, target, epoch):\n",
      "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
      "    gen_output = generator(input_image, training=True)\n",
      "\n",
      "    disc_real_output = discriminator([input_image, target], training=True)\n",
      "    disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
      "\n",
      "    gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n",
      "    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
      "\n",
      "  generator_gradients = gen_tape.gradient(gen_total_loss,\n",
      "                                          generator.trainable_variables)\n",
      "  discriminator_gradients = disc_tape.gradient(disc_loss,\n",
      "                                               discriminator.trainable_variables)\n",
      "\n",
      "  generator_optimizer.apply_gradients(zip(generator_gradients,\n",
      "                                          generator.trainable_variables))\n",
      "  discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
      "                                              discriminator.trainable_variables))\n",
      "\n",
      "  with summary_writer.as_default():\n",
      "    tf.summary.scalar('gen_total_loss', gen_total_loss, step=epoch)\n",
      "    tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=epoch)\n",
      "    tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=epoch)\n",
      "    tf.summary.scalar('disc_loss', disc_loss, step=epoch)\n",
      "31/39:\n",
      "def fit(train_ds, epochs, test_ds):\n",
      "  for epoch in range(epochs):\n",
      "    start = time.time()\n",
      "\n",
      "    display.clear_output(wait=True)\n",
      "\n",
      "    for example_input, example_target in test_ds.take(1):\n",
      "      generate_images(generator, example_input, example_target)\n",
      "    print(\"Epoch: \", epoch)\n",
      "\n",
      "    # Train\n",
      "    for n, (input_image, target) in train_ds.enumerate():\n",
      "      print('.', end='')\n",
      "      if (n+1) % 100 == 0:\n",
      "        print()\n",
      "      train_step(input_image, target, epoch)\n",
      "    print()\n",
      "\n",
      "    # saving (checkpoint) the model every 20 epochs\n",
      "    if (epoch + 1) % 20 == 0:\n",
      "      checkpoint.save(file_prefix=checkpoint_prefix)\n",
      "\n",
      "    print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n",
      "                                                        time.time()-start))\n",
      "  checkpoint.save(file_prefix=checkpoint_prefix)\n",
      "31/40:\n",
      "#docs_infra: no_execute\n",
      "%load_ext tensorboard\n",
      "%tensorboard --logdir {log_dir}\n",
      "31/41: fit(train_dataset, EPOCHS, test_dataset)\n",
      "32/1:\n",
      "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "# https://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "33/1:\n",
      "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "# https://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "33/2:\n",
      "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "# https://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "33/3:\n",
      "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "# https://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "34/1:\n",
      "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "# https://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "34/2:\n",
      "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "# https://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "34/3:\n",
      "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "# https://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "from tensorflow-gpu.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "34/4:\n",
      "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "# https://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "34/5:\n",
      "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "# https://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "34/6:\n",
      "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "# https://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "34/7:\n",
      "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "# https://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "34/8:\n",
      "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "# https://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "34/9:\n",
      "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "# https://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "34/10:\n",
      "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "# https://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "import tensorflow as tf\n",
      "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
      "34/11:\n",
      "import tensorflow as tf\n",
      "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
      "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
      "34/12:\n",
      "import tensorflow as tf\n",
      "physical_devices = tf.compat.v1.ConfigProto.experimental.list_physical_devices('GPU')\n",
      "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
      "34/13:\n",
      "# import tensorflow as tf\n",
      "# physical_devices = tf.compat.v1.ConfigProto.experimental.list_physical_devices('GPU')\n",
      "# tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
      "34/14:\n",
      "import tensorflow as tf\n",
      "\n",
      "import os\n",
      "import time\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython import display\n",
      "34/15: !pip install -q -U tensorboard\n",
      "34/16:\n",
      "_URL = 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz'\n",
      "\n",
      "path_to_zip = tf.keras.utils.get_file('facades.tar.gz',\n",
      "                                      origin=_URL,\n",
      "                                      extract=True)\n",
      "\n",
      "PATH = os.path.join(os.path.dirname(path_to_zip), 'facades/')\n",
      "35/1:\n",
      "# import tensorflow as tf\n",
      "# physical_devices = tf.compat.v1.ConfigProto.experimental.list_physical_devices('GPU')\n",
      "# tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
      "35/2:\n",
      "import tensorflow as tf\n",
      "\n",
      "import os\n",
      "import time\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython import display\n",
      "35/3:\n",
      "_URL = 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz'\n",
      "\n",
      "path_to_zip = tf.keras.utils.get_file('facades.tar.gz',\n",
      "                                      origin=_URL,\n",
      "                                      extract=True)\n",
      "\n",
      "PATH = os.path.join(os.path.dirname(path_to_zip), 'facades/')\n",
      "35/4:\n",
      "import tensorflow as tf\n",
      "\n",
      "import os\n",
      "import time\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython import display\n",
      "35/5: !pip install -q -U tensorboard\n",
      "35/6:\n",
      "_URL = 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz'\n",
      "\n",
      "path_to_zip = tf.keras.utils.get_file('facades.tar.gz',\n",
      "                                      origin=_URL,\n",
      "                                      extract=True)\n",
      "\n",
      "PATH = os.path.join(os.path.dirname(path_to_zip), 'facades/')\n",
      "35/7:\n",
      "BUFFER_SIZE = 400\n",
      "BATCH_SIZE = 1\n",
      "IMG_WIDTH = 256\n",
      "IMG_HEIGHT = 256\n",
      "35/8:\n",
      "def load(image_file):\n",
      "  image = tf.io.read_file(image_file)\n",
      "  image = tf.image.decode_jpeg(image)\n",
      "\n",
      "  w = tf.shape(image)[1]\n",
      "\n",
      "  w = w // 2\n",
      "  real_image = image[:, :w, :]\n",
      "  input_image = image[:, w:, :]\n",
      "\n",
      "  input_image = tf.cast(input_image, tf.float32)\n",
      "  real_image = tf.cast(real_image, tf.float32)\n",
      "\n",
      "  return input_image, real_image\n",
      "35/9:\n",
      "inp, re = load(PATH+'train/100.jpg')\n",
      "# casting to int for matplotlib to show the image\n",
      "plt.figure()\n",
      "plt.imshow(inp/255.0)\n",
      "plt.figure()\n",
      "plt.imshow(re/255.0)\n",
      "35/10:\n",
      "def resize(input_image, real_image, height, width):\n",
      "  input_image = tf.image.resize(input_image, [height, width],\n",
      "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "  real_image = tf.image.resize(real_image, [height, width],\n",
      "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "\n",
      "  return input_image, real_image\n",
      "35/11:\n",
      "def random_crop(input_image, real_image):\n",
      "  stacked_image = tf.stack([input_image, real_image], axis=0)\n",
      "  cropped_image = tf.image.random_crop(\n",
      "      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
      "\n",
      "  return cropped_image[0], cropped_image[1]\n",
      "35/12:\n",
      "# normalizing the images to [-1, 1]\n",
      "\n",
      "def normalize(input_image, real_image):\n",
      "  input_image = (input_image / 127.5) - 1\n",
      "  real_image = (real_image / 127.5) - 1\n",
      "\n",
      "  return input_image, real_image\n",
      "35/13:\n",
      "@tf.function()\n",
      "def random_jitter(input_image, real_image):\n",
      "  # resizing to 286 x 286 x 3\n",
      "  input_image, real_image = resize(input_image, real_image, 286, 286)\n",
      "\n",
      "  # randomly cropping to 256 x 256 x 3\n",
      "  input_image, real_image = random_crop(input_image, real_image)\n",
      "\n",
      "  if tf.random.uniform(()) > 0.5:\n",
      "    # random mirroring\n",
      "    input_image = tf.image.flip_left_right(input_image)\n",
      "    real_image = tf.image.flip_left_right(real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "35/14:\n",
      "plt.figure(figsize=(6, 6))\n",
      "for i in range(4):\n",
      "  rj_inp, rj_re = random_jitter(inp, re)\n",
      "  plt.subplot(2, 2, i+1)\n",
      "  plt.imshow(rj_inp/255.0)\n",
      "  plt.axis('off')\n",
      "plt.show()\n",
      "35/15:\n",
      "def load_image_train(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = random_jitter(input_image, real_image)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "35/16:\n",
      "def load_image_test(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = resize(input_image, real_image,\n",
      "                                   IMG_HEIGHT, IMG_WIDTH)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "35/17:\n",
      "train_dataset = tf.data.Dataset.list_files(PATH+'train/*.jpg')\n",
      "train_dataset = train_dataset.map(load_image_train,\n",
      "                                  num_parallel_calls=tf.data.AUTOTUNE)\n",
      "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
      "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
      "35/18:\n",
      "test_dataset = tf.data.Dataset.list_files(PATH+'test/*.jpg')\n",
      "test_dataset = test_dataset.map(load_image_test)\n",
      "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
      "35/19: OUTPUT_CHANNELS = 3\n",
      "35/20:\n",
      "def downsample(filters, size, apply_batchnorm=True):\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "\n",
      "  result = tf.keras.Sequential()\n",
      "  result.add(\n",
      "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
      "                             kernel_initializer=initializer, use_bias=False))\n",
      "\n",
      "  if apply_batchnorm:\n",
      "    result.add(tf.keras.layers.BatchNormalization())\n",
      "\n",
      "  result.add(tf.keras.layers.LeakyReLU())\n",
      "\n",
      "  return result\n",
      "35/21:\n",
      "down_model = downsample(3, 4)\n",
      "down_result = down_model(tf.expand_dims(inp, 0))\n",
      "print (down_result.shape)\n",
      "35/22:\n",
      "def upsample(filters, size, apply_dropout=False):\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "\n",
      "  result = tf.keras.Sequential()\n",
      "  result.add(\n",
      "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
      "                                    padding='same',\n",
      "                                    kernel_initializer=initializer,\n",
      "                                    use_bias=False))\n",
      "\n",
      "  result.add(tf.keras.layers.BatchNormalization())\n",
      "\n",
      "  if apply_dropout:\n",
      "      result.add(tf.keras.layers.Dropout(0.5))\n",
      "\n",
      "  result.add(tf.keras.layers.ReLU())\n",
      "\n",
      "  return result\n",
      "35/23:\n",
      "up_model = upsample(3, 4)\n",
      "up_result = up_model(down_result)\n",
      "print (up_result.shape)\n",
      "35/24:\n",
      "def Generator():\n",
      "  inputs = tf.keras.layers.Input(shape=[256, 256, 3])\n",
      "\n",
      "  down_stack = [\n",
      "    downsample(64, 4, apply_batchnorm=False),  # (bs, 128, 128, 64)\n",
      "    downsample(128, 4),  # (bs, 64, 64, 128)\n",
      "    downsample(256, 4),  # (bs, 32, 32, 256)\n",
      "    downsample(512, 4),  # (bs, 16, 16, 512)\n",
      "    downsample(512, 4),  # (bs, 8, 8, 512)\n",
      "    downsample(512, 4),  # (bs, 4, 4, 512)\n",
      "    downsample(512, 4),  # (bs, 2, 2, 512)\n",
      "    downsample(512, 4),  # (bs, 1, 1, 512)\n",
      "  ]\n",
      "\n",
      "  up_stack = [\n",
      "    upsample(512, 4, apply_dropout=True),  # (bs, 2, 2, 1024)\n",
      "    upsample(512, 4, apply_dropout=True),  # (bs, 4, 4, 1024)\n",
      "    upsample(512, 4, apply_dropout=True),  # (bs, 8, 8, 1024)\n",
      "    upsample(512, 4),  # (bs, 16, 16, 1024)\n",
      "    upsample(256, 4),  # (bs, 32, 32, 512)\n",
      "    upsample(128, 4),  # (bs, 64, 64, 256)\n",
      "    upsample(64, 4),  # (bs, 128, 128, 128)\n",
      "  ]\n",
      "\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "  last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
      "                                         strides=2,\n",
      "                                         padding='same',\n",
      "                                         kernel_initializer=initializer,\n",
      "                                         activation='tanh')  # (bs, 256, 256, 3)\n",
      "\n",
      "  x = inputs\n",
      "\n",
      "  # Downsampling through the model\n",
      "  skips = []\n",
      "  for down in down_stack:\n",
      "    x = down(x)\n",
      "    skips.append(x)\n",
      "\n",
      "  skips = reversed(skips[:-1])\n",
      "\n",
      "  # Upsampling and establishing the skip connections\n",
      "  for up, skip in zip(up_stack, skips):\n",
      "    x = up(x)\n",
      "    x = tf.keras.layers.Concatenate()([x, skip])\n",
      "\n",
      "  x = last(x)\n",
      "\n",
      "  return tf.keras.Model(inputs=inputs, outputs=x)\n",
      "35/25:\n",
      "generator = Generator()\n",
      "tf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)\n",
      "35/26:\n",
      "gen_output = generator(inp[tf.newaxis, ...], training=False)\n",
      "plt.imshow(gen_output[0, ...])\n",
      "35/27: LAMBDA = 100\n",
      "35/28: loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
      "35/29:\n",
      "def generator_loss(disc_generated_output, gen_output, target):\n",
      "  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
      "\n",
      "  # mean absolute error\n",
      "  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
      "\n",
      "  total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
      "\n",
      "  return total_gen_loss, gan_loss, l1_loss\n",
      "35/30:\n",
      "def Discriminator():\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "\n",
      "  inp = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image')\n",
      "  tar = tf.keras.layers.Input(shape=[256, 256, 3], name='target_image')\n",
      "\n",
      "  x = tf.keras.layers.concatenate([inp, tar])  # (bs, 256, 256, channels*2)\n",
      "\n",
      "  down1 = downsample(64, 4, False)(x)  # (bs, 128, 128, 64)\n",
      "  down2 = downsample(128, 4)(down1)  # (bs, 64, 64, 128)\n",
      "  down3 = downsample(256, 4)(down2)  # (bs, 32, 32, 256)\n",
      "\n",
      "  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)  # (bs, 34, 34, 256)\n",
      "  conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n",
      "                                kernel_initializer=initializer,\n",
      "                                use_bias=False)(zero_pad1)  # (bs, 31, 31, 512)\n",
      "\n",
      "  batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
      "\n",
      "  leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
      "\n",
      "  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)  # (bs, 33, 33, 512)\n",
      "\n",
      "  last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
      "                                kernel_initializer=initializer)(zero_pad2)  # (bs, 30, 30, 1)\n",
      "\n",
      "  return tf.keras.Model(inputs=[inp, tar], outputs=last)\n",
      "35/31:\n",
      "discriminator = Discriminator()\n",
      "tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)\n",
      "35/32:\n",
      "disc_out = discriminator([inp[tf.newaxis, ...], gen_output], training=False)\n",
      "plt.imshow(disc_out[0, ..., -1], vmin=-20, vmax=20, cmap='RdBu_r')\n",
      "plt.colorbar()\n",
      "35/33:\n",
      "def discriminator_loss(disc_real_output, disc_generated_output):\n",
      "  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
      "\n",
      "  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
      "\n",
      "  total_disc_loss = real_loss + generated_loss\n",
      "\n",
      "  return total_disc_loss\n",
      "35/34:\n",
      "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
      "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
      "35/35:\n",
      "checkpoint_dir = './training_checkpoints'\n",
      "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
      "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
      "                                 discriminator_optimizer=discriminator_optimizer,\n",
      "                                 generator=generator,\n",
      "                                 discriminator=discriminator)\n",
      "35/36:\n",
      "def generate_images(model, test_input, tar):\n",
      "  prediction = model(test_input, training=True)\n",
      "  plt.figure(figsize=(15, 15))\n",
      "\n",
      "  display_list = [test_input[0], tar[0], prediction[0]]\n",
      "  title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
      "\n",
      "  for i in range(3):\n",
      "    plt.subplot(1, 3, i+1)\n",
      "    plt.title(title[i])\n",
      "    # getting the pixel values between [0, 1] to plot it.\n",
      "    plt.imshow(display_list[i] * 0.5 + 0.5)\n",
      "    plt.axis('off')\n",
      "  plt.show()\n",
      "35/37:\n",
      "for example_input, example_target in test_dataset.take(1):\n",
      "  generate_images(generator, example_input, example_target)\n",
      "35/38: EPOCHS = 150\n",
      "35/39:\n",
      "import datetime\n",
      "log_dir=\"logs/\"\n",
      "\n",
      "summary_writer = tf.summary.create_file_writer(\n",
      "  log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
      "35/40:\n",
      "@tf.function\n",
      "def train_step(input_image, target, epoch):\n",
      "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
      "    gen_output = generator(input_image, training=True)\n",
      "\n",
      "    disc_real_output = discriminator([input_image, target], training=True)\n",
      "    disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
      "\n",
      "    gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n",
      "    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
      "\n",
      "  generator_gradients = gen_tape.gradient(gen_total_loss,\n",
      "                                          generator.trainable_variables)\n",
      "  discriminator_gradients = disc_tape.gradient(disc_loss,\n",
      "                                               discriminator.trainable_variables)\n",
      "\n",
      "  generator_optimizer.apply_gradients(zip(generator_gradients,\n",
      "                                          generator.trainable_variables))\n",
      "  discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
      "                                              discriminator.trainable_variables))\n",
      "\n",
      "  with summary_writer.as_default():\n",
      "    tf.summary.scalar('gen_total_loss', gen_total_loss, step=epoch)\n",
      "    tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=epoch)\n",
      "    tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=epoch)\n",
      "    tf.summary.scalar('disc_loss', disc_loss, step=epoch)\n",
      "35/41:\n",
      "def fit(train_ds, epochs, test_ds):\n",
      "  for epoch in range(epochs):\n",
      "    start = time.time()\n",
      "\n",
      "    display.clear_output(wait=True)\n",
      "\n",
      "    for example_input, example_target in test_ds.take(1):\n",
      "      generate_images(generator, example_input, example_target)\n",
      "    print(\"Epoch: \", epoch)\n",
      "\n",
      "    # Train\n",
      "    for n, (input_image, target) in train_ds.enumerate():\n",
      "      print('.', end='')\n",
      "      if (n+1) % 100 == 0:\n",
      "        print()\n",
      "      train_step(input_image, target, epoch)\n",
      "    print()\n",
      "\n",
      "    # saving (checkpoint) the model every 20 epochs\n",
      "    if (epoch + 1) % 20 == 0:\n",
      "      checkpoint.save(file_prefix=checkpoint_prefix)\n",
      "\n",
      "    print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n",
      "                                                        time.time()-start))\n",
      "  checkpoint.save(file_prefix=checkpoint_prefix)\n",
      "35/42:\n",
      "#docs_infra: no_execute\n",
      "%load_ext tensorboard\n",
      "%tensorboard --logdir {log_dir}\n",
      "35/43: fit(train_dataset, EPOCHS, test_dataset)\n",
      "35/44:\n",
      "import tensorflow as tf\n",
      "\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "\n",
      "import os\n",
      "import time\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython import display\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/45:\n",
      "\n",
      "import tensorflow as tf\n",
      "\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "\n",
      "import os\n",
      "import time\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython import display\n",
      "35/46:\n",
      "\n",
      "!pip install -q -U tensorboard\n",
      "35/47:\n",
      "\n",
      "_URL = 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz'\n",
      "\n",
      "path_to_zip = tf.keras.utils.get_file('facades.tar.gz',\n",
      "                                      origin=_URL,\n",
      "                                      extract=True)\n",
      "\n",
      "PATH = os.path.join(os.path.dirname(path_to_zip), 'facades/')\n",
      "36/1:\n",
      "import tensorflow as tf\n",
      "\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "\n",
      "import os\n",
      "import time\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython import display\n",
      "37/1:\n",
      "import tensorflow as tf\n",
      "\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "\n",
      "import os\n",
      "import time\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython import display\n",
      "37/2:\n",
      "import tensorflow-gpu as tf\n",
      "\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "\n",
      "import os\n",
      "import time\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython import display\n",
      "37/3:\n",
      "import tensorflow as tf\n",
      "\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "\n",
      "import os\n",
      "import time\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython import display\n",
      "37/4:\n",
      "import tensorflow as tf\n",
      "\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "tf.config.list_physical devices()\n",
      "\n",
      "import os\n",
      "import time\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython import display\n",
      "37/5:\n",
      "import tensorflow as tf\n",
      "\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "tf.config.list_physical_devices()\n",
      "\n",
      "import os\n",
      "import time\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython import display\n",
      "37/6:\n",
      "import tensorflow as tf\n",
      "\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "print(tf.config.list_physical_devices())\n",
      "\n",
      "import os\n",
      "import time\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython import display\n",
      "37/7:\n",
      "import tensorflow as tf\n",
      "\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "print(tf.config.list_physical_devices())\n",
      "\n",
      "import os\n",
      "import time\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython import display\n",
      "38/1:\n",
      "import tensorflow as tf\n",
      "\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "print(tf.config.list_physical_devices())\n",
      "\n",
      "import os\n",
      "import time\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython import display\n",
      "38/2:\n",
      "import tensorflow as tf\n",
      "\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "print(tf.config.list_physical_devices())\n",
      "\n",
      "import os\n",
      "import time\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython import display\n",
      "38/3: !pip install -q -U tensorboard\n",
      "38/4:\n",
      "_URL = 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz'\n",
      "\n",
      "path_to_zip = tf.keras.utils.get_file('facades.tar.gz',\n",
      "                                      origin=_URL,\n",
      "                                      extract=True)\n",
      "\n",
      "PATH = os.path.join(os.path.dirname(path_to_zip), 'facades/')\n",
      "38/5:\n",
      "BUFFER_SIZE = 400\n",
      "BATCH_SIZE = 1\n",
      "IMG_WIDTH = 256\n",
      "IMG_HEIGHT = 256\n",
      "38/6:\n",
      "def load(image_file):\n",
      "  image = tf.io.read_file(image_file)\n",
      "  image = tf.image.decode_jpeg(image)\n",
      "\n",
      "  w = tf.shape(image)[1]\n",
      "\n",
      "  w = w // 2\n",
      "  real_image = image[:, :w, :]\n",
      "  input_image = image[:, w:, :]\n",
      "\n",
      "  input_image = tf.cast(input_image, tf.float32)\n",
      "  real_image = tf.cast(real_image, tf.float32)\n",
      "\n",
      "  return input_image, real_image\n",
      "38/7:\n",
      "inp, re = load(PATH+'train/100.jpg')\n",
      "# casting to int for matplotlib to show the image\n",
      "plt.figure()\n",
      "plt.imshow(inp/255.0)\n",
      "plt.figure()\n",
      "plt.imshow(re/255.0)\n",
      "38/8:\n",
      "def resize(input_image, real_image, height, width):\n",
      "  input_image = tf.image.resize(input_image, [height, width],\n",
      "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "  real_image = tf.image.resize(real_image, [height, width],\n",
      "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "\n",
      "  return input_image, real_image\n",
      "38/9:\n",
      "def random_crop(input_image, real_image):\n",
      "  stacked_image = tf.stack([input_image, real_image], axis=0)\n",
      "  cropped_image = tf.image.random_crop(\n",
      "      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
      "\n",
      "  return cropped_image[0], cropped_image[1]\n",
      "38/10:\n",
      "# normalizing the images to [-1, 1]\n",
      "\n",
      "def normalize(input_image, real_image):\n",
      "  input_image = (input_image / 127.5) - 1\n",
      "  real_image = (real_image / 127.5) - 1\n",
      "\n",
      "  return input_image, real_image\n",
      "38/11:\n",
      "@tf.function()\n",
      "def random_jitter(input_image, real_image):\n",
      "  # resizing to 286 x 286 x 3\n",
      "  input_image, real_image = resize(input_image, real_image, 286, 286)\n",
      "\n",
      "  # randomly cropping to 256 x 256 x 3\n",
      "  input_image, real_image = random_crop(input_image, real_image)\n",
      "\n",
      "  if tf.random.uniform(()) > 0.5:\n",
      "    # random mirroring\n",
      "    input_image = tf.image.flip_left_right(input_image)\n",
      "    real_image = tf.image.flip_left_right(real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "38/12:\n",
      "plt.figure(figsize=(6, 6))\n",
      "for i in range(4):\n",
      "  rj_inp, rj_re = random_jitter(inp, re)\n",
      "  plt.subplot(2, 2, i+1)\n",
      "  plt.imshow(rj_inp/255.0)\n",
      "  plt.axis('off')\n",
      "plt.show()\n",
      "38/13:\n",
      "def load_image_train(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = random_jitter(input_image, real_image)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "38/14:\n",
      "def load_image_test(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = resize(input_image, real_image,\n",
      "                                   IMG_HEIGHT, IMG_WIDTH)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "38/15:\n",
      "train_dataset = tf.data.Dataset.list_files(PATH+'train/*.jpg')\n",
      "train_dataset = train_dataset.map(load_image_train,\n",
      "                                  num_parallel_calls=tf.data.AUTOTUNE)\n",
      "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
      "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
      "38/16:\n",
      "test_dataset = tf.data.Dataset.list_files(PATH+'test/*.jpg')\n",
      "test_dataset = test_dataset.map(load_image_test)\n",
      "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
      "38/17: OUTPUT_CHANNELS = 3\n",
      "38/18:\n",
      "def downsample(filters, size, apply_batchnorm=True):\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "\n",
      "  result = tf.keras.Sequential()\n",
      "  result.add(\n",
      "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
      "                             kernel_initializer=initializer, use_bias=False))\n",
      "\n",
      "  if apply_batchnorm:\n",
      "    result.add(tf.keras.layers.BatchNormalization())\n",
      "\n",
      "  result.add(tf.keras.layers.LeakyReLU())\n",
      "\n",
      "  return result\n",
      "38/19:\n",
      "down_model = downsample(3, 4)\n",
      "down_result = down_model(tf.expand_dims(inp, 0))\n",
      "print (down_result.shape)\n",
      "39/1:\n",
      "import tensorflow as tf\n",
      "\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "print(tf.config.list_physical_devices())\n",
      "\n",
      "import os\n",
      "import time\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython import display\n",
      "39/2: !pip install -q -U tensorboard\n",
      "39/3:\n",
      "_URL = 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz'\n",
      "\n",
      "path_to_zip = tf.keras.utils.get_file('facades.tar.gz',\n",
      "                                      origin=_URL,\n",
      "                                      extract=True)\n",
      "\n",
      "PATH = os.path.join(os.path.dirname(path_to_zip), 'facades/')\n",
      "39/4:\n",
      "BUFFER_SIZE = 400\n",
      "BATCH_SIZE = 1\n",
      "IMG_WIDTH = 256\n",
      "IMG_HEIGHT = 256\n",
      "39/5:\n",
      "def load(image_file):\n",
      "  image = tf.io.read_file(image_file)\n",
      "  image = tf.image.decode_jpeg(image)\n",
      "\n",
      "  w = tf.shape(image)[1]\n",
      "\n",
      "  w = w // 2\n",
      "  real_image = image[:, :w, :]\n",
      "  input_image = image[:, w:, :]\n",
      "\n",
      "  input_image = tf.cast(input_image, tf.float32)\n",
      "  real_image = tf.cast(real_image, tf.float32)\n",
      "\n",
      "  return input_image, real_image\n",
      "39/6:\n",
      "inp, re = load(PATH+'train/100.jpg')\n",
      "# casting to int for matplotlib to show the image\n",
      "plt.figure()\n",
      "plt.imshow(inp/255.0)\n",
      "plt.figure()\n",
      "plt.imshow(re/255.0)\n",
      "39/7:\n",
      "def resize(input_image, real_image, height, width):\n",
      "  input_image = tf.image.resize(input_image, [height, width],\n",
      "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "  real_image = tf.image.resize(real_image, [height, width],\n",
      "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "\n",
      "  return input_image, real_image\n",
      "39/8:\n",
      "def random_crop(input_image, real_image):\n",
      "  stacked_image = tf.stack([input_image, real_image], axis=0)\n",
      "  cropped_image = tf.image.random_crop(\n",
      "      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
      "\n",
      "  return cropped_image[0], cropped_image[1]\n",
      "39/9:\n",
      "# normalizing the images to [-1, 1]\n",
      "\n",
      "def normalize(input_image, real_image):\n",
      "  input_image = (input_image / 127.5) - 1\n",
      "  real_image = (real_image / 127.5) - 1\n",
      "\n",
      "  return input_image, real_image\n",
      "39/10:\n",
      "@tf.function()\n",
      "def random_jitter(input_image, real_image):\n",
      "  # resizing to 286 x 286 x 3\n",
      "  input_image, real_image = resize(input_image, real_image, 286, 286)\n",
      "\n",
      "  # randomly cropping to 256 x 256 x 3\n",
      "  input_image, real_image = random_crop(input_image, real_image)\n",
      "\n",
      "  if tf.random.uniform(()) > 0.5:\n",
      "    # random mirroring\n",
      "    input_image = tf.image.flip_left_right(input_image)\n",
      "    real_image = tf.image.flip_left_right(real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "39/11:\n",
      "plt.figure(figsize=(6, 6))\n",
      "for i in range(4):\n",
      "  rj_inp, rj_re = random_jitter(inp, re)\n",
      "  plt.subplot(2, 2, i+1)\n",
      "  plt.imshow(rj_inp/255.0)\n",
      "  plt.axis('off')\n",
      "plt.show()\n",
      "39/12:\n",
      "def load_image_train(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = random_jitter(input_image, real_image)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "39/13:\n",
      "def load_image_test(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = resize(input_image, real_image,\n",
      "                                   IMG_HEIGHT, IMG_WIDTH)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "39/14:\n",
      "train_dataset = tf.data.Dataset.list_files(PATH+'train/*.jpg')\n",
      "train_dataset = train_dataset.map(load_image_train,\n",
      "                                  num_parallel_calls=tf.data.AUTOTUNE)\n",
      "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
      "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
      "39/15:\n",
      "test_dataset = tf.data.Dataset.list_files(PATH+'test/*.jpg')\n",
      "test_dataset = test_dataset.map(load_image_test)\n",
      "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
      "39/16: OUTPUT_CHANNELS = 3\n",
      "39/17:\n",
      "def downsample(filters, size, apply_batchnorm=True):\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "\n",
      "  result = tf.keras.Sequential()\n",
      "  result.add(\n",
      "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
      "                             kernel_initializer=initializer, use_bias=False))\n",
      "\n",
      "  if apply_batchnorm:\n",
      "    result.add(tf.keras.layers.BatchNormalization())\n",
      "\n",
      "  result.add(tf.keras.layers.LeakyReLU())\n",
      "\n",
      "  return result\n",
      "39/18:\n",
      "down_model = downsample(3, 4)\n",
      "down_result = down_model(tf.expand_dims(inp, 0))\n",
      "print (down_result.shape)\n",
      "40/1:\n",
      "import tensorflow as tf\n",
      "\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "print(tf.config.list_physical_devices())\n",
      "\n",
      "import os\n",
      "import time\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython import display\n",
      "40/2: !pip install -q -U tensorboard\n",
      "40/3:\n",
      "_URL = 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz'\n",
      "\n",
      "path_to_zip = tf.keras.utils.get_file('facades.tar.gz',\n",
      "                                      origin=_URL,\n",
      "                                      extract=True)\n",
      "\n",
      "PATH = os.path.join(os.path.dirname(path_to_zip), 'facades/')\n",
      "40/4:\n",
      "BUFFER_SIZE = 400\n",
      "BATCH_SIZE = 1\n",
      "IMG_WIDTH = 256\n",
      "IMG_HEIGHT = 256\n",
      "40/5:\n",
      "def load(image_file):\n",
      "  image = tf.io.read_file(image_file)\n",
      "  image = tf.image.decode_jpeg(image)\n",
      "\n",
      "  w = tf.shape(image)[1]\n",
      "\n",
      "  w = w // 2\n",
      "  real_image = image[:, :w, :]\n",
      "  input_image = image[:, w:, :]\n",
      "\n",
      "  input_image = tf.cast(input_image, tf.float32)\n",
      "  real_image = tf.cast(real_image, tf.float32)\n",
      "\n",
      "  return input_image, real_image\n",
      "40/6:\n",
      "inp, re = load(PATH+'train/100.jpg')\n",
      "# casting to int for matplotlib to show the image\n",
      "plt.figure()\n",
      "plt.imshow(inp/255.0)\n",
      "plt.figure()\n",
      "plt.imshow(re/255.0)\n",
      "40/7:\n",
      "def resize(input_image, real_image, height, width):\n",
      "  input_image = tf.image.resize(input_image, [height, width],\n",
      "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "  real_image = tf.image.resize(real_image, [height, width],\n",
      "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "\n",
      "  return input_image, real_image\n",
      "40/8:\n",
      "def random_crop(input_image, real_image):\n",
      "  stacked_image = tf.stack([input_image, real_image], axis=0)\n",
      "  cropped_image = tf.image.random_crop(\n",
      "      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
      "\n",
      "  return cropped_image[0], cropped_image[1]\n",
      "40/9:\n",
      "# normalizing the images to [-1, 1]\n",
      "\n",
      "def normalize(input_image, real_image):\n",
      "  input_image = (input_image / 127.5) - 1\n",
      "  real_image = (real_image / 127.5) - 1\n",
      "\n",
      "  return input_image, real_image\n",
      "40/10:\n",
      "@tf.function()\n",
      "def random_jitter(input_image, real_image):\n",
      "  # resizing to 286 x 286 x 3\n",
      "  input_image, real_image = resize(input_image, real_image, 286, 286)\n",
      "\n",
      "  # randomly cropping to 256 x 256 x 3\n",
      "  input_image, real_image = random_crop(input_image, real_image)\n",
      "\n",
      "  if tf.random.uniform(()) > 0.5:\n",
      "    # random mirroring\n",
      "    input_image = tf.image.flip_left_right(input_image)\n",
      "    real_image = tf.image.flip_left_right(real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "40/11:\n",
      "plt.figure(figsize=(6, 6))\n",
      "for i in range(4):\n",
      "  rj_inp, rj_re = random_jitter(inp, re)\n",
      "  plt.subplot(2, 2, i+1)\n",
      "  plt.imshow(rj_inp/255.0)\n",
      "  plt.axis('off')\n",
      "plt.show()\n",
      "40/12:\n",
      "def load_image_train(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = random_jitter(input_image, real_image)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "40/13:\n",
      "def load_image_test(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = resize(input_image, real_image,\n",
      "                                   IMG_HEIGHT, IMG_WIDTH)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "40/14:\n",
      "train_dataset = tf.data.Dataset.list_files(PATH+'train/*.jpg')\n",
      "train_dataset = train_dataset.map(load_image_train,\n",
      "                                  num_parallel_calls=tf.data.AUTOTUNE)\n",
      "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
      "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
      "40/15:\n",
      "test_dataset = tf.data.Dataset.list_files(PATH+'test/*.jpg')\n",
      "test_dataset = test_dataset.map(load_image_test)\n",
      "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
      "40/16: OUTPUT_CHANNELS = 3\n",
      "40/17:\n",
      "def downsample(filters, size, apply_batchnorm=True):\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "\n",
      "  result = tf.keras.Sequential()\n",
      "  result.add(\n",
      "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
      "                             kernel_initializer=initializer, use_bias=False))\n",
      "\n",
      "  if apply_batchnorm:\n",
      "    result.add(tf.keras.layers.BatchNormalization())\n",
      "\n",
      "  result.add(tf.keras.layers.LeakyReLU())\n",
      "\n",
      "  return result\n",
      "40/18:\n",
      "down_model = downsample(3, 4)\n",
      "down_result = down_model(tf.expand_dims(inp, 0))\n",
      "print (down_result.shape)\n",
      "41/1:\n",
      "import tensorflow as tf\n",
      "\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "print(tf.config.list_physical_devices())\n",
      "\n",
      "import os\n",
      "import time\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython import display\n",
      "41/2: !pip install -q -U tensorboard\n",
      "41/3:\n",
      "_URL = 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz'\n",
      "\n",
      "path_to_zip = tf.keras.utils.get_file('facades.tar.gz',\n",
      "                                      origin=_URL,\n",
      "                                      extract=True)\n",
      "\n",
      "PATH = os.path.join(os.path.dirname(path_to_zip), 'facades/')\n",
      "41/4:\n",
      "BUFFER_SIZE = 400\n",
      "BATCH_SIZE = 1\n",
      "IMG_WIDTH = 256\n",
      "IMG_HEIGHT = 256\n",
      "41/5:\n",
      "def load(image_file):\n",
      "  image = tf.io.read_file(image_file)\n",
      "  image = tf.image.decode_jpeg(image)\n",
      "\n",
      "  w = tf.shape(image)[1]\n",
      "\n",
      "  w = w // 2\n",
      "  real_image = image[:, :w, :]\n",
      "  input_image = image[:, w:, :]\n",
      "\n",
      "  input_image = tf.cast(input_image, tf.float32)\n",
      "  real_image = tf.cast(real_image, tf.float32)\n",
      "\n",
      "  return input_image, real_image\n",
      "41/6:\n",
      "inp, re = load(PATH+'train/100.jpg')\n",
      "# casting to int for matplotlib to show the image\n",
      "plt.figure()\n",
      "plt.imshow(inp/255.0)\n",
      "plt.figure()\n",
      "plt.imshow(re/255.0)\n",
      "41/7:\n",
      "def resize(input_image, real_image, height, width):\n",
      "  input_image = tf.image.resize(input_image, [height, width],\n",
      "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "  real_image = tf.image.resize(real_image, [height, width],\n",
      "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "\n",
      "  return input_image, real_image\n",
      "41/8:\n",
      "def random_crop(input_image, real_image):\n",
      "  stacked_image = tf.stack([input_image, real_image], axis=0)\n",
      "  cropped_image = tf.image.random_crop(\n",
      "      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
      "\n",
      "  return cropped_image[0], cropped_image[1]\n",
      "41/9:\n",
      "# normalizing the images to [-1, 1]\n",
      "\n",
      "def normalize(input_image, real_image):\n",
      "  input_image = (input_image / 127.5) - 1\n",
      "  real_image = (real_image / 127.5) - 1\n",
      "\n",
      "  return input_image, real_image\n",
      "41/10:\n",
      "@tf.function()\n",
      "def random_jitter(input_image, real_image):\n",
      "  # resizing to 286 x 286 x 3\n",
      "  input_image, real_image = resize(input_image, real_image, 286, 286)\n",
      "\n",
      "  # randomly cropping to 256 x 256 x 3\n",
      "  input_image, real_image = random_crop(input_image, real_image)\n",
      "\n",
      "  if tf.random.uniform(()) > 0.5:\n",
      "    # random mirroring\n",
      "    input_image = tf.image.flip_left_right(input_image)\n",
      "    real_image = tf.image.flip_left_right(real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "41/11:\n",
      "plt.figure(figsize=(6, 6))\n",
      "for i in range(4):\n",
      "  rj_inp, rj_re = random_jitter(inp, re)\n",
      "  plt.subplot(2, 2, i+1)\n",
      "  plt.imshow(rj_inp/255.0)\n",
      "  plt.axis('off')\n",
      "plt.show()\n",
      "41/12:\n",
      "def load_image_train(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = random_jitter(input_image, real_image)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "41/13:\n",
      "def load_image_test(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = resize(input_image, real_image,\n",
      "                                   IMG_HEIGHT, IMG_WIDTH)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "41/14:\n",
      "train_dataset = tf.data.Dataset.list_files(PATH+'train/*.jpg')\n",
      "train_dataset = train_dataset.map(load_image_train,\n",
      "                                  num_parallel_calls=tf.data.AUTOTUNE)\n",
      "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
      "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
      "41/15:\n",
      "test_dataset = tf.data.Dataset.list_files(PATH+'test/*.jpg')\n",
      "test_dataset = test_dataset.map(load_image_test)\n",
      "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
      "41/16: OUTPUT_CHANNELS = 3\n",
      "41/17:\n",
      "# def downsample(filters, size, apply_batchnorm=True):\n",
      "#   initializer = tf.random_normal_initializer(0., 0.02)\n",
      "\n",
      "#   result = tf.keras.Sequential()\n",
      "#   result.add(\n",
      "#       tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
      "#                              kernel_initializer=initializer, use_bias=False))\n",
      "\n",
      "#   if apply_batchnorm:\n",
      "#     result.add(tf.keras.layers.BatchNormalization())\n",
      "\n",
      "#   result.add(tf.keras.layers.LeakyReLU())\n",
      "\n",
      "#   return result\n",
      "41/18:\n",
      "down_model = downsample(3, 4)\n",
      "down_result = down_model(tf.expand_dims(inp, 0))\n",
      "print (down_result.shape)\n",
      "41/19:\n",
      "def downsample(filters, size, apply_batchnorm=True):\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "\n",
      "  result = tf.keras.Sequential()\n",
      "  result.add(\n",
      "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
      "                             kernel_initializer=initializer, use_bias=False))\n",
      "\n",
      "  if apply_batchnorm:\n",
      "    result.add(tf.keras.layers.BatchNormalization())\n",
      "\n",
      "  result.add(tf.keras.layers.LeakyReLU())\n",
      "\n",
      "  return result\n",
      "41/20:\n",
      "def downsample(filters, size, apply_batchnorm=True):\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "\n",
      "  result = tf.keras.Sequential()\n",
      "  result.add(\n",
      "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
      "                             kernel_initializer=initializer, use_bias=False))\n",
      "\n",
      "  if apply_batchnorm:\n",
      "    result.add(tf.keras.layers.BatchNormalization())\n",
      "\n",
      "  result.add(tf.keras.layers.LeakyReLU())\n",
      "\n",
      "  return result\n",
      "41/21:\n",
      "down_model = downsample(3, 4)\n",
      "down_result = down_model(tf.expand_dims(inp, 0))\n",
      "print (down_result.shape)\n",
      "42/1:\n",
      "down_model = downsample(3, 4)\n",
      "down_result = down_model(tf.expand_dims(inp, 0))\n",
      "print (down_result.shape)\n",
      "42/2:\n",
      "def downsample(filters, size, apply_batchnorm=True):\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "\n",
      "  result = tf.keras.Sequential()\n",
      "  result.add(\n",
      "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
      "                             kernel_initializer=initializer, use_bias=False))\n",
      "\n",
      "  if apply_batchnorm:\n",
      "    result.add(tf.keras.layers.BatchNormalization())\n",
      "\n",
      "  result.add(tf.keras.layers.LeakyReLU())\n",
      "\n",
      "  return result\n",
      "42/3:\n",
      "down_model = downsample(3, 4)\n",
      "down_result = down_model(tf.expand_dims(inp, 0))\n",
      "print (down_result.shape)\n",
      "42/4:\n",
      "import tensorflow as tf\n",
      "\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "print(tf.config.list_physical_devices())\n",
      "\n",
      "import os\n",
      "import time\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython import display\n",
      "42/5:\n",
      "down_model = downsample(3, 4)\n",
      "down_result = down_model(tf.expand_dims(inp, 0))\n",
      "print (down_result.shape)\n",
      "42/6:\n",
      "import tensorflow as tf\n",
      "\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "print(tf.config.list_physical_devices())\n",
      "\n",
      "import os\n",
      "import time\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython import display\n",
      "42/7: !pip install -q -U tensorboard\n",
      "42/8:\n",
      "_URL = 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz'\n",
      "\n",
      "path_to_zip = tf.keras.utils.get_file('facades.tar.gz',\n",
      "                                      origin=_URL,\n",
      "                                      extract=True)\n",
      "\n",
      "PATH = os.path.join(os.path.dirname(path_to_zip), 'facades/')\n",
      "42/9:\n",
      "BUFFER_SIZE = 400\n",
      "BATCH_SIZE = 1\n",
      "IMG_WIDTH = 256\n",
      "IMG_HEIGHT = 256\n",
      "42/10:\n",
      "def load(image_file):\n",
      "  image = tf.io.read_file(image_file)\n",
      "  image = tf.image.decode_jpeg(image)\n",
      "\n",
      "  w = tf.shape(image)[1]\n",
      "\n",
      "  w = w // 2\n",
      "  real_image = image[:, :w, :]\n",
      "  input_image = image[:, w:, :]\n",
      "\n",
      "  input_image = tf.cast(input_image, tf.float32)\n",
      "  real_image = tf.cast(real_image, tf.float32)\n",
      "\n",
      "  return input_image, real_image\n",
      "42/11:\n",
      "inp, re = load(PATH+'train/100.jpg')\n",
      "# casting to int for matplotlib to show the image\n",
      "plt.figure()\n",
      "plt.imshow(inp/255.0)\n",
      "plt.figure()\n",
      "plt.imshow(re/255.0)\n",
      "42/12:\n",
      "def resize(input_image, real_image, height, width):\n",
      "  input_image = tf.image.resize(input_image, [height, width],\n",
      "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "  real_image = tf.image.resize(real_image, [height, width],\n",
      "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "\n",
      "  return input_image, real_image\n",
      "42/13:\n",
      "def random_crop(input_image, real_image):\n",
      "  stacked_image = tf.stack([input_image, real_image], axis=0)\n",
      "  cropped_image = tf.image.random_crop(\n",
      "      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
      "\n",
      "  return cropped_image[0], cropped_image[1]\n",
      "42/14:\n",
      "# normalizing the images to [-1, 1]\n",
      "\n",
      "def normalize(input_image, real_image):\n",
      "  input_image = (input_image / 127.5) - 1\n",
      "  real_image = (real_image / 127.5) - 1\n",
      "\n",
      "  return input_image, real_image\n",
      "42/15:\n",
      "@tf.function()\n",
      "def random_jitter(input_image, real_image):\n",
      "  # resizing to 286 x 286 x 3\n",
      "  input_image, real_image = resize(input_image, real_image, 286, 286)\n",
      "\n",
      "  # randomly cropping to 256 x 256 x 3\n",
      "  input_image, real_image = random_crop(input_image, real_image)\n",
      "\n",
      "  if tf.random.uniform(()) > 0.5:\n",
      "    # random mirroring\n",
      "    input_image = tf.image.flip_left_right(input_image)\n",
      "    real_image = tf.image.flip_left_right(real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "42/16:\n",
      "plt.figure(figsize=(6, 6))\n",
      "for i in range(4):\n",
      "  rj_inp, rj_re = random_jitter(inp, re)\n",
      "  plt.subplot(2, 2, i+1)\n",
      "  plt.imshow(rj_inp/255.0)\n",
      "  plt.axis('off')\n",
      "plt.show()\n",
      "42/17:\n",
      "def load_image_train(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = random_jitter(input_image, real_image)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "42/18:\n",
      "def load_image_test(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = resize(input_image, real_image,\n",
      "                                   IMG_HEIGHT, IMG_WIDTH)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "42/19:\n",
      "train_dataset = tf.data.Dataset.list_files(PATH+'train/*.jpg')\n",
      "train_dataset = train_dataset.map(load_image_train,\n",
      "                                  num_parallel_calls=tf.data.AUTOTUNE)\n",
      "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
      "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
      "42/20:\n",
      "test_dataset = tf.data.Dataset.list_files(PATH+'test/*.jpg')\n",
      "test_dataset = test_dataset.map(load_image_test)\n",
      "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
      "42/21: OUTPUT_CHANNELS = 3\n",
      "42/22:\n",
      "def downsample(filters, size, apply_batchnorm=True):\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "\n",
      "  result = tf.keras.Sequential()\n",
      "  result.add(\n",
      "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
      "                             kernel_initializer=initializer, use_bias=False))\n",
      "\n",
      "  if apply_batchnorm:\n",
      "    result.add(tf.keras.layers.BatchNormalization())\n",
      "\n",
      "  result.add(tf.keras.layers.LeakyReLU())\n",
      "\n",
      "  return result\n",
      "42/23:\n",
      "down_model = downsample(3, 4)\n",
      "down_result = down_model(tf.expand_dims(inp, 0))\n",
      "# print (down_result.shape)\n",
      "43/1:\n",
      "import tensorflow as tf\n",
      "\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "print(tf.config.list_physical_devices())\n",
      "\n",
      "import os\n",
      "import time\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython import display\n",
      "43/2: !pip install -q -U tensorboard\n",
      "43/3:\n",
      "_URL = 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz'\n",
      "\n",
      "path_to_zip = tf.keras.utils.get_file('facades.tar.gz',\n",
      "                                      origin=_URL,\n",
      "                                      extract=True)\n",
      "\n",
      "PATH = os.path.join(os.path.dirname(path_to_zip), 'facades/')\n",
      "43/4:\n",
      "BUFFER_SIZE = 400\n",
      "BATCH_SIZE = 1\n",
      "IMG_WIDTH = 256\n",
      "IMG_HEIGHT = 256\n",
      "43/5:\n",
      "def load(image_file):\n",
      "  image = tf.io.read_file(image_file)\n",
      "  image = tf.image.decode_jpeg(image)\n",
      "\n",
      "  w = tf.shape(image)[1]\n",
      "\n",
      "  w = w // 2\n",
      "  real_image = image[:, :w, :]\n",
      "  input_image = image[:, w:, :]\n",
      "\n",
      "  input_image = tf.cast(input_image, tf.float32)\n",
      "  real_image = tf.cast(real_image, tf.float32)\n",
      "\n",
      "  return input_image, real_image\n",
      "43/6:\n",
      "inp, re = load(PATH+'train/100.jpg')\n",
      "# casting to int for matplotlib to show the image\n",
      "plt.figure()\n",
      "plt.imshow(inp/255.0)\n",
      "plt.figure()\n",
      "plt.imshow(re/255.0)\n",
      "43/7:\n",
      "def resize(input_image, real_image, height, width):\n",
      "  input_image = tf.image.resize(input_image, [height, width],\n",
      "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "  real_image = tf.image.resize(real_image, [height, width],\n",
      "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "\n",
      "  return input_image, real_image\n",
      "43/8:\n",
      "def random_crop(input_image, real_image):\n",
      "  stacked_image = tf.stack([input_image, real_image], axis=0)\n",
      "  cropped_image = tf.image.random_crop(\n",
      "      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
      "\n",
      "  return cropped_image[0], cropped_image[1]\n",
      "43/9:\n",
      "# normalizing the images to [-1, 1]\n",
      "\n",
      "def normalize(input_image, real_image):\n",
      "  input_image = (input_image / 127.5) - 1\n",
      "  real_image = (real_image / 127.5) - 1\n",
      "\n",
      "  return input_image, real_image\n",
      "43/10:\n",
      "@tf.function()\n",
      "def random_jitter(input_image, real_image):\n",
      "  # resizing to 286 x 286 x 3\n",
      "  input_image, real_image = resize(input_image, real_image, 286, 286)\n",
      "\n",
      "  # randomly cropping to 256 x 256 x 3\n",
      "  input_image, real_image = random_crop(input_image, real_image)\n",
      "\n",
      "  if tf.random.uniform(()) > 0.5:\n",
      "    # random mirroring\n",
      "    input_image = tf.image.flip_left_right(input_image)\n",
      "    real_image = tf.image.flip_left_right(real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "43/11:\n",
      "plt.figure(figsize=(6, 6))\n",
      "for i in range(4):\n",
      "  rj_inp, rj_re = random_jitter(inp, re)\n",
      "  plt.subplot(2, 2, i+1)\n",
      "  plt.imshow(rj_inp/255.0)\n",
      "  plt.axis('off')\n",
      "plt.show()\n",
      "43/12:\n",
      "def load_image_train(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = random_jitter(input_image, real_image)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "43/13:\n",
      "def load_image_test(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = resize(input_image, real_image,\n",
      "                                   IMG_HEIGHT, IMG_WIDTH)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "43/14:\n",
      "train_dataset = tf.data.Dataset.list_files(PATH+'train/*.jpg')\n",
      "train_dataset = train_dataset.map(load_image_train,\n",
      "                                  num_parallel_calls=tf.data.AUTOTUNE)\n",
      "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
      "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
      "43/15:\n",
      "test_dataset = tf.data.Dataset.list_files(PATH+'test/*.jpg')\n",
      "test_dataset = test_dataset.map(load_image_test)\n",
      "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
      "43/16: OUTPUT_CHANNELS = 3\n",
      "43/17:\n",
      "def downsample(filters, size, apply_batchnorm=True):\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "\n",
      "  result = tf.keras.Sequential()\n",
      "  result.add(\n",
      "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
      "                             kernel_initializer=initializer, use_bias=False))\n",
      "\n",
      "  if apply_batchnorm:\n",
      "    result.add(tf.keras.layers.BatchNormalization())\n",
      "\n",
      "  result.add(tf.keras.layers.LeakyReLU())\n",
      "\n",
      "  return result\n",
      "43/18:\n",
      "# down_model = downsample(3, 4)\n",
      "# down_result = down_model(tf.expand_dims(inp, 0))\n",
      "# print (down_result.shape)\n",
      "43/19:\n",
      "def upsample(filters, size, apply_dropout=False):\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "\n",
      "  result = tf.keras.Sequential()\n",
      "  result.add(\n",
      "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
      "                                    padding='same',\n",
      "                                    kernel_initializer=initializer,\n",
      "                                    use_bias=False))\n",
      "\n",
      "  result.add(tf.keras.layers.BatchNormalization())\n",
      "\n",
      "  if apply_dropout:\n",
      "      result.add(tf.keras.layers.Dropout(0.5))\n",
      "\n",
      "  result.add(tf.keras.layers.ReLU())\n",
      "\n",
      "  return result\n",
      "43/20:\n",
      "up_model = upsample(3, 4)\n",
      "up_result = up_model(down_result)\n",
      "# print (up_result.shape)\n",
      "43/21:\n",
      "import tensorflow as tf\n",
      "\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "print(tf.config.list_physical_devices())\n",
      "\n",
      "import os\n",
      "import time\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython import display\n",
      "43/22: !pip install -q -U tensorboard\n",
      "43/23:\n",
      "_URL = 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz'\n",
      "\n",
      "path_to_zip = tf.keras.utils.get_file('facades.tar.gz',\n",
      "                                      origin=_URL,\n",
      "                                      extract=True)\n",
      "\n",
      "PATH = os.path.join(os.path.dirname(path_to_zip), 'facades/')\n",
      "43/24:\n",
      "BUFFER_SIZE = 400\n",
      "BATCH_SIZE = 1\n",
      "IMG_WIDTH = 256\n",
      "IMG_HEIGHT = 256\n",
      "43/25:\n",
      "def load(image_file):\n",
      "  image = tf.io.read_file(image_file)\n",
      "  image = tf.image.decode_jpeg(image)\n",
      "\n",
      "  w = tf.shape(image)[1]\n",
      "\n",
      "  w = w // 2\n",
      "  real_image = image[:, :w, :]\n",
      "  input_image = image[:, w:, :]\n",
      "\n",
      "  input_image = tf.cast(input_image, tf.float32)\n",
      "  real_image = tf.cast(real_image, tf.float32)\n",
      "\n",
      "  return input_image, real_image\n",
      "43/26:\n",
      "inp, re = load(PATH+'train/100.jpg')\n",
      "# casting to int for matplotlib to show the image\n",
      "plt.figure()\n",
      "plt.imshow(inp/255.0)\n",
      "plt.figure()\n",
      "plt.imshow(re/255.0)\n",
      "43/27:\n",
      "def resize(input_image, real_image, height, width):\n",
      "  input_image = tf.image.resize(input_image, [height, width],\n",
      "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "  real_image = tf.image.resize(real_image, [height, width],\n",
      "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "\n",
      "  return input_image, real_image\n",
      "43/28:\n",
      "def random_crop(input_image, real_image):\n",
      "  stacked_image = tf.stack([input_image, real_image], axis=0)\n",
      "  cropped_image = tf.image.random_crop(\n",
      "      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
      "\n",
      "  return cropped_image[0], cropped_image[1]\n",
      "43/29:\n",
      "# normalizing the images to [-1, 1]\n",
      "\n",
      "def normalize(input_image, real_image):\n",
      "  input_image = (input_image / 127.5) - 1\n",
      "  real_image = (real_image / 127.5) - 1\n",
      "\n",
      "  return input_image, real_image\n",
      "43/30:\n",
      "@tf.function()\n",
      "def random_jitter(input_image, real_image):\n",
      "  # resizing to 286 x 286 x 3\n",
      "  input_image, real_image = resize(input_image, real_image, 286, 286)\n",
      "\n",
      "  # randomly cropping to 256 x 256 x 3\n",
      "  input_image, real_image = random_crop(input_image, real_image)\n",
      "\n",
      "  if tf.random.uniform(()) > 0.5:\n",
      "    # random mirroring\n",
      "    input_image = tf.image.flip_left_right(input_image)\n",
      "    real_image = tf.image.flip_left_right(real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "43/31:\n",
      "plt.figure(figsize=(6, 6))\n",
      "for i in range(4):\n",
      "  rj_inp, rj_re = random_jitter(inp, re)\n",
      "  plt.subplot(2, 2, i+1)\n",
      "  plt.imshow(rj_inp/255.0)\n",
      "  plt.axis('off')\n",
      "plt.show()\n",
      "43/32:\n",
      "def load_image_train(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = random_jitter(input_image, real_image)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "43/33:\n",
      "def load_image_test(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = resize(input_image, real_image,\n",
      "                                   IMG_HEIGHT, IMG_WIDTH)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "43/34:\n",
      "train_dataset = tf.data.Dataset.list_files(PATH+'train/*.jpg')\n",
      "train_dataset = train_dataset.map(load_image_train,\n",
      "                                  num_parallel_calls=tf.data.AUTOTUNE)\n",
      "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
      "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
      "43/35:\n",
      "test_dataset = tf.data.Dataset.list_files(PATH+'test/*.jpg')\n",
      "test_dataset = test_dataset.map(load_image_test)\n",
      "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
      "43/36: OUTPUT_CHANNELS = 3\n",
      "43/37:\n",
      "def downsample(filters, size, apply_batchnorm=True):\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "\n",
      "  result = tf.keras.Sequential()\n",
      "  result.add(\n",
      "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
      "                             kernel_initializer=initializer, use_bias=False))\n",
      "\n",
      "  if apply_batchnorm:\n",
      "    result.add(tf.keras.layers.BatchNormalization())\n",
      "\n",
      "  result.add(tf.keras.layers.LeakyReLU())\n",
      "\n",
      "  return result\n",
      "43/38:\n",
      "# down_model = downsample(3, 4)\n",
      "# down_result = down_model(tf.expand_dims(inp, 0))\n",
      "# print (down_result.shape)\n",
      "43/39:\n",
      "def upsample(filters, size, apply_dropout=False):\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "\n",
      "  result = tf.keras.Sequential()\n",
      "  result.add(\n",
      "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
      "                                    padding='same',\n",
      "                                    kernel_initializer=initializer,\n",
      "                                    use_bias=False))\n",
      "\n",
      "  result.add(tf.keras.layers.BatchNormalization())\n",
      "\n",
      "  if apply_dropout:\n",
      "      result.add(tf.keras.layers.Dropout(0.5))\n",
      "\n",
      "  result.add(tf.keras.layers.ReLU())\n",
      "\n",
      "  return result\n",
      "43/40:\n",
      "# up_model = upsample(3, 4)\n",
      "# up_result = up_model(down_result)\n",
      "# print (up_result.shape)\n",
      "43/41:\n",
      "def Generator():\n",
      "  inputs = tf.keras.layers.Input(shape=[256, 256, 3])\n",
      "\n",
      "  down_stack = [\n",
      "    downsample(64, 4, apply_batchnorm=False),  # (bs, 128, 128, 64)\n",
      "    downsample(128, 4),  # (bs, 64, 64, 128)\n",
      "    downsample(256, 4),  # (bs, 32, 32, 256)\n",
      "    downsample(512, 4),  # (bs, 16, 16, 512)\n",
      "    downsample(512, 4),  # (bs, 8, 8, 512)\n",
      "    downsample(512, 4),  # (bs, 4, 4, 512)\n",
      "    downsample(512, 4),  # (bs, 2, 2, 512)\n",
      "    downsample(512, 4),  # (bs, 1, 1, 512)\n",
      "  ]\n",
      "\n",
      "  up_stack = [\n",
      "    upsample(512, 4, apply_dropout=True),  # (bs, 2, 2, 1024)\n",
      "    upsample(512, 4, apply_dropout=True),  # (bs, 4, 4, 1024)\n",
      "    upsample(512, 4, apply_dropout=True),  # (bs, 8, 8, 1024)\n",
      "    upsample(512, 4),  # (bs, 16, 16, 1024)\n",
      "    upsample(256, 4),  # (bs, 32, 32, 512)\n",
      "    upsample(128, 4),  # (bs, 64, 64, 256)\n",
      "    upsample(64, 4),  # (bs, 128, 128, 128)\n",
      "  ]\n",
      "\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "  last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
      "                                         strides=2,\n",
      "                                         padding='same',\n",
      "                                         kernel_initializer=initializer,\n",
      "                                         activation='tanh')  # (bs, 256, 256, 3)\n",
      "\n",
      "  x = inputs\n",
      "\n",
      "  # Downsampling through the model\n",
      "  skips = []\n",
      "  for down in down_stack:\n",
      "    x = down(x)\n",
      "    skips.append(x)\n",
      "\n",
      "  skips = reversed(skips[:-1])\n",
      "\n",
      "  # Upsampling and establishing the skip connections\n",
      "  for up, skip in zip(up_stack, skips):\n",
      "    x = up(x)\n",
      "    x = tf.keras.layers.Concatenate()([x, skip])\n",
      "\n",
      "  x = last(x)\n",
      "\n",
      "  return tf.keras.Model(inputs=inputs, outputs=x)\n",
      "43/42:\n",
      "generator = Generator()\n",
      "tf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)\n",
      "44/1:\n",
      "import tensorflow as tf\n",
      "\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "print(tf.config.list_physical_devices())\n",
      "\n",
      "import os\n",
      "import time\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython import display\n",
      "44/2: !pip install -q -U tensorboard\n",
      "44/3:\n",
      "_URL = 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz'\n",
      "\n",
      "path_to_zip = tf.keras.utils.get_file('facades.tar.gz',\n",
      "                                      origin=_URL,\n",
      "                                      extract=True)\n",
      "\n",
      "PATH = os.path.join(os.path.dirname(path_to_zip), 'facades/')\n",
      "44/4:\n",
      "BUFFER_SIZE = 400\n",
      "BATCH_SIZE = 1\n",
      "IMG_WIDTH = 256\n",
      "IMG_HEIGHT = 256\n",
      "44/5:\n",
      "def load(image_file):\n",
      "  image = tf.io.read_file(image_file)\n",
      "  image = tf.image.decode_jpeg(image)\n",
      "\n",
      "  w = tf.shape(image)[1]\n",
      "\n",
      "  w = w // 2\n",
      "  real_image = image[:, :w, :]\n",
      "  input_image = image[:, w:, :]\n",
      "\n",
      "  input_image = tf.cast(input_image, tf.float32)\n",
      "  real_image = tf.cast(real_image, tf.float32)\n",
      "\n",
      "  return input_image, real_image\n",
      "44/6:\n",
      "inp, re = load(PATH+'train/100.jpg')\n",
      "# casting to int for matplotlib to show the image\n",
      "plt.figure()\n",
      "plt.imshow(inp/255.0)\n",
      "plt.figure()\n",
      "plt.imshow(re/255.0)\n",
      "44/7:\n",
      "def resize(input_image, real_image, height, width):\n",
      "  input_image = tf.image.resize(input_image, [height, width],\n",
      "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "  real_image = tf.image.resize(real_image, [height, width],\n",
      "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "\n",
      "  return input_image, real_image\n",
      "44/8:\n",
      "def random_crop(input_image, real_image):\n",
      "  stacked_image = tf.stack([input_image, real_image], axis=0)\n",
      "  cropped_image = tf.image.random_crop(\n",
      "      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
      "\n",
      "  return cropped_image[0], cropped_image[1]\n",
      "44/9:\n",
      "# normalizing the images to [-1, 1]\n",
      "\n",
      "def normalize(input_image, real_image):\n",
      "  input_image = (input_image / 127.5) - 1\n",
      "  real_image = (real_image / 127.5) - 1\n",
      "\n",
      "  return input_image, real_image\n",
      "44/10:\n",
      "@tf.function()\n",
      "def random_jitter(input_image, real_image):\n",
      "  # resizing to 286 x 286 x 3\n",
      "  input_image, real_image = resize(input_image, real_image, 286, 286)\n",
      "\n",
      "  # randomly cropping to 256 x 256 x 3\n",
      "  input_image, real_image = random_crop(input_image, real_image)\n",
      "\n",
      "  if tf.random.uniform(()) > 0.5:\n",
      "    # random mirroring\n",
      "    input_image = tf.image.flip_left_right(input_image)\n",
      "    real_image = tf.image.flip_left_right(real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "44/11:\n",
      "plt.figure(figsize=(6, 6))\n",
      "for i in range(4):\n",
      "  rj_inp, rj_re = random_jitter(inp, re)\n",
      "  plt.subplot(2, 2, i+1)\n",
      "  plt.imshow(rj_inp/255.0)\n",
      "  plt.axis('off')\n",
      "plt.show()\n",
      "44/12:\n",
      "def load_image_train(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = random_jitter(input_image, real_image)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "44/13:\n",
      "def load_image_test(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = resize(input_image, real_image,\n",
      "                                   IMG_HEIGHT, IMG_WIDTH)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "44/14:\n",
      "train_dataset = tf.data.Dataset.list_files(PATH+'train/*.jpg')\n",
      "train_dataset = train_dataset.map(load_image_train,\n",
      "                                  num_parallel_calls=tf.data.AUTOTUNE)\n",
      "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
      "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
      "44/15:\n",
      "test_dataset = tf.data.Dataset.list_files(PATH+'test/*.jpg')\n",
      "test_dataset = test_dataset.map(load_image_test)\n",
      "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
      "44/16: OUTPUT_CHANNELS = 3\n",
      "44/17:\n",
      "def downsample(filters, size, apply_batchnorm=True):\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "\n",
      "  result = tf.keras.Sequential()\n",
      "  result.add(\n",
      "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
      "                             kernel_initializer=initializer, use_bias=False))\n",
      "\n",
      "  if apply_batchnorm:\n",
      "    result.add(tf.keras.layers.BatchNormalization())\n",
      "\n",
      "  result.add(tf.keras.layers.LeakyReLU())\n",
      "\n",
      "  return result\n",
      "44/18:\n",
      "# down_model = downsample(3, 4)\n",
      "# down_result = down_model(tf.expand_dims(inp, 0))\n",
      "# print (down_result.shape)\n",
      "44/19:\n",
      "def upsample(filters, size, apply_dropout=False):\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "\n",
      "  result = tf.keras.Sequential()\n",
      "  result.add(\n",
      "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
      "                                    padding='same',\n",
      "                                    kernel_initializer=initializer,\n",
      "                                    use_bias=False))\n",
      "\n",
      "  result.add(tf.keras.layers.BatchNormalization())\n",
      "\n",
      "  if apply_dropout:\n",
      "      result.add(tf.keras.layers.Dropout(0.5))\n",
      "\n",
      "  result.add(tf.keras.layers.ReLU())\n",
      "\n",
      "  return result\n",
      "44/20:\n",
      "# up_model = upsample(3, 4)\n",
      "# up_result = up_model(down_result)\n",
      "# print (up_result.shape)\n",
      "44/21:\n",
      "def Generator():\n",
      "  inputs = tf.keras.layers.Input(shape=[256, 256, 3])\n",
      "\n",
      "  down_stack = [\n",
      "    downsample(64, 4, apply_batchnorm=False),  # (bs, 128, 128, 64)\n",
      "    downsample(128, 4),  # (bs, 64, 64, 128)\n",
      "    downsample(256, 4),  # (bs, 32, 32, 256)\n",
      "    downsample(512, 4),  # (bs, 16, 16, 512)\n",
      "    downsample(512, 4),  # (bs, 8, 8, 512)\n",
      "    downsample(512, 4),  # (bs, 4, 4, 512)\n",
      "    downsample(512, 4),  # (bs, 2, 2, 512)\n",
      "    downsample(512, 4),  # (bs, 1, 1, 512)\n",
      "  ]\n",
      "\n",
      "  up_stack = [\n",
      "    upsample(512, 4, apply_dropout=True),  # (bs, 2, 2, 1024)\n",
      "    upsample(512, 4, apply_dropout=True),  # (bs, 4, 4, 1024)\n",
      "    upsample(512, 4, apply_dropout=True),  # (bs, 8, 8, 1024)\n",
      "    upsample(512, 4),  # (bs, 16, 16, 1024)\n",
      "    upsample(256, 4),  # (bs, 32, 32, 512)\n",
      "    upsample(128, 4),  # (bs, 64, 64, 256)\n",
      "    upsample(64, 4),  # (bs, 128, 128, 128)\n",
      "  ]\n",
      "\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "  last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
      "                                         strides=2,\n",
      "                                         padding='same',\n",
      "                                         kernel_initializer=initializer,\n",
      "                                         activation='tanh')  # (bs, 256, 256, 3)\n",
      "\n",
      "  x = inputs\n",
      "\n",
      "  # Downsampling through the model\n",
      "  skips = []\n",
      "  for down in down_stack:\n",
      "    x = down(x)\n",
      "    skips.append(x)\n",
      "\n",
      "  skips = reversed(skips[:-1])\n",
      "\n",
      "  # Upsampling and establishing the skip connections\n",
      "  for up, skip in zip(up_stack, skips):\n",
      "    x = up(x)\n",
      "    x = tf.keras.layers.Concatenate()([x, skip])\n",
      "\n",
      "  x = last(x)\n",
      "\n",
      "  return tf.keras.Model(inputs=inputs, outputs=x)\n",
      "44/22:\n",
      "generator = Generator()\n",
      "tf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)\n",
      "45/1:\n",
      "import tensorflow as tf\n",
      "\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "print(tf.config.list_physical_devices())\n",
      "\n",
      "import os\n",
      "import time\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython import display\n",
      "45/2: !pip install -q -U tensorboard\n",
      "45/3:\n",
      "_URL = 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz'\n",
      "\n",
      "path_to_zip = tf.keras.utils.get_file('facades.tar.gz',\n",
      "                                      origin=_URL,\n",
      "                                      extract=True)\n",
      "\n",
      "PATH = os.path.join(os.path.dirname(path_to_zip), 'facades/')\n",
      "45/4:\n",
      "BUFFER_SIZE = 400\n",
      "BATCH_SIZE = 1\n",
      "IMG_WIDTH = 256\n",
      "IMG_HEIGHT = 256\n",
      "45/5:\n",
      "def load(image_file):\n",
      "  image = tf.io.read_file(image_file)\n",
      "  image = tf.image.decode_jpeg(image)\n",
      "\n",
      "  w = tf.shape(image)[1]\n",
      "\n",
      "  w = w // 2\n",
      "  real_image = image[:, :w, :]\n",
      "  input_image = image[:, w:, :]\n",
      "\n",
      "  input_image = tf.cast(input_image, tf.float32)\n",
      "  real_image = tf.cast(real_image, tf.float32)\n",
      "\n",
      "  return input_image, real_image\n",
      "45/6:\n",
      "inp, re = load(PATH+'train/100.jpg')\n",
      "# casting to int for matplotlib to show the image\n",
      "plt.figure()\n",
      "plt.imshow(inp/255.0)\n",
      "plt.figure()\n",
      "plt.imshow(re/255.0)\n",
      "45/7:\n",
      "def resize(input_image, real_image, height, width):\n",
      "  input_image = tf.image.resize(input_image, [height, width],\n",
      "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "  real_image = tf.image.resize(real_image, [height, width],\n",
      "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "\n",
      "  return input_image, real_image\n",
      "45/8:\n",
      "def random_crop(input_image, real_image):\n",
      "  stacked_image = tf.stack([input_image, real_image], axis=0)\n",
      "  cropped_image = tf.image.random_crop(\n",
      "      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
      "\n",
      "  return cropped_image[0], cropped_image[1]\n",
      "45/9:\n",
      "# normalizing the images to [-1, 1]\n",
      "\n",
      "def normalize(input_image, real_image):\n",
      "  input_image = (input_image / 127.5) - 1\n",
      "  real_image = (real_image / 127.5) - 1\n",
      "\n",
      "  return input_image, real_image\n",
      "45/10:\n",
      "@tf.function()\n",
      "def random_jitter(input_image, real_image):\n",
      "  # resizing to 286 x 286 x 3\n",
      "  input_image, real_image = resize(input_image, real_image, 286, 286)\n",
      "\n",
      "  # randomly cropping to 256 x 256 x 3\n",
      "  input_image, real_image = random_crop(input_image, real_image)\n",
      "\n",
      "  if tf.random.uniform(()) > 0.5:\n",
      "    # random mirroring\n",
      "    input_image = tf.image.flip_left_right(input_image)\n",
      "    real_image = tf.image.flip_left_right(real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "45/11:\n",
      "plt.figure(figsize=(6, 6))\n",
      "for i in range(4):\n",
      "  rj_inp, rj_re = random_jitter(inp, re)\n",
      "  plt.subplot(2, 2, i+1)\n",
      "  plt.imshow(rj_inp/255.0)\n",
      "  plt.axis('off')\n",
      "plt.show()\n",
      "45/12:\n",
      "def load_image_train(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = random_jitter(input_image, real_image)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "45/13:\n",
      "def load_image_test(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = resize(input_image, real_image,\n",
      "                                   IMG_HEIGHT, IMG_WIDTH)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "45/14:\n",
      "train_dataset = tf.data.Dataset.list_files(PATH+'train/*.jpg')\n",
      "train_dataset = train_dataset.map(load_image_train,\n",
      "                                  num_parallel_calls=tf.data.AUTOTUNE)\n",
      "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
      "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
      "45/15:\n",
      "test_dataset = tf.data.Dataset.list_files(PATH+'test/*.jpg')\n",
      "test_dataset = test_dataset.map(load_image_test)\n",
      "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
      "45/16: OUTPUT_CHANNELS = 3\n",
      "45/17:\n",
      "def downsample(filters, size, apply_batchnorm=True):\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "\n",
      "  result = tf.keras.Sequential()\n",
      "  result.add(\n",
      "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
      "                             kernel_initializer=initializer, use_bias=False))\n",
      "\n",
      "  if apply_batchnorm:\n",
      "    result.add(tf.keras.layers.BatchNormalization())\n",
      "\n",
      "  result.add(tf.keras.layers.LeakyReLU())\n",
      "\n",
      "  return result\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/18:\n",
      "# down_model = downsample(3, 4)\n",
      "# down_result = down_model(tf.expand_dims(inp, 0))\n",
      "# print (down_result.shape)\n",
      "45/19:\n",
      "def upsample(filters, size, apply_dropout=False):\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "\n",
      "  result = tf.keras.Sequential()\n",
      "  result.add(\n",
      "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
      "                                    padding='same',\n",
      "                                    kernel_initializer=initializer,\n",
      "                                    use_bias=False))\n",
      "\n",
      "  result.add(tf.keras.layers.BatchNormalization())\n",
      "\n",
      "  if apply_dropout:\n",
      "      result.add(tf.keras.layers.Dropout(0.5))\n",
      "\n",
      "  result.add(tf.keras.layers.ReLU())\n",
      "\n",
      "  return result\n",
      "45/20:\n",
      "# up_model = upsample(3, 4)\n",
      "# up_result = up_model(down_result)\n",
      "# print (up_result.shape)\n",
      "45/21:\n",
      "def Generator():\n",
      "  inputs = tf.keras.layers.Input(shape=[256, 256, 3])\n",
      "\n",
      "  down_stack = [\n",
      "    downsample(64, 4, apply_batchnorm=False),  # (bs, 128, 128, 64)\n",
      "    downsample(128, 4),  # (bs, 64, 64, 128)\n",
      "    downsample(256, 4),  # (bs, 32, 32, 256)\n",
      "    downsample(512, 4),  # (bs, 16, 16, 512)\n",
      "    downsample(512, 4),  # (bs, 8, 8, 512)\n",
      "    downsample(512, 4),  # (bs, 4, 4, 512)\n",
      "    downsample(512, 4),  # (bs, 2, 2, 512)\n",
      "    downsample(512, 4),  # (bs, 1, 1, 512)\n",
      "  ]\n",
      "\n",
      "  up_stack = [\n",
      "    upsample(512, 4, apply_dropout=True),  # (bs, 2, 2, 1024)\n",
      "    upsample(512, 4, apply_dropout=True),  # (bs, 4, 4, 1024)\n",
      "    upsample(512, 4, apply_dropout=True),  # (bs, 8, 8, 1024)\n",
      "    upsample(512, 4),  # (bs, 16, 16, 1024)\n",
      "    upsample(256, 4),  # (bs, 32, 32, 512)\n",
      "    upsample(128, 4),  # (bs, 64, 64, 256)\n",
      "    upsample(64, 4),  # (bs, 128, 128, 128)\n",
      "  ]\n",
      "\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "  last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
      "                                         strides=2,\n",
      "                                         padding='same',\n",
      "                                         kernel_initializer=initializer,\n",
      "                                         activation='tanh')  # (bs, 256, 256, 3)\n",
      "\n",
      "  x = inputs\n",
      "\n",
      "  # Downsampling through the model\n",
      "  skips = []\n",
      "  for down in down_stack:\n",
      "    x = down(x)\n",
      "    skips.append(x)\n",
      "\n",
      "  skips = reversed(skips[:-1])\n",
      "\n",
      "  # Upsampling and establishing the skip connections\n",
      "  for up, skip in zip(up_stack, skips):\n",
      "    x = up(x)\n",
      "    x = tf.keras.layers.Concatenate()([x, skip])\n",
      "\n",
      "  x = last(x)\n",
      "\n",
      "  return tf.keras.Model(inputs=inputs, outputs=x)\n",
      "45/22:\n",
      "# generator = Generator()\n",
      "# tf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)\n",
      "45/23:\n",
      "# gen_output = generator(inp[tf.newaxis, ...], training=False)\n",
      "# plt.imshow(gen_output[0, ...])\n",
      "45/24: LAMBDA = 100\n",
      "45/25: loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
      "45/26:\n",
      "def generator_loss(disc_generated_output, gen_output, target):\n",
      "  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
      "\n",
      "  # mean absolute error\n",
      "  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
      "\n",
      "  total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
      "\n",
      "  return total_gen_loss, gan_loss, l1_loss\n",
      "45/27:\n",
      "def Discriminator():\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "\n",
      "  inp = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image')\n",
      "  tar = tf.keras.layers.Input(shape=[256, 256, 3], name='target_image')\n",
      "\n",
      "  x = tf.keras.layers.concatenate([inp, tar])  # (bs, 256, 256, channels*2)\n",
      "\n",
      "  down1 = downsample(64, 4, False)(x)  # (bs, 128, 128, 64)\n",
      "  down2 = downsample(128, 4)(down1)  # (bs, 64, 64, 128)\n",
      "  down3 = downsample(256, 4)(down2)  # (bs, 32, 32, 256)\n",
      "\n",
      "  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)  # (bs, 34, 34, 256)\n",
      "  conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n",
      "                                kernel_initializer=initializer,\n",
      "                                use_bias=False)(zero_pad1)  # (bs, 31, 31, 512)\n",
      "\n",
      "  batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
      "\n",
      "  leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
      "\n",
      "  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)  # (bs, 33, 33, 512)\n",
      "\n",
      "  last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
      "                                kernel_initializer=initializer)(zero_pad2)  # (bs, 30, 30, 1)\n",
      "\n",
      "  return tf.keras.Model(inputs=[inp, tar], outputs=last)\n",
      "45/28:\n",
      "discriminator = Discriminator()\n",
      "tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)\n",
      "45/29:\n",
      "disc_out = discriminator([inp[tf.newaxis, ...], gen_output], training=False)\n",
      "plt.imshow(disc_out[0, ..., -1], vmin=-20, vmax=20, cmap='RdBu_r')\n",
      "plt.colorbar()\n",
      "45/30:\n",
      "import tensorflow as tf\n",
      "\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "print(tf.config.list_physical_devices())\n",
      "\n",
      "import os\n",
      "import time\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython import display\n",
      "45/31: !pip install -q -U tensorboard\n",
      "45/32:\n",
      "_URL = 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz'\n",
      "\n",
      "path_to_zip = tf.keras.utils.get_file('facades.tar.gz',\n",
      "                                      origin=_URL,\n",
      "                                      extract=True)\n",
      "\n",
      "PATH = os.path.join(os.path.dirname(path_to_zip), 'facades/')\n",
      "45/33:\n",
      "BUFFER_SIZE = 400\n",
      "BATCH_SIZE = 1\n",
      "IMG_WIDTH = 256\n",
      "IMG_HEIGHT = 256\n",
      "45/34:\n",
      "def load(image_file):\n",
      "  image = tf.io.read_file(image_file)\n",
      "  image = tf.image.decode_jpeg(image)\n",
      "\n",
      "  w = tf.shape(image)[1]\n",
      "\n",
      "  w = w // 2\n",
      "  real_image = image[:, :w, :]\n",
      "  input_image = image[:, w:, :]\n",
      "\n",
      "  input_image = tf.cast(input_image, tf.float32)\n",
      "  real_image = tf.cast(real_image, tf.float32)\n",
      "\n",
      "  return input_image, real_image\n",
      "45/35:\n",
      "inp, re = load(PATH+'train/100.jpg')\n",
      "# casting to int for matplotlib to show the image\n",
      "plt.figure()\n",
      "plt.imshow(inp/255.0)\n",
      "plt.figure()\n",
      "plt.imshow(re/255.0)\n",
      "45/36:\n",
      "def resize(input_image, real_image, height, width):\n",
      "  input_image = tf.image.resize(input_image, [height, width],\n",
      "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "  real_image = tf.image.resize(real_image, [height, width],\n",
      "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "\n",
      "  return input_image, real_image\n",
      "45/37:\n",
      "def random_crop(input_image, real_image):\n",
      "  stacked_image = tf.stack([input_image, real_image], axis=0)\n",
      "  cropped_image = tf.image.random_crop(\n",
      "      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
      "\n",
      "  return cropped_image[0], cropped_image[1]\n",
      "45/38:\n",
      "# normalizing the images to [-1, 1]\n",
      "\n",
      "def normalize(input_image, real_image):\n",
      "  input_image = (input_image / 127.5) - 1\n",
      "  real_image = (real_image / 127.5) - 1\n",
      "\n",
      "  return input_image, real_image\n",
      "45/39:\n",
      "@tf.function()\n",
      "def random_jitter(input_image, real_image):\n",
      "  # resizing to 286 x 286 x 3\n",
      "  input_image, real_image = resize(input_image, real_image, 286, 286)\n",
      "\n",
      "  # randomly cropping to 256 x 256 x 3\n",
      "  input_image, real_image = random_crop(input_image, real_image)\n",
      "\n",
      "  if tf.random.uniform(()) > 0.5:\n",
      "    # random mirroring\n",
      "    input_image = tf.image.flip_left_right(input_image)\n",
      "    real_image = tf.image.flip_left_right(real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "45/40:\n",
      "plt.figure(figsize=(6, 6))\n",
      "for i in range(4):\n",
      "  rj_inp, rj_re = random_jitter(inp, re)\n",
      "  plt.subplot(2, 2, i+1)\n",
      "  plt.imshow(rj_inp/255.0)\n",
      "  plt.axis('off')\n",
      "plt.show()\n",
      "45/41:\n",
      "def load_image_train(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = random_jitter(input_image, real_image)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "45/42:\n",
      "def load_image_test(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = resize(input_image, real_image,\n",
      "                                   IMG_HEIGHT, IMG_WIDTH)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "45/43:\n",
      "train_dataset = tf.data.Dataset.list_files(PATH+'train/*.jpg')\n",
      "train_dataset = train_dataset.map(load_image_train,\n",
      "                                  num_parallel_calls=tf.data.AUTOTUNE)\n",
      "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
      "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
      "45/44:\n",
      "test_dataset = tf.data.Dataset.list_files(PATH+'test/*.jpg')\n",
      "test_dataset = test_dataset.map(load_image_test)\n",
      "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
      "45/45: OUTPUT_CHANNELS = 3\n",
      "45/46:\n",
      "def downsample(filters, size, apply_batchnorm=True):\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "\n",
      "  result = tf.keras.Sequential()\n",
      "  result.add(\n",
      "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
      "                             kernel_initializer=initializer, use_bias=False))\n",
      "\n",
      "  if apply_batchnorm:\n",
      "    result.add(tf.keras.layers.BatchNormalization())\n",
      "\n",
      "  result.add(tf.keras.layers.LeakyReLU())\n",
      "\n",
      "  return result\n",
      "45/47:\n",
      "# down_model = downsample(3, 4)\n",
      "# down_result = down_model(tf.expand_dims(inp, 0))\n",
      "# print (down_result.shape)\n",
      "45/48:\n",
      "def upsample(filters, size, apply_dropout=False):\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "\n",
      "  result = tf.keras.Sequential()\n",
      "  result.add(\n",
      "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
      "                                    padding='same',\n",
      "                                    kernel_initializer=initializer,\n",
      "                                    use_bias=False))\n",
      "\n",
      "  result.add(tf.keras.layers.BatchNormalization())\n",
      "\n",
      "  if apply_dropout:\n",
      "      result.add(tf.keras.layers.Dropout(0.5))\n",
      "\n",
      "  result.add(tf.keras.layers.ReLU())\n",
      "\n",
      "  return result\n",
      "45/49:\n",
      "# up_model = upsample(3, 4)\n",
      "# up_result = up_model(down_result)\n",
      "# print (up_result.shape)\n",
      "45/50:\n",
      "def Generator():\n",
      "  inputs = tf.keras.layers.Input(shape=[256, 256, 3])\n",
      "\n",
      "  down_stack = [\n",
      "    downsample(64, 4, apply_batchnorm=False),  # (bs, 128, 128, 64)\n",
      "    downsample(128, 4),  # (bs, 64, 64, 128)\n",
      "    downsample(256, 4),  # (bs, 32, 32, 256)\n",
      "    downsample(512, 4),  # (bs, 16, 16, 512)\n",
      "    downsample(512, 4),  # (bs, 8, 8, 512)\n",
      "    downsample(512, 4),  # (bs, 4, 4, 512)\n",
      "    downsample(512, 4),  # (bs, 2, 2, 512)\n",
      "    downsample(512, 4),  # (bs, 1, 1, 512)\n",
      "  ]\n",
      "\n",
      "  up_stack = [\n",
      "    upsample(512, 4, apply_dropout=True),  # (bs, 2, 2, 1024)\n",
      "    upsample(512, 4, apply_dropout=True),  # (bs, 4, 4, 1024)\n",
      "    upsample(512, 4, apply_dropout=True),  # (bs, 8, 8, 1024)\n",
      "    upsample(512, 4),  # (bs, 16, 16, 1024)\n",
      "    upsample(256, 4),  # (bs, 32, 32, 512)\n",
      "    upsample(128, 4),  # (bs, 64, 64, 256)\n",
      "    upsample(64, 4),  # (bs, 128, 128, 128)\n",
      "  ]\n",
      "\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "  last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
      "                                         strides=2,\n",
      "                                         padding='same',\n",
      "                                         kernel_initializer=initializer,\n",
      "                                         activation='tanh')  # (bs, 256, 256, 3)\n",
      "\n",
      "  x = inputs\n",
      "\n",
      "  # Downsampling through the model\n",
      "  skips = []\n",
      "  for down in down_stack:\n",
      "    x = down(x)\n",
      "    skips.append(x)\n",
      "\n",
      "  skips = reversed(skips[:-1])\n",
      "\n",
      "  # Upsampling and establishing the skip connections\n",
      "  for up, skip in zip(up_stack, skips):\n",
      "    x = up(x)\n",
      "    x = tf.keras.layers.Concatenate()([x, skip])\n",
      "\n",
      "  x = last(x)\n",
      "\n",
      "  return tf.keras.Model(inputs=inputs, outputs=x)\n",
      "45/51:\n",
      "generator = Generator()\n",
      "# tf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)\n",
      "46/1:\n",
      "import tensorflow as tf\n",
      "\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "print(tf.config.list_physical_devices())\n",
      "\n",
      "import os\n",
      "import time\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython import display\n",
      "46/2: !pip install -q -U tensorboard\n",
      "46/3:\n",
      "_URL = 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz'\n",
      "\n",
      "path_to_zip = tf.keras.utils.get_file('facades.tar.gz',\n",
      "                                      origin=_URL,\n",
      "                                      extract=True)\n",
      "\n",
      "PATH = os.path.join(os.path.dirname(path_to_zip), 'facades/')\n",
      "46/4:\n",
      "BUFFER_SIZE = 400\n",
      "BATCH_SIZE = 1\n",
      "IMG_WIDTH = 256\n",
      "IMG_HEIGHT = 256\n",
      "46/5:\n",
      "def load(image_file):\n",
      "  image = tf.io.read_file(image_file)\n",
      "  image = tf.image.decode_jpeg(image)\n",
      "\n",
      "  w = tf.shape(image)[1]\n",
      "\n",
      "  w = w // 2\n",
      "  real_image = image[:, :w, :]\n",
      "  input_image = image[:, w:, :]\n",
      "\n",
      "  input_image = tf.cast(input_image, tf.float32)\n",
      "  real_image = tf.cast(real_image, tf.float32)\n",
      "\n",
      "  return input_image, real_image\n",
      "46/6:\n",
      "inp, re = load(PATH+'train/100.jpg')\n",
      "# casting to int for matplotlib to show the image\n",
      "plt.figure()\n",
      "plt.imshow(inp/255.0)\n",
      "plt.figure()\n",
      "plt.imshow(re/255.0)\n",
      "46/7:\n",
      "def resize(input_image, real_image, height, width):\n",
      "  input_image = tf.image.resize(input_image, [height, width],\n",
      "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "  real_image = tf.image.resize(real_image, [height, width],\n",
      "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "\n",
      "  return input_image, real_image\n",
      "46/8:\n",
      "def random_crop(input_image, real_image):\n",
      "  stacked_image = tf.stack([input_image, real_image], axis=0)\n",
      "  cropped_image = tf.image.random_crop(\n",
      "      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
      "\n",
      "  return cropped_image[0], cropped_image[1]\n",
      "46/9:\n",
      "# normalizing the images to [-1, 1]\n",
      "\n",
      "def normalize(input_image, real_image):\n",
      "  input_image = (input_image / 127.5) - 1\n",
      "  real_image = (real_image / 127.5) - 1\n",
      "\n",
      "  return input_image, real_image\n",
      "46/10:\n",
      "@tf.function()\n",
      "def random_jitter(input_image, real_image):\n",
      "  # resizing to 286 x 286 x 3\n",
      "  input_image, real_image = resize(input_image, real_image, 286, 286)\n",
      "\n",
      "  # randomly cropping to 256 x 256 x 3\n",
      "  input_image, real_image = random_crop(input_image, real_image)\n",
      "\n",
      "  if tf.random.uniform(()) > 0.5:\n",
      "    # random mirroring\n",
      "    input_image = tf.image.flip_left_right(input_image)\n",
      "    real_image = tf.image.flip_left_right(real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "46/11:\n",
      "plt.figure(figsize=(6, 6))\n",
      "for i in range(4):\n",
      "  rj_inp, rj_re = random_jitter(inp, re)\n",
      "  plt.subplot(2, 2, i+1)\n",
      "  plt.imshow(rj_inp/255.0)\n",
      "  plt.axis('off')\n",
      "plt.show()\n",
      "46/12:\n",
      "def load_image_train(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = random_jitter(input_image, real_image)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "46/13:\n",
      "def load_image_test(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = resize(input_image, real_image,\n",
      "                                   IMG_HEIGHT, IMG_WIDTH)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "46/14:\n",
      "train_dataset = tf.data.Dataset.list_files(PATH+'train/*.jpg')\n",
      "train_dataset = train_dataset.map(load_image_train,\n",
      "                                  num_parallel_calls=tf.data.AUTOTUNE)\n",
      "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
      "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
      "46/15:\n",
      "test_dataset = tf.data.Dataset.list_files(PATH+'test/*.jpg')\n",
      "test_dataset = test_dataset.map(load_image_test)\n",
      "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
      "46/16: OUTPUT_CHANNELS = 3\n",
      "46/17:\n",
      "def downsample(filters, size, apply_batchnorm=True):\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "\n",
      "  result = tf.keras.Sequential()\n",
      "  result.add(\n",
      "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
      "                             kernel_initializer=initializer, use_bias=False))\n",
      "\n",
      "#   if apply_batchnorm:\n",
      "#     result.add(tf.keras.layers.BatchNormalization())\n",
      "\n",
      "  result.add(tf.keras.layers.LeakyReLU())\n",
      "\n",
      "  return result\n",
      "46/18:\n",
      "down_model = downsample(3, 4)\n",
      "down_result = down_model(tf.expand_dims(inp, 0))\n",
      "print (down_result.shape)\n",
      "48/1:\n",
      "print('PyDev console: using IPython 7.19.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['C:\\\\Users\\\\luker\\\\Documents\\\\Github\\\\magisterka', 'C:/Users/luker/Documents/Github/magisterka'])\n",
      "47/1:\n",
      "import tensorflow as tf\n",
      "\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "print(tf.config.list_physical_devices())\n",
      "config = tf.compat.v1.ConfigProto(gpu_options = \n",
      "                         tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
      "# device_count = {'GPU': 1}\n",
      ")\n",
      "config.gpu_options.allow_growth = True\n",
      "session = tf.compat.v1.Session(config=config)\n",
      "tf.compat.v1.keras.backend.set_session(session)\n",
      "\n",
      "import os\n",
      "import time\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython import display\n",
      "47/2:\n",
      "import tensorflow as tf\n",
      "\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "print(tf.config.list_physical_devices())\n",
      "config = tf.compat.v1.ConfigProto(gpu_options = \n",
      "                         tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
      "# device_count = {'GPU': 1}\n",
      ")\n",
      "config.gpu_options.allow_growth = True\n",
      "session = tf.compat.v1.Session(config=config)\n",
      "tf.compat.v1.keras.backend.set_session(session)\n",
      "\n",
      "import os\n",
      "import time\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython import display\n",
      "47/3: !pip install -q -U tensorboard\n",
      "47/4:\n",
      "_URL = 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz'\n",
      "\n",
      "path_to_zip = tf.keras.utils.get_file('facades.tar.gz',\n",
      "                                      origin=_URL,\n",
      "                                      extract=True)\n",
      "\n",
      "PATH = os.path.join(os.path.dirname(path_to_zip), 'facades/')\n",
      "47/5:\n",
      "BUFFER_SIZE = 400\n",
      "BATCH_SIZE = 1\n",
      "IMG_WIDTH = 256\n",
      "IMG_HEIGHT = 256\n",
      "47/6:\n",
      "def load(image_file):\n",
      "  image = tf.io.read_file(image_file)\n",
      "  image = tf.image.decode_jpeg(image)\n",
      "\n",
      "  w = tf.shape(image)[1]\n",
      "\n",
      "  w = w // 2\n",
      "  real_image = image[:, :w, :]\n",
      "  input_image = image[:, w:, :]\n",
      "\n",
      "  input_image = tf.cast(input_image, tf.float32)\n",
      "  real_image = tf.cast(real_image, tf.float32)\n",
      "\n",
      "  return input_image, real_image\n",
      "47/7:\n",
      "inp, re = load(PATH+'train/100.jpg')\n",
      "# casting to int for matplotlib to show the image\n",
      "plt.figure()\n",
      "plt.imshow(inp/255.0)\n",
      "plt.figure()\n",
      "plt.imshow(re/255.0)\n",
      "47/8:\n",
      "def resize(input_image, real_image, height, width):\n",
      "  input_image = tf.image.resize(input_image, [height, width],\n",
      "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "  real_image = tf.image.resize(real_image, [height, width],\n",
      "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "\n",
      "  return input_image, real_image\n",
      "47/9:\n",
      "def random_crop(input_image, real_image):\n",
      "  stacked_image = tf.stack([input_image, real_image], axis=0)\n",
      "  cropped_image = tf.image.random_crop(\n",
      "      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
      "\n",
      "  return cropped_image[0], cropped_image[1]\n",
      "47/10:\n",
      "# normalizing the images to [-1, 1]\n",
      "\n",
      "def normalize(input_image, real_image):\n",
      "  input_image = (input_image / 127.5) - 1\n",
      "  real_image = (real_image / 127.5) - 1\n",
      "\n",
      "  return input_image, real_image\n",
      "47/11:\n",
      "@tf.function()\n",
      "def random_jitter(input_image, real_image):\n",
      "  # resizing to 286 x 286 x 3\n",
      "  input_image, real_image = resize(input_image, real_image, 286, 286)\n",
      "\n",
      "  # randomly cropping to 256 x 256 x 3\n",
      "  input_image, real_image = random_crop(input_image, real_image)\n",
      "\n",
      "  if tf.random.uniform(()) > 0.5:\n",
      "    # random mirroring\n",
      "    input_image = tf.image.flip_left_right(input_image)\n",
      "    real_image = tf.image.flip_left_right(real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "47/12:\n",
      "plt.figure(figsize=(6, 6))\n",
      "for i in range(4):\n",
      "  rj_inp, rj_re = random_jitter(inp, re)\n",
      "  plt.subplot(2, 2, i+1)\n",
      "  plt.imshow(rj_inp/255.0)\n",
      "  plt.axis('off')\n",
      "plt.show()\n",
      "47/13:\n",
      "def load_image_train(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = random_jitter(input_image, real_image)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "47/14:\n",
      "def load_image_test(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = resize(input_image, real_image,\n",
      "                                   IMG_HEIGHT, IMG_WIDTH)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "47/15:\n",
      "train_dataset = tf.data.Dataset.list_files(PATH+'train/*.jpg')\n",
      "train_dataset = train_dataset.map(load_image_train,\n",
      "                                  num_parallel_calls=tf.data.AUTOTUNE)\n",
      "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
      "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
      "47/16:\n",
      "test_dataset = tf.data.Dataset.list_files(PATH+'test/*.jpg')\n",
      "test_dataset = test_dataset.map(load_image_test)\n",
      "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
      "47/17: OUTPUT_CHANNELS = 3\n",
      "47/18:\n",
      "def downsample(filters, size, apply_batchnorm=True):\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "\n",
      "  result = tf.keras.Sequential()\n",
      "  result.add(\n",
      "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
      "                             kernel_initializer=initializer, use_bias=False))\n",
      "\n",
      "#   if apply_batchnorm:\n",
      "#     result.add(tf.keras.layers.BatchNormalization())\n",
      "\n",
      "  result.add(tf.keras.layers.LeakyReLU())\n",
      "\n",
      "  return result\n",
      "47/19:\n",
      "down_model = downsample(3, 4)\n",
      "down_result = down_model(tf.expand_dims(inp, 0))\n",
      "print (down_result.shape)\n",
      "49/1:\n",
      "import tensorflow as tf\n",
      "\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "print(tf.config.list_physical_devices())\n",
      "config = tf.compat.v1.ConfigProto(gpu_options = \n",
      "                         tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
      "# device_count = {'GPU': 1}\n",
      ")\n",
      "config.gpu_options.allow_growth = True\n",
      "session = tf.compat.v1.Session(config=config)\n",
      "tf.compat.v1.keras.backend.set_session(session)\n",
      "\n",
      "import os\n",
      "import time\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython import display\n",
      "49/2: !pip install -q -U tensorboard\n",
      "49/3:\n",
      "_URL = 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz'\n",
      "\n",
      "path_to_zip = tf.keras.utils.get_file('facades.tar.gz',\n",
      "                                      origin=_URL,\n",
      "                                      extract=True)\n",
      "\n",
      "PATH = os.path.join(os.path.dirname(path_to_zip), 'facades/')\n",
      "49/4:\n",
      "BUFFER_SIZE = 400\n",
      "BATCH_SIZE = 1\n",
      "IMG_WIDTH = 256\n",
      "IMG_HEIGHT = 256\n",
      "49/5:\n",
      "def load(image_file):\n",
      "  image = tf.io.read_file(image_file)\n",
      "  image = tf.image.decode_jpeg(image)\n",
      "\n",
      "  w = tf.shape(image)[1]\n",
      "\n",
      "  w = w // 2\n",
      "  real_image = image[:, :w, :]\n",
      "  input_image = image[:, w:, :]\n",
      "\n",
      "  input_image = tf.cast(input_image, tf.float32)\n",
      "  real_image = tf.cast(real_image, tf.float32)\n",
      "\n",
      "  return input_image, real_image\n",
      "49/6:\n",
      "inp, re = load(PATH+'train/100.jpg')\n",
      "# casting to int for matplotlib to show the image\n",
      "plt.figure()\n",
      "plt.imshow(inp/255.0)\n",
      "plt.figure()\n",
      "plt.imshow(re/255.0)\n",
      "49/7:\n",
      "def resize(input_image, real_image, height, width):\n",
      "  input_image = tf.image.resize(input_image, [height, width],\n",
      "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "  real_image = tf.image.resize(real_image, [height, width],\n",
      "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "\n",
      "  return input_image, real_image\n",
      "49/8:\n",
      "def random_crop(input_image, real_image):\n",
      "  stacked_image = tf.stack([input_image, real_image], axis=0)\n",
      "  cropped_image = tf.image.random_crop(\n",
      "      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
      "\n",
      "  return cropped_image[0], cropped_image[1]\n",
      "49/9:\n",
      "# normalizing the images to [-1, 1]\n",
      "\n",
      "def normalize(input_image, real_image):\n",
      "  input_image = (input_image / 127.5) - 1\n",
      "  real_image = (real_image / 127.5) - 1\n",
      "\n",
      "  return input_image, real_image\n",
      "49/10:\n",
      "@tf.function()\n",
      "def random_jitter(input_image, real_image):\n",
      "  # resizing to 286 x 286 x 3\n",
      "  input_image, real_image = resize(input_image, real_image, 286, 286)\n",
      "\n",
      "  # randomly cropping to 256 x 256 x 3\n",
      "  input_image, real_image = random_crop(input_image, real_image)\n",
      "\n",
      "  if tf.random.uniform(()) > 0.5:\n",
      "    # random mirroring\n",
      "    input_image = tf.image.flip_left_right(input_image)\n",
      "    real_image = tf.image.flip_left_right(real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "49/11:\n",
      "plt.figure(figsize=(6, 6))\n",
      "for i in range(4):\n",
      "  rj_inp, rj_re = random_jitter(inp, re)\n",
      "  plt.subplot(2, 2, i+1)\n",
      "  plt.imshow(rj_inp/255.0)\n",
      "  plt.axis('off')\n",
      "plt.show()\n",
      "49/12:\n",
      "def load_image_train(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = random_jitter(input_image, real_image)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "49/13:\n",
      "def load_image_test(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = resize(input_image, real_image,\n",
      "                                   IMG_HEIGHT, IMG_WIDTH)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "49/14:\n",
      "train_dataset = tf.data.Dataset.list_files(PATH+'train/*.jpg')\n",
      "train_dataset = train_dataset.map(load_image_train,\n",
      "                                  num_parallel_calls=tf.data.AUTOTUNE)\n",
      "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
      "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
      "49/15:\n",
      "test_dataset = tf.data.Dataset.list_files(PATH+'test/*.jpg')\n",
      "test_dataset = test_dataset.map(load_image_test)\n",
      "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
      "49/16: OUTPUT_CHANNELS = 3\n",
      "49/17:\n",
      "def downsample(filters, size, apply_batchnorm=True):\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "\n",
      "  result = tf.keras.Sequential()\n",
      "#   result.add(\n",
      "#       tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
      "#                              kernel_initializer=initializer, use_bias=False))\n",
      "\n",
      "  if apply_batchnorm:\n",
      "    result.add(tf.keras.layers.BatchNormalization())\n",
      "\n",
      "  result.add(tf.keras.layers.LeakyReLU())\n",
      "\n",
      "  return result\n",
      "49/18:\n",
      "down_model = downsample(3, 4)\n",
      "down_result = down_model(tf.expand_dims(inp, 0))\n",
      "print (down_result.shape)\n",
      "50/1:\n",
      "import tensorflow as tf\n",
      "\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "print(tf.config.list_physical_devices())\n",
      "config = tf.compat.v1.ConfigProto(gpu_options = \n",
      "                         tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
      "# device_count = {'GPU': 1}\n",
      ")\n",
      "config.gpu_options.allow_growth = True\n",
      "session = tf.compat.v1.Session(config=config)\n",
      "tf.compat.v1.keras.backend.set_session(session)\n",
      "\n",
      "import os\n",
      "import time\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython import display\n",
      "50/2: !pip install -q -U tensorboard\n",
      "50/3:\n",
      "_URL = 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz'\n",
      "\n",
      "path_to_zip = tf.keras.utils.get_file('facades.tar.gz',\n",
      "                                      origin=_URL,\n",
      "                                      extract=True)\n",
      "\n",
      "PATH = os.path.join(os.path.dirname(path_to_zip), 'facades/')\n",
      "50/4:\n",
      "BUFFER_SIZE = 400\n",
      "BATCH_SIZE = 1\n",
      "IMG_WIDTH = 256\n",
      "IMG_HEIGHT = 256\n",
      "50/5:\n",
      "def load(image_file):\n",
      "  image = tf.io.read_file(image_file)\n",
      "  image = tf.image.decode_jpeg(image)\n",
      "\n",
      "  w = tf.shape(image)[1]\n",
      "\n",
      "  w = w // 2\n",
      "  real_image = image[:, :w, :]\n",
      "  input_image = image[:, w:, :]\n",
      "\n",
      "  input_image = tf.cast(input_image, tf.float32)\n",
      "  real_image = tf.cast(real_image, tf.float32)\n",
      "\n",
      "  return input_image, real_image\n",
      "50/6:\n",
      "inp, re = load(PATH+'train/100.jpg')\n",
      "# casting to int for matplotlib to show the image\n",
      "plt.figure()\n",
      "plt.imshow(inp/255.0)\n",
      "plt.figure()\n",
      "plt.imshow(re/255.0)\n",
      "50/7:\n",
      "def resize(input_image, real_image, height, width):\n",
      "  input_image = tf.image.resize(input_image, [height, width],\n",
      "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "  real_image = tf.image.resize(real_image, [height, width],\n",
      "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "\n",
      "  return input_image, real_image\n",
      "50/8:\n",
      "def random_crop(input_image, real_image):\n",
      "  stacked_image = tf.stack([input_image, real_image], axis=0)\n",
      "  cropped_image = tf.image.random_crop(\n",
      "      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
      "\n",
      "  return cropped_image[0], cropped_image[1]\n",
      "50/9:\n",
      "# normalizing the images to [-1, 1]\n",
      "\n",
      "def normalize(input_image, real_image):\n",
      "  input_image = (input_image / 127.5) - 1\n",
      "  real_image = (real_image / 127.5) - 1\n",
      "\n",
      "  return input_image, real_image\n",
      "50/10:\n",
      "@tf.function()\n",
      "def random_jitter(input_image, real_image):\n",
      "  # resizing to 286 x 286 x 3\n",
      "  input_image, real_image = resize(input_image, real_image, 286, 286)\n",
      "\n",
      "  # randomly cropping to 256 x 256 x 3\n",
      "  input_image, real_image = random_crop(input_image, real_image)\n",
      "\n",
      "  if tf.random.uniform(()) > 0.5:\n",
      "    # random mirroring\n",
      "    input_image = tf.image.flip_left_right(input_image)\n",
      "    real_image = tf.image.flip_left_right(real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "50/11:\n",
      "plt.figure(figsize=(6, 6))\n",
      "for i in range(4):\n",
      "  rj_inp, rj_re = random_jitter(inp, re)\n",
      "  plt.subplot(2, 2, i+1)\n",
      "  plt.imshow(rj_inp/255.0)\n",
      "  plt.axis('off')\n",
      "plt.show()\n",
      "50/12:\n",
      "def load_image_train(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = random_jitter(input_image, real_image)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "50/13:\n",
      "def load_image_test(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = resize(input_image, real_image,\n",
      "                                   IMG_HEIGHT, IMG_WIDTH)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "50/14:\n",
      "train_dataset = tf.data.Dataset.list_files(PATH+'train/*.jpg')\n",
      "train_dataset = train_dataset.map(load_image_train,\n",
      "                                  num_parallel_calls=tf.data.AUTOTUNE)\n",
      "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
      "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
      "50/15:\n",
      "test_dataset = tf.data.Dataset.list_files(PATH+'test/*.jpg')\n",
      "test_dataset = test_dataset.map(load_image_test)\n",
      "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
      "50/16: OUTPUT_CHANNELS = 3\n",
      "50/17:\n",
      "def downsample(filters, size, apply_batchnorm=True):\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "\n",
      "  result = tf.keras.Sequential()\n",
      "#   result.add(\n",
      "#       tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
      "#                              kernel_initializer=initializer, use_bias=False))\n",
      "\n",
      "#   if apply_batchnorm:\n",
      "#     result.add(tf.keras.layers.BatchNormalization())\n",
      "\n",
      "#   result.add(tf.keras.layers.LeakyReLU())\n",
      "\n",
      "  return result\n",
      "50/18:\n",
      "down_model = downsample(3, 4)\n",
      "down_result = down_model(tf.expand_dims(inp, 0))\n",
      "print (down_result.shape)\n",
      "50/19:\n",
      "def upsample(filters, size, apply_dropout=False):\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "\n",
      "  result = tf.keras.Sequential()\n",
      "  result.add(\n",
      "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
      "                                    padding='same',\n",
      "                                    kernel_initializer=initializer,\n",
      "                                    use_bias=False))\n",
      "\n",
      "  result.add(tf.keras.layers.BatchNormalization())\n",
      "\n",
      "  if apply_dropout:\n",
      "      result.add(tf.keras.layers.Dropout(0.5))\n",
      "\n",
      "  result.add(tf.keras.layers.ReLU())\n",
      "\n",
      "  return result\n",
      "50/20:\n",
      "up_model = upsample(3, 4)\n",
      "up_result = up_model(down_result)\n",
      "print (up_result.shape)\n",
      "51/1:\n",
      "import tensorflow as tf\n",
      "\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "print(tf.config.list_physical_devices())\n",
      "config = tf.compat.v1.ConfigProto(gpu_options = \n",
      "                         tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
      "# device_count = {'GPU': 1}\n",
      ")\n",
      "config.gpu_options.allow_growth = True\n",
      "session = tf.compat.v1.Session(config=config)\n",
      "tf.compat.v1.keras.backend.set_session(session)\n",
      "\n",
      "import os\n",
      "import time\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython import display\n",
      "51/2: !pip install -q -U tensorboard\n",
      "51/3:\n",
      "_URL = 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz'\n",
      "\n",
      "path_to_zip = tf.keras.utils.get_file('facades.tar.gz',\n",
      "                                      origin=_URL,\n",
      "                                      extract=True)\n",
      "\n",
      "PATH = os.path.join(os.path.dirname(path_to_zip), 'facades/')\n",
      "51/4:\n",
      "BUFFER_SIZE = 400\n",
      "BATCH_SIZE = 1\n",
      "IMG_WIDTH = 256\n",
      "IMG_HEIGHT = 256\n",
      "51/5:\n",
      "def load(image_file):\n",
      "  image = tf.io.read_file(image_file)\n",
      "  image = tf.image.decode_jpeg(image)\n",
      "\n",
      "  w = tf.shape(image)[1]\n",
      "\n",
      "  w = w // 2\n",
      "  real_image = image[:, :w, :]\n",
      "  input_image = image[:, w:, :]\n",
      "\n",
      "  input_image = tf.cast(input_image, tf.float32)\n",
      "  real_image = tf.cast(real_image, tf.float32)\n",
      "\n",
      "  return input_image, real_image\n",
      "51/6:\n",
      "inp, re = load(PATH+'train/100.jpg')\n",
      "# casting to int for matplotlib to show the image\n",
      "plt.figure()\n",
      "plt.imshow(inp/255.0)\n",
      "plt.figure()\n",
      "plt.imshow(re/255.0)\n",
      "51/7:\n",
      "def resize(input_image, real_image, height, width):\n",
      "  input_image = tf.image.resize(input_image, [height, width],\n",
      "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "  real_image = tf.image.resize(real_image, [height, width],\n",
      "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "\n",
      "  return input_image, real_image\n",
      "51/8:\n",
      "def random_crop(input_image, real_image):\n",
      "  stacked_image = tf.stack([input_image, real_image], axis=0)\n",
      "  cropped_image = tf.image.random_crop(\n",
      "      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
      "\n",
      "  return cropped_image[0], cropped_image[1]\n",
      "51/9:\n",
      "# normalizing the images to [-1, 1]\n",
      "\n",
      "def normalize(input_image, real_image):\n",
      "  input_image = (input_image / 127.5) - 1\n",
      "  real_image = (real_image / 127.5) - 1\n",
      "\n",
      "  return input_image, real_image\n",
      "51/10:\n",
      "@tf.function()\n",
      "def random_jitter(input_image, real_image):\n",
      "  # resizing to 286 x 286 x 3\n",
      "  input_image, real_image = resize(input_image, real_image, 286, 286)\n",
      "\n",
      "  # randomly cropping to 256 x 256 x 3\n",
      "  input_image, real_image = random_crop(input_image, real_image)\n",
      "\n",
      "  if tf.random.uniform(()) > 0.5:\n",
      "    # random mirroring\n",
      "    input_image = tf.image.flip_left_right(input_image)\n",
      "    real_image = tf.image.flip_left_right(real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "51/11:\n",
      "plt.figure(figsize=(6, 6))\n",
      "for i in range(4):\n",
      "  rj_inp, rj_re = random_jitter(inp, re)\n",
      "  plt.subplot(2, 2, i+1)\n",
      "  plt.imshow(rj_inp/255.0)\n",
      "  plt.axis('off')\n",
      "plt.show()\n",
      "51/12:\n",
      "def load_image_train(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = random_jitter(input_image, real_image)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "51/13:\n",
      "def load_image_test(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = resize(input_image, real_image,\n",
      "                                   IMG_HEIGHT, IMG_WIDTH)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "51/14:\n",
      "train_dataset = tf.data.Dataset.list_files(PATH+'train/*.jpg')\n",
      "train_dataset = train_dataset.map(load_image_train,\n",
      "                                  num_parallel_calls=tf.data.AUTOTUNE)\n",
      "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
      "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
      "51/15:\n",
      "test_dataset = tf.data.Dataset.list_files(PATH+'test/*.jpg')\n",
      "test_dataset = test_dataset.map(load_image_test)\n",
      "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
      "51/16: OUTPUT_CHANNELS = 3\n",
      "51/17:\n",
      "def downsample(filters, size, apply_batchnorm=True):\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "\n",
      "  result = tf.keras.Sequential()\n",
      "  result.add(\n",
      "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
      "                             kernel_initializer=initializer, use_bias=False))\n",
      "\n",
      "  if apply_batchnorm:\n",
      "    result.add(tf.keras.layers.BatchNormalization())\n",
      "\n",
      "  #result.add(tf.keras.layers.ReLU())\n",
      "\n",
      "  return result\n",
      "51/18:\n",
      "down_model = downsample(3, 4)\n",
      "down_result = down_model(tf.expand_dims(inp, 0))\n",
      "print (down_result.shape)\n",
      "52/1:\n",
      "import tensorflow as tf\n",
      "\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "print(tf.config.list_physical_devices())\n",
      "config = tf.compat.v1.ConfigProto(gpu_options = \n",
      "                         tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
      "# device_count = {'GPU': 1}\n",
      ")\n",
      "config.gpu_options.allow_growth = True\n",
      "session = tf.compat.v1.Session(config=config)\n",
      "tf.compat.v1.keras.backend.set_session(session)\n",
      "\n",
      "import os\n",
      "import time\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython import display\n",
      "52/2: !pip install -q -U tensorboard\n",
      "52/3:\n",
      "_URL = 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz'\n",
      "\n",
      "path_to_zip = tf.keras.utils.get_file('facades.tar.gz',\n",
      "                                      origin=_URL,\n",
      "                                      extract=True)\n",
      "\n",
      "PATH = os.path.join(os.path.dirname(path_to_zip), 'facades/')\n",
      "52/4:\n",
      "BUFFER_SIZE = 400\n",
      "BATCH_SIZE = 1\n",
      "IMG_WIDTH = 256\n",
      "IMG_HEIGHT = 256\n",
      "52/5:\n",
      "def load(image_file):\n",
      "  image = tf.io.read_file(image_file)\n",
      "  image = tf.image.decode_jpeg(image)\n",
      "\n",
      "  w = tf.shape(image)[1]\n",
      "\n",
      "  w = w // 2\n",
      "  real_image = image[:, :w, :]\n",
      "  input_image = image[:, w:, :]\n",
      "\n",
      "  input_image = tf.cast(input_image, tf.float32)\n",
      "  real_image = tf.cast(real_image, tf.float32)\n",
      "\n",
      "  return input_image, real_image\n",
      "52/6:\n",
      "inp, re = load(PATH+'train/100.jpg')\n",
      "# casting to int for matplotlib to show the image\n",
      "plt.figure()\n",
      "plt.imshow(inp/255.0)\n",
      "plt.figure()\n",
      "plt.imshow(re/255.0)\n",
      "52/7:\n",
      "def resize(input_image, real_image, height, width):\n",
      "  input_image = tf.image.resize(input_image, [height, width],\n",
      "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "  real_image = tf.image.resize(real_image, [height, width],\n",
      "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "\n",
      "  return input_image, real_image\n",
      "52/8:\n",
      "def random_crop(input_image, real_image):\n",
      "  stacked_image = tf.stack([input_image, real_image], axis=0)\n",
      "  cropped_image = tf.image.random_crop(\n",
      "      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
      "\n",
      "  return cropped_image[0], cropped_image[1]\n",
      "52/9:\n",
      "# normalizing the images to [-1, 1]\n",
      "\n",
      "def normalize(input_image, real_image):\n",
      "  input_image = (input_image / 127.5) - 1\n",
      "  real_image = (real_image / 127.5) - 1\n",
      "\n",
      "  return input_image, real_image\n",
      "52/10:\n",
      "@tf.function()\n",
      "def random_jitter(input_image, real_image):\n",
      "  # resizing to 286 x 286 x 3\n",
      "  input_image, real_image = resize(input_image, real_image, 286, 286)\n",
      "\n",
      "  # randomly cropping to 256 x 256 x 3\n",
      "  input_image, real_image = random_crop(input_image, real_image)\n",
      "\n",
      "  if tf.random.uniform(()) > 0.5:\n",
      "    # random mirroring\n",
      "    input_image = tf.image.flip_left_right(input_image)\n",
      "    real_image = tf.image.flip_left_right(real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "52/11:\n",
      "plt.figure(figsize=(6, 6))\n",
      "for i in range(4):\n",
      "  rj_inp, rj_re = random_jitter(inp, re)\n",
      "  plt.subplot(2, 2, i+1)\n",
      "  plt.imshow(rj_inp/255.0)\n",
      "  plt.axis('off')\n",
      "plt.show()\n",
      "52/12:\n",
      "def load_image_train(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = random_jitter(input_image, real_image)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "52/13:\n",
      "def load_image_test(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = resize(input_image, real_image,\n",
      "                                   IMG_HEIGHT, IMG_WIDTH)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "52/14:\n",
      "train_dataset = tf.data.Dataset.list_files(PATH+'train/*.jpg')\n",
      "train_dataset = train_dataset.map(load_image_train,\n",
      "                                  num_parallel_calls=tf.data.AUTOTUNE)\n",
      "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
      "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
      "52/15:\n",
      "test_dataset = tf.data.Dataset.list_files(PATH+'test/*.jpg')\n",
      "test_dataset = test_dataset.map(load_image_test)\n",
      "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
      "52/16: OUTPUT_CHANNELS = 3\n",
      "52/17:\n",
      "def downsample(filters, size, apply_batchnorm=True):\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "\n",
      "  result = tf.keras.Sequential()\n",
      "#   result.add(\n",
      "#       tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
      "#                              kernel_initializer=initializer, use_bias=False))\n",
      "\n",
      "  if apply_batchnorm:\n",
      "    result.add(tf.keras.layers.BatchNormalization())\n",
      "\n",
      "  #result.add(tf.keras.layers.ReLU())\n",
      "\n",
      "  return result\n",
      "52/18:\n",
      "down_model = downsample(3, 4)\n",
      "down_result = down_model(tf.expand_dims(inp, 0))\n",
      "print (down_result.shape)\n",
      "53/1:\n",
      "import tensorflow as tf\n",
      "\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "print(tf.config.list_physical_devices())\n",
      "config = tf.compat.v1.ConfigProto(gpu_options = \n",
      "                         tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
      "# device_count = {'GPU': 1}\n",
      ")\n",
      "config.gpu_options.allow_growth = True\n",
      "session = tf.compat.v1.Session(config=config)\n",
      "tf.compat.v1.keras.backend.set_session(session)\n",
      "\n",
      "import os\n",
      "import time\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython import display\n",
      "53/2: !pip install -q -U tensorboard\n",
      "53/3:\n",
      "_URL = 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz'\n",
      "\n",
      "path_to_zip = tf.keras.utils.get_file('facades.tar.gz',\n",
      "                                      origin=_URL,\n",
      "                                      extract=True)\n",
      "\n",
      "PATH = os.path.join(os.path.dirname(path_to_zip), 'facades/')\n",
      "53/4:\n",
      "BUFFER_SIZE = 400\n",
      "BATCH_SIZE = 1\n",
      "IMG_WIDTH = 256\n",
      "IMG_HEIGHT = 256\n",
      "53/5:\n",
      "def load(image_file):\n",
      "  image = tf.io.read_file(image_file)\n",
      "  image = tf.image.decode_jpeg(image)\n",
      "\n",
      "  w = tf.shape(image)[1]\n",
      "\n",
      "  w = w // 2\n",
      "  real_image = image[:, :w, :]\n",
      "  input_image = image[:, w:, :]\n",
      "\n",
      "  input_image = tf.cast(input_image, tf.float32)\n",
      "  real_image = tf.cast(real_image, tf.float32)\n",
      "\n",
      "  return input_image, real_image\n",
      "53/6:\n",
      "inp, re = load(PATH+'train/100.jpg')\n",
      "# casting to int for matplotlib to show the image\n",
      "plt.figure()\n",
      "plt.imshow(inp/255.0)\n",
      "plt.figure()\n",
      "plt.imshow(re/255.0)\n",
      "53/7:\n",
      "def resize(input_image, real_image, height, width):\n",
      "  input_image = tf.image.resize(input_image, [height, width],\n",
      "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "  real_image = tf.image.resize(real_image, [height, width],\n",
      "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "\n",
      "  return input_image, real_image\n",
      "53/8:\n",
      "def random_crop(input_image, real_image):\n",
      "  stacked_image = tf.stack([input_image, real_image], axis=0)\n",
      "  cropped_image = tf.image.random_crop(\n",
      "      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
      "\n",
      "  return cropped_image[0], cropped_image[1]\n",
      "53/9:\n",
      "# normalizing the images to [-1, 1]\n",
      "\n",
      "def normalize(input_image, real_image):\n",
      "  input_image = (input_image / 127.5) - 1\n",
      "  real_image = (real_image / 127.5) - 1\n",
      "\n",
      "  return input_image, real_image\n",
      "53/10:\n",
      "@tf.function()\n",
      "def random_jitter(input_image, real_image):\n",
      "  # resizing to 286 x 286 x 3\n",
      "  input_image, real_image = resize(input_image, real_image, 286, 286)\n",
      "\n",
      "  # randomly cropping to 256 x 256 x 3\n",
      "  input_image, real_image = random_crop(input_image, real_image)\n",
      "\n",
      "  if tf.random.uniform(()) > 0.5:\n",
      "    # random mirroring\n",
      "    input_image = tf.image.flip_left_right(input_image)\n",
      "    real_image = tf.image.flip_left_right(real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "53/11:\n",
      "plt.figure(figsize=(6, 6))\n",
      "for i in range(4):\n",
      "  rj_inp, rj_re = random_jitter(inp, re)\n",
      "  plt.subplot(2, 2, i+1)\n",
      "  plt.imshow(rj_inp/255.0)\n",
      "  plt.axis('off')\n",
      "plt.show()\n",
      "53/12:\n",
      "def load_image_train(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = random_jitter(input_image, real_image)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "53/13:\n",
      "def load_image_test(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = resize(input_image, real_image,\n",
      "                                   IMG_HEIGHT, IMG_WIDTH)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "53/14:\n",
      "train_dataset = tf.data.Dataset.list_files(PATH+'train/*.jpg')\n",
      "train_dataset = train_dataset.map(load_image_train,\n",
      "                                  num_parallel_calls=tf.data.AUTOTUNE)\n",
      "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
      "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
      "53/15:\n",
      "test_dataset = tf.data.Dataset.list_files(PATH+'test/*.jpg')\n",
      "test_dataset = test_dataset.map(load_image_test)\n",
      "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
      "53/16: OUTPUT_CHANNELS = 3\n",
      "53/17:\n",
      "def downsample(filters, size, apply_batchnorm=True):\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "\n",
      "  result = tf.keras.Sequential()\n",
      "#   result.add(\n",
      "#       tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
      "#                              kernel_initializer=initializer, use_bias=False))\n",
      "\n",
      "#   if apply_batchnorm:\n",
      "#     result.add(tf.keras.layers.BatchNormalization())\n",
      "\n",
      "  #result.add(tf.keras.layers.ReLU())\n",
      "\n",
      "  return result\n",
      "53/18:\n",
      "down_model = downsample(3, 4)\n",
      "down_result = down_model(tf.expand_dims(inp, 0))\n",
      "print (down_result.shape)\n",
      "53/19:\n",
      "def upsample(filters, size, apply_dropout=False):\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "\n",
      "  result = tf.keras.Sequential()\n",
      "  result.add(\n",
      "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
      "                                    padding='same',\n",
      "                                    kernel_initializer=initializer,\n",
      "                                    use_bias=False))\n",
      "\n",
      "  result.add(tf.keras.layers.BatchNormalization())\n",
      "\n",
      "  if apply_dropout:\n",
      "      result.add(tf.keras.layers.Dropout(0.5))\n",
      "\n",
      "  #result.add(tf.keras.layers.ReLU())\n",
      "\n",
      "  return result\n",
      "53/20:\n",
      "up_model = upsample(3, 4)\n",
      "up_result = up_model(down_result)\n",
      "print (up_result.shape)\n",
      "54/1:\n",
      "import tensorflow as tf\n",
      "\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "print(tf.config.list_physical_devices())\n",
      "config = tf.compat.v1.ConfigProto(gpu_options = \n",
      "                         tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
      "# device_count = {'GPU': 1}\n",
      ")\n",
      "config.gpu_options.allow_growth = True\n",
      "session = tf.compat.v1.Session(config=config)\n",
      "tf.compat.v1.keras.backend.set_session(session)\n",
      "\n",
      "import os\n",
      "import time\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython import display\n",
      "54/2: !pip install -q -U tensorboard\n",
      "54/3:\n",
      "_URL = 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz'\n",
      "\n",
      "path_to_zip = tf.keras.utils.get_file('facades.tar.gz',\n",
      "                                      origin=_URL,\n",
      "                                      extract=True)\n",
      "\n",
      "PATH = os.path.join(os.path.dirname(path_to_zip), 'facades/')\n",
      "54/4:\n",
      "BUFFER_SIZE = 400\n",
      "BATCH_SIZE = 1\n",
      "IMG_WIDTH = 256\n",
      "IMG_HEIGHT = 256\n",
      "54/5:\n",
      "def load(image_file):\n",
      "  image = tf.io.read_file(image_file)\n",
      "  image = tf.image.decode_jpeg(image)\n",
      "\n",
      "  w = tf.shape(image)[1]\n",
      "\n",
      "  w = w // 2\n",
      "  real_image = image[:, :w, :]\n",
      "  input_image = image[:, w:, :]\n",
      "\n",
      "  input_image = tf.cast(input_image, tf.float32)\n",
      "  real_image = tf.cast(real_image, tf.float32)\n",
      "\n",
      "  return input_image, real_image\n",
      "54/6:\n",
      "inp, re = load(PATH+'train/100.jpg')\n",
      "# casting to int for matplotlib to show the image\n",
      "plt.figure()\n",
      "plt.imshow(inp/255.0)\n",
      "plt.figure()\n",
      "plt.imshow(re/255.0)\n",
      "54/7:\n",
      "def resize(input_image, real_image, height, width):\n",
      "  input_image = tf.image.resize(input_image, [height, width],\n",
      "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "  real_image = tf.image.resize(real_image, [height, width],\n",
      "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "\n",
      "  return input_image, real_image\n",
      "54/8:\n",
      "def random_crop(input_image, real_image):\n",
      "  stacked_image = tf.stack([input_image, real_image], axis=0)\n",
      "  cropped_image = tf.image.random_crop(\n",
      "      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
      "\n",
      "  return cropped_image[0], cropped_image[1]\n",
      "54/9:\n",
      "# normalizing the images to [-1, 1]\n",
      "\n",
      "def normalize(input_image, real_image):\n",
      "  input_image = (input_image / 127.5) - 1\n",
      "  real_image = (real_image / 127.5) - 1\n",
      "\n",
      "  return input_image, real_image\n",
      "54/10:\n",
      "@tf.function()\n",
      "def random_jitter(input_image, real_image):\n",
      "  # resizing to 286 x 286 x 3\n",
      "  input_image, real_image = resize(input_image, real_image, 286, 286)\n",
      "\n",
      "  # randomly cropping to 256 x 256 x 3\n",
      "  input_image, real_image = random_crop(input_image, real_image)\n",
      "\n",
      "  if tf.random.uniform(()) > 0.5:\n",
      "    # random mirroring\n",
      "    input_image = tf.image.flip_left_right(input_image)\n",
      "    real_image = tf.image.flip_left_right(real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "54/11:\n",
      "plt.figure(figsize=(6, 6))\n",
      "for i in range(4):\n",
      "  rj_inp, rj_re = random_jitter(inp, re)\n",
      "  plt.subplot(2, 2, i+1)\n",
      "  plt.imshow(rj_inp/255.0)\n",
      "  plt.axis('off')\n",
      "plt.show()\n",
      "54/12:\n",
      "def load_image_train(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = random_jitter(input_image, real_image)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "54/13:\n",
      "def load_image_test(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = resize(input_image, real_image,\n",
      "                                   IMG_HEIGHT, IMG_WIDTH)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "54/14:\n",
      "train_dataset = tf.data.Dataset.list_files(PATH+'train/*.jpg')\n",
      "train_dataset = train_dataset.map(load_image_train,\n",
      "                                  num_parallel_calls=tf.data.AUTOTUNE)\n",
      "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
      "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
      "54/15:\n",
      "test_dataset = tf.data.Dataset.list_files(PATH+'test/*.jpg')\n",
      "test_dataset = test_dataset.map(load_image_test)\n",
      "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
      "54/16: OUTPUT_CHANNELS = 3\n",
      "54/17:\n",
      "def downsample(filters, size, apply_batchnorm=True):\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "\n",
      "  result = tf.keras.Sequential()\n",
      "  result.add(\n",
      "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
      "                             kernel_initializer=initializer, use_bias=False))\n",
      "\n",
      "#   if apply_batchnorm:\n",
      "#     result.add(tf.keras.layers.BatchNormalization())\n",
      "\n",
      "  #result.add(tf.keras.layers.ReLU())\n",
      "\n",
      "  return result\n",
      "54/18:\n",
      "down_model = downsample(3, 4)\n",
      "down_result = down_model(tf.expand_dims(inp, 0))\n",
      "print (down_result.shape)\n",
      "55/1:\n",
      "import tensorflow as tf\n",
      "\n",
      "from tensorflow.python.client import device_lib \n",
      "print(device_lib.list_local_devices())\n",
      "print(tf.config.list_physical_devices())\n",
      "config = tf.compat.v1.ConfigProto(gpu_options = \n",
      "                         tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
      "# device_count = {'GPU': 1}\n",
      ")\n",
      "tf.config.gpu.set_per_process_memory_fraction(0.4)\n",
      "\n",
      "import os\n",
      "import time\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython import display\n",
      "55/2:\n",
      "import tensorflow as tf\n",
      "\n",
      "import os\n",
      "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
      "\n",
      "import os\n",
      "import time\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython import display\n",
      "55/3:\n",
      "import tensorflow as tf\n",
      "\n",
      "import os\n",
      "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
      "\n",
      "\n",
      "import time\n",
      "\n",
      "from matplotlib import pyplot as plt\n",
      "from IPython import display\n",
      "55/4: !pip install -q -U tensorboard\n",
      "55/5:\n",
      "_URL = 'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz'\n",
      "\n",
      "path_to_zip = tf.keras.utils.get_file('facades.tar.gz',\n",
      "                                      origin=_URL,\n",
      "                                      extract=True)\n",
      "\n",
      "PATH = os.path.join(os.path.dirname(path_to_zip), 'facades/')\n",
      "55/6:\n",
      "BUFFER_SIZE = 400\n",
      "BATCH_SIZE = 1\n",
      "IMG_WIDTH = 256\n",
      "IMG_HEIGHT = 256\n",
      "55/7:\n",
      "def load(image_file):\n",
      "  image = tf.io.read_file(image_file)\n",
      "  image = tf.image.decode_jpeg(image)\n",
      "\n",
      "  w = tf.shape(image)[1]\n",
      "\n",
      "  w = w // 2\n",
      "  real_image = image[:, :w, :]\n",
      "  input_image = image[:, w:, :]\n",
      "\n",
      "  input_image = tf.cast(input_image, tf.float32)\n",
      "  real_image = tf.cast(real_image, tf.float32)\n",
      "\n",
      "  return input_image, real_image\n",
      "55/8:\n",
      "inp, re = load(PATH+'train/100.jpg')\n",
      "# casting to int for matplotlib to show the image\n",
      "plt.figure()\n",
      "plt.imshow(inp/255.0)\n",
      "plt.figure()\n",
      "plt.imshow(re/255.0)\n",
      "55/9:\n",
      "def resize(input_image, real_image, height, width):\n",
      "  input_image = tf.image.resize(input_image, [height, width],\n",
      "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "  real_image = tf.image.resize(real_image, [height, width],\n",
      "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
      "\n",
      "  return input_image, real_image\n",
      "55/10:\n",
      "def random_crop(input_image, real_image):\n",
      "  stacked_image = tf.stack([input_image, real_image], axis=0)\n",
      "  cropped_image = tf.image.random_crop(\n",
      "      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
      "\n",
      "  return cropped_image[0], cropped_image[1]\n",
      "55/11:\n",
      "# normalizing the images to [-1, 1]\n",
      "\n",
      "def normalize(input_image, real_image):\n",
      "  input_image = (input_image / 127.5) - 1\n",
      "  real_image = (real_image / 127.5) - 1\n",
      "\n",
      "  return input_image, real_image\n",
      "55/12:\n",
      "@tf.function()\n",
      "def random_jitter(input_image, real_image):\n",
      "  # resizing to 286 x 286 x 3\n",
      "  input_image, real_image = resize(input_image, real_image, 286, 286)\n",
      "\n",
      "  # randomly cropping to 256 x 256 x 3\n",
      "  input_image, real_image = random_crop(input_image, real_image)\n",
      "\n",
      "  if tf.random.uniform(()) > 0.5:\n",
      "    # random mirroring\n",
      "    input_image = tf.image.flip_left_right(input_image)\n",
      "    real_image = tf.image.flip_left_right(real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "55/13:\n",
      "plt.figure(figsize=(6, 6))\n",
      "for i in range(4):\n",
      "  rj_inp, rj_re = random_jitter(inp, re)\n",
      "  plt.subplot(2, 2, i+1)\n",
      "  plt.imshow(rj_inp/255.0)\n",
      "  plt.axis('off')\n",
      "plt.show()\n",
      "55/14:\n",
      "def load_image_train(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = random_jitter(input_image, real_image)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "55/15:\n",
      "def load_image_test(image_file):\n",
      "  input_image, real_image = load(image_file)\n",
      "  input_image, real_image = resize(input_image, real_image,\n",
      "                                   IMG_HEIGHT, IMG_WIDTH)\n",
      "  input_image, real_image = normalize(input_image, real_image)\n",
      "\n",
      "  return input_image, real_image\n",
      "55/16:\n",
      "train_dataset = tf.data.Dataset.list_files(PATH+'train/*.jpg')\n",
      "train_dataset = train_dataset.map(load_image_train,\n",
      "                                  num_parallel_calls=tf.data.AUTOTUNE)\n",
      "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
      "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
      "55/17:\n",
      "test_dataset = tf.data.Dataset.list_files(PATH+'test/*.jpg')\n",
      "test_dataset = test_dataset.map(load_image_test)\n",
      "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
      "55/18: OUTPUT_CHANNELS = 3\n",
      "55/19:\n",
      "def downsample(filters, size, apply_batchnorm=True):\n",
      "  initializer = tf.random_normal_initializer(0., 0.02)\n",
      "\n",
      "  result = tf.keras.Sequential()\n",
      "  result.add(\n",
      "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
      "                             kernel_initializer=initializer, use_bias=False))\n",
      "\n",
      "  if apply_batchnorm:\n",
      "    result.add(tf.keras.layers.BatchNormalization())\n",
      "\n",
      "  result.add(tf.keras.layers.ReLU())\n",
      "\n",
      "  return result\n",
      "55/20:\n",
      "down_model = downsample(3, 4)\n",
      "down_result = down_model(tf.expand_dims(inp, 0))\n",
      "print (down_result.shape)\n",
      "57/1:\n",
      "from tensorflow.keras.applications import EfficientNetB4\n",
      "from tensorflow.image import resize\n",
      "model = EfficientNetB4(weights='imagenet')\n",
      "\n",
      "def center_crop(img, new_width=None, new_height=None):        \n",
      "\n",
      "    width = img.shape[1]\n",
      "    height = img.shape[0]\n",
      "\n",
      "    if new_width is None:\n",
      "        new_width = min(width, height)\n",
      "\n",
      "    if new_height is None:\n",
      "        new_height = min(width, height)\n",
      "\n",
      "    left = int(np.ceil((width - new_width) / 2))\n",
      "    right = width - int(np.floor((width - new_width) / 2))\n",
      "\n",
      "    top = int(np.ceil((height - new_height) / 2))\n",
      "    bottom = height - int(np.floor((height - new_height) / 2))\n",
      "\n",
      "    if len(img.shape) == 2:\n",
      "        center_cropped_img = img[top:bottom, left:right]\n",
      "    else:\n",
      "        center_cropped_img = img[top:bottom, left:right, ...]\n",
      "\n",
      "    return center_cropped_img\n",
      "\n",
      "class_dict = {0: 'tench, Tinca tinca',\n",
      " 1: 'goldfish, Carassius auratus',\n",
      " 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n",
      " 3: 'tiger shark, Galeocerdo cuvieri',\n",
      " 4: 'hammerhead, hammerhead shark',\n",
      " 5: 'electric ray, crampfish, numbfish, torpedo',\n",
      " 6: 'stingray',\n",
      " 7: 'cock',\n",
      " 8: 'hen',\n",
      " 9: 'ostrich, Struthio camelus',\n",
      " 10: 'brambling, Fringilla montifringilla',\n",
      " 11: 'goldfinch, Carduelis carduelis',\n",
      " 12: 'house finch, linnet, Carpodacus mexicanus',\n",
      " 13: 'junco, snowbird',\n",
      " 14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea',\n",
      " 15: 'robin, American robin, Turdus migratorius',\n",
      " 16: 'bulbul',\n",
      " 17: 'jay',\n",
      " 18: 'magpie',\n",
      " 19: 'chickadee',\n",
      " 20: 'water ouzel, dipper',\n",
      " 21: 'kite',\n",
      " 22: 'bald eagle, American eagle, Haliaeetus leucocephalus',\n",
      " 23: 'vulture',\n",
      " 24: 'great grey owl, great gray owl, Strix nebulosa',\n",
      " 25: 'European fire salamander, Salamandra salamandra',\n",
      " 26: 'common newt, Triturus vulgaris',\n",
      " 27: 'eft',\n",
      " 28: 'spotted salamander, Ambystoma maculatum',\n",
      " 29: 'axolotl, mud puppy, Ambystoma mexicanum',\n",
      " 30: 'bullfrog, Rana catesbeiana',\n",
      " 31: 'tree frog, tree-frog',\n",
      " 32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui',\n",
      " 33: 'loggerhead, loggerhead turtle, Caretta caretta',\n",
      " 34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea',\n",
      " 35: 'mud turtle',\n",
      " 36: 'terrapin',\n",
      " 37: 'box turtle, box tortoise',\n",
      " 38: 'banded gecko',\n",
      " 39: 'common iguana, iguana, Iguana iguana',\n",
      " 40: 'American chameleon, anole, Anolis carolinensis',\n",
      " 41: 'whiptail, whiptail lizard',\n",
      " 42: 'agama',\n",
      " 43: 'frilled lizard, Chlamydosaurus kingi',\n",
      " 44: 'alligator lizard',\n",
      " 45: 'Gila monster, Heloderma suspectum',\n",
      " 46: 'green lizard, Lacerta viridis',\n",
      " 47: 'African chameleon, Chamaeleo chamaeleon',\n",
      " 48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis',\n",
      " 49: 'African crocodile, Nile crocodile, Crocodylus niloticus',\n",
      " 50: 'American alligator, Alligator mississipiensis',\n",
      " 51: 'triceratops',\n",
      " 52: 'thunder snake, worm snake, Carphophis amoenus',\n",
      " 53: 'ringneck snake, ring-necked snake, ring snake',\n",
      " 54: 'hognose snake, puff adder, sand viper',\n",
      " 55: 'green snake, grass snake',\n",
      " 56: 'king snake, kingsnake',\n",
      " 57: 'garter snake, grass snake',\n",
      " 58: 'water snake',\n",
      " 59: 'vine snake',\n",
      " 60: 'night snake, Hypsiglena torquata',\n",
      " 61: 'boa constrictor, Constrictor constrictor',\n",
      " 62: 'rock python, rock snake, Python sebae',\n",
      " 63: 'Indian cobra, Naja naja',\n",
      " 64: 'green mamba',\n",
      " 65: 'sea snake',\n",
      " 66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus',\n",
      " 67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus',\n",
      " 68: 'sidewinder, horned rattlesnake, Crotalus cerastes',\n",
      " 69: 'trilobite',\n",
      " 70: 'harvestman, daddy longlegs, Phalangium opilio',\n",
      " 71: 'scorpion',\n",
      " 72: 'black and gold garden spider, Argiope aurantia',\n",
      " 73: 'barn spider, Araneus cavaticus',\n",
      " 74: 'garden spider, Aranea diademata',\n",
      " 75: 'black widow, Latrodectus mactans',\n",
      " 76: 'tarantula',\n",
      " 77: 'wolf spider, hunting spider',\n",
      " 78: 'tick',\n",
      " 79: 'centipede',\n",
      " 80: 'black grouse',\n",
      " 81: 'ptarmigan',\n",
      " 82: 'ruffed grouse, partridge, Bonasa umbellus',\n",
      " 83: 'prairie chicken, prairie grouse, prairie fowl',\n",
      " 84: 'peacock',\n",
      " 85: 'quail',\n",
      " 86: 'partridge',\n",
      " 87: 'African grey, African gray, Psittacus erithacus',\n",
      " 88: 'macaw',\n",
      " 89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n",
      " 90: 'lorikeet',\n",
      " 91: 'coucal',\n",
      " 92: 'bee eater',\n",
      " 93: 'hornbill',\n",
      " 94: 'hummingbird',\n",
      " 95: 'jacamar',\n",
      " 96: 'toucan',\n",
      " 97: 'drake',\n",
      " 98: 'red-breasted merganser, Mergus serrator',\n",
      " 99: 'goose',\n",
      " 100: 'black swan, Cygnus atratus',\n",
      " 101: 'tusker',\n",
      " 102: 'echidna, spiny anteater, anteater',\n",
      " 103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n",
      " 104: 'wallaby, brush kangaroo',\n",
      " 105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n",
      " 106: 'wombat',\n",
      " 107: 'jellyfish',\n",
      " 108: 'sea anemone, anemone',\n",
      " 109: 'brain coral',\n",
      " 110: 'flatworm, platyhelminth',\n",
      " 111: 'nematode, nematode worm, roundworm',\n",
      " 112: 'conch',\n",
      " 113: 'snail',\n",
      " 114: 'slug',\n",
      " 115: 'sea slug, nudibranch',\n",
      " 116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore',\n",
      " 117: 'chambered nautilus, pearly nautilus, nautilus',\n",
      " 118: 'Dungeness crab, Cancer magister',\n",
      " 119: 'rock crab, Cancer irroratus',\n",
      " 120: 'fiddler crab',\n",
      " 121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica',\n",
      " 122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus',\n",
      " 123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish',\n",
      " 124: 'crayfish, crawfish, crawdad, crawdaddy',\n",
      " 125: 'hermit crab',\n",
      " 126: 'isopod',\n",
      " 127: 'white stork, Ciconia ciconia',\n",
      " 128: 'black stork, Ciconia nigra',\n",
      " 129: 'spoonbill',\n",
      " 130: 'flamingo',\n",
      " 131: 'little blue heron, Egretta caerulea',\n",
      " 132: 'American egret, great white heron, Egretta albus',\n",
      " 133: 'bittern',\n",
      " 134: 'crane',\n",
      " 135: 'limpkin, Aramus pictus',\n",
      " 136: 'European gallinule, Porphyrio porphyrio',\n",
      " 137: 'American coot, marsh hen, mud hen, water hen, Fulica americana',\n",
      " 138: 'bustard',\n",
      " 139: 'ruddy turnstone, Arenaria interpres',\n",
      " 140: 'red-backed sandpiper, dunlin, Erolia alpina',\n",
      " 141: 'redshank, Tringa totanus',\n",
      " 142: 'dowitcher',\n",
      " 143: 'oystercatcher, oyster catcher',\n",
      " 144: 'pelican',\n",
      " 145: 'king penguin, Aptenodytes patagonica',\n",
      " 146: 'albatross, mollymawk',\n",
      " 147: 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus',\n",
      " 148: 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca',\n",
      " 149: 'dugong, Dugong dugon',\n",
      " 150: 'sea lion',\n",
      " 151: 'Chihuahua',\n",
      " 152: 'Japanese spaniel',\n",
      " 153: 'Maltese dog, Maltese terrier, Maltese',\n",
      " 154: 'Pekinese, Pekingese, Peke',\n",
      " 155: 'Shih-Tzu',\n",
      " 156: 'Blenheim spaniel',\n",
      " 157: 'papillon',\n",
      " 158: 'toy terrier',\n",
      " 159: 'Rhodesian ridgeback',\n",
      " 160: 'Afghan hound, Afghan',\n",
      " 161: 'basset, basset hound',\n",
      " 162: 'beagle',\n",
      " 163: 'bloodhound, sleuthhound',\n",
      " 164: 'bluetick',\n",
      " 165: 'black-and-tan coonhound',\n",
      " 166: 'Walker hound, Walker foxhound',\n",
      " 167: 'English foxhound',\n",
      " 168: 'redbone',\n",
      " 169: 'borzoi, Russian wolfhound',\n",
      " 170: 'Irish wolfhound',\n",
      " 171: 'Italian greyhound',\n",
      " 172: 'whippet',\n",
      " 173: 'Ibizan hound, Ibizan Podenco',\n",
      " 174: 'Norwegian elkhound, elkhound',\n",
      " 175: 'otterhound, otter hound',\n",
      " 176: 'Saluki, gazelle hound',\n",
      " 177: 'Scottish deerhound, deerhound',\n",
      " 178: 'Weimaraner',\n",
      " 179: 'Staffordshire bullterrier, Staffordshire bull terrier',\n",
      " 180: 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier',\n",
      " 181: 'Bedlington terrier',\n",
      " 182: 'Border terrier',\n",
      " 183: 'Kerry blue terrier',\n",
      " 184: 'Irish terrier',\n",
      " 185: 'Norfolk terrier',\n",
      " 186: 'Norwich terrier',\n",
      " 187: 'Yorkshire terrier',\n",
      " 188: 'wire-haired fox terrier',\n",
      " 189: 'Lakeland terrier',\n",
      " 190: 'Sealyham terrier, Sealyham',\n",
      " 191: 'Airedale, Airedale terrier',\n",
      " 192: 'cairn, cairn terrier',\n",
      " 193: 'Australian terrier',\n",
      " 194: 'Dandie Dinmont, Dandie Dinmont terrier',\n",
      " 195: 'Boston bull, Boston terrier',\n",
      " 196: 'miniature schnauzer',\n",
      " 197: 'giant schnauzer',\n",
      " 198: 'standard schnauzer',\n",
      " 199: 'Scotch terrier, Scottish terrier, Scottie',\n",
      " 200: 'Tibetan terrier, chrysanthemum dog',\n",
      " 201: 'silky terrier, Sydney silky',\n",
      " 202: 'soft-coated wheaten terrier',\n",
      " 203: 'West Highland white terrier',\n",
      " 204: 'Lhasa, Lhasa apso',\n",
      " 205: 'flat-coated retriever',\n",
      " 206: 'curly-coated retriever',\n",
      " 207: 'golden retriever',\n",
      " 208: 'Labrador retriever',\n",
      " 209: 'Chesapeake Bay retriever',\n",
      " 210: 'German short-haired pointer',\n",
      " 211: 'vizsla, Hungarian pointer',\n",
      " 212: 'English setter',\n",
      " 213: 'Irish setter, red setter',\n",
      " 214: 'Gordon setter',\n",
      " 215: 'Brittany spaniel',\n",
      " 216: 'clumber, clumber spaniel',\n",
      " 217: 'English springer, English springer spaniel',\n",
      " 218: 'Welsh springer spaniel',\n",
      " 219: 'cocker spaniel, English cocker spaniel, cocker',\n",
      " 220: 'Sussex spaniel',\n",
      " 221: 'Irish water spaniel',\n",
      " 222: 'kuvasz',\n",
      " 223: 'schipperke',\n",
      " 224: 'groenendael',\n",
      " 225: 'malinois',\n",
      " 226: 'briard',\n",
      " 227: 'kelpie',\n",
      " 228: 'komondor',\n",
      " 229: 'Old English sheepdog, bobtail',\n",
      " 230: 'Shetland sheepdog, Shetland sheep dog, Shetland',\n",
      " 231: 'collie',\n",
      " 232: 'Border collie',\n",
      " 233: 'Bouvier des Flandres, Bouviers des Flandres',\n",
      " 234: 'Rottweiler',\n",
      " 235: 'German shepherd, German shepherd dog, German police dog, alsatian',\n",
      " 236: 'Doberman, Doberman pinscher',\n",
      " 237: 'miniature pinscher',\n",
      " 238: 'Greater Swiss Mountain dog',\n",
      " 239: 'Bernese mountain dog',\n",
      " 240: 'Appenzeller',\n",
      " 241: 'EntleBucher',\n",
      " 242: 'boxer',\n",
      " 243: 'bull mastiff',\n",
      " 244: 'Tibetan mastiff',\n",
      " 245: 'French bulldog',\n",
      " 246: 'Great Dane',\n",
      " 247: 'Saint Bernard, St Bernard',\n",
      " 248: 'Eskimo dog, husky',\n",
      " 249: 'malamute, malemute, Alaskan malamute',\n",
      " 250: 'Siberian husky',\n",
      " 251: 'dalmatian, coach dog, carriage dog',\n",
      " 252: 'affenpinscher, monkey pinscher, monkey dog',\n",
      " 253: 'basenji',\n",
      " 254: 'pug, pug-dog',\n",
      " 255: 'Leonberg',\n",
      " 256: 'Newfoundland, Newfoundland dog',\n",
      " 257: 'Great Pyrenees',\n",
      " 258: 'Samoyed, Samoyede',\n",
      " 259: 'Pomeranian',\n",
      " 260: 'chow, chow chow',\n",
      " 261: 'keeshond',\n",
      " 262: 'Brabancon griffon',\n",
      " 263: 'Pembroke, Pembroke Welsh corgi',\n",
      " 264: 'Cardigan, Cardigan Welsh corgi',\n",
      " 265: 'toy poodle',\n",
      " 266: 'miniature poodle',\n",
      " 267: 'standard poodle',\n",
      " 268: 'Mexican hairless',\n",
      " 269: 'timber wolf, grey wolf, gray wolf, Canis lupus',\n",
      " 270: 'white wolf, Arctic wolf, Canis lupus tundrarum',\n",
      " 271: 'red wolf, maned wolf, Canis rufus, Canis niger',\n",
      " 272: 'coyote, prairie wolf, brush wolf, Canis latrans',\n",
      " 273: 'dingo, warrigal, warragal, Canis dingo',\n",
      " 274: 'dhole, Cuon alpinus',\n",
      " 275: 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus',\n",
      " 276: 'hyena, hyaena',\n",
      " 277: 'red fox, Vulpes vulpes',\n",
      " 278: 'kit fox, Vulpes macrotis',\n",
      " 279: 'Arctic fox, white fox, Alopex lagopus',\n",
      " 280: 'grey fox, gray fox, Urocyon cinereoargenteus',\n",
      " 281: 'tabby, tabby cat',\n",
      " 282: 'tiger cat',\n",
      " 283: 'Persian cat',\n",
      " 284: 'Siamese cat, Siamese',\n",
      " 285: 'Egyptian cat',\n",
      " 286: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n",
      " 287: 'lynx, catamount',\n",
      " 288: 'leopard, Panthera pardus',\n",
      " 289: 'snow leopard, ounce, Panthera uncia',\n",
      " 290: 'jaguar, panther, Panthera onca, Felis onca',\n",
      " 291: 'lion, king of beasts, Panthera leo',\n",
      " 292: 'tiger, Panthera tigris',\n",
      " 293: 'cheetah, chetah, Acinonyx jubatus',\n",
      " 294: 'brown bear, bruin, Ursus arctos',\n",
      " 295: 'American black bear, black bear, Ursus americanus, Euarctos americanus',\n",
      " 296: 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus',\n",
      " 297: 'sloth bear, Melursus ursinus, Ursus ursinus',\n",
      " 298: 'mongoose',\n",
      " 299: 'meerkat, mierkat',\n",
      " 300: 'tiger beetle',\n",
      " 301: 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle',\n",
      " 302: 'ground beetle, carabid beetle',\n",
      " 303: 'long-horned beetle, longicorn, longicorn beetle',\n",
      " 304: 'leaf beetle, chrysomelid',\n",
      " 305: 'dung beetle',\n",
      " 306: 'rhinoceros beetle',\n",
      " 307: 'weevil',\n",
      " 308: 'fly',\n",
      " 309: 'bee',\n",
      " 310: 'ant, emmet, pismire',\n",
      " 311: 'grasshopper, hopper',\n",
      " 312: 'cricket',\n",
      " 313: 'walking stick, walkingstick, stick insect',\n",
      " 314: 'cockroach, roach',\n",
      " 315: 'mantis, mantid',\n",
      " 316: 'cicada, cicala',\n",
      " 317: 'leafhopper',\n",
      " 318: 'lacewing, lacewing fly',\n",
      " 319: \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\",\n",
      " 320: 'damselfly',\n",
      " 321: 'admiral',\n",
      " 322: 'ringlet, ringlet butterfly',\n",
      " 323: 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus',\n",
      " 324: 'cabbage butterfly',\n",
      " 325: 'sulphur butterfly, sulfur butterfly',\n",
      " 326: 'lycaenid, lycaenid butterfly',\n",
      " 327: 'starfish, sea star',\n",
      " 328: 'sea urchin',\n",
      " 329: 'sea cucumber, holothurian',\n",
      " 330: 'wood rabbit, cottontail, cottontail rabbit',\n",
      " 331: 'hare',\n",
      " 332: 'Angora, Angora rabbit',\n",
      " 333: 'hamster',\n",
      " 334: 'porcupine, hedgehog',\n",
      " 335: 'fox squirrel, eastern fox squirrel, Sciurus niger',\n",
      " 336: 'marmot',\n",
      " 337: 'beaver',\n",
      " 338: 'guinea pig, Cavia cobaya',\n",
      " 339: 'sorrel',\n",
      " 340: 'zebra',\n",
      " 341: 'hog, pig, grunter, squealer, Sus scrofa',\n",
      " 342: 'wild boar, boar, Sus scrofa',\n",
      " 343: 'warthog',\n",
      " 344: 'hippopotamus, hippo, river horse, Hippopotamus amphibius',\n",
      " 345: 'ox',\n",
      " 346: 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis',\n",
      " 347: 'bison',\n",
      " 348: 'ram, tup',\n",
      " 349: 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis',\n",
      " 350: 'ibex, Capra ibex',\n",
      " 351: 'hartebeest',\n",
      " 352: 'impala, Aepyceros melampus',\n",
      " 353: 'gazelle',\n",
      " 354: 'Arabian camel, dromedary, Camelus dromedarius',\n",
      " 355: 'llama',\n",
      " 356: 'weasel',\n",
      " 357: 'mink',\n",
      " 358: 'polecat, fitch, foulmart, foumart, Mustela putorius',\n",
      " 359: 'black-footed ferret, ferret, Mustela nigripes',\n",
      " 360: 'otter',\n",
      " 361: 'skunk, polecat, wood pussy',\n",
      " 362: 'badger',\n",
      " 363: 'armadillo',\n",
      " 364: 'three-toed sloth, ai, Bradypus tridactylus',\n",
      " 365: 'orangutan, orang, orangutang, Pongo pygmaeus',\n",
      " 366: 'gorilla, Gorilla gorilla',\n",
      " 367: 'chimpanzee, chimp, Pan troglodytes',\n",
      " 368: 'gibbon, Hylobates lar',\n",
      " 369: 'siamang, Hylobates syndactylus, Symphalangus syndactylus',\n",
      " 370: 'guenon, guenon monkey',\n",
      " 371: 'patas, hussar monkey, Erythrocebus patas',\n",
      " 372: 'baboon',\n",
      " 373: 'macaque',\n",
      " 374: 'langur',\n",
      " 375: 'colobus, colobus monkey',\n",
      " 376: 'proboscis monkey, Nasalis larvatus',\n",
      " 377: 'marmoset',\n",
      " 378: 'capuchin, ringtail, Cebus capucinus',\n",
      " 379: 'howler monkey, howler',\n",
      " 380: 'titi, titi monkey',\n",
      " 381: 'spider monkey, Ateles geoffroyi',\n",
      " 382: 'squirrel monkey, Saimiri sciureus',\n",
      " 383: 'Madagascar cat, ring-tailed lemur, Lemur catta',\n",
      " 384: 'indri, indris, Indri indri, Indri brevicaudatus',\n",
      " 385: 'Indian elephant, Elephas maximus',\n",
      " 386: 'African elephant, Loxodonta africana',\n",
      " 387: 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens',\n",
      " 388: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca',\n",
      " 389: 'barracouta, snoek',\n",
      " 390: 'eel',\n",
      " 391: 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch',\n",
      " 392: 'rock beauty, Holocanthus tricolor',\n",
      " 393: 'anemone fish',\n",
      " 394: 'sturgeon',\n",
      " 395: 'gar, garfish, garpike, billfish, Lepisosteus osseus',\n",
      " 396: 'lionfish',\n",
      " 397: 'puffer, pufferfish, blowfish, globefish',\n",
      " 398: 'abacus',\n",
      " 399: 'abaya',\n",
      " 400: \"academic gown, academic robe, judge's robe\",\n",
      " 401: 'accordion, piano accordion, squeeze box',\n",
      " 402: 'acoustic guitar',\n",
      " 403: 'aircraft carrier, carrier, flattop, attack aircraft carrier',\n",
      " 404: 'airliner',\n",
      " 405: 'airship, dirigible',\n",
      " 406: 'altar',\n",
      " 407: 'ambulance',\n",
      " 408: 'amphibian, amphibious vehicle',\n",
      " 409: 'analog clock',\n",
      " 410: 'apiary, bee house',\n",
      " 411: 'apron',\n",
      " 412: 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin',\n",
      " 413: 'assault rifle, assault gun',\n",
      " 414: 'backpack, back pack, knapsack, packsack, rucksack, haversack',\n",
      " 415: 'bakery, bakeshop, bakehouse',\n",
      " 416: 'balance beam, beam',\n",
      " 417: 'balloon',\n",
      " 418: 'ballpoint, ballpoint pen, ballpen, Biro',\n",
      " 419: 'Band Aid',\n",
      " 420: 'banjo',\n",
      " 421: 'bannister, banister, balustrade, balusters, handrail',\n",
      " 422: 'barbell',\n",
      " 423: 'barber chair',\n",
      " 424: 'barbershop',\n",
      " 425: 'barn',\n",
      " 426: 'barometer',\n",
      " 427: 'barrel, cask',\n",
      " 428: 'barrow, garden cart, lawn cart, wheelbarrow',\n",
      " 429: 'baseball',\n",
      " 430: 'basketball',\n",
      " 431: 'bassinet',\n",
      " 432: 'bassoon',\n",
      " 433: 'bathing cap, swimming cap',\n",
      " 434: 'bath towel',\n",
      " 435: 'bathtub, bathing tub, bath, tub',\n",
      " 436: 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon',\n",
      " 437: 'beacon, lighthouse, beacon light, pharos',\n",
      " 438: 'beaker',\n",
      " 439: 'bearskin, busby, shako',\n",
      " 440: 'beer bottle',\n",
      " 441: 'beer glass',\n",
      " 442: 'bell cote, bell cot',\n",
      " 443: 'bib',\n",
      " 444: 'bicycle-built-for-two, tandem bicycle, tandem',\n",
      " 445: 'bikini, two-piece',\n",
      " 446: 'binder, ring-binder',\n",
      " 447: 'binoculars, field glasses, opera glasses',\n",
      " 448: 'birdhouse',\n",
      " 449: 'boathouse',\n",
      " 450: 'bobsled, bobsleigh, bob',\n",
      " 451: 'bolo tie, bolo, bola tie, bola',\n",
      " 452: 'bonnet, poke bonnet',\n",
      " 453: 'bookcase',\n",
      " 454: 'bookshop, bookstore, bookstall',\n",
      " 455: 'bottlecap',\n",
      " 456: 'bow',\n",
      " 457: 'bow tie, bow-tie, bowtie',\n",
      " 458: 'brass, memorial tablet, plaque',\n",
      " 459: 'brassiere, bra, bandeau',\n",
      " 460: 'breakwater, groin, groyne, mole, bulwark, seawall, jetty',\n",
      " 461: 'breastplate, aegis, egis',\n",
      " 462: 'broom',\n",
      " 463: 'bucket, pail',\n",
      " 464: 'buckle',\n",
      " 465: 'bulletproof vest',\n",
      " 466: 'bullet train, bullet',\n",
      " 467: 'butcher shop, meat market',\n",
      " 468: 'cab, hack, taxi, taxicab',\n",
      " 469: 'caldron, cauldron',\n",
      " 470: 'candle, taper, wax light',\n",
      " 471: 'cannon',\n",
      " 472: 'canoe',\n",
      " 473: 'can opener, tin opener',\n",
      " 474: 'cardigan',\n",
      " 475: 'car mirror',\n",
      " 476: 'carousel, carrousel, merry-go-round, roundabout, whirligig',\n",
      " 477: \"carpenter's kit, tool kit\",\n",
      " 478: 'carton',\n",
      " 479: 'car wheel',\n",
      " 480: 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM',\n",
      " 481: 'cassette',\n",
      " 482: 'cassette player',\n",
      " 483: 'castle',\n",
      " 484: 'catamaran',\n",
      " 485: 'CD player',\n",
      " 486: 'cello, violoncello',\n",
      " 487: 'cellular telephone, cellular phone, cellphone, cell, mobile phone',\n",
      " 488: 'chain',\n",
      " 489: 'chainlink fence',\n",
      " 490: 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour',\n",
      " 491: 'chain saw, chainsaw',\n",
      " 492: 'chest',\n",
      " 493: 'chiffonier, commode',\n",
      " 494: 'chime, bell, gong',\n",
      " 495: 'china cabinet, china closet',\n",
      " 496: 'Christmas stocking',\n",
      " 497: 'church, church building',\n",
      " 498: 'cinema, movie theater, movie theatre, movie house, picture palace',\n",
      " 499: 'cleaver, meat cleaver, chopper',\n",
      " 500: 'cliff dwelling',\n",
      " 501: 'cloak',\n",
      " 502: 'clog, geta, patten, sabot',\n",
      " 503: 'cocktail shaker',\n",
      " 504: 'coffee mug',\n",
      " 505: 'coffeepot',\n",
      " 506: 'coil, spiral, volute, whorl, helix',\n",
      " 507: 'combination lock',\n",
      " 508: 'computer keyboard, keypad',\n",
      " 509: 'confectionery, confectionary, candy store',\n",
      " 510: 'container ship, containership, container vessel',\n",
      " 511: 'convertible',\n",
      " 512: 'corkscrew, bottle screw',\n",
      " 513: 'cornet, horn, trumpet, trump',\n",
      " 514: 'cowboy boot',\n",
      " 515: 'cowboy hat, ten-gallon hat',\n",
      " 516: 'cradle',\n",
      " 517: 'crane',\n",
      " 518: 'crash helmet',\n",
      " 519: 'crate',\n",
      " 520: 'crib, cot',\n",
      " 521: 'Crock Pot',\n",
      " 522: 'croquet ball',\n",
      " 523: 'crutch',\n",
      " 524: 'cuirass',\n",
      " 525: 'dam, dike, dyke',\n",
      " 526: 'desk',\n",
      " 527: 'desktop computer',\n",
      " 528: 'dial telephone, dial phone',\n",
      " 529: 'diaper, nappy, napkin',\n",
      " 530: 'digital clock',\n",
      " 531: 'digital watch',\n",
      " 532: 'dining table, board',\n",
      " 533: 'dishrag, dishcloth',\n",
      " 534: 'dishwasher, dish washer, dishwashing machine',\n",
      " 535: 'disk brake, disc brake',\n",
      " 536: 'dock, dockage, docking facility',\n",
      " 537: 'dogsled, dog sled, dog sleigh',\n",
      " 538: 'dome',\n",
      " 539: 'doormat, welcome mat',\n",
      " 540: 'drilling platform, offshore rig',\n",
      " 541: 'drum, membranophone, tympan',\n",
      " 542: 'drumstick',\n",
      " 543: 'dumbbell',\n",
      " 544: 'Dutch oven',\n",
      " 545: 'electric fan, blower',\n",
      " 546: 'electric guitar',\n",
      " 547: 'electric locomotive',\n",
      " 548: 'entertainment center',\n",
      " 549: 'envelope',\n",
      " 550: 'espresso maker',\n",
      " 551: 'face powder',\n",
      " 552: 'feather boa, boa',\n",
      " 553: 'file, file cabinet, filing cabinet',\n",
      " 554: 'fireboat',\n",
      " 555: 'fire engine, fire truck',\n",
      " 556: 'fire screen, fireguard',\n",
      " 557: 'flagpole, flagstaff',\n",
      " 558: 'flute, transverse flute',\n",
      " 559: 'folding chair',\n",
      " 560: 'football helmet',\n",
      " 561: 'forklift',\n",
      " 562: 'fountain',\n",
      " 563: 'fountain pen',\n",
      " 564: 'four-poster',\n",
      " 565: 'freight car',\n",
      " 566: 'French horn, horn',\n",
      " 567: 'frying pan, frypan, skillet',\n",
      " 568: 'fur coat',\n",
      " 569: 'garbage truck, dustcart',\n",
      " 570: 'gasmask, respirator, gas helmet',\n",
      " 571: 'gas pump, gasoline pump, petrol pump, island dispenser',\n",
      " 572: 'goblet',\n",
      " 573: 'go-kart',\n",
      " 574: 'golf ball',\n",
      " 575: 'golfcart, golf cart',\n",
      " 576: 'gondola',\n",
      " 577: 'gong, tam-tam',\n",
      " 578: 'gown',\n",
      " 579: 'grand piano, grand',\n",
      " 580: 'greenhouse, nursery, glasshouse',\n",
      " 581: 'grille, radiator grille',\n",
      " 582: 'grocery store, grocery, food market, market',\n",
      " 583: 'guillotine',\n",
      " 584: 'hair slide',\n",
      " 585: 'hair spray',\n",
      " 586: 'half track',\n",
      " 587: 'hammer',\n",
      " 588: 'hamper',\n",
      " 589: 'hand blower, blow dryer, blow drier, hair dryer, hair drier',\n",
      " 590: 'hand-held computer, hand-held microcomputer',\n",
      " 591: 'handkerchief, hankie, hanky, hankey',\n",
      " 592: 'hard disc, hard disk, fixed disk',\n",
      " 593: 'harmonica, mouth organ, harp, mouth harp',\n",
      " 594: 'harp',\n",
      " 595: 'harvester, reaper',\n",
      " 596: 'hatchet',\n",
      " 597: 'holster',\n",
      " 598: 'home theater, home theatre',\n",
      " 599: 'honeycomb',\n",
      " 600: 'hook, claw',\n",
      " 601: 'hoopskirt, crinoline',\n",
      " 602: 'horizontal bar, high bar',\n",
      " 603: 'horse cart, horse-cart',\n",
      " 604: 'hourglass',\n",
      " 605: 'iPod',\n",
      " 606: 'iron, smoothing iron',\n",
      " 607: \"jack-o'-lantern\",\n",
      " 608: 'jean, blue jean, denim',\n",
      " 609: 'jeep, landrover',\n",
      " 610: 'jersey, T-shirt, tee shirt',\n",
      " 611: 'jigsaw puzzle',\n",
      " 612: 'jinrikisha, ricksha, rickshaw',\n",
      " 613: 'joystick',\n",
      " 614: 'kimono',\n",
      " 615: 'knee pad',\n",
      " 616: 'knot',\n",
      " 617: 'lab coat, laboratory coat',\n",
      " 618: 'ladle',\n",
      " 619: 'lampshade, lamp shade',\n",
      " 620: 'laptop, laptop computer',\n",
      " 621: 'lawn mower, mower',\n",
      " 622: 'lens cap, lens cover',\n",
      " 623: 'letter opener, paper knife, paperknife',\n",
      " 624: 'library',\n",
      " 625: 'lifeboat',\n",
      " 626: 'lighter, light, igniter, ignitor',\n",
      " 627: 'limousine, limo',\n",
      " 628: 'liner, ocean liner',\n",
      " 629: 'lipstick, lip rouge',\n",
      " 630: 'Loafer',\n",
      " 631: 'lotion',\n",
      " 632: 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system',\n",
      " 633: \"loupe, jeweler's loupe\",\n",
      " 634: 'lumbermill, sawmill',\n",
      " 635: 'magnetic compass',\n",
      " 636: 'mailbag, postbag',\n",
      " 637: 'mailbox, letter box',\n",
      " 638: 'maillot',\n",
      " 639: 'maillot, tank suit',\n",
      " 640: 'manhole cover',\n",
      " 641: 'maraca',\n",
      " 642: 'marimba, xylophone',\n",
      " 643: 'mask',\n",
      " 644: 'matchstick',\n",
      " 645: 'maypole',\n",
      " 646: 'maze, labyrinth',\n",
      " 647: 'measuring cup',\n",
      " 648: 'medicine chest, medicine cabinet',\n",
      " 649: 'megalith, megalithic structure',\n",
      " 650: 'microphone, mike',\n",
      " 651: 'microwave, microwave oven',\n",
      " 652: 'military uniform',\n",
      " 653: 'milk can',\n",
      " 654: 'minibus',\n",
      " 655: 'miniskirt, mini',\n",
      " 656: 'minivan',\n",
      " 657: 'missile',\n",
      " 658: 'mitten',\n",
      " 659: 'mixing bowl',\n",
      " 660: 'mobile home, manufactured home',\n",
      " 661: 'Model T',\n",
      " 662: 'modem',\n",
      " 663: 'monastery',\n",
      " 664: 'monitor',\n",
      " 665: 'moped',\n",
      " 666: 'mortar',\n",
      " 667: 'mortarboard',\n",
      " 668: 'mosque',\n",
      " 669: 'mosquito net',\n",
      " 670: 'motor scooter, scooter',\n",
      " 671: 'mountain bike, all-terrain bike, off-roader',\n",
      " 672: 'mountain tent',\n",
      " 673: 'mouse, computer mouse',\n",
      " 674: 'mousetrap',\n",
      " 675: 'moving van',\n",
      " 676: 'muzzle',\n",
      " 677: 'nail',\n",
      " 678: 'neck brace',\n",
      " 679: 'necklace',\n",
      " 680: 'nipple',\n",
      " 681: 'notebook, notebook computer',\n",
      " 682: 'obelisk',\n",
      " 683: 'oboe, hautboy, hautbois',\n",
      " 684: 'ocarina, sweet potato',\n",
      " 685: 'odometer, hodometer, mileometer, milometer',\n",
      " 686: 'oil filter',\n",
      " 687: 'organ, pipe organ',\n",
      " 688: 'oscilloscope, scope, cathode-ray oscilloscope, CRO',\n",
      " 689: 'overskirt',\n",
      " 690: 'oxcart',\n",
      " 691: 'oxygen mask',\n",
      " 692: 'packet',\n",
      " 693: 'paddle, boat paddle',\n",
      " 694: 'paddlewheel, paddle wheel',\n",
      " 695: 'padlock',\n",
      " 696: 'paintbrush',\n",
      " 697: \"pajama, pyjama, pj's, jammies\",\n",
      " 698: 'palace',\n",
      " 699: 'panpipe, pandean pipe, syrinx',\n",
      " 700: 'paper towel',\n",
      " 701: 'parachute, chute',\n",
      " 702: 'parallel bars, bars',\n",
      " 703: 'park bench',\n",
      " 704: 'parking meter',\n",
      " 705: 'passenger car, coach, carriage',\n",
      " 706: 'patio, terrace',\n",
      " 707: 'pay-phone, pay-station',\n",
      " 708: 'pedestal, plinth, footstall',\n",
      " 709: 'pencil box, pencil case',\n",
      " 710: 'pencil sharpener',\n",
      " 711: 'perfume, essence',\n",
      " 712: 'Petri dish',\n",
      " 713: 'photocopier',\n",
      " 714: 'pick, plectrum, plectron',\n",
      " 715: 'pickelhaube',\n",
      " 716: 'picket fence, paling',\n",
      " 717: 'pickup, pickup truck',\n",
      " 718: 'pier',\n",
      " 719: 'piggy bank, penny bank',\n",
      " 720: 'pill bottle',\n",
      " 721: 'pillow',\n",
      " 722: 'ping-pong ball',\n",
      " 723: 'pinwheel',\n",
      " 724: 'pirate, pirate ship',\n",
      " 725: 'pitcher, ewer',\n",
      " 726: \"plane, carpenter's plane, woodworking plane\",\n",
      " 727: 'planetarium',\n",
      " 728: 'plastic bag',\n",
      " 729: 'plate rack',\n",
      " 730: 'plow, plough',\n",
      " 731: \"plunger, plumber's helper\",\n",
      " 732: 'Polaroid camera, Polaroid Land camera',\n",
      " 733: 'pole',\n",
      " 734: 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria',\n",
      " 735: 'poncho',\n",
      " 736: 'pool table, billiard table, snooker table',\n",
      " 737: 'pop bottle, soda bottle',\n",
      " 738: 'pot, flowerpot',\n",
      " 739: \"potter's wheel\",\n",
      " 740: 'power drill',\n",
      " 741: 'prayer rug, prayer mat',\n",
      " 742: 'printer',\n",
      " 743: 'prison, prison house',\n",
      " 744: 'projectile, missile',\n",
      " 745: 'projector',\n",
      " 746: 'puck, hockey puck',\n",
      " 747: 'punching bag, punch bag, punching ball, punchball',\n",
      " 748: 'purse',\n",
      " 749: 'quill, quill pen',\n",
      " 750: 'quilt, comforter, comfort, puff',\n",
      " 751: 'racer, race car, racing car',\n",
      " 752: 'racket, racquet',\n",
      " 753: 'radiator',\n",
      " 754: 'radio, wireless',\n",
      " 755: 'radio telescope, radio reflector',\n",
      " 756: 'rain barrel',\n",
      " 757: 'recreational vehicle, RV, R.V.',\n",
      " 758: 'reel',\n",
      " 759: 'reflex camera',\n",
      " 760: 'refrigerator, icebox',\n",
      " 761: 'remote control, remote',\n",
      " 762: 'restaurant, eating house, eating place, eatery',\n",
      " 763: 'revolver, six-gun, six-shooter',\n",
      " 764: 'rifle',\n",
      " 765: 'rocking chair, rocker',\n",
      " 766: 'rotisserie',\n",
      " 767: 'rubber eraser, rubber, pencil eraser',\n",
      " 768: 'rugby ball',\n",
      " 769: 'rule, ruler',\n",
      " 770: 'running shoe',\n",
      " 771: 'safe',\n",
      " 772: 'safety pin',\n",
      " 773: 'saltshaker, salt shaker',\n",
      " 774: 'sandal',\n",
      " 775: 'sarong',\n",
      " 776: 'sax, saxophone',\n",
      " 777: 'scabbard',\n",
      " 778: 'scale, weighing machine',\n",
      " 779: 'school bus',\n",
      " 780: 'schooner',\n",
      " 781: 'scoreboard',\n",
      " 782: 'screen, CRT screen',\n",
      " 783: 'screw',\n",
      " 784: 'screwdriver',\n",
      " 785: 'seat belt, seatbelt',\n",
      " 786: 'sewing machine',\n",
      " 787: 'shield, buckler',\n",
      " 788: 'shoe shop, shoe-shop, shoe store',\n",
      " 789: 'shoji',\n",
      " 790: 'shopping basket',\n",
      " 791: 'shopping cart',\n",
      " 792: 'shovel',\n",
      " 793: 'shower cap',\n",
      " 794: 'shower curtain',\n",
      " 795: 'ski',\n",
      " 796: 'ski mask',\n",
      " 797: 'sleeping bag',\n",
      " 798: 'slide rule, slipstick',\n",
      " 799: 'sliding door',\n",
      " 800: 'slot, one-armed bandit',\n",
      " 801: 'snorkel',\n",
      " 802: 'snowmobile',\n",
      " 803: 'snowplow, snowplough',\n",
      " 804: 'soap dispenser',\n",
      " 805: 'soccer ball',\n",
      " 806: 'sock',\n",
      " 807: 'solar dish, solar collector, solar furnace',\n",
      " 808: 'sombrero',\n",
      " 809: 'soup bowl',\n",
      " 810: 'space bar',\n",
      " 811: 'space heater',\n",
      " 812: 'space shuttle',\n",
      " 813: 'spatula',\n",
      " 814: 'speedboat',\n",
      " 815: \"spider web, spider's web\",\n",
      " 816: 'spindle',\n",
      " 817: 'sports car, sport car',\n",
      " 818: 'spotlight, spot',\n",
      " 819: 'stage',\n",
      " 820: 'steam locomotive',\n",
      " 821: 'steel arch bridge',\n",
      " 822: 'steel drum',\n",
      " 823: 'stethoscope',\n",
      " 824: 'stole',\n",
      " 825: 'stone wall',\n",
      " 826: 'stopwatch, stop watch',\n",
      " 827: 'stove',\n",
      " 828: 'strainer',\n",
      " 829: 'streetcar, tram, tramcar, trolley, trolley car',\n",
      " 830: 'stretcher',\n",
      " 831: 'studio couch, day bed',\n",
      " 832: 'stupa, tope',\n",
      " 833: 'submarine, pigboat, sub, U-boat',\n",
      " 834: 'suit, suit of clothes',\n",
      " 835: 'sundial',\n",
      " 836: 'sunglass',\n",
      " 837: 'sunglasses, dark glasses, shades',\n",
      " 838: 'sunscreen, sunblock, sun blocker',\n",
      " 839: 'suspension bridge',\n",
      " 840: 'swab, swob, mop',\n",
      " 841: 'sweatshirt',\n",
      " 842: 'swimming trunks, bathing trunks',\n",
      " 843: 'swing',\n",
      " 844: 'switch, electric switch, electrical switch',\n",
      " 845: 'syringe',\n",
      " 846: 'table lamp',\n",
      " 847: 'tank, army tank, armored combat vehicle, armoured combat vehicle',\n",
      " 848: 'tape player',\n",
      " 849: 'teapot',\n",
      " 850: 'teddy, teddy bear',\n",
      " 851: 'television, television system',\n",
      " 852: 'tennis ball',\n",
      " 853: 'thatch, thatched roof',\n",
      " 854: 'theater curtain, theatre curtain',\n",
      " 855: 'thimble',\n",
      " 856: 'thresher, thrasher, threshing machine',\n",
      " 857: 'throne',\n",
      " 858: 'tile roof',\n",
      " 859: 'toaster',\n",
      " 860: 'tobacco shop, tobacconist shop, tobacconist',\n",
      " 861: 'toilet seat',\n",
      " 862: 'torch',\n",
      " 863: 'totem pole',\n",
      " 864: 'tow truck, tow car, wrecker',\n",
      " 865: 'toyshop',\n",
      " 866: 'tractor',\n",
      " 867: 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi',\n",
      " 868: 'tray',\n",
      " 869: 'trench coat',\n",
      " 870: 'tricycle, trike, velocipede',\n",
      " 871: 'trimaran',\n",
      " 872: 'tripod',\n",
      " 873: 'triumphal arch',\n",
      " 874: 'trolleybus, trolley coach, trackless trolley',\n",
      " 875: 'trombone',\n",
      " 876: 'tub, vat',\n",
      " 877: 'turnstile',\n",
      " 878: 'typewriter keyboard',\n",
      " 879: 'umbrella',\n",
      " 880: 'unicycle, monocycle',\n",
      " 881: 'upright, upright piano',\n",
      " 882: 'vacuum, vacuum cleaner',\n",
      " 883: 'vase',\n",
      " 884: 'vault',\n",
      " 885: 'velvet',\n",
      " 886: 'vending machine',\n",
      " 887: 'vestment',\n",
      " 888: 'viaduct',\n",
      " 889: 'violin, fiddle',\n",
      " 890: 'volleyball',\n",
      " 891: 'waffle iron',\n",
      " 892: 'wall clock',\n",
      " 893: 'wallet, billfold, notecase, pocketbook',\n",
      " 894: 'wardrobe, closet, press',\n",
      " 895: 'warplane, military plane',\n",
      " 896: 'washbasin, handbasin, washbowl, lavabo, wash-hand basin',\n",
      " 897: 'washer, automatic washer, washing machine',\n",
      " 898: 'water bottle',\n",
      " 899: 'water jug',\n",
      " 900: 'water tower',\n",
      " 901: 'whiskey jug',\n",
      " 902: 'whistle',\n",
      " 903: 'wig',\n",
      " 904: 'window screen',\n",
      " 905: 'window shade',\n",
      " 906: 'Windsor tie',\n",
      " 907: 'wine bottle',\n",
      " 908: 'wing',\n",
      " 909: 'wok',\n",
      " 910: 'wooden spoon',\n",
      " 911: 'wool, woolen, woollen',\n",
      " 912: 'worm fence, snake fence, snake-rail fence, Virginia fence',\n",
      " 913: 'wreck',\n",
      " 914: 'yawl',\n",
      " 915: 'yurt',\n",
      " 916: 'web site, website, internet site, site',\n",
      " 917: 'comic book',\n",
      " 918: 'crossword puzzle, crossword',\n",
      " 919: 'street sign',\n",
      " 920: 'traffic light, traffic signal, stoplight',\n",
      " 921: 'book jacket, dust cover, dust jacket, dust wrapper',\n",
      " 922: 'menu',\n",
      " 923: 'plate',\n",
      " 924: 'guacamole',\n",
      " 925: 'consomme',\n",
      " 926: 'hot pot, hotpot',\n",
      " 927: 'trifle',\n",
      " 928: 'ice cream, icecream',\n",
      " 929: 'ice lolly, lolly, lollipop, popsicle',\n",
      " 930: 'French loaf',\n",
      " 931: 'bagel, beigel',\n",
      " 932: 'pretzel',\n",
      " 933: 'cheeseburger',\n",
      " 934: 'hotdog, hot dog, red hot',\n",
      " 935: 'mashed potato',\n",
      " 936: 'head cabbage',\n",
      " 937: 'broccoli',\n",
      " 938: 'cauliflower',\n",
      " 939: 'zucchini, courgette',\n",
      " 940: 'spaghetti squash',\n",
      " 941: 'acorn squash',\n",
      " 942: 'butternut squash',\n",
      " 943: 'cucumber, cuke',\n",
      " 944: 'artichoke, globe artichoke',\n",
      " 945: 'bell pepper',\n",
      " 946: 'cardoon',\n",
      " 947: 'mushroom',\n",
      " 948: 'Granny Smith',\n",
      " 949: 'strawberry',\n",
      " 950: 'orange',\n",
      " 951: 'lemon',\n",
      " 952: 'fig',\n",
      " 953: 'pineapple, ananas',\n",
      " 954: 'banana',\n",
      " 955: 'jackfruit, jak, jack',\n",
      " 956: 'custard apple',\n",
      " 957: 'pomegranate',\n",
      " 958: 'hay',\n",
      " 959: 'carbonara',\n",
      " 960: 'chocolate sauce, chocolate syrup',\n",
      " 961: 'dough',\n",
      " 962: 'meat loaf, meatloaf',\n",
      " 963: 'pizza, pizza pie',\n",
      " 964: 'potpie',\n",
      " 965: 'burrito',\n",
      " 966: 'red wine',\n",
      " 967: 'espresso',\n",
      " 968: 'cup',\n",
      " 969: 'eggnog',\n",
      " 970: 'alp',\n",
      " 971: 'bubble',\n",
      " 972: 'cliff, drop, drop-off',\n",
      " 973: 'coral reef',\n",
      " 974: 'geyser',\n",
      " 975: 'lakeside, lakeshore',\n",
      " 976: 'promontory, headland, head, foreland',\n",
      " 977: 'sandbar, sand bar',\n",
      " 978: 'seashore, coast, seacoast, sea-coast',\n",
      " 979: 'valley, vale',\n",
      " 980: 'volcano',\n",
      " 981: 'ballplayer, baseball player',\n",
      " 982: 'groom, bridegroom',\n",
      " 983: 'scuba diver',\n",
      " 984: 'rapeseed',\n",
      " 985: 'daisy',\n",
      " 986: \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\",\n",
      " 987: 'corn',\n",
      " 988: 'acorn',\n",
      " 989: 'hip, rose hip, rosehip',\n",
      " 990: 'buckeye, horse chestnut, conker',\n",
      " 991: 'coral fungus',\n",
      " 992: 'agaric',\n",
      " 993: 'gyromitra',\n",
      " 994: 'stinkhorn, carrion fungus',\n",
      " 995: 'earthstar',\n",
      " 996: 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa',\n",
      " 997: 'bolete',\n",
      " 998: 'ear, spike, capitulum',\n",
      " 999: 'toilet tissue, toilet paper, bathroom tissue'}\n",
      "57/2:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 224\n",
      "\n",
      "def prep_image(image):\n",
      "    res = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "    res = res.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "    return res\n",
      "\n",
      "def predict(image):\n",
      "    cropped_image = center_crop(image)\n",
      "    prediction = (model.predict(prep_image(image)), model.predict(prep_image(cropped_image)))\n",
      "    if np.max(prediction[0]>=prediction[1]):\n",
      "        return np.argmax(prediction[0], axis=1)[0]\n",
      "    else:\n",
      "        return np.argmax(prediction[1], axis=1)[1]\n",
      "    \n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "plt.imshow(image)\n",
      "\n",
      "print(class_dict[predict(image)])\n",
      "57/3:\n",
      "from tensorflow.keras.applications import EfficientNetB4\n",
      "from tensorflow.image import resize\n",
      "model = EfficientNetB4(weights='imagenet')\n",
      "\n",
      "def center_crop(img, new_width=None, new_height=None):        \n",
      "\n",
      "    width = img.shape[1]\n",
      "    height = img.shape[0]\n",
      "\n",
      "    if new_width is None:\n",
      "        new_width = min(width, height)\n",
      "\n",
      "    if new_height is None:\n",
      "        new_height = min(width, height)\n",
      "\n",
      "    left = int(np.ceil((width - new_width) / 2))\n",
      "    right = width - int(np.floor((width - new_width) / 2))\n",
      "\n",
      "    top = int(np.ceil((height - new_height) / 2))\n",
      "    bottom = height - int(np.floor((height - new_height) / 2))\n",
      "\n",
      "    if len(img.shape) == 2:\n",
      "        center_cropped_img = img[top:bottom, left:right]\n",
      "    else:\n",
      "        center_cropped_img = img[top:bottom, left:right, ...]\n",
      "\n",
      "    return center_cropped_img\n",
      "\n",
      "class_dict = {0: 'tench, Tinca tinca',\n",
      " 1: 'goldfish, Carassius auratus',\n",
      " 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n",
      " 3: 'tiger shark, Galeocerdo cuvieri',\n",
      " 4: 'hammerhead, hammerhead shark',\n",
      " 5: 'electric ray, crampfish, numbfish, torpedo',\n",
      " 6: 'stingray',\n",
      " 7: 'cock',\n",
      " 8: 'hen',\n",
      " 9: 'ostrich, Struthio camelus',\n",
      " 10: 'brambling, Fringilla montifringilla',\n",
      " 11: 'goldfinch, Carduelis carduelis',\n",
      " 12: 'house finch, linnet, Carpodacus mexicanus',\n",
      " 13: 'junco, snowbird',\n",
      " 14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea',\n",
      " 15: 'robin, American robin, Turdus migratorius',\n",
      " 16: 'bulbul',\n",
      " 17: 'jay',\n",
      " 18: 'magpie',\n",
      " 19: 'chickadee',\n",
      " 20: 'water ouzel, dipper',\n",
      " 21: 'kite',\n",
      " 22: 'bald eagle, American eagle, Haliaeetus leucocephalus',\n",
      " 23: 'vulture',\n",
      " 24: 'great grey owl, great gray owl, Strix nebulosa',\n",
      " 25: 'European fire salamander, Salamandra salamandra',\n",
      " 26: 'common newt, Triturus vulgaris',\n",
      " 27: 'eft',\n",
      " 28: 'spotted salamander, Ambystoma maculatum',\n",
      " 29: 'axolotl, mud puppy, Ambystoma mexicanum',\n",
      " 30: 'bullfrog, Rana catesbeiana',\n",
      " 31: 'tree frog, tree-frog',\n",
      " 32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui',\n",
      " 33: 'loggerhead, loggerhead turtle, Caretta caretta',\n",
      " 34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea',\n",
      " 35: 'mud turtle',\n",
      " 36: 'terrapin',\n",
      " 37: 'box turtle, box tortoise',\n",
      " 38: 'banded gecko',\n",
      " 39: 'common iguana, iguana, Iguana iguana',\n",
      " 40: 'American chameleon, anole, Anolis carolinensis',\n",
      " 41: 'whiptail, whiptail lizard',\n",
      " 42: 'agama',\n",
      " 43: 'frilled lizard, Chlamydosaurus kingi',\n",
      " 44: 'alligator lizard',\n",
      " 45: 'Gila monster, Heloderma suspectum',\n",
      " 46: 'green lizard, Lacerta viridis',\n",
      " 47: 'African chameleon, Chamaeleo chamaeleon',\n",
      " 48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis',\n",
      " 49: 'African crocodile, Nile crocodile, Crocodylus niloticus',\n",
      " 50: 'American alligator, Alligator mississipiensis',\n",
      " 51: 'triceratops',\n",
      " 52: 'thunder snake, worm snake, Carphophis amoenus',\n",
      " 53: 'ringneck snake, ring-necked snake, ring snake',\n",
      " 54: 'hognose snake, puff adder, sand viper',\n",
      " 55: 'green snake, grass snake',\n",
      " 56: 'king snake, kingsnake',\n",
      " 57: 'garter snake, grass snake',\n",
      " 58: 'water snake',\n",
      " 59: 'vine snake',\n",
      " 60: 'night snake, Hypsiglena torquata',\n",
      " 61: 'boa constrictor, Constrictor constrictor',\n",
      " 62: 'rock python, rock snake, Python sebae',\n",
      " 63: 'Indian cobra, Naja naja',\n",
      " 64: 'green mamba',\n",
      " 65: 'sea snake',\n",
      " 66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus',\n",
      " 67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus',\n",
      " 68: 'sidewinder, horned rattlesnake, Crotalus cerastes',\n",
      " 69: 'trilobite',\n",
      " 70: 'harvestman, daddy longlegs, Phalangium opilio',\n",
      " 71: 'scorpion',\n",
      " 72: 'black and gold garden spider, Argiope aurantia',\n",
      " 73: 'barn spider, Araneus cavaticus',\n",
      " 74: 'garden spider, Aranea diademata',\n",
      " 75: 'black widow, Latrodectus mactans',\n",
      " 76: 'tarantula',\n",
      " 77: 'wolf spider, hunting spider',\n",
      " 78: 'tick',\n",
      " 79: 'centipede',\n",
      " 80: 'black grouse',\n",
      " 81: 'ptarmigan',\n",
      " 82: 'ruffed grouse, partridge, Bonasa umbellus',\n",
      " 83: 'prairie chicken, prairie grouse, prairie fowl',\n",
      " 84: 'peacock',\n",
      " 85: 'quail',\n",
      " 86: 'partridge',\n",
      " 87: 'African grey, African gray, Psittacus erithacus',\n",
      " 88: 'macaw',\n",
      " 89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n",
      " 90: 'lorikeet',\n",
      " 91: 'coucal',\n",
      " 92: 'bee eater',\n",
      " 93: 'hornbill',\n",
      " 94: 'hummingbird',\n",
      " 95: 'jacamar',\n",
      " 96: 'toucan',\n",
      " 97: 'drake',\n",
      " 98: 'red-breasted merganser, Mergus serrator',\n",
      " 99: 'goose',\n",
      " 100: 'black swan, Cygnus atratus',\n",
      " 101: 'tusker',\n",
      " 102: 'echidna, spiny anteater, anteater',\n",
      " 103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n",
      " 104: 'wallaby, brush kangaroo',\n",
      " 105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n",
      " 106: 'wombat',\n",
      " 107: 'jellyfish',\n",
      " 108: 'sea anemone, anemone',\n",
      " 109: 'brain coral',\n",
      " 110: 'flatworm, platyhelminth',\n",
      " 111: 'nematode, nematode worm, roundworm',\n",
      " 112: 'conch',\n",
      " 113: 'snail',\n",
      " 114: 'slug',\n",
      " 115: 'sea slug, nudibranch',\n",
      " 116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore',\n",
      " 117: 'chambered nautilus, pearly nautilus, nautilus',\n",
      " 118: 'Dungeness crab, Cancer magister',\n",
      " 119: 'rock crab, Cancer irroratus',\n",
      " 120: 'fiddler crab',\n",
      " 121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica',\n",
      " 122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus',\n",
      " 123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish',\n",
      " 124: 'crayfish, crawfish, crawdad, crawdaddy',\n",
      " 125: 'hermit crab',\n",
      " 126: 'isopod',\n",
      " 127: 'white stork, Ciconia ciconia',\n",
      " 128: 'black stork, Ciconia nigra',\n",
      " 129: 'spoonbill',\n",
      " 130: 'flamingo',\n",
      " 131: 'little blue heron, Egretta caerulea',\n",
      " 132: 'American egret, great white heron, Egretta albus',\n",
      " 133: 'bittern',\n",
      " 134: 'crane',\n",
      " 135: 'limpkin, Aramus pictus',\n",
      " 136: 'European gallinule, Porphyrio porphyrio',\n",
      " 137: 'American coot, marsh hen, mud hen, water hen, Fulica americana',\n",
      " 138: 'bustard',\n",
      " 139: 'ruddy turnstone, Arenaria interpres',\n",
      " 140: 'red-backed sandpiper, dunlin, Erolia alpina',\n",
      " 141: 'redshank, Tringa totanus',\n",
      " 142: 'dowitcher',\n",
      " 143: 'oystercatcher, oyster catcher',\n",
      " 144: 'pelican',\n",
      " 145: 'king penguin, Aptenodytes patagonica',\n",
      " 146: 'albatross, mollymawk',\n",
      " 147: 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus',\n",
      " 148: 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca',\n",
      " 149: 'dugong, Dugong dugon',\n",
      " 150: 'sea lion',\n",
      " 151: 'Chihuahua',\n",
      " 152: 'Japanese spaniel',\n",
      " 153: 'Maltese dog, Maltese terrier, Maltese',\n",
      " 154: 'Pekinese, Pekingese, Peke',\n",
      " 155: 'Shih-Tzu',\n",
      " 156: 'Blenheim spaniel',\n",
      " 157: 'papillon',\n",
      " 158: 'toy terrier',\n",
      " 159: 'Rhodesian ridgeback',\n",
      " 160: 'Afghan hound, Afghan',\n",
      " 161: 'basset, basset hound',\n",
      " 162: 'beagle',\n",
      " 163: 'bloodhound, sleuthhound',\n",
      " 164: 'bluetick',\n",
      " 165: 'black-and-tan coonhound',\n",
      " 166: 'Walker hound, Walker foxhound',\n",
      " 167: 'English foxhound',\n",
      " 168: 'redbone',\n",
      " 169: 'borzoi, Russian wolfhound',\n",
      " 170: 'Irish wolfhound',\n",
      " 171: 'Italian greyhound',\n",
      " 172: 'whippet',\n",
      " 173: 'Ibizan hound, Ibizan Podenco',\n",
      " 174: 'Norwegian elkhound, elkhound',\n",
      " 175: 'otterhound, otter hound',\n",
      " 176: 'Saluki, gazelle hound',\n",
      " 177: 'Scottish deerhound, deerhound',\n",
      " 178: 'Weimaraner',\n",
      " 179: 'Staffordshire bullterrier, Staffordshire bull terrier',\n",
      " 180: 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier',\n",
      " 181: 'Bedlington terrier',\n",
      " 182: 'Border terrier',\n",
      " 183: 'Kerry blue terrier',\n",
      " 184: 'Irish terrier',\n",
      " 185: 'Norfolk terrier',\n",
      " 186: 'Norwich terrier',\n",
      " 187: 'Yorkshire terrier',\n",
      " 188: 'wire-haired fox terrier',\n",
      " 189: 'Lakeland terrier',\n",
      " 190: 'Sealyham terrier, Sealyham',\n",
      " 191: 'Airedale, Airedale terrier',\n",
      " 192: 'cairn, cairn terrier',\n",
      " 193: 'Australian terrier',\n",
      " 194: 'Dandie Dinmont, Dandie Dinmont terrier',\n",
      " 195: 'Boston bull, Boston terrier',\n",
      " 196: 'miniature schnauzer',\n",
      " 197: 'giant schnauzer',\n",
      " 198: 'standard schnauzer',\n",
      " 199: 'Scotch terrier, Scottish terrier, Scottie',\n",
      " 200: 'Tibetan terrier, chrysanthemum dog',\n",
      " 201: 'silky terrier, Sydney silky',\n",
      " 202: 'soft-coated wheaten terrier',\n",
      " 203: 'West Highland white terrier',\n",
      " 204: 'Lhasa, Lhasa apso',\n",
      " 205: 'flat-coated retriever',\n",
      " 206: 'curly-coated retriever',\n",
      " 207: 'golden retriever',\n",
      " 208: 'Labrador retriever',\n",
      " 209: 'Chesapeake Bay retriever',\n",
      " 210: 'German short-haired pointer',\n",
      " 211: 'vizsla, Hungarian pointer',\n",
      " 212: 'English setter',\n",
      " 213: 'Irish setter, red setter',\n",
      " 214: 'Gordon setter',\n",
      " 215: 'Brittany spaniel',\n",
      " 216: 'clumber, clumber spaniel',\n",
      " 217: 'English springer, English springer spaniel',\n",
      " 218: 'Welsh springer spaniel',\n",
      " 219: 'cocker spaniel, English cocker spaniel, cocker',\n",
      " 220: 'Sussex spaniel',\n",
      " 221: 'Irish water spaniel',\n",
      " 222: 'kuvasz',\n",
      " 223: 'schipperke',\n",
      " 224: 'groenendael',\n",
      " 225: 'malinois',\n",
      " 226: 'briard',\n",
      " 227: 'kelpie',\n",
      " 228: 'komondor',\n",
      " 229: 'Old English sheepdog, bobtail',\n",
      " 230: 'Shetland sheepdog, Shetland sheep dog, Shetland',\n",
      " 231: 'collie',\n",
      " 232: 'Border collie',\n",
      " 233: 'Bouvier des Flandres, Bouviers des Flandres',\n",
      " 234: 'Rottweiler',\n",
      " 235: 'German shepherd, German shepherd dog, German police dog, alsatian',\n",
      " 236: 'Doberman, Doberman pinscher',\n",
      " 237: 'miniature pinscher',\n",
      " 238: 'Greater Swiss Mountain dog',\n",
      " 239: 'Bernese mountain dog',\n",
      " 240: 'Appenzeller',\n",
      " 241: 'EntleBucher',\n",
      " 242: 'boxer',\n",
      " 243: 'bull mastiff',\n",
      " 244: 'Tibetan mastiff',\n",
      " 245: 'French bulldog',\n",
      " 246: 'Great Dane',\n",
      " 247: 'Saint Bernard, St Bernard',\n",
      " 248: 'Eskimo dog, husky',\n",
      " 249: 'malamute, malemute, Alaskan malamute',\n",
      " 250: 'Siberian husky',\n",
      " 251: 'dalmatian, coach dog, carriage dog',\n",
      " 252: 'affenpinscher, monkey pinscher, monkey dog',\n",
      " 253: 'basenji',\n",
      " 254: 'pug, pug-dog',\n",
      " 255: 'Leonberg',\n",
      " 256: 'Newfoundland, Newfoundland dog',\n",
      " 257: 'Great Pyrenees',\n",
      " 258: 'Samoyed, Samoyede',\n",
      " 259: 'Pomeranian',\n",
      " 260: 'chow, chow chow',\n",
      " 261: 'keeshond',\n",
      " 262: 'Brabancon griffon',\n",
      " 263: 'Pembroke, Pembroke Welsh corgi',\n",
      " 264: 'Cardigan, Cardigan Welsh corgi',\n",
      " 265: 'toy poodle',\n",
      " 266: 'miniature poodle',\n",
      " 267: 'standard poodle',\n",
      " 268: 'Mexican hairless',\n",
      " 269: 'timber wolf, grey wolf, gray wolf, Canis lupus',\n",
      " 270: 'white wolf, Arctic wolf, Canis lupus tundrarum',\n",
      " 271: 'red wolf, maned wolf, Canis rufus, Canis niger',\n",
      " 272: 'coyote, prairie wolf, brush wolf, Canis latrans',\n",
      " 273: 'dingo, warrigal, warragal, Canis dingo',\n",
      " 274: 'dhole, Cuon alpinus',\n",
      " 275: 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus',\n",
      " 276: 'hyena, hyaena',\n",
      " 277: 'red fox, Vulpes vulpes',\n",
      " 278: 'kit fox, Vulpes macrotis',\n",
      " 279: 'Arctic fox, white fox, Alopex lagopus',\n",
      " 280: 'grey fox, gray fox, Urocyon cinereoargenteus',\n",
      " 281: 'tabby, tabby cat',\n",
      " 282: 'tiger cat',\n",
      " 283: 'Persian cat',\n",
      " 284: 'Siamese cat, Siamese',\n",
      " 285: 'Egyptian cat',\n",
      " 286: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n",
      " 287: 'lynx, catamount',\n",
      " 288: 'leopard, Panthera pardus',\n",
      " 289: 'snow leopard, ounce, Panthera uncia',\n",
      " 290: 'jaguar, panther, Panthera onca, Felis onca',\n",
      " 291: 'lion, king of beasts, Panthera leo',\n",
      " 292: 'tiger, Panthera tigris',\n",
      " 293: 'cheetah, chetah, Acinonyx jubatus',\n",
      " 294: 'brown bear, bruin, Ursus arctos',\n",
      " 295: 'American black bear, black bear, Ursus americanus, Euarctos americanus',\n",
      " 296: 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus',\n",
      " 297: 'sloth bear, Melursus ursinus, Ursus ursinus',\n",
      " 298: 'mongoose',\n",
      " 299: 'meerkat, mierkat',\n",
      " 300: 'tiger beetle',\n",
      " 301: 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle',\n",
      " 302: 'ground beetle, carabid beetle',\n",
      " 303: 'long-horned beetle, longicorn, longicorn beetle',\n",
      " 304: 'leaf beetle, chrysomelid',\n",
      " 305: 'dung beetle',\n",
      " 306: 'rhinoceros beetle',\n",
      " 307: 'weevil',\n",
      " 308: 'fly',\n",
      " 309: 'bee',\n",
      " 310: 'ant, emmet, pismire',\n",
      " 311: 'grasshopper, hopper',\n",
      " 312: 'cricket',\n",
      " 313: 'walking stick, walkingstick, stick insect',\n",
      " 314: 'cockroach, roach',\n",
      " 315: 'mantis, mantid',\n",
      " 316: 'cicada, cicala',\n",
      " 317: 'leafhopper',\n",
      " 318: 'lacewing, lacewing fly',\n",
      " 319: \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\",\n",
      " 320: 'damselfly',\n",
      " 321: 'admiral',\n",
      " 322: 'ringlet, ringlet butterfly',\n",
      " 323: 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus',\n",
      " 324: 'cabbage butterfly',\n",
      " 325: 'sulphur butterfly, sulfur butterfly',\n",
      " 326: 'lycaenid, lycaenid butterfly',\n",
      " 327: 'starfish, sea star',\n",
      " 328: 'sea urchin',\n",
      " 329: 'sea cucumber, holothurian',\n",
      " 330: 'wood rabbit, cottontail, cottontail rabbit',\n",
      " 331: 'hare',\n",
      " 332: 'Angora, Angora rabbit',\n",
      " 333: 'hamster',\n",
      " 334: 'porcupine, hedgehog',\n",
      " 335: 'fox squirrel, eastern fox squirrel, Sciurus niger',\n",
      " 336: 'marmot',\n",
      " 337: 'beaver',\n",
      " 338: 'guinea pig, Cavia cobaya',\n",
      " 339: 'sorrel',\n",
      " 340: 'zebra',\n",
      " 341: 'hog, pig, grunter, squealer, Sus scrofa',\n",
      " 342: 'wild boar, boar, Sus scrofa',\n",
      " 343: 'warthog',\n",
      " 344: 'hippopotamus, hippo, river horse, Hippopotamus amphibius',\n",
      " 345: 'ox',\n",
      " 346: 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis',\n",
      " 347: 'bison',\n",
      " 348: 'ram, tup',\n",
      " 349: 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis',\n",
      " 350: 'ibex, Capra ibex',\n",
      " 351: 'hartebeest',\n",
      " 352: 'impala, Aepyceros melampus',\n",
      " 353: 'gazelle',\n",
      " 354: 'Arabian camel, dromedary, Camelus dromedarius',\n",
      " 355: 'llama',\n",
      " 356: 'weasel',\n",
      " 357: 'mink',\n",
      " 358: 'polecat, fitch, foulmart, foumart, Mustela putorius',\n",
      " 359: 'black-footed ferret, ferret, Mustela nigripes',\n",
      " 360: 'otter',\n",
      " 361: 'skunk, polecat, wood pussy',\n",
      " 362: 'badger',\n",
      " 363: 'armadillo',\n",
      " 364: 'three-toed sloth, ai, Bradypus tridactylus',\n",
      " 365: 'orangutan, orang, orangutang, Pongo pygmaeus',\n",
      " 366: 'gorilla, Gorilla gorilla',\n",
      " 367: 'chimpanzee, chimp, Pan troglodytes',\n",
      " 368: 'gibbon, Hylobates lar',\n",
      " 369: 'siamang, Hylobates syndactylus, Symphalangus syndactylus',\n",
      " 370: 'guenon, guenon monkey',\n",
      " 371: 'patas, hussar monkey, Erythrocebus patas',\n",
      " 372: 'baboon',\n",
      " 373: 'macaque',\n",
      " 374: 'langur',\n",
      " 375: 'colobus, colobus monkey',\n",
      " 376: 'proboscis monkey, Nasalis larvatus',\n",
      " 377: 'marmoset',\n",
      " 378: 'capuchin, ringtail, Cebus capucinus',\n",
      " 379: 'howler monkey, howler',\n",
      " 380: 'titi, titi monkey',\n",
      " 381: 'spider monkey, Ateles geoffroyi',\n",
      " 382: 'squirrel monkey, Saimiri sciureus',\n",
      " 383: 'Madagascar cat, ring-tailed lemur, Lemur catta',\n",
      " 384: 'indri, indris, Indri indri, Indri brevicaudatus',\n",
      " 385: 'Indian elephant, Elephas maximus',\n",
      " 386: 'African elephant, Loxodonta africana',\n",
      " 387: 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens',\n",
      " 388: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca',\n",
      " 389: 'barracouta, snoek',\n",
      " 390: 'eel',\n",
      " 391: 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch',\n",
      " 392: 'rock beauty, Holocanthus tricolor',\n",
      " 393: 'anemone fish',\n",
      " 394: 'sturgeon',\n",
      " 395: 'gar, garfish, garpike, billfish, Lepisosteus osseus',\n",
      " 396: 'lionfish',\n",
      " 397: 'puffer, pufferfish, blowfish, globefish',\n",
      " 398: 'abacus',\n",
      " 399: 'abaya',\n",
      " 400: \"academic gown, academic robe, judge's robe\",\n",
      " 401: 'accordion, piano accordion, squeeze box',\n",
      " 402: 'acoustic guitar',\n",
      " 403: 'aircraft carrier, carrier, flattop, attack aircraft carrier',\n",
      " 404: 'airliner',\n",
      " 405: 'airship, dirigible',\n",
      " 406: 'altar',\n",
      " 407: 'ambulance',\n",
      " 408: 'amphibian, amphibious vehicle',\n",
      " 409: 'analog clock',\n",
      " 410: 'apiary, bee house',\n",
      " 411: 'apron',\n",
      " 412: 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin',\n",
      " 413: 'assault rifle, assault gun',\n",
      " 414: 'backpack, back pack, knapsack, packsack, rucksack, haversack',\n",
      " 415: 'bakery, bakeshop, bakehouse',\n",
      " 416: 'balance beam, beam',\n",
      " 417: 'balloon',\n",
      " 418: 'ballpoint, ballpoint pen, ballpen, Biro',\n",
      " 419: 'Band Aid',\n",
      " 420: 'banjo',\n",
      " 421: 'bannister, banister, balustrade, balusters, handrail',\n",
      " 422: 'barbell',\n",
      " 423: 'barber chair',\n",
      " 424: 'barbershop',\n",
      " 425: 'barn',\n",
      " 426: 'barometer',\n",
      " 427: 'barrel, cask',\n",
      " 428: 'barrow, garden cart, lawn cart, wheelbarrow',\n",
      " 429: 'baseball',\n",
      " 430: 'basketball',\n",
      " 431: 'bassinet',\n",
      " 432: 'bassoon',\n",
      " 433: 'bathing cap, swimming cap',\n",
      " 434: 'bath towel',\n",
      " 435: 'bathtub, bathing tub, bath, tub',\n",
      " 436: 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon',\n",
      " 437: 'beacon, lighthouse, beacon light, pharos',\n",
      " 438: 'beaker',\n",
      " 439: 'bearskin, busby, shako',\n",
      " 440: 'beer bottle',\n",
      " 441: 'beer glass',\n",
      " 442: 'bell cote, bell cot',\n",
      " 443: 'bib',\n",
      " 444: 'bicycle-built-for-two, tandem bicycle, tandem',\n",
      " 445: 'bikini, two-piece',\n",
      " 446: 'binder, ring-binder',\n",
      " 447: 'binoculars, field glasses, opera glasses',\n",
      " 448: 'birdhouse',\n",
      " 449: 'boathouse',\n",
      " 450: 'bobsled, bobsleigh, bob',\n",
      " 451: 'bolo tie, bolo, bola tie, bola',\n",
      " 452: 'bonnet, poke bonnet',\n",
      " 453: 'bookcase',\n",
      " 454: 'bookshop, bookstore, bookstall',\n",
      " 455: 'bottlecap',\n",
      " 456: 'bow',\n",
      " 457: 'bow tie, bow-tie, bowtie',\n",
      " 458: 'brass, memorial tablet, plaque',\n",
      " 459: 'brassiere, bra, bandeau',\n",
      " 460: 'breakwater, groin, groyne, mole, bulwark, seawall, jetty',\n",
      " 461: 'breastplate, aegis, egis',\n",
      " 462: 'broom',\n",
      " 463: 'bucket, pail',\n",
      " 464: 'buckle',\n",
      " 465: 'bulletproof vest',\n",
      " 466: 'bullet train, bullet',\n",
      " 467: 'butcher shop, meat market',\n",
      " 468: 'cab, hack, taxi, taxicab',\n",
      " 469: 'caldron, cauldron',\n",
      " 470: 'candle, taper, wax light',\n",
      " 471: 'cannon',\n",
      " 472: 'canoe',\n",
      " 473: 'can opener, tin opener',\n",
      " 474: 'cardigan',\n",
      " 475: 'car mirror',\n",
      " 476: 'carousel, carrousel, merry-go-round, roundabout, whirligig',\n",
      " 477: \"carpenter's kit, tool kit\",\n",
      " 478: 'carton',\n",
      " 479: 'car wheel',\n",
      " 480: 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM',\n",
      " 481: 'cassette',\n",
      " 482: 'cassette player',\n",
      " 483: 'castle',\n",
      " 484: 'catamaran',\n",
      " 485: 'CD player',\n",
      " 486: 'cello, violoncello',\n",
      " 487: 'cellular telephone, cellular phone, cellphone, cell, mobile phone',\n",
      " 488: 'chain',\n",
      " 489: 'chainlink fence',\n",
      " 490: 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour',\n",
      " 491: 'chain saw, chainsaw',\n",
      " 492: 'chest',\n",
      " 493: 'chiffonier, commode',\n",
      " 494: 'chime, bell, gong',\n",
      " 495: 'china cabinet, china closet',\n",
      " 496: 'Christmas stocking',\n",
      " 497: 'church, church building',\n",
      " 498: 'cinema, movie theater, movie theatre, movie house, picture palace',\n",
      " 499: 'cleaver, meat cleaver, chopper',\n",
      " 500: 'cliff dwelling',\n",
      " 501: 'cloak',\n",
      " 502: 'clog, geta, patten, sabot',\n",
      " 503: 'cocktail shaker',\n",
      " 504: 'coffee mug',\n",
      " 505: 'coffeepot',\n",
      " 506: 'coil, spiral, volute, whorl, helix',\n",
      " 507: 'combination lock',\n",
      " 508: 'computer keyboard, keypad',\n",
      " 509: 'confectionery, confectionary, candy store',\n",
      " 510: 'container ship, containership, container vessel',\n",
      " 511: 'convertible',\n",
      " 512: 'corkscrew, bottle screw',\n",
      " 513: 'cornet, horn, trumpet, trump',\n",
      " 514: 'cowboy boot',\n",
      " 515: 'cowboy hat, ten-gallon hat',\n",
      " 516: 'cradle',\n",
      " 517: 'crane',\n",
      " 518: 'crash helmet',\n",
      " 519: 'crate',\n",
      " 520: 'crib, cot',\n",
      " 521: 'Crock Pot',\n",
      " 522: 'croquet ball',\n",
      " 523: 'crutch',\n",
      " 524: 'cuirass',\n",
      " 525: 'dam, dike, dyke',\n",
      " 526: 'desk',\n",
      " 527: 'desktop computer',\n",
      " 528: 'dial telephone, dial phone',\n",
      " 529: 'diaper, nappy, napkin',\n",
      " 530: 'digital clock',\n",
      " 531: 'digital watch',\n",
      " 532: 'dining table, board',\n",
      " 533: 'dishrag, dishcloth',\n",
      " 534: 'dishwasher, dish washer, dishwashing machine',\n",
      " 535: 'disk brake, disc brake',\n",
      " 536: 'dock, dockage, docking facility',\n",
      " 537: 'dogsled, dog sled, dog sleigh',\n",
      " 538: 'dome',\n",
      " 539: 'doormat, welcome mat',\n",
      " 540: 'drilling platform, offshore rig',\n",
      " 541: 'drum, membranophone, tympan',\n",
      " 542: 'drumstick',\n",
      " 543: 'dumbbell',\n",
      " 544: 'Dutch oven',\n",
      " 545: 'electric fan, blower',\n",
      " 546: 'electric guitar',\n",
      " 547: 'electric locomotive',\n",
      " 548: 'entertainment center',\n",
      " 549: 'envelope',\n",
      " 550: 'espresso maker',\n",
      " 551: 'face powder',\n",
      " 552: 'feather boa, boa',\n",
      " 553: 'file, file cabinet, filing cabinet',\n",
      " 554: 'fireboat',\n",
      " 555: 'fire engine, fire truck',\n",
      " 556: 'fire screen, fireguard',\n",
      " 557: 'flagpole, flagstaff',\n",
      " 558: 'flute, transverse flute',\n",
      " 559: 'folding chair',\n",
      " 560: 'football helmet',\n",
      " 561: 'forklift',\n",
      " 562: 'fountain',\n",
      " 563: 'fountain pen',\n",
      " 564: 'four-poster',\n",
      " 565: 'freight car',\n",
      " 566: 'French horn, horn',\n",
      " 567: 'frying pan, frypan, skillet',\n",
      " 568: 'fur coat',\n",
      " 569: 'garbage truck, dustcart',\n",
      " 570: 'gasmask, respirator, gas helmet',\n",
      " 571: 'gas pump, gasoline pump, petrol pump, island dispenser',\n",
      " 572: 'goblet',\n",
      " 573: 'go-kart',\n",
      " 574: 'golf ball',\n",
      " 575: 'golfcart, golf cart',\n",
      " 576: 'gondola',\n",
      " 577: 'gong, tam-tam',\n",
      " 578: 'gown',\n",
      " 579: 'grand piano, grand',\n",
      " 580: 'greenhouse, nursery, glasshouse',\n",
      " 581: 'grille, radiator grille',\n",
      " 582: 'grocery store, grocery, food market, market',\n",
      " 583: 'guillotine',\n",
      " 584: 'hair slide',\n",
      " 585: 'hair spray',\n",
      " 586: 'half track',\n",
      " 587: 'hammer',\n",
      " 588: 'hamper',\n",
      " 589: 'hand blower, blow dryer, blow drier, hair dryer, hair drier',\n",
      " 590: 'hand-held computer, hand-held microcomputer',\n",
      " 591: 'handkerchief, hankie, hanky, hankey',\n",
      " 592: 'hard disc, hard disk, fixed disk',\n",
      " 593: 'harmonica, mouth organ, harp, mouth harp',\n",
      " 594: 'harp',\n",
      " 595: 'harvester, reaper',\n",
      " 596: 'hatchet',\n",
      " 597: 'holster',\n",
      " 598: 'home theater, home theatre',\n",
      " 599: 'honeycomb',\n",
      " 600: 'hook, claw',\n",
      " 601: 'hoopskirt, crinoline',\n",
      " 602: 'horizontal bar, high bar',\n",
      " 603: 'horse cart, horse-cart',\n",
      " 604: 'hourglass',\n",
      " 605: 'iPod',\n",
      " 606: 'iron, smoothing iron',\n",
      " 607: \"jack-o'-lantern\",\n",
      " 608: 'jean, blue jean, denim',\n",
      " 609: 'jeep, landrover',\n",
      " 610: 'jersey, T-shirt, tee shirt',\n",
      " 611: 'jigsaw puzzle',\n",
      " 612: 'jinrikisha, ricksha, rickshaw',\n",
      " 613: 'joystick',\n",
      " 614: 'kimono',\n",
      " 615: 'knee pad',\n",
      " 616: 'knot',\n",
      " 617: 'lab coat, laboratory coat',\n",
      " 618: 'ladle',\n",
      " 619: 'lampshade, lamp shade',\n",
      " 620: 'laptop, laptop computer',\n",
      " 621: 'lawn mower, mower',\n",
      " 622: 'lens cap, lens cover',\n",
      " 623: 'letter opener, paper knife, paperknife',\n",
      " 624: 'library',\n",
      " 625: 'lifeboat',\n",
      " 626: 'lighter, light, igniter, ignitor',\n",
      " 627: 'limousine, limo',\n",
      " 628: 'liner, ocean liner',\n",
      " 629: 'lipstick, lip rouge',\n",
      " 630: 'Loafer',\n",
      " 631: 'lotion',\n",
      " 632: 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system',\n",
      " 633: \"loupe, jeweler's loupe\",\n",
      " 634: 'lumbermill, sawmill',\n",
      " 635: 'magnetic compass',\n",
      " 636: 'mailbag, postbag',\n",
      " 637: 'mailbox, letter box',\n",
      " 638: 'maillot',\n",
      " 639: 'maillot, tank suit',\n",
      " 640: 'manhole cover',\n",
      " 641: 'maraca',\n",
      " 642: 'marimba, xylophone',\n",
      " 643: 'mask',\n",
      " 644: 'matchstick',\n",
      " 645: 'maypole',\n",
      " 646: 'maze, labyrinth',\n",
      " 647: 'measuring cup',\n",
      " 648: 'medicine chest, medicine cabinet',\n",
      " 649: 'megalith, megalithic structure',\n",
      " 650: 'microphone, mike',\n",
      " 651: 'microwave, microwave oven',\n",
      " 652: 'military uniform',\n",
      " 653: 'milk can',\n",
      " 654: 'minibus',\n",
      " 655: 'miniskirt, mini',\n",
      " 656: 'minivan',\n",
      " 657: 'missile',\n",
      " 658: 'mitten',\n",
      " 659: 'mixing bowl',\n",
      " 660: 'mobile home, manufactured home',\n",
      " 661: 'Model T',\n",
      " 662: 'modem',\n",
      " 663: 'monastery',\n",
      " 664: 'monitor',\n",
      " 665: 'moped',\n",
      " 666: 'mortar',\n",
      " 667: 'mortarboard',\n",
      " 668: 'mosque',\n",
      " 669: 'mosquito net',\n",
      " 670: 'motor scooter, scooter',\n",
      " 671: 'mountain bike, all-terrain bike, off-roader',\n",
      " 672: 'mountain tent',\n",
      " 673: 'mouse, computer mouse',\n",
      " 674: 'mousetrap',\n",
      " 675: 'moving van',\n",
      " 676: 'muzzle',\n",
      " 677: 'nail',\n",
      " 678: 'neck brace',\n",
      " 679: 'necklace',\n",
      " 680: 'nipple',\n",
      " 681: 'notebook, notebook computer',\n",
      " 682: 'obelisk',\n",
      " 683: 'oboe, hautboy, hautbois',\n",
      " 684: 'ocarina, sweet potato',\n",
      " 685: 'odometer, hodometer, mileometer, milometer',\n",
      " 686: 'oil filter',\n",
      " 687: 'organ, pipe organ',\n",
      " 688: 'oscilloscope, scope, cathode-ray oscilloscope, CRO',\n",
      " 689: 'overskirt',\n",
      " 690: 'oxcart',\n",
      " 691: 'oxygen mask',\n",
      " 692: 'packet',\n",
      " 693: 'paddle, boat paddle',\n",
      " 694: 'paddlewheel, paddle wheel',\n",
      " 695: 'padlock',\n",
      " 696: 'paintbrush',\n",
      " 697: \"pajama, pyjama, pj's, jammies\",\n",
      " 698: 'palace',\n",
      " 699: 'panpipe, pandean pipe, syrinx',\n",
      " 700: 'paper towel',\n",
      " 701: 'parachute, chute',\n",
      " 702: 'parallel bars, bars',\n",
      " 703: 'park bench',\n",
      " 704: 'parking meter',\n",
      " 705: 'passenger car, coach, carriage',\n",
      " 706: 'patio, terrace',\n",
      " 707: 'pay-phone, pay-station',\n",
      " 708: 'pedestal, plinth, footstall',\n",
      " 709: 'pencil box, pencil case',\n",
      " 710: 'pencil sharpener',\n",
      " 711: 'perfume, essence',\n",
      " 712: 'Petri dish',\n",
      " 713: 'photocopier',\n",
      " 714: 'pick, plectrum, plectron',\n",
      " 715: 'pickelhaube',\n",
      " 716: 'picket fence, paling',\n",
      " 717: 'pickup, pickup truck',\n",
      " 718: 'pier',\n",
      " 719: 'piggy bank, penny bank',\n",
      " 720: 'pill bottle',\n",
      " 721: 'pillow',\n",
      " 722: 'ping-pong ball',\n",
      " 723: 'pinwheel',\n",
      " 724: 'pirate, pirate ship',\n",
      " 725: 'pitcher, ewer',\n",
      " 726: \"plane, carpenter's plane, woodworking plane\",\n",
      " 727: 'planetarium',\n",
      " 728: 'plastic bag',\n",
      " 729: 'plate rack',\n",
      " 730: 'plow, plough',\n",
      " 731: \"plunger, plumber's helper\",\n",
      " 732: 'Polaroid camera, Polaroid Land camera',\n",
      " 733: 'pole',\n",
      " 734: 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria',\n",
      " 735: 'poncho',\n",
      " 736: 'pool table, billiard table, snooker table',\n",
      " 737: 'pop bottle, soda bottle',\n",
      " 738: 'pot, flowerpot',\n",
      " 739: \"potter's wheel\",\n",
      " 740: 'power drill',\n",
      " 741: 'prayer rug, prayer mat',\n",
      " 742: 'printer',\n",
      " 743: 'prison, prison house',\n",
      " 744: 'projectile, missile',\n",
      " 745: 'projector',\n",
      " 746: 'puck, hockey puck',\n",
      " 747: 'punching bag, punch bag, punching ball, punchball',\n",
      " 748: 'purse',\n",
      " 749: 'quill, quill pen',\n",
      " 750: 'quilt, comforter, comfort, puff',\n",
      " 751: 'racer, race car, racing car',\n",
      " 752: 'racket, racquet',\n",
      " 753: 'radiator',\n",
      " 754: 'radio, wireless',\n",
      " 755: 'radio telescope, radio reflector',\n",
      " 756: 'rain barrel',\n",
      " 757: 'recreational vehicle, RV, R.V.',\n",
      " 758: 'reel',\n",
      " 759: 'reflex camera',\n",
      " 760: 'refrigerator, icebox',\n",
      " 761: 'remote control, remote',\n",
      " 762: 'restaurant, eating house, eating place, eatery',\n",
      " 763: 'revolver, six-gun, six-shooter',\n",
      " 764: 'rifle',\n",
      " 765: 'rocking chair, rocker',\n",
      " 766: 'rotisserie',\n",
      " 767: 'rubber eraser, rubber, pencil eraser',\n",
      " 768: 'rugby ball',\n",
      " 769: 'rule, ruler',\n",
      " 770: 'running shoe',\n",
      " 771: 'safe',\n",
      " 772: 'safety pin',\n",
      " 773: 'saltshaker, salt shaker',\n",
      " 774: 'sandal',\n",
      " 775: 'sarong',\n",
      " 776: 'sax, saxophone',\n",
      " 777: 'scabbard',\n",
      " 778: 'scale, weighing machine',\n",
      " 779: 'school bus',\n",
      " 780: 'schooner',\n",
      " 781: 'scoreboard',\n",
      " 782: 'screen, CRT screen',\n",
      " 783: 'screw',\n",
      " 784: 'screwdriver',\n",
      " 785: 'seat belt, seatbelt',\n",
      " 786: 'sewing machine',\n",
      " 787: 'shield, buckler',\n",
      " 788: 'shoe shop, shoe-shop, shoe store',\n",
      " 789: 'shoji',\n",
      " 790: 'shopping basket',\n",
      " 791: 'shopping cart',\n",
      " 792: 'shovel',\n",
      " 793: 'shower cap',\n",
      " 794: 'shower curtain',\n",
      " 795: 'ski',\n",
      " 796: 'ski mask',\n",
      " 797: 'sleeping bag',\n",
      " 798: 'slide rule, slipstick',\n",
      " 799: 'sliding door',\n",
      " 800: 'slot, one-armed bandit',\n",
      " 801: 'snorkel',\n",
      " 802: 'snowmobile',\n",
      " 803: 'snowplow, snowplough',\n",
      " 804: 'soap dispenser',\n",
      " 805: 'soccer ball',\n",
      " 806: 'sock',\n",
      " 807: 'solar dish, solar collector, solar furnace',\n",
      " 808: 'sombrero',\n",
      " 809: 'soup bowl',\n",
      " 810: 'space bar',\n",
      " 811: 'space heater',\n",
      " 812: 'space shuttle',\n",
      " 813: 'spatula',\n",
      " 814: 'speedboat',\n",
      " 815: \"spider web, spider's web\",\n",
      " 816: 'spindle',\n",
      " 817: 'sports car, sport car',\n",
      " 818: 'spotlight, spot',\n",
      " 819: 'stage',\n",
      " 820: 'steam locomotive',\n",
      " 821: 'steel arch bridge',\n",
      " 822: 'steel drum',\n",
      " 823: 'stethoscope',\n",
      " 824: 'stole',\n",
      " 825: 'stone wall',\n",
      " 826: 'stopwatch, stop watch',\n",
      " 827: 'stove',\n",
      " 828: 'strainer',\n",
      " 829: 'streetcar, tram, tramcar, trolley, trolley car',\n",
      " 830: 'stretcher',\n",
      " 831: 'studio couch, day bed',\n",
      " 832: 'stupa, tope',\n",
      " 833: 'submarine, pigboat, sub, U-boat',\n",
      " 834: 'suit, suit of clothes',\n",
      " 835: 'sundial',\n",
      " 836: 'sunglass',\n",
      " 837: 'sunglasses, dark glasses, shades',\n",
      " 838: 'sunscreen, sunblock, sun blocker',\n",
      " 839: 'suspension bridge',\n",
      " 840: 'swab, swob, mop',\n",
      " 841: 'sweatshirt',\n",
      " 842: 'swimming trunks, bathing trunks',\n",
      " 843: 'swing',\n",
      " 844: 'switch, electric switch, electrical switch',\n",
      " 845: 'syringe',\n",
      " 846: 'table lamp',\n",
      " 847: 'tank, army tank, armored combat vehicle, armoured combat vehicle',\n",
      " 848: 'tape player',\n",
      " 849: 'teapot',\n",
      " 850: 'teddy, teddy bear',\n",
      " 851: 'television, television system',\n",
      " 852: 'tennis ball',\n",
      " 853: 'thatch, thatched roof',\n",
      " 854: 'theater curtain, theatre curtain',\n",
      " 855: 'thimble',\n",
      " 856: 'thresher, thrasher, threshing machine',\n",
      " 857: 'throne',\n",
      " 858: 'tile roof',\n",
      " 859: 'toaster',\n",
      " 860: 'tobacco shop, tobacconist shop, tobacconist',\n",
      " 861: 'toilet seat',\n",
      " 862: 'torch',\n",
      " 863: 'totem pole',\n",
      " 864: 'tow truck, tow car, wrecker',\n",
      " 865: 'toyshop',\n",
      " 866: 'tractor',\n",
      " 867: 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi',\n",
      " 868: 'tray',\n",
      " 869: 'trench coat',\n",
      " 870: 'tricycle, trike, velocipede',\n",
      " 871: 'trimaran',\n",
      " 872: 'tripod',\n",
      " 873: 'triumphal arch',\n",
      " 874: 'trolleybus, trolley coach, trackless trolley',\n",
      " 875: 'trombone',\n",
      " 876: 'tub, vat',\n",
      " 877: 'turnstile',\n",
      " 878: 'typewriter keyboard',\n",
      " 879: 'umbrella',\n",
      " 880: 'unicycle, monocycle',\n",
      " 881: 'upright, upright piano',\n",
      " 882: 'vacuum, vacuum cleaner',\n",
      " 883: 'vase',\n",
      " 884: 'vault',\n",
      " 885: 'velvet',\n",
      " 886: 'vending machine',\n",
      " 887: 'vestment',\n",
      " 888: 'viaduct',\n",
      " 889: 'violin, fiddle',\n",
      " 890: 'volleyball',\n",
      " 891: 'waffle iron',\n",
      " 892: 'wall clock',\n",
      " 893: 'wallet, billfold, notecase, pocketbook',\n",
      " 894: 'wardrobe, closet, press',\n",
      " 895: 'warplane, military plane',\n",
      " 896: 'washbasin, handbasin, washbowl, lavabo, wash-hand basin',\n",
      " 897: 'washer, automatic washer, washing machine',\n",
      " 898: 'water bottle',\n",
      " 899: 'water jug',\n",
      " 900: 'water tower',\n",
      " 901: 'whiskey jug',\n",
      " 902: 'whistle',\n",
      " 903: 'wig',\n",
      " 904: 'window screen',\n",
      " 905: 'window shade',\n",
      " 906: 'Windsor tie',\n",
      " 907: 'wine bottle',\n",
      " 908: 'wing',\n",
      " 909: 'wok',\n",
      " 910: 'wooden spoon',\n",
      " 911: 'wool, woolen, woollen',\n",
      " 912: 'worm fence, snake fence, snake-rail fence, Virginia fence',\n",
      " 913: 'wreck',\n",
      " 914: 'yawl',\n",
      " 915: 'yurt',\n",
      " 916: 'web site, website, internet site, site',\n",
      " 917: 'comic book',\n",
      " 918: 'crossword puzzle, crossword',\n",
      " 919: 'street sign',\n",
      " 920: 'traffic light, traffic signal, stoplight',\n",
      " 921: 'book jacket, dust cover, dust jacket, dust wrapper',\n",
      " 922: 'menu',\n",
      " 923: 'plate',\n",
      " 924: 'guacamole',\n",
      " 925: 'consomme',\n",
      " 926: 'hot pot, hotpot',\n",
      " 927: 'trifle',\n",
      " 928: 'ice cream, icecream',\n",
      " 929: 'ice lolly, lolly, lollipop, popsicle',\n",
      " 930: 'French loaf',\n",
      " 931: 'bagel, beigel',\n",
      " 932: 'pretzel',\n",
      " 933: 'cheeseburger',\n",
      " 934: 'hotdog, hot dog, red hot',\n",
      " 935: 'mashed potato',\n",
      " 936: 'head cabbage',\n",
      " 937: 'broccoli',\n",
      " 938: 'cauliflower',\n",
      " 939: 'zucchini, courgette',\n",
      " 940: 'spaghetti squash',\n",
      " 941: 'acorn squash',\n",
      " 942: 'butternut squash',\n",
      " 943: 'cucumber, cuke',\n",
      " 944: 'artichoke, globe artichoke',\n",
      " 945: 'bell pepper',\n",
      " 946: 'cardoon',\n",
      " 947: 'mushroom',\n",
      " 948: 'Granny Smith',\n",
      " 949: 'strawberry',\n",
      " 950: 'orange',\n",
      " 951: 'lemon',\n",
      " 952: 'fig',\n",
      " 953: 'pineapple, ananas',\n",
      " 954: 'banana',\n",
      " 955: 'jackfruit, jak, jack',\n",
      " 956: 'custard apple',\n",
      " 957: 'pomegranate',\n",
      " 958: 'hay',\n",
      " 959: 'carbonara',\n",
      " 960: 'chocolate sauce, chocolate syrup',\n",
      " 961: 'dough',\n",
      " 962: 'meat loaf, meatloaf',\n",
      " 963: 'pizza, pizza pie',\n",
      " 964: 'potpie',\n",
      " 965: 'burrito',\n",
      " 966: 'red wine',\n",
      " 967: 'espresso',\n",
      " 968: 'cup',\n",
      " 969: 'eggnog',\n",
      " 970: 'alp',\n",
      " 971: 'bubble',\n",
      " 972: 'cliff, drop, drop-off',\n",
      " 973: 'coral reef',\n",
      " 974: 'geyser',\n",
      " 975: 'lakeside, lakeshore',\n",
      " 976: 'promontory, headland, head, foreland',\n",
      " 977: 'sandbar, sand bar',\n",
      " 978: 'seashore, coast, seacoast, sea-coast',\n",
      " 979: 'valley, vale',\n",
      " 980: 'volcano',\n",
      " 981: 'ballplayer, baseball player',\n",
      " 982: 'groom, bridegroom',\n",
      " 983: 'scuba diver',\n",
      " 984: 'rapeseed',\n",
      " 985: 'daisy',\n",
      " 986: \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\",\n",
      " 987: 'corn',\n",
      " 988: 'acorn',\n",
      " 989: 'hip, rose hip, rosehip',\n",
      " 990: 'buckeye, horse chestnut, conker',\n",
      " 991: 'coral fungus',\n",
      " 992: 'agaric',\n",
      " 993: 'gyromitra',\n",
      " 994: 'stinkhorn, carrion fungus',\n",
      " 995: 'earthstar',\n",
      " 996: 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa',\n",
      " 997: 'bolete',\n",
      " 998: 'ear, spike, capitulum',\n",
      " 999: 'toilet tissue, toilet paper, bathroom tissue'}\n",
      "57/4:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 380\n",
      "\n",
      "def prep_image(image):\n",
      "    res = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "    res = res.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "    return res\n",
      "\n",
      "def predict(image):\n",
      "    cropped_image = center_crop(image)\n",
      "    prediction = (model.predict(prep_image(image)), model.predict(prep_image(cropped_image)))\n",
      "    if np.max(prediction[0]>=prediction[1]):\n",
      "        return np.argmax(prediction[0], axis=1)[0]\n",
      "    else:\n",
      "        return np.argmax(prediction[1], axis=1)[1]\n",
      "    \n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "plt.imshow(image)\n",
      "\n",
      "print(class_dict[predict(image)])\n",
      "58/1:\n",
      "from tensorflow.keras.applications import EfficientNetB4\n",
      "from tensorflow.image import resize\n",
      "model = EfficientNetB4(weights='imagenet')\n",
      "\n",
      "def center_crop(img, new_width=None, new_height=None):        \n",
      "\n",
      "    width = img.shape[1]\n",
      "    height = img.shape[0]\n",
      "\n",
      "    if new_width is None:\n",
      "        new_width = min(width, height)\n",
      "\n",
      "    if new_height is None:\n",
      "        new_height = min(width, height)\n",
      "\n",
      "    left = int(np.ceil((width - new_width) / 2))\n",
      "    right = width - int(np.floor((width - new_width) / 2))\n",
      "\n",
      "    top = int(np.ceil((height - new_height) / 2))\n",
      "    bottom = height - int(np.floor((height - new_height) / 2))\n",
      "\n",
      "    if len(img.shape) == 2:\n",
      "        center_cropped_img = img[top:bottom, left:right]\n",
      "    else:\n",
      "        center_cropped_img = img[top:bottom, left:right, ...]\n",
      "\n",
      "    return center_cropped_img\n",
      "\n",
      "class_dict = {0: 'tench, Tinca tinca',\n",
      " 1: 'goldfish, Carassius auratus',\n",
      " 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n",
      " 3: 'tiger shark, Galeocerdo cuvieri',\n",
      " 4: 'hammerhead, hammerhead shark',\n",
      " 5: 'electric ray, crampfish, numbfish, torpedo',\n",
      " 6: 'stingray',\n",
      " 7: 'cock',\n",
      " 8: 'hen',\n",
      " 9: 'ostrich, Struthio camelus',\n",
      " 10: 'brambling, Fringilla montifringilla',\n",
      " 11: 'goldfinch, Carduelis carduelis',\n",
      " 12: 'house finch, linnet, Carpodacus mexicanus',\n",
      " 13: 'junco, snowbird',\n",
      " 14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea',\n",
      " 15: 'robin, American robin, Turdus migratorius',\n",
      " 16: 'bulbul',\n",
      " 17: 'jay',\n",
      " 18: 'magpie',\n",
      " 19: 'chickadee',\n",
      " 20: 'water ouzel, dipper',\n",
      " 21: 'kite',\n",
      " 22: 'bald eagle, American eagle, Haliaeetus leucocephalus',\n",
      " 23: 'vulture',\n",
      " 24: 'great grey owl, great gray owl, Strix nebulosa',\n",
      " 25: 'European fire salamander, Salamandra salamandra',\n",
      " 26: 'common newt, Triturus vulgaris',\n",
      " 27: 'eft',\n",
      " 28: 'spotted salamander, Ambystoma maculatum',\n",
      " 29: 'axolotl, mud puppy, Ambystoma mexicanum',\n",
      " 30: 'bullfrog, Rana catesbeiana',\n",
      " 31: 'tree frog, tree-frog',\n",
      " 32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui',\n",
      " 33: 'loggerhead, loggerhead turtle, Caretta caretta',\n",
      " 34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea',\n",
      " 35: 'mud turtle',\n",
      " 36: 'terrapin',\n",
      " 37: 'box turtle, box tortoise',\n",
      " 38: 'banded gecko',\n",
      " 39: 'common iguana, iguana, Iguana iguana',\n",
      " 40: 'American chameleon, anole, Anolis carolinensis',\n",
      " 41: 'whiptail, whiptail lizard',\n",
      " 42: 'agama',\n",
      " 43: 'frilled lizard, Chlamydosaurus kingi',\n",
      " 44: 'alligator lizard',\n",
      " 45: 'Gila monster, Heloderma suspectum',\n",
      " 46: 'green lizard, Lacerta viridis',\n",
      " 47: 'African chameleon, Chamaeleo chamaeleon',\n",
      " 48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis',\n",
      " 49: 'African crocodile, Nile crocodile, Crocodylus niloticus',\n",
      " 50: 'American alligator, Alligator mississipiensis',\n",
      " 51: 'triceratops',\n",
      " 52: 'thunder snake, worm snake, Carphophis amoenus',\n",
      " 53: 'ringneck snake, ring-necked snake, ring snake',\n",
      " 54: 'hognose snake, puff adder, sand viper',\n",
      " 55: 'green snake, grass snake',\n",
      " 56: 'king snake, kingsnake',\n",
      " 57: 'garter snake, grass snake',\n",
      " 58: 'water snake',\n",
      " 59: 'vine snake',\n",
      " 60: 'night snake, Hypsiglena torquata',\n",
      " 61: 'boa constrictor, Constrictor constrictor',\n",
      " 62: 'rock python, rock snake, Python sebae',\n",
      " 63: 'Indian cobra, Naja naja',\n",
      " 64: 'green mamba',\n",
      " 65: 'sea snake',\n",
      " 66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus',\n",
      " 67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus',\n",
      " 68: 'sidewinder, horned rattlesnake, Crotalus cerastes',\n",
      " 69: 'trilobite',\n",
      " 70: 'harvestman, daddy longlegs, Phalangium opilio',\n",
      " 71: 'scorpion',\n",
      " 72: 'black and gold garden spider, Argiope aurantia',\n",
      " 73: 'barn spider, Araneus cavaticus',\n",
      " 74: 'garden spider, Aranea diademata',\n",
      " 75: 'black widow, Latrodectus mactans',\n",
      " 76: 'tarantula',\n",
      " 77: 'wolf spider, hunting spider',\n",
      " 78: 'tick',\n",
      " 79: 'centipede',\n",
      " 80: 'black grouse',\n",
      " 81: 'ptarmigan',\n",
      " 82: 'ruffed grouse, partridge, Bonasa umbellus',\n",
      " 83: 'prairie chicken, prairie grouse, prairie fowl',\n",
      " 84: 'peacock',\n",
      " 85: 'quail',\n",
      " 86: 'partridge',\n",
      " 87: 'African grey, African gray, Psittacus erithacus',\n",
      " 88: 'macaw',\n",
      " 89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n",
      " 90: 'lorikeet',\n",
      " 91: 'coucal',\n",
      " 92: 'bee eater',\n",
      " 93: 'hornbill',\n",
      " 94: 'hummingbird',\n",
      " 95: 'jacamar',\n",
      " 96: 'toucan',\n",
      " 97: 'drake',\n",
      " 98: 'red-breasted merganser, Mergus serrator',\n",
      " 99: 'goose',\n",
      " 100: 'black swan, Cygnus atratus',\n",
      " 101: 'tusker',\n",
      " 102: 'echidna, spiny anteater, anteater',\n",
      " 103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n",
      " 104: 'wallaby, brush kangaroo',\n",
      " 105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n",
      " 106: 'wombat',\n",
      " 107: 'jellyfish',\n",
      " 108: 'sea anemone, anemone',\n",
      " 109: 'brain coral',\n",
      " 110: 'flatworm, platyhelminth',\n",
      " 111: 'nematode, nematode worm, roundworm',\n",
      " 112: 'conch',\n",
      " 113: 'snail',\n",
      " 114: 'slug',\n",
      " 115: 'sea slug, nudibranch',\n",
      " 116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore',\n",
      " 117: 'chambered nautilus, pearly nautilus, nautilus',\n",
      " 118: 'Dungeness crab, Cancer magister',\n",
      " 119: 'rock crab, Cancer irroratus',\n",
      " 120: 'fiddler crab',\n",
      " 121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica',\n",
      " 122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus',\n",
      " 123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish',\n",
      " 124: 'crayfish, crawfish, crawdad, crawdaddy',\n",
      " 125: 'hermit crab',\n",
      " 126: 'isopod',\n",
      " 127: 'white stork, Ciconia ciconia',\n",
      " 128: 'black stork, Ciconia nigra',\n",
      " 129: 'spoonbill',\n",
      " 130: 'flamingo',\n",
      " 131: 'little blue heron, Egretta caerulea',\n",
      " 132: 'American egret, great white heron, Egretta albus',\n",
      " 133: 'bittern',\n",
      " 134: 'crane',\n",
      " 135: 'limpkin, Aramus pictus',\n",
      " 136: 'European gallinule, Porphyrio porphyrio',\n",
      " 137: 'American coot, marsh hen, mud hen, water hen, Fulica americana',\n",
      " 138: 'bustard',\n",
      " 139: 'ruddy turnstone, Arenaria interpres',\n",
      " 140: 'red-backed sandpiper, dunlin, Erolia alpina',\n",
      " 141: 'redshank, Tringa totanus',\n",
      " 142: 'dowitcher',\n",
      " 143: 'oystercatcher, oyster catcher',\n",
      " 144: 'pelican',\n",
      " 145: 'king penguin, Aptenodytes patagonica',\n",
      " 146: 'albatross, mollymawk',\n",
      " 147: 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus',\n",
      " 148: 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca',\n",
      " 149: 'dugong, Dugong dugon',\n",
      " 150: 'sea lion',\n",
      " 151: 'Chihuahua',\n",
      " 152: 'Japanese spaniel',\n",
      " 153: 'Maltese dog, Maltese terrier, Maltese',\n",
      " 154: 'Pekinese, Pekingese, Peke',\n",
      " 155: 'Shih-Tzu',\n",
      " 156: 'Blenheim spaniel',\n",
      " 157: 'papillon',\n",
      " 158: 'toy terrier',\n",
      " 159: 'Rhodesian ridgeback',\n",
      " 160: 'Afghan hound, Afghan',\n",
      " 161: 'basset, basset hound',\n",
      " 162: 'beagle',\n",
      " 163: 'bloodhound, sleuthhound',\n",
      " 164: 'bluetick',\n",
      " 165: 'black-and-tan coonhound',\n",
      " 166: 'Walker hound, Walker foxhound',\n",
      " 167: 'English foxhound',\n",
      " 168: 'redbone',\n",
      " 169: 'borzoi, Russian wolfhound',\n",
      " 170: 'Irish wolfhound',\n",
      " 171: 'Italian greyhound',\n",
      " 172: 'whippet',\n",
      " 173: 'Ibizan hound, Ibizan Podenco',\n",
      " 174: 'Norwegian elkhound, elkhound',\n",
      " 175: 'otterhound, otter hound',\n",
      " 176: 'Saluki, gazelle hound',\n",
      " 177: 'Scottish deerhound, deerhound',\n",
      " 178: 'Weimaraner',\n",
      " 179: 'Staffordshire bullterrier, Staffordshire bull terrier',\n",
      " 180: 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier',\n",
      " 181: 'Bedlington terrier',\n",
      " 182: 'Border terrier',\n",
      " 183: 'Kerry blue terrier',\n",
      " 184: 'Irish terrier',\n",
      " 185: 'Norfolk terrier',\n",
      " 186: 'Norwich terrier',\n",
      " 187: 'Yorkshire terrier',\n",
      " 188: 'wire-haired fox terrier',\n",
      " 189: 'Lakeland terrier',\n",
      " 190: 'Sealyham terrier, Sealyham',\n",
      " 191: 'Airedale, Airedale terrier',\n",
      " 192: 'cairn, cairn terrier',\n",
      " 193: 'Australian terrier',\n",
      " 194: 'Dandie Dinmont, Dandie Dinmont terrier',\n",
      " 195: 'Boston bull, Boston terrier',\n",
      " 196: 'miniature schnauzer',\n",
      " 197: 'giant schnauzer',\n",
      " 198: 'standard schnauzer',\n",
      " 199: 'Scotch terrier, Scottish terrier, Scottie',\n",
      " 200: 'Tibetan terrier, chrysanthemum dog',\n",
      " 201: 'silky terrier, Sydney silky',\n",
      " 202: 'soft-coated wheaten terrier',\n",
      " 203: 'West Highland white terrier',\n",
      " 204: 'Lhasa, Lhasa apso',\n",
      " 205: 'flat-coated retriever',\n",
      " 206: 'curly-coated retriever',\n",
      " 207: 'golden retriever',\n",
      " 208: 'Labrador retriever',\n",
      " 209: 'Chesapeake Bay retriever',\n",
      " 210: 'German short-haired pointer',\n",
      " 211: 'vizsla, Hungarian pointer',\n",
      " 212: 'English setter',\n",
      " 213: 'Irish setter, red setter',\n",
      " 214: 'Gordon setter',\n",
      " 215: 'Brittany spaniel',\n",
      " 216: 'clumber, clumber spaniel',\n",
      " 217: 'English springer, English springer spaniel',\n",
      " 218: 'Welsh springer spaniel',\n",
      " 219: 'cocker spaniel, English cocker spaniel, cocker',\n",
      " 220: 'Sussex spaniel',\n",
      " 221: 'Irish water spaniel',\n",
      " 222: 'kuvasz',\n",
      " 223: 'schipperke',\n",
      " 224: 'groenendael',\n",
      " 225: 'malinois',\n",
      " 226: 'briard',\n",
      " 227: 'kelpie',\n",
      " 228: 'komondor',\n",
      " 229: 'Old English sheepdog, bobtail',\n",
      " 230: 'Shetland sheepdog, Shetland sheep dog, Shetland',\n",
      " 231: 'collie',\n",
      " 232: 'Border collie',\n",
      " 233: 'Bouvier des Flandres, Bouviers des Flandres',\n",
      " 234: 'Rottweiler',\n",
      " 235: 'German shepherd, German shepherd dog, German police dog, alsatian',\n",
      " 236: 'Doberman, Doberman pinscher',\n",
      " 237: 'miniature pinscher',\n",
      " 238: 'Greater Swiss Mountain dog',\n",
      " 239: 'Bernese mountain dog',\n",
      " 240: 'Appenzeller',\n",
      " 241: 'EntleBucher',\n",
      " 242: 'boxer',\n",
      " 243: 'bull mastiff',\n",
      " 244: 'Tibetan mastiff',\n",
      " 245: 'French bulldog',\n",
      " 246: 'Great Dane',\n",
      " 247: 'Saint Bernard, St Bernard',\n",
      " 248: 'Eskimo dog, husky',\n",
      " 249: 'malamute, malemute, Alaskan malamute',\n",
      " 250: 'Siberian husky',\n",
      " 251: 'dalmatian, coach dog, carriage dog',\n",
      " 252: 'affenpinscher, monkey pinscher, monkey dog',\n",
      " 253: 'basenji',\n",
      " 254: 'pug, pug-dog',\n",
      " 255: 'Leonberg',\n",
      " 256: 'Newfoundland, Newfoundland dog',\n",
      " 257: 'Great Pyrenees',\n",
      " 258: 'Samoyed, Samoyede',\n",
      " 259: 'Pomeranian',\n",
      " 260: 'chow, chow chow',\n",
      " 261: 'keeshond',\n",
      " 262: 'Brabancon griffon',\n",
      " 263: 'Pembroke, Pembroke Welsh corgi',\n",
      " 264: 'Cardigan, Cardigan Welsh corgi',\n",
      " 265: 'toy poodle',\n",
      " 266: 'miniature poodle',\n",
      " 267: 'standard poodle',\n",
      " 268: 'Mexican hairless',\n",
      " 269: 'timber wolf, grey wolf, gray wolf, Canis lupus',\n",
      " 270: 'white wolf, Arctic wolf, Canis lupus tundrarum',\n",
      " 271: 'red wolf, maned wolf, Canis rufus, Canis niger',\n",
      " 272: 'coyote, prairie wolf, brush wolf, Canis latrans',\n",
      " 273: 'dingo, warrigal, warragal, Canis dingo',\n",
      " 274: 'dhole, Cuon alpinus',\n",
      " 275: 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus',\n",
      " 276: 'hyena, hyaena',\n",
      " 277: 'red fox, Vulpes vulpes',\n",
      " 278: 'kit fox, Vulpes macrotis',\n",
      " 279: 'Arctic fox, white fox, Alopex lagopus',\n",
      " 280: 'grey fox, gray fox, Urocyon cinereoargenteus',\n",
      " 281: 'tabby, tabby cat',\n",
      " 282: 'tiger cat',\n",
      " 283: 'Persian cat',\n",
      " 284: 'Siamese cat, Siamese',\n",
      " 285: 'Egyptian cat',\n",
      " 286: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n",
      " 287: 'lynx, catamount',\n",
      " 288: 'leopard, Panthera pardus',\n",
      " 289: 'snow leopard, ounce, Panthera uncia',\n",
      " 290: 'jaguar, panther, Panthera onca, Felis onca',\n",
      " 291: 'lion, king of beasts, Panthera leo',\n",
      " 292: 'tiger, Panthera tigris',\n",
      " 293: 'cheetah, chetah, Acinonyx jubatus',\n",
      " 294: 'brown bear, bruin, Ursus arctos',\n",
      " 295: 'American black bear, black bear, Ursus americanus, Euarctos americanus',\n",
      " 296: 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus',\n",
      " 297: 'sloth bear, Melursus ursinus, Ursus ursinus',\n",
      " 298: 'mongoose',\n",
      " 299: 'meerkat, mierkat',\n",
      " 300: 'tiger beetle',\n",
      " 301: 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle',\n",
      " 302: 'ground beetle, carabid beetle',\n",
      " 303: 'long-horned beetle, longicorn, longicorn beetle',\n",
      " 304: 'leaf beetle, chrysomelid',\n",
      " 305: 'dung beetle',\n",
      " 306: 'rhinoceros beetle',\n",
      " 307: 'weevil',\n",
      " 308: 'fly',\n",
      " 309: 'bee',\n",
      " 310: 'ant, emmet, pismire',\n",
      " 311: 'grasshopper, hopper',\n",
      " 312: 'cricket',\n",
      " 313: 'walking stick, walkingstick, stick insect',\n",
      " 314: 'cockroach, roach',\n",
      " 315: 'mantis, mantid',\n",
      " 316: 'cicada, cicala',\n",
      " 317: 'leafhopper',\n",
      " 318: 'lacewing, lacewing fly',\n",
      " 319: \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\",\n",
      " 320: 'damselfly',\n",
      " 321: 'admiral',\n",
      " 322: 'ringlet, ringlet butterfly',\n",
      " 323: 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus',\n",
      " 324: 'cabbage butterfly',\n",
      " 325: 'sulphur butterfly, sulfur butterfly',\n",
      " 326: 'lycaenid, lycaenid butterfly',\n",
      " 327: 'starfish, sea star',\n",
      " 328: 'sea urchin',\n",
      " 329: 'sea cucumber, holothurian',\n",
      " 330: 'wood rabbit, cottontail, cottontail rabbit',\n",
      " 331: 'hare',\n",
      " 332: 'Angora, Angora rabbit',\n",
      " 333: 'hamster',\n",
      " 334: 'porcupine, hedgehog',\n",
      " 335: 'fox squirrel, eastern fox squirrel, Sciurus niger',\n",
      " 336: 'marmot',\n",
      " 337: 'beaver',\n",
      " 338: 'guinea pig, Cavia cobaya',\n",
      " 339: 'sorrel',\n",
      " 340: 'zebra',\n",
      " 341: 'hog, pig, grunter, squealer, Sus scrofa',\n",
      " 342: 'wild boar, boar, Sus scrofa',\n",
      " 343: 'warthog',\n",
      " 344: 'hippopotamus, hippo, river horse, Hippopotamus amphibius',\n",
      " 345: 'ox',\n",
      " 346: 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis',\n",
      " 347: 'bison',\n",
      " 348: 'ram, tup',\n",
      " 349: 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis',\n",
      " 350: 'ibex, Capra ibex',\n",
      " 351: 'hartebeest',\n",
      " 352: 'impala, Aepyceros melampus',\n",
      " 353: 'gazelle',\n",
      " 354: 'Arabian camel, dromedary, Camelus dromedarius',\n",
      " 355: 'llama',\n",
      " 356: 'weasel',\n",
      " 357: 'mink',\n",
      " 358: 'polecat, fitch, foulmart, foumart, Mustela putorius',\n",
      " 359: 'black-footed ferret, ferret, Mustela nigripes',\n",
      " 360: 'otter',\n",
      " 361: 'skunk, polecat, wood pussy',\n",
      " 362: 'badger',\n",
      " 363: 'armadillo',\n",
      " 364: 'three-toed sloth, ai, Bradypus tridactylus',\n",
      " 365: 'orangutan, orang, orangutang, Pongo pygmaeus',\n",
      " 366: 'gorilla, Gorilla gorilla',\n",
      " 367: 'chimpanzee, chimp, Pan troglodytes',\n",
      " 368: 'gibbon, Hylobates lar',\n",
      " 369: 'siamang, Hylobates syndactylus, Symphalangus syndactylus',\n",
      " 370: 'guenon, guenon monkey',\n",
      " 371: 'patas, hussar monkey, Erythrocebus patas',\n",
      " 372: 'baboon',\n",
      " 373: 'macaque',\n",
      " 374: 'langur',\n",
      " 375: 'colobus, colobus monkey',\n",
      " 376: 'proboscis monkey, Nasalis larvatus',\n",
      " 377: 'marmoset',\n",
      " 378: 'capuchin, ringtail, Cebus capucinus',\n",
      " 379: 'howler monkey, howler',\n",
      " 380: 'titi, titi monkey',\n",
      " 381: 'spider monkey, Ateles geoffroyi',\n",
      " 382: 'squirrel monkey, Saimiri sciureus',\n",
      " 383: 'Madagascar cat, ring-tailed lemur, Lemur catta',\n",
      " 384: 'indri, indris, Indri indri, Indri brevicaudatus',\n",
      " 385: 'Indian elephant, Elephas maximus',\n",
      " 386: 'African elephant, Loxodonta africana',\n",
      " 387: 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens',\n",
      " 388: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca',\n",
      " 389: 'barracouta, snoek',\n",
      " 390: 'eel',\n",
      " 391: 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch',\n",
      " 392: 'rock beauty, Holocanthus tricolor',\n",
      " 393: 'anemone fish',\n",
      " 394: 'sturgeon',\n",
      " 395: 'gar, garfish, garpike, billfish, Lepisosteus osseus',\n",
      " 396: 'lionfish',\n",
      " 397: 'puffer, pufferfish, blowfish, globefish',\n",
      " 398: 'abacus',\n",
      " 399: 'abaya',\n",
      " 400: \"academic gown, academic robe, judge's robe\",\n",
      " 401: 'accordion, piano accordion, squeeze box',\n",
      " 402: 'acoustic guitar',\n",
      " 403: 'aircraft carrier, carrier, flattop, attack aircraft carrier',\n",
      " 404: 'airliner',\n",
      " 405: 'airship, dirigible',\n",
      " 406: 'altar',\n",
      " 407: 'ambulance',\n",
      " 408: 'amphibian, amphibious vehicle',\n",
      " 409: 'analog clock',\n",
      " 410: 'apiary, bee house',\n",
      " 411: 'apron',\n",
      " 412: 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin',\n",
      " 413: 'assault rifle, assault gun',\n",
      " 414: 'backpack, back pack, knapsack, packsack, rucksack, haversack',\n",
      " 415: 'bakery, bakeshop, bakehouse',\n",
      " 416: 'balance beam, beam',\n",
      " 417: 'balloon',\n",
      " 418: 'ballpoint, ballpoint pen, ballpen, Biro',\n",
      " 419: 'Band Aid',\n",
      " 420: 'banjo',\n",
      " 421: 'bannister, banister, balustrade, balusters, handrail',\n",
      " 422: 'barbell',\n",
      " 423: 'barber chair',\n",
      " 424: 'barbershop',\n",
      " 425: 'barn',\n",
      " 426: 'barometer',\n",
      " 427: 'barrel, cask',\n",
      " 428: 'barrow, garden cart, lawn cart, wheelbarrow',\n",
      " 429: 'baseball',\n",
      " 430: 'basketball',\n",
      " 431: 'bassinet',\n",
      " 432: 'bassoon',\n",
      " 433: 'bathing cap, swimming cap',\n",
      " 434: 'bath towel',\n",
      " 435: 'bathtub, bathing tub, bath, tub',\n",
      " 436: 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon',\n",
      " 437: 'beacon, lighthouse, beacon light, pharos',\n",
      " 438: 'beaker',\n",
      " 439: 'bearskin, busby, shako',\n",
      " 440: 'beer bottle',\n",
      " 441: 'beer glass',\n",
      " 442: 'bell cote, bell cot',\n",
      " 443: 'bib',\n",
      " 444: 'bicycle-built-for-two, tandem bicycle, tandem',\n",
      " 445: 'bikini, two-piece',\n",
      " 446: 'binder, ring-binder',\n",
      " 447: 'binoculars, field glasses, opera glasses',\n",
      " 448: 'birdhouse',\n",
      " 449: 'boathouse',\n",
      " 450: 'bobsled, bobsleigh, bob',\n",
      " 451: 'bolo tie, bolo, bola tie, bola',\n",
      " 452: 'bonnet, poke bonnet',\n",
      " 453: 'bookcase',\n",
      " 454: 'bookshop, bookstore, bookstall',\n",
      " 455: 'bottlecap',\n",
      " 456: 'bow',\n",
      " 457: 'bow tie, bow-tie, bowtie',\n",
      " 458: 'brass, memorial tablet, plaque',\n",
      " 459: 'brassiere, bra, bandeau',\n",
      " 460: 'breakwater, groin, groyne, mole, bulwark, seawall, jetty',\n",
      " 461: 'breastplate, aegis, egis',\n",
      " 462: 'broom',\n",
      " 463: 'bucket, pail',\n",
      " 464: 'buckle',\n",
      " 465: 'bulletproof vest',\n",
      " 466: 'bullet train, bullet',\n",
      " 467: 'butcher shop, meat market',\n",
      " 468: 'cab, hack, taxi, taxicab',\n",
      " 469: 'caldron, cauldron',\n",
      " 470: 'candle, taper, wax light',\n",
      " 471: 'cannon',\n",
      " 472: 'canoe',\n",
      " 473: 'can opener, tin opener',\n",
      " 474: 'cardigan',\n",
      " 475: 'car mirror',\n",
      " 476: 'carousel, carrousel, merry-go-round, roundabout, whirligig',\n",
      " 477: \"carpenter's kit, tool kit\",\n",
      " 478: 'carton',\n",
      " 479: 'car wheel',\n",
      " 480: 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM',\n",
      " 481: 'cassette',\n",
      " 482: 'cassette player',\n",
      " 483: 'castle',\n",
      " 484: 'catamaran',\n",
      " 485: 'CD player',\n",
      " 486: 'cello, violoncello',\n",
      " 487: 'cellular telephone, cellular phone, cellphone, cell, mobile phone',\n",
      " 488: 'chain',\n",
      " 489: 'chainlink fence',\n",
      " 490: 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour',\n",
      " 491: 'chain saw, chainsaw',\n",
      " 492: 'chest',\n",
      " 493: 'chiffonier, commode',\n",
      " 494: 'chime, bell, gong',\n",
      " 495: 'china cabinet, china closet',\n",
      " 496: 'Christmas stocking',\n",
      " 497: 'church, church building',\n",
      " 498: 'cinema, movie theater, movie theatre, movie house, picture palace',\n",
      " 499: 'cleaver, meat cleaver, chopper',\n",
      " 500: 'cliff dwelling',\n",
      " 501: 'cloak',\n",
      " 502: 'clog, geta, patten, sabot',\n",
      " 503: 'cocktail shaker',\n",
      " 504: 'coffee mug',\n",
      " 505: 'coffeepot',\n",
      " 506: 'coil, spiral, volute, whorl, helix',\n",
      " 507: 'combination lock',\n",
      " 508: 'computer keyboard, keypad',\n",
      " 509: 'confectionery, confectionary, candy store',\n",
      " 510: 'container ship, containership, container vessel',\n",
      " 511: 'convertible',\n",
      " 512: 'corkscrew, bottle screw',\n",
      " 513: 'cornet, horn, trumpet, trump',\n",
      " 514: 'cowboy boot',\n",
      " 515: 'cowboy hat, ten-gallon hat',\n",
      " 516: 'cradle',\n",
      " 517: 'crane',\n",
      " 518: 'crash helmet',\n",
      " 519: 'crate',\n",
      " 520: 'crib, cot',\n",
      " 521: 'Crock Pot',\n",
      " 522: 'croquet ball',\n",
      " 523: 'crutch',\n",
      " 524: 'cuirass',\n",
      " 525: 'dam, dike, dyke',\n",
      " 526: 'desk',\n",
      " 527: 'desktop computer',\n",
      " 528: 'dial telephone, dial phone',\n",
      " 529: 'diaper, nappy, napkin',\n",
      " 530: 'digital clock',\n",
      " 531: 'digital watch',\n",
      " 532: 'dining table, board',\n",
      " 533: 'dishrag, dishcloth',\n",
      " 534: 'dishwasher, dish washer, dishwashing machine',\n",
      " 535: 'disk brake, disc brake',\n",
      " 536: 'dock, dockage, docking facility',\n",
      " 537: 'dogsled, dog sled, dog sleigh',\n",
      " 538: 'dome',\n",
      " 539: 'doormat, welcome mat',\n",
      " 540: 'drilling platform, offshore rig',\n",
      " 541: 'drum, membranophone, tympan',\n",
      " 542: 'drumstick',\n",
      " 543: 'dumbbell',\n",
      " 544: 'Dutch oven',\n",
      " 545: 'electric fan, blower',\n",
      " 546: 'electric guitar',\n",
      " 547: 'electric locomotive',\n",
      " 548: 'entertainment center',\n",
      " 549: 'envelope',\n",
      " 550: 'espresso maker',\n",
      " 551: 'face powder',\n",
      " 552: 'feather boa, boa',\n",
      " 553: 'file, file cabinet, filing cabinet',\n",
      " 554: 'fireboat',\n",
      " 555: 'fire engine, fire truck',\n",
      " 556: 'fire screen, fireguard',\n",
      " 557: 'flagpole, flagstaff',\n",
      " 558: 'flute, transverse flute',\n",
      " 559: 'folding chair',\n",
      " 560: 'football helmet',\n",
      " 561: 'forklift',\n",
      " 562: 'fountain',\n",
      " 563: 'fountain pen',\n",
      " 564: 'four-poster',\n",
      " 565: 'freight car',\n",
      " 566: 'French horn, horn',\n",
      " 567: 'frying pan, frypan, skillet',\n",
      " 568: 'fur coat',\n",
      " 569: 'garbage truck, dustcart',\n",
      " 570: 'gasmask, respirator, gas helmet',\n",
      " 571: 'gas pump, gasoline pump, petrol pump, island dispenser',\n",
      " 572: 'goblet',\n",
      " 573: 'go-kart',\n",
      " 574: 'golf ball',\n",
      " 575: 'golfcart, golf cart',\n",
      " 576: 'gondola',\n",
      " 577: 'gong, tam-tam',\n",
      " 578: 'gown',\n",
      " 579: 'grand piano, grand',\n",
      " 580: 'greenhouse, nursery, glasshouse',\n",
      " 581: 'grille, radiator grille',\n",
      " 582: 'grocery store, grocery, food market, market',\n",
      " 583: 'guillotine',\n",
      " 584: 'hair slide',\n",
      " 585: 'hair spray',\n",
      " 586: 'half track',\n",
      " 587: 'hammer',\n",
      " 588: 'hamper',\n",
      " 589: 'hand blower, blow dryer, blow drier, hair dryer, hair drier',\n",
      " 590: 'hand-held computer, hand-held microcomputer',\n",
      " 591: 'handkerchief, hankie, hanky, hankey',\n",
      " 592: 'hard disc, hard disk, fixed disk',\n",
      " 593: 'harmonica, mouth organ, harp, mouth harp',\n",
      " 594: 'harp',\n",
      " 595: 'harvester, reaper',\n",
      " 596: 'hatchet',\n",
      " 597: 'holster',\n",
      " 598: 'home theater, home theatre',\n",
      " 599: 'honeycomb',\n",
      " 600: 'hook, claw',\n",
      " 601: 'hoopskirt, crinoline',\n",
      " 602: 'horizontal bar, high bar',\n",
      " 603: 'horse cart, horse-cart',\n",
      " 604: 'hourglass',\n",
      " 605: 'iPod',\n",
      " 606: 'iron, smoothing iron',\n",
      " 607: \"jack-o'-lantern\",\n",
      " 608: 'jean, blue jean, denim',\n",
      " 609: 'jeep, landrover',\n",
      " 610: 'jersey, T-shirt, tee shirt',\n",
      " 611: 'jigsaw puzzle',\n",
      " 612: 'jinrikisha, ricksha, rickshaw',\n",
      " 613: 'joystick',\n",
      " 614: 'kimono',\n",
      " 615: 'knee pad',\n",
      " 616: 'knot',\n",
      " 617: 'lab coat, laboratory coat',\n",
      " 618: 'ladle',\n",
      " 619: 'lampshade, lamp shade',\n",
      " 620: 'laptop, laptop computer',\n",
      " 621: 'lawn mower, mower',\n",
      " 622: 'lens cap, lens cover',\n",
      " 623: 'letter opener, paper knife, paperknife',\n",
      " 624: 'library',\n",
      " 625: 'lifeboat',\n",
      " 626: 'lighter, light, igniter, ignitor',\n",
      " 627: 'limousine, limo',\n",
      " 628: 'liner, ocean liner',\n",
      " 629: 'lipstick, lip rouge',\n",
      " 630: 'Loafer',\n",
      " 631: 'lotion',\n",
      " 632: 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system',\n",
      " 633: \"loupe, jeweler's loupe\",\n",
      " 634: 'lumbermill, sawmill',\n",
      " 635: 'magnetic compass',\n",
      " 636: 'mailbag, postbag',\n",
      " 637: 'mailbox, letter box',\n",
      " 638: 'maillot',\n",
      " 639: 'maillot, tank suit',\n",
      " 640: 'manhole cover',\n",
      " 641: 'maraca',\n",
      " 642: 'marimba, xylophone',\n",
      " 643: 'mask',\n",
      " 644: 'matchstick',\n",
      " 645: 'maypole',\n",
      " 646: 'maze, labyrinth',\n",
      " 647: 'measuring cup',\n",
      " 648: 'medicine chest, medicine cabinet',\n",
      " 649: 'megalith, megalithic structure',\n",
      " 650: 'microphone, mike',\n",
      " 651: 'microwave, microwave oven',\n",
      " 652: 'military uniform',\n",
      " 653: 'milk can',\n",
      " 654: 'minibus',\n",
      " 655: 'miniskirt, mini',\n",
      " 656: 'minivan',\n",
      " 657: 'missile',\n",
      " 658: 'mitten',\n",
      " 659: 'mixing bowl',\n",
      " 660: 'mobile home, manufactured home',\n",
      " 661: 'Model T',\n",
      " 662: 'modem',\n",
      " 663: 'monastery',\n",
      " 664: 'monitor',\n",
      " 665: 'moped',\n",
      " 666: 'mortar',\n",
      " 667: 'mortarboard',\n",
      " 668: 'mosque',\n",
      " 669: 'mosquito net',\n",
      " 670: 'motor scooter, scooter',\n",
      " 671: 'mountain bike, all-terrain bike, off-roader',\n",
      " 672: 'mountain tent',\n",
      " 673: 'mouse, computer mouse',\n",
      " 674: 'mousetrap',\n",
      " 675: 'moving van',\n",
      " 676: 'muzzle',\n",
      " 677: 'nail',\n",
      " 678: 'neck brace',\n",
      " 679: 'necklace',\n",
      " 680: 'nipple',\n",
      " 681: 'notebook, notebook computer',\n",
      " 682: 'obelisk',\n",
      " 683: 'oboe, hautboy, hautbois',\n",
      " 684: 'ocarina, sweet potato',\n",
      " 685: 'odometer, hodometer, mileometer, milometer',\n",
      " 686: 'oil filter',\n",
      " 687: 'organ, pipe organ',\n",
      " 688: 'oscilloscope, scope, cathode-ray oscilloscope, CRO',\n",
      " 689: 'overskirt',\n",
      " 690: 'oxcart',\n",
      " 691: 'oxygen mask',\n",
      " 692: 'packet',\n",
      " 693: 'paddle, boat paddle',\n",
      " 694: 'paddlewheel, paddle wheel',\n",
      " 695: 'padlock',\n",
      " 696: 'paintbrush',\n",
      " 697: \"pajama, pyjama, pj's, jammies\",\n",
      " 698: 'palace',\n",
      " 699: 'panpipe, pandean pipe, syrinx',\n",
      " 700: 'paper towel',\n",
      " 701: 'parachute, chute',\n",
      " 702: 'parallel bars, bars',\n",
      " 703: 'park bench',\n",
      " 704: 'parking meter',\n",
      " 705: 'passenger car, coach, carriage',\n",
      " 706: 'patio, terrace',\n",
      " 707: 'pay-phone, pay-station',\n",
      " 708: 'pedestal, plinth, footstall',\n",
      " 709: 'pencil box, pencil case',\n",
      " 710: 'pencil sharpener',\n",
      " 711: 'perfume, essence',\n",
      " 712: 'Petri dish',\n",
      " 713: 'photocopier',\n",
      " 714: 'pick, plectrum, plectron',\n",
      " 715: 'pickelhaube',\n",
      " 716: 'picket fence, paling',\n",
      " 717: 'pickup, pickup truck',\n",
      " 718: 'pier',\n",
      " 719: 'piggy bank, penny bank',\n",
      " 720: 'pill bottle',\n",
      " 721: 'pillow',\n",
      " 722: 'ping-pong ball',\n",
      " 723: 'pinwheel',\n",
      " 724: 'pirate, pirate ship',\n",
      " 725: 'pitcher, ewer',\n",
      " 726: \"plane, carpenter's plane, woodworking plane\",\n",
      " 727: 'planetarium',\n",
      " 728: 'plastic bag',\n",
      " 729: 'plate rack',\n",
      " 730: 'plow, plough',\n",
      " 731: \"plunger, plumber's helper\",\n",
      " 732: 'Polaroid camera, Polaroid Land camera',\n",
      " 733: 'pole',\n",
      " 734: 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria',\n",
      " 735: 'poncho',\n",
      " 736: 'pool table, billiard table, snooker table',\n",
      " 737: 'pop bottle, soda bottle',\n",
      " 738: 'pot, flowerpot',\n",
      " 739: \"potter's wheel\",\n",
      " 740: 'power drill',\n",
      " 741: 'prayer rug, prayer mat',\n",
      " 742: 'printer',\n",
      " 743: 'prison, prison house',\n",
      " 744: 'projectile, missile',\n",
      " 745: 'projector',\n",
      " 746: 'puck, hockey puck',\n",
      " 747: 'punching bag, punch bag, punching ball, punchball',\n",
      " 748: 'purse',\n",
      " 749: 'quill, quill pen',\n",
      " 750: 'quilt, comforter, comfort, puff',\n",
      " 751: 'racer, race car, racing car',\n",
      " 752: 'racket, racquet',\n",
      " 753: 'radiator',\n",
      " 754: 'radio, wireless',\n",
      " 755: 'radio telescope, radio reflector',\n",
      " 756: 'rain barrel',\n",
      " 757: 'recreational vehicle, RV, R.V.',\n",
      " 758: 'reel',\n",
      " 759: 'reflex camera',\n",
      " 760: 'refrigerator, icebox',\n",
      " 761: 'remote control, remote',\n",
      " 762: 'restaurant, eating house, eating place, eatery',\n",
      " 763: 'revolver, six-gun, six-shooter',\n",
      " 764: 'rifle',\n",
      " 765: 'rocking chair, rocker',\n",
      " 766: 'rotisserie',\n",
      " 767: 'rubber eraser, rubber, pencil eraser',\n",
      " 768: 'rugby ball',\n",
      " 769: 'rule, ruler',\n",
      " 770: 'running shoe',\n",
      " 771: 'safe',\n",
      " 772: 'safety pin',\n",
      " 773: 'saltshaker, salt shaker',\n",
      " 774: 'sandal',\n",
      " 775: 'sarong',\n",
      " 776: 'sax, saxophone',\n",
      " 777: 'scabbard',\n",
      " 778: 'scale, weighing machine',\n",
      " 779: 'school bus',\n",
      " 780: 'schooner',\n",
      " 781: 'scoreboard',\n",
      " 782: 'screen, CRT screen',\n",
      " 783: 'screw',\n",
      " 784: 'screwdriver',\n",
      " 785: 'seat belt, seatbelt',\n",
      " 786: 'sewing machine',\n",
      " 787: 'shield, buckler',\n",
      " 788: 'shoe shop, shoe-shop, shoe store',\n",
      " 789: 'shoji',\n",
      " 790: 'shopping basket',\n",
      " 791: 'shopping cart',\n",
      " 792: 'shovel',\n",
      " 793: 'shower cap',\n",
      " 794: 'shower curtain',\n",
      " 795: 'ski',\n",
      " 796: 'ski mask',\n",
      " 797: 'sleeping bag',\n",
      " 798: 'slide rule, slipstick',\n",
      " 799: 'sliding door',\n",
      " 800: 'slot, one-armed bandit',\n",
      " 801: 'snorkel',\n",
      " 802: 'snowmobile',\n",
      " 803: 'snowplow, snowplough',\n",
      " 804: 'soap dispenser',\n",
      " 805: 'soccer ball',\n",
      " 806: 'sock',\n",
      " 807: 'solar dish, solar collector, solar furnace',\n",
      " 808: 'sombrero',\n",
      " 809: 'soup bowl',\n",
      " 810: 'space bar',\n",
      " 811: 'space heater',\n",
      " 812: 'space shuttle',\n",
      " 813: 'spatula',\n",
      " 814: 'speedboat',\n",
      " 815: \"spider web, spider's web\",\n",
      " 816: 'spindle',\n",
      " 817: 'sports car, sport car',\n",
      " 818: 'spotlight, spot',\n",
      " 819: 'stage',\n",
      " 820: 'steam locomotive',\n",
      " 821: 'steel arch bridge',\n",
      " 822: 'steel drum',\n",
      " 823: 'stethoscope',\n",
      " 824: 'stole',\n",
      " 825: 'stone wall',\n",
      " 826: 'stopwatch, stop watch',\n",
      " 827: 'stove',\n",
      " 828: 'strainer',\n",
      " 829: 'streetcar, tram, tramcar, trolley, trolley car',\n",
      " 830: 'stretcher',\n",
      " 831: 'studio couch, day bed',\n",
      " 832: 'stupa, tope',\n",
      " 833: 'submarine, pigboat, sub, U-boat',\n",
      " 834: 'suit, suit of clothes',\n",
      " 835: 'sundial',\n",
      " 836: 'sunglass',\n",
      " 837: 'sunglasses, dark glasses, shades',\n",
      " 838: 'sunscreen, sunblock, sun blocker',\n",
      " 839: 'suspension bridge',\n",
      " 840: 'swab, swob, mop',\n",
      " 841: 'sweatshirt',\n",
      " 842: 'swimming trunks, bathing trunks',\n",
      " 843: 'swing',\n",
      " 844: 'switch, electric switch, electrical switch',\n",
      " 845: 'syringe',\n",
      " 846: 'table lamp',\n",
      " 847: 'tank, army tank, armored combat vehicle, armoured combat vehicle',\n",
      " 848: 'tape player',\n",
      " 849: 'teapot',\n",
      " 850: 'teddy, teddy bear',\n",
      " 851: 'television, television system',\n",
      " 852: 'tennis ball',\n",
      " 853: 'thatch, thatched roof',\n",
      " 854: 'theater curtain, theatre curtain',\n",
      " 855: 'thimble',\n",
      " 856: 'thresher, thrasher, threshing machine',\n",
      " 857: 'throne',\n",
      " 858: 'tile roof',\n",
      " 859: 'toaster',\n",
      " 860: 'tobacco shop, tobacconist shop, tobacconist',\n",
      " 861: 'toilet seat',\n",
      " 862: 'torch',\n",
      " 863: 'totem pole',\n",
      " 864: 'tow truck, tow car, wrecker',\n",
      " 865: 'toyshop',\n",
      " 866: 'tractor',\n",
      " 867: 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi',\n",
      " 868: 'tray',\n",
      " 869: 'trench coat',\n",
      " 870: 'tricycle, trike, velocipede',\n",
      " 871: 'trimaran',\n",
      " 872: 'tripod',\n",
      " 873: 'triumphal arch',\n",
      " 874: 'trolleybus, trolley coach, trackless trolley',\n",
      " 875: 'trombone',\n",
      " 876: 'tub, vat',\n",
      " 877: 'turnstile',\n",
      " 878: 'typewriter keyboard',\n",
      " 879: 'umbrella',\n",
      " 880: 'unicycle, monocycle',\n",
      " 881: 'upright, upright piano',\n",
      " 882: 'vacuum, vacuum cleaner',\n",
      " 883: 'vase',\n",
      " 884: 'vault',\n",
      " 885: 'velvet',\n",
      " 886: 'vending machine',\n",
      " 887: 'vestment',\n",
      " 888: 'viaduct',\n",
      " 889: 'violin, fiddle',\n",
      " 890: 'volleyball',\n",
      " 891: 'waffle iron',\n",
      " 892: 'wall clock',\n",
      " 893: 'wallet, billfold, notecase, pocketbook',\n",
      " 894: 'wardrobe, closet, press',\n",
      " 895: 'warplane, military plane',\n",
      " 896: 'washbasin, handbasin, washbowl, lavabo, wash-hand basin',\n",
      " 897: 'washer, automatic washer, washing machine',\n",
      " 898: 'water bottle',\n",
      " 899: 'water jug',\n",
      " 900: 'water tower',\n",
      " 901: 'whiskey jug',\n",
      " 902: 'whistle',\n",
      " 903: 'wig',\n",
      " 904: 'window screen',\n",
      " 905: 'window shade',\n",
      " 906: 'Windsor tie',\n",
      " 907: 'wine bottle',\n",
      " 908: 'wing',\n",
      " 909: 'wok',\n",
      " 910: 'wooden spoon',\n",
      " 911: 'wool, woolen, woollen',\n",
      " 912: 'worm fence, snake fence, snake-rail fence, Virginia fence',\n",
      " 913: 'wreck',\n",
      " 914: 'yawl',\n",
      " 915: 'yurt',\n",
      " 916: 'web site, website, internet site, site',\n",
      " 917: 'comic book',\n",
      " 918: 'crossword puzzle, crossword',\n",
      " 919: 'street sign',\n",
      " 920: 'traffic light, traffic signal, stoplight',\n",
      " 921: 'book jacket, dust cover, dust jacket, dust wrapper',\n",
      " 922: 'menu',\n",
      " 923: 'plate',\n",
      " 924: 'guacamole',\n",
      " 925: 'consomme',\n",
      " 926: 'hot pot, hotpot',\n",
      " 927: 'trifle',\n",
      " 928: 'ice cream, icecream',\n",
      " 929: 'ice lolly, lolly, lollipop, popsicle',\n",
      " 930: 'French loaf',\n",
      " 931: 'bagel, beigel',\n",
      " 932: 'pretzel',\n",
      " 933: 'cheeseburger',\n",
      " 934: 'hotdog, hot dog, red hot',\n",
      " 935: 'mashed potato',\n",
      " 936: 'head cabbage',\n",
      " 937: 'broccoli',\n",
      " 938: 'cauliflower',\n",
      " 939: 'zucchini, courgette',\n",
      " 940: 'spaghetti squash',\n",
      " 941: 'acorn squash',\n",
      " 942: 'butternut squash',\n",
      " 943: 'cucumber, cuke',\n",
      " 944: 'artichoke, globe artichoke',\n",
      " 945: 'bell pepper',\n",
      " 946: 'cardoon',\n",
      " 947: 'mushroom',\n",
      " 948: 'Granny Smith',\n",
      " 949: 'strawberry',\n",
      " 950: 'orange',\n",
      " 951: 'lemon',\n",
      " 952: 'fig',\n",
      " 953: 'pineapple, ananas',\n",
      " 954: 'banana',\n",
      " 955: 'jackfruit, jak, jack',\n",
      " 956: 'custard apple',\n",
      " 957: 'pomegranate',\n",
      " 958: 'hay',\n",
      " 959: 'carbonara',\n",
      " 960: 'chocolate sauce, chocolate syrup',\n",
      " 961: 'dough',\n",
      " 962: 'meat loaf, meatloaf',\n",
      " 963: 'pizza, pizza pie',\n",
      " 964: 'potpie',\n",
      " 965: 'burrito',\n",
      " 966: 'red wine',\n",
      " 967: 'espresso',\n",
      " 968: 'cup',\n",
      " 969: 'eggnog',\n",
      " 970: 'alp',\n",
      " 971: 'bubble',\n",
      " 972: 'cliff, drop, drop-off',\n",
      " 973: 'coral reef',\n",
      " 974: 'geyser',\n",
      " 975: 'lakeside, lakeshore',\n",
      " 976: 'promontory, headland, head, foreland',\n",
      " 977: 'sandbar, sand bar',\n",
      " 978: 'seashore, coast, seacoast, sea-coast',\n",
      " 979: 'valley, vale',\n",
      " 980: 'volcano',\n",
      " 981: 'ballplayer, baseball player',\n",
      " 982: 'groom, bridegroom',\n",
      " 983: 'scuba diver',\n",
      " 984: 'rapeseed',\n",
      " 985: 'daisy',\n",
      " 986: \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\",\n",
      " 987: 'corn',\n",
      " 988: 'acorn',\n",
      " 989: 'hip, rose hip, rosehip',\n",
      " 990: 'buckeye, horse chestnut, conker',\n",
      " 991: 'coral fungus',\n",
      " 992: 'agaric',\n",
      " 993: 'gyromitra',\n",
      " 994: 'stinkhorn, carrion fungus',\n",
      " 995: 'earthstar',\n",
      " 996: 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa',\n",
      " 997: 'bolete',\n",
      " 998: 'ear, spike, capitulum',\n",
      " 999: 'toilet tissue, toilet paper, bathroom tissue'}\n",
      "58/2:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 380\n",
      "\n",
      "def prep_image(image):\n",
      "    res = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "    res = res.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "    return res\n",
      "\n",
      "def predict(image):\n",
      "    cropped_image = center_crop(image)\n",
      "    prediction = (model.predict(prep_image(image)), model.predict(prep_image(cropped_image)))\n",
      "    if np.max(prediction[0]>=prediction[1]):\n",
      "        return np.argmax(prediction[0], axis=1)[0]\n",
      "    else:\n",
      "        return np.argmax(prediction[1], axis=1)[1]\n",
      "    \n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "plt.imshow(image)\n",
      "\n",
      "print(class_dict[predict(image)])\n",
      "59/1:\n",
      "from tensorflow.keras.applications import EfficientNetB4\n",
      "from tensorflow.image import resize\n",
      "model = EfficientNetB4(weights='imagenet')\n",
      "\n",
      "def center_crop(img, new_width=None, new_height=None):        \n",
      "\n",
      "    width = img.shape[1]\n",
      "    height = img.shape[0]\n",
      "\n",
      "    if new_width is None:\n",
      "        new_width = min(width, height)\n",
      "\n",
      "    if new_height is None:\n",
      "        new_height = min(width, height)\n",
      "\n",
      "    left = int(np.ceil((width - new_width) / 2))\n",
      "    right = width - int(np.floor((width - new_width) / 2))\n",
      "\n",
      "    top = int(np.ceil((height - new_height) / 2))\n",
      "    bottom = height - int(np.floor((height - new_height) / 2))\n",
      "\n",
      "    if len(img.shape) == 2:\n",
      "        center_cropped_img = img[top:bottom, left:right]\n",
      "    else:\n",
      "        center_cropped_img = img[top:bottom, left:right, ...]\n",
      "\n",
      "    return center_cropped_img\n",
      "\n",
      "class_dict = {0: 'tench, Tinca tinca',\n",
      " 1: 'goldfish, Carassius auratus',\n",
      " 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n",
      " 3: 'tiger shark, Galeocerdo cuvieri',\n",
      " 4: 'hammerhead, hammerhead shark',\n",
      " 5: 'electric ray, crampfish, numbfish, torpedo',\n",
      " 6: 'stingray',\n",
      " 7: 'cock',\n",
      " 8: 'hen',\n",
      " 9: 'ostrich, Struthio camelus',\n",
      " 10: 'brambling, Fringilla montifringilla',\n",
      " 11: 'goldfinch, Carduelis carduelis',\n",
      " 12: 'house finch, linnet, Carpodacus mexicanus',\n",
      " 13: 'junco, snowbird',\n",
      " 14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea',\n",
      " 15: 'robin, American robin, Turdus migratorius',\n",
      " 16: 'bulbul',\n",
      " 17: 'jay',\n",
      " 18: 'magpie',\n",
      " 19: 'chickadee',\n",
      " 20: 'water ouzel, dipper',\n",
      " 21: 'kite',\n",
      " 22: 'bald eagle, American eagle, Haliaeetus leucocephalus',\n",
      " 23: 'vulture',\n",
      " 24: 'great grey owl, great gray owl, Strix nebulosa',\n",
      " 25: 'European fire salamander, Salamandra salamandra',\n",
      " 26: 'common newt, Triturus vulgaris',\n",
      " 27: 'eft',\n",
      " 28: 'spotted salamander, Ambystoma maculatum',\n",
      " 29: 'axolotl, mud puppy, Ambystoma mexicanum',\n",
      " 30: 'bullfrog, Rana catesbeiana',\n",
      " 31: 'tree frog, tree-frog',\n",
      " 32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui',\n",
      " 33: 'loggerhead, loggerhead turtle, Caretta caretta',\n",
      " 34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea',\n",
      " 35: 'mud turtle',\n",
      " 36: 'terrapin',\n",
      " 37: 'box turtle, box tortoise',\n",
      " 38: 'banded gecko',\n",
      " 39: 'common iguana, iguana, Iguana iguana',\n",
      " 40: 'American chameleon, anole, Anolis carolinensis',\n",
      " 41: 'whiptail, whiptail lizard',\n",
      " 42: 'agama',\n",
      " 43: 'frilled lizard, Chlamydosaurus kingi',\n",
      " 44: 'alligator lizard',\n",
      " 45: 'Gila monster, Heloderma suspectum',\n",
      " 46: 'green lizard, Lacerta viridis',\n",
      " 47: 'African chameleon, Chamaeleo chamaeleon',\n",
      " 48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis',\n",
      " 49: 'African crocodile, Nile crocodile, Crocodylus niloticus',\n",
      " 50: 'American alligator, Alligator mississipiensis',\n",
      " 51: 'triceratops',\n",
      " 52: 'thunder snake, worm snake, Carphophis amoenus',\n",
      " 53: 'ringneck snake, ring-necked snake, ring snake',\n",
      " 54: 'hognose snake, puff adder, sand viper',\n",
      " 55: 'green snake, grass snake',\n",
      " 56: 'king snake, kingsnake',\n",
      " 57: 'garter snake, grass snake',\n",
      " 58: 'water snake',\n",
      " 59: 'vine snake',\n",
      " 60: 'night snake, Hypsiglena torquata',\n",
      " 61: 'boa constrictor, Constrictor constrictor',\n",
      " 62: 'rock python, rock snake, Python sebae',\n",
      " 63: 'Indian cobra, Naja naja',\n",
      " 64: 'green mamba',\n",
      " 65: 'sea snake',\n",
      " 66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus',\n",
      " 67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus',\n",
      " 68: 'sidewinder, horned rattlesnake, Crotalus cerastes',\n",
      " 69: 'trilobite',\n",
      " 70: 'harvestman, daddy longlegs, Phalangium opilio',\n",
      " 71: 'scorpion',\n",
      " 72: 'black and gold garden spider, Argiope aurantia',\n",
      " 73: 'barn spider, Araneus cavaticus',\n",
      " 74: 'garden spider, Aranea diademata',\n",
      " 75: 'black widow, Latrodectus mactans',\n",
      " 76: 'tarantula',\n",
      " 77: 'wolf spider, hunting spider',\n",
      " 78: 'tick',\n",
      " 79: 'centipede',\n",
      " 80: 'black grouse',\n",
      " 81: 'ptarmigan',\n",
      " 82: 'ruffed grouse, partridge, Bonasa umbellus',\n",
      " 83: 'prairie chicken, prairie grouse, prairie fowl',\n",
      " 84: 'peacock',\n",
      " 85: 'quail',\n",
      " 86: 'partridge',\n",
      " 87: 'African grey, African gray, Psittacus erithacus',\n",
      " 88: 'macaw',\n",
      " 89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n",
      " 90: 'lorikeet',\n",
      " 91: 'coucal',\n",
      " 92: 'bee eater',\n",
      " 93: 'hornbill',\n",
      " 94: 'hummingbird',\n",
      " 95: 'jacamar',\n",
      " 96: 'toucan',\n",
      " 97: 'drake',\n",
      " 98: 'red-breasted merganser, Mergus serrator',\n",
      " 99: 'goose',\n",
      " 100: 'black swan, Cygnus atratus',\n",
      " 101: 'tusker',\n",
      " 102: 'echidna, spiny anteater, anteater',\n",
      " 103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n",
      " 104: 'wallaby, brush kangaroo',\n",
      " 105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n",
      " 106: 'wombat',\n",
      " 107: 'jellyfish',\n",
      " 108: 'sea anemone, anemone',\n",
      " 109: 'brain coral',\n",
      " 110: 'flatworm, platyhelminth',\n",
      " 111: 'nematode, nematode worm, roundworm',\n",
      " 112: 'conch',\n",
      " 113: 'snail',\n",
      " 114: 'slug',\n",
      " 115: 'sea slug, nudibranch',\n",
      " 116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore',\n",
      " 117: 'chambered nautilus, pearly nautilus, nautilus',\n",
      " 118: 'Dungeness crab, Cancer magister',\n",
      " 119: 'rock crab, Cancer irroratus',\n",
      " 120: 'fiddler crab',\n",
      " 121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica',\n",
      " 122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus',\n",
      " 123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish',\n",
      " 124: 'crayfish, crawfish, crawdad, crawdaddy',\n",
      " 125: 'hermit crab',\n",
      " 126: 'isopod',\n",
      " 127: 'white stork, Ciconia ciconia',\n",
      " 128: 'black stork, Ciconia nigra',\n",
      " 129: 'spoonbill',\n",
      " 130: 'flamingo',\n",
      " 131: 'little blue heron, Egretta caerulea',\n",
      " 132: 'American egret, great white heron, Egretta albus',\n",
      " 133: 'bittern',\n",
      " 134: 'crane',\n",
      " 135: 'limpkin, Aramus pictus',\n",
      " 136: 'European gallinule, Porphyrio porphyrio',\n",
      " 137: 'American coot, marsh hen, mud hen, water hen, Fulica americana',\n",
      " 138: 'bustard',\n",
      " 139: 'ruddy turnstone, Arenaria interpres',\n",
      " 140: 'red-backed sandpiper, dunlin, Erolia alpina',\n",
      " 141: 'redshank, Tringa totanus',\n",
      " 142: 'dowitcher',\n",
      " 143: 'oystercatcher, oyster catcher',\n",
      " 144: 'pelican',\n",
      " 145: 'king penguin, Aptenodytes patagonica',\n",
      " 146: 'albatross, mollymawk',\n",
      " 147: 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus',\n",
      " 148: 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca',\n",
      " 149: 'dugong, Dugong dugon',\n",
      " 150: 'sea lion',\n",
      " 151: 'Chihuahua',\n",
      " 152: 'Japanese spaniel',\n",
      " 153: 'Maltese dog, Maltese terrier, Maltese',\n",
      " 154: 'Pekinese, Pekingese, Peke',\n",
      " 155: 'Shih-Tzu',\n",
      " 156: 'Blenheim spaniel',\n",
      " 157: 'papillon',\n",
      " 158: 'toy terrier',\n",
      " 159: 'Rhodesian ridgeback',\n",
      " 160: 'Afghan hound, Afghan',\n",
      " 161: 'basset, basset hound',\n",
      " 162: 'beagle',\n",
      " 163: 'bloodhound, sleuthhound',\n",
      " 164: 'bluetick',\n",
      " 165: 'black-and-tan coonhound',\n",
      " 166: 'Walker hound, Walker foxhound',\n",
      " 167: 'English foxhound',\n",
      " 168: 'redbone',\n",
      " 169: 'borzoi, Russian wolfhound',\n",
      " 170: 'Irish wolfhound',\n",
      " 171: 'Italian greyhound',\n",
      " 172: 'whippet',\n",
      " 173: 'Ibizan hound, Ibizan Podenco',\n",
      " 174: 'Norwegian elkhound, elkhound',\n",
      " 175: 'otterhound, otter hound',\n",
      " 176: 'Saluki, gazelle hound',\n",
      " 177: 'Scottish deerhound, deerhound',\n",
      " 178: 'Weimaraner',\n",
      " 179: 'Staffordshire bullterrier, Staffordshire bull terrier',\n",
      " 180: 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier',\n",
      " 181: 'Bedlington terrier',\n",
      " 182: 'Border terrier',\n",
      " 183: 'Kerry blue terrier',\n",
      " 184: 'Irish terrier',\n",
      " 185: 'Norfolk terrier',\n",
      " 186: 'Norwich terrier',\n",
      " 187: 'Yorkshire terrier',\n",
      " 188: 'wire-haired fox terrier',\n",
      " 189: 'Lakeland terrier',\n",
      " 190: 'Sealyham terrier, Sealyham',\n",
      " 191: 'Airedale, Airedale terrier',\n",
      " 192: 'cairn, cairn terrier',\n",
      " 193: 'Australian terrier',\n",
      " 194: 'Dandie Dinmont, Dandie Dinmont terrier',\n",
      " 195: 'Boston bull, Boston terrier',\n",
      " 196: 'miniature schnauzer',\n",
      " 197: 'giant schnauzer',\n",
      " 198: 'standard schnauzer',\n",
      " 199: 'Scotch terrier, Scottish terrier, Scottie',\n",
      " 200: 'Tibetan terrier, chrysanthemum dog',\n",
      " 201: 'silky terrier, Sydney silky',\n",
      " 202: 'soft-coated wheaten terrier',\n",
      " 203: 'West Highland white terrier',\n",
      " 204: 'Lhasa, Lhasa apso',\n",
      " 205: 'flat-coated retriever',\n",
      " 206: 'curly-coated retriever',\n",
      " 207: 'golden retriever',\n",
      " 208: 'Labrador retriever',\n",
      " 209: 'Chesapeake Bay retriever',\n",
      " 210: 'German short-haired pointer',\n",
      " 211: 'vizsla, Hungarian pointer',\n",
      " 212: 'English setter',\n",
      " 213: 'Irish setter, red setter',\n",
      " 214: 'Gordon setter',\n",
      " 215: 'Brittany spaniel',\n",
      " 216: 'clumber, clumber spaniel',\n",
      " 217: 'English springer, English springer spaniel',\n",
      " 218: 'Welsh springer spaniel',\n",
      " 219: 'cocker spaniel, English cocker spaniel, cocker',\n",
      " 220: 'Sussex spaniel',\n",
      " 221: 'Irish water spaniel',\n",
      " 222: 'kuvasz',\n",
      " 223: 'schipperke',\n",
      " 224: 'groenendael',\n",
      " 225: 'malinois',\n",
      " 226: 'briard',\n",
      " 227: 'kelpie',\n",
      " 228: 'komondor',\n",
      " 229: 'Old English sheepdog, bobtail',\n",
      " 230: 'Shetland sheepdog, Shetland sheep dog, Shetland',\n",
      " 231: 'collie',\n",
      " 232: 'Border collie',\n",
      " 233: 'Bouvier des Flandres, Bouviers des Flandres',\n",
      " 234: 'Rottweiler',\n",
      " 235: 'German shepherd, German shepherd dog, German police dog, alsatian',\n",
      " 236: 'Doberman, Doberman pinscher',\n",
      " 237: 'miniature pinscher',\n",
      " 238: 'Greater Swiss Mountain dog',\n",
      " 239: 'Bernese mountain dog',\n",
      " 240: 'Appenzeller',\n",
      " 241: 'EntleBucher',\n",
      " 242: 'boxer',\n",
      " 243: 'bull mastiff',\n",
      " 244: 'Tibetan mastiff',\n",
      " 245: 'French bulldog',\n",
      " 246: 'Great Dane',\n",
      " 247: 'Saint Bernard, St Bernard',\n",
      " 248: 'Eskimo dog, husky',\n",
      " 249: 'malamute, malemute, Alaskan malamute',\n",
      " 250: 'Siberian husky',\n",
      " 251: 'dalmatian, coach dog, carriage dog',\n",
      " 252: 'affenpinscher, monkey pinscher, monkey dog',\n",
      " 253: 'basenji',\n",
      " 254: 'pug, pug-dog',\n",
      " 255: 'Leonberg',\n",
      " 256: 'Newfoundland, Newfoundland dog',\n",
      " 257: 'Great Pyrenees',\n",
      " 258: 'Samoyed, Samoyede',\n",
      " 259: 'Pomeranian',\n",
      " 260: 'chow, chow chow',\n",
      " 261: 'keeshond',\n",
      " 262: 'Brabancon griffon',\n",
      " 263: 'Pembroke, Pembroke Welsh corgi',\n",
      " 264: 'Cardigan, Cardigan Welsh corgi',\n",
      " 265: 'toy poodle',\n",
      " 266: 'miniature poodle',\n",
      " 267: 'standard poodle',\n",
      " 268: 'Mexican hairless',\n",
      " 269: 'timber wolf, grey wolf, gray wolf, Canis lupus',\n",
      " 270: 'white wolf, Arctic wolf, Canis lupus tundrarum',\n",
      " 271: 'red wolf, maned wolf, Canis rufus, Canis niger',\n",
      " 272: 'coyote, prairie wolf, brush wolf, Canis latrans',\n",
      " 273: 'dingo, warrigal, warragal, Canis dingo',\n",
      " 274: 'dhole, Cuon alpinus',\n",
      " 275: 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus',\n",
      " 276: 'hyena, hyaena',\n",
      " 277: 'red fox, Vulpes vulpes',\n",
      " 278: 'kit fox, Vulpes macrotis',\n",
      " 279: 'Arctic fox, white fox, Alopex lagopus',\n",
      " 280: 'grey fox, gray fox, Urocyon cinereoargenteus',\n",
      " 281: 'tabby, tabby cat',\n",
      " 282: 'tiger cat',\n",
      " 283: 'Persian cat',\n",
      " 284: 'Siamese cat, Siamese',\n",
      " 285: 'Egyptian cat',\n",
      " 286: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n",
      " 287: 'lynx, catamount',\n",
      " 288: 'leopard, Panthera pardus',\n",
      " 289: 'snow leopard, ounce, Panthera uncia',\n",
      " 290: 'jaguar, panther, Panthera onca, Felis onca',\n",
      " 291: 'lion, king of beasts, Panthera leo',\n",
      " 292: 'tiger, Panthera tigris',\n",
      " 293: 'cheetah, chetah, Acinonyx jubatus',\n",
      " 294: 'brown bear, bruin, Ursus arctos',\n",
      " 295: 'American black bear, black bear, Ursus americanus, Euarctos americanus',\n",
      " 296: 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus',\n",
      " 297: 'sloth bear, Melursus ursinus, Ursus ursinus',\n",
      " 298: 'mongoose',\n",
      " 299: 'meerkat, mierkat',\n",
      " 300: 'tiger beetle',\n",
      " 301: 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle',\n",
      " 302: 'ground beetle, carabid beetle',\n",
      " 303: 'long-horned beetle, longicorn, longicorn beetle',\n",
      " 304: 'leaf beetle, chrysomelid',\n",
      " 305: 'dung beetle',\n",
      " 306: 'rhinoceros beetle',\n",
      " 307: 'weevil',\n",
      " 308: 'fly',\n",
      " 309: 'bee',\n",
      " 310: 'ant, emmet, pismire',\n",
      " 311: 'grasshopper, hopper',\n",
      " 312: 'cricket',\n",
      " 313: 'walking stick, walkingstick, stick insect',\n",
      " 314: 'cockroach, roach',\n",
      " 315: 'mantis, mantid',\n",
      " 316: 'cicada, cicala',\n",
      " 317: 'leafhopper',\n",
      " 318: 'lacewing, lacewing fly',\n",
      " 319: \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\",\n",
      " 320: 'damselfly',\n",
      " 321: 'admiral',\n",
      " 322: 'ringlet, ringlet butterfly',\n",
      " 323: 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus',\n",
      " 324: 'cabbage butterfly',\n",
      " 325: 'sulphur butterfly, sulfur butterfly',\n",
      " 326: 'lycaenid, lycaenid butterfly',\n",
      " 327: 'starfish, sea star',\n",
      " 328: 'sea urchin',\n",
      " 329: 'sea cucumber, holothurian',\n",
      " 330: 'wood rabbit, cottontail, cottontail rabbit',\n",
      " 331: 'hare',\n",
      " 332: 'Angora, Angora rabbit',\n",
      " 333: 'hamster',\n",
      " 334: 'porcupine, hedgehog',\n",
      " 335: 'fox squirrel, eastern fox squirrel, Sciurus niger',\n",
      " 336: 'marmot',\n",
      " 337: 'beaver',\n",
      " 338: 'guinea pig, Cavia cobaya',\n",
      " 339: 'sorrel',\n",
      " 340: 'zebra',\n",
      " 341: 'hog, pig, grunter, squealer, Sus scrofa',\n",
      " 342: 'wild boar, boar, Sus scrofa',\n",
      " 343: 'warthog',\n",
      " 344: 'hippopotamus, hippo, river horse, Hippopotamus amphibius',\n",
      " 345: 'ox',\n",
      " 346: 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis',\n",
      " 347: 'bison',\n",
      " 348: 'ram, tup',\n",
      " 349: 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis',\n",
      " 350: 'ibex, Capra ibex',\n",
      " 351: 'hartebeest',\n",
      " 352: 'impala, Aepyceros melampus',\n",
      " 353: 'gazelle',\n",
      " 354: 'Arabian camel, dromedary, Camelus dromedarius',\n",
      " 355: 'llama',\n",
      " 356: 'weasel',\n",
      " 357: 'mink',\n",
      " 358: 'polecat, fitch, foulmart, foumart, Mustela putorius',\n",
      " 359: 'black-footed ferret, ferret, Mustela nigripes',\n",
      " 360: 'otter',\n",
      " 361: 'skunk, polecat, wood pussy',\n",
      " 362: 'badger',\n",
      " 363: 'armadillo',\n",
      " 364: 'three-toed sloth, ai, Bradypus tridactylus',\n",
      " 365: 'orangutan, orang, orangutang, Pongo pygmaeus',\n",
      " 366: 'gorilla, Gorilla gorilla',\n",
      " 367: 'chimpanzee, chimp, Pan troglodytes',\n",
      " 368: 'gibbon, Hylobates lar',\n",
      " 369: 'siamang, Hylobates syndactylus, Symphalangus syndactylus',\n",
      " 370: 'guenon, guenon monkey',\n",
      " 371: 'patas, hussar monkey, Erythrocebus patas',\n",
      " 372: 'baboon',\n",
      " 373: 'macaque',\n",
      " 374: 'langur',\n",
      " 375: 'colobus, colobus monkey',\n",
      " 376: 'proboscis monkey, Nasalis larvatus',\n",
      " 377: 'marmoset',\n",
      " 378: 'capuchin, ringtail, Cebus capucinus',\n",
      " 379: 'howler monkey, howler',\n",
      " 380: 'titi, titi monkey',\n",
      " 381: 'spider monkey, Ateles geoffroyi',\n",
      " 382: 'squirrel monkey, Saimiri sciureus',\n",
      " 383: 'Madagascar cat, ring-tailed lemur, Lemur catta',\n",
      " 384: 'indri, indris, Indri indri, Indri brevicaudatus',\n",
      " 385: 'Indian elephant, Elephas maximus',\n",
      " 386: 'African elephant, Loxodonta africana',\n",
      " 387: 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens',\n",
      " 388: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca',\n",
      " 389: 'barracouta, snoek',\n",
      " 390: 'eel',\n",
      " 391: 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch',\n",
      " 392: 'rock beauty, Holocanthus tricolor',\n",
      " 393: 'anemone fish',\n",
      " 394: 'sturgeon',\n",
      " 395: 'gar, garfish, garpike, billfish, Lepisosteus osseus',\n",
      " 396: 'lionfish',\n",
      " 397: 'puffer, pufferfish, blowfish, globefish',\n",
      " 398: 'abacus',\n",
      " 399: 'abaya',\n",
      " 400: \"academic gown, academic robe, judge's robe\",\n",
      " 401: 'accordion, piano accordion, squeeze box',\n",
      " 402: 'acoustic guitar',\n",
      " 403: 'aircraft carrier, carrier, flattop, attack aircraft carrier',\n",
      " 404: 'airliner',\n",
      " 405: 'airship, dirigible',\n",
      " 406: 'altar',\n",
      " 407: 'ambulance',\n",
      " 408: 'amphibian, amphibious vehicle',\n",
      " 409: 'analog clock',\n",
      " 410: 'apiary, bee house',\n",
      " 411: 'apron',\n",
      " 412: 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin',\n",
      " 413: 'assault rifle, assault gun',\n",
      " 414: 'backpack, back pack, knapsack, packsack, rucksack, haversack',\n",
      " 415: 'bakery, bakeshop, bakehouse',\n",
      " 416: 'balance beam, beam',\n",
      " 417: 'balloon',\n",
      " 418: 'ballpoint, ballpoint pen, ballpen, Biro',\n",
      " 419: 'Band Aid',\n",
      " 420: 'banjo',\n",
      " 421: 'bannister, banister, balustrade, balusters, handrail',\n",
      " 422: 'barbell',\n",
      " 423: 'barber chair',\n",
      " 424: 'barbershop',\n",
      " 425: 'barn',\n",
      " 426: 'barometer',\n",
      " 427: 'barrel, cask',\n",
      " 428: 'barrow, garden cart, lawn cart, wheelbarrow',\n",
      " 429: 'baseball',\n",
      " 430: 'basketball',\n",
      " 431: 'bassinet',\n",
      " 432: 'bassoon',\n",
      " 433: 'bathing cap, swimming cap',\n",
      " 434: 'bath towel',\n",
      " 435: 'bathtub, bathing tub, bath, tub',\n",
      " 436: 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon',\n",
      " 437: 'beacon, lighthouse, beacon light, pharos',\n",
      " 438: 'beaker',\n",
      " 439: 'bearskin, busby, shako',\n",
      " 440: 'beer bottle',\n",
      " 441: 'beer glass',\n",
      " 442: 'bell cote, bell cot',\n",
      " 443: 'bib',\n",
      " 444: 'bicycle-built-for-two, tandem bicycle, tandem',\n",
      " 445: 'bikini, two-piece',\n",
      " 446: 'binder, ring-binder',\n",
      " 447: 'binoculars, field glasses, opera glasses',\n",
      " 448: 'birdhouse',\n",
      " 449: 'boathouse',\n",
      " 450: 'bobsled, bobsleigh, bob',\n",
      " 451: 'bolo tie, bolo, bola tie, bola',\n",
      " 452: 'bonnet, poke bonnet',\n",
      " 453: 'bookcase',\n",
      " 454: 'bookshop, bookstore, bookstall',\n",
      " 455: 'bottlecap',\n",
      " 456: 'bow',\n",
      " 457: 'bow tie, bow-tie, bowtie',\n",
      " 458: 'brass, memorial tablet, plaque',\n",
      " 459: 'brassiere, bra, bandeau',\n",
      " 460: 'breakwater, groin, groyne, mole, bulwark, seawall, jetty',\n",
      " 461: 'breastplate, aegis, egis',\n",
      " 462: 'broom',\n",
      " 463: 'bucket, pail',\n",
      " 464: 'buckle',\n",
      " 465: 'bulletproof vest',\n",
      " 466: 'bullet train, bullet',\n",
      " 467: 'butcher shop, meat market',\n",
      " 468: 'cab, hack, taxi, taxicab',\n",
      " 469: 'caldron, cauldron',\n",
      " 470: 'candle, taper, wax light',\n",
      " 471: 'cannon',\n",
      " 472: 'canoe',\n",
      " 473: 'can opener, tin opener',\n",
      " 474: 'cardigan',\n",
      " 475: 'car mirror',\n",
      " 476: 'carousel, carrousel, merry-go-round, roundabout, whirligig',\n",
      " 477: \"carpenter's kit, tool kit\",\n",
      " 478: 'carton',\n",
      " 479: 'car wheel',\n",
      " 480: 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM',\n",
      " 481: 'cassette',\n",
      " 482: 'cassette player',\n",
      " 483: 'castle',\n",
      " 484: 'catamaran',\n",
      " 485: 'CD player',\n",
      " 486: 'cello, violoncello',\n",
      " 487: 'cellular telephone, cellular phone, cellphone, cell, mobile phone',\n",
      " 488: 'chain',\n",
      " 489: 'chainlink fence',\n",
      " 490: 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour',\n",
      " 491: 'chain saw, chainsaw',\n",
      " 492: 'chest',\n",
      " 493: 'chiffonier, commode',\n",
      " 494: 'chime, bell, gong',\n",
      " 495: 'china cabinet, china closet',\n",
      " 496: 'Christmas stocking',\n",
      " 497: 'church, church building',\n",
      " 498: 'cinema, movie theater, movie theatre, movie house, picture palace',\n",
      " 499: 'cleaver, meat cleaver, chopper',\n",
      " 500: 'cliff dwelling',\n",
      " 501: 'cloak',\n",
      " 502: 'clog, geta, patten, sabot',\n",
      " 503: 'cocktail shaker',\n",
      " 504: 'coffee mug',\n",
      " 505: 'coffeepot',\n",
      " 506: 'coil, spiral, volute, whorl, helix',\n",
      " 507: 'combination lock',\n",
      " 508: 'computer keyboard, keypad',\n",
      " 509: 'confectionery, confectionary, candy store',\n",
      " 510: 'container ship, containership, container vessel',\n",
      " 511: 'convertible',\n",
      " 512: 'corkscrew, bottle screw',\n",
      " 513: 'cornet, horn, trumpet, trump',\n",
      " 514: 'cowboy boot',\n",
      " 515: 'cowboy hat, ten-gallon hat',\n",
      " 516: 'cradle',\n",
      " 517: 'crane',\n",
      " 518: 'crash helmet',\n",
      " 519: 'crate',\n",
      " 520: 'crib, cot',\n",
      " 521: 'Crock Pot',\n",
      " 522: 'croquet ball',\n",
      " 523: 'crutch',\n",
      " 524: 'cuirass',\n",
      " 525: 'dam, dike, dyke',\n",
      " 526: 'desk',\n",
      " 527: 'desktop computer',\n",
      " 528: 'dial telephone, dial phone',\n",
      " 529: 'diaper, nappy, napkin',\n",
      " 530: 'digital clock',\n",
      " 531: 'digital watch',\n",
      " 532: 'dining table, board',\n",
      " 533: 'dishrag, dishcloth',\n",
      " 534: 'dishwasher, dish washer, dishwashing machine',\n",
      " 535: 'disk brake, disc brake',\n",
      " 536: 'dock, dockage, docking facility',\n",
      " 537: 'dogsled, dog sled, dog sleigh',\n",
      " 538: 'dome',\n",
      " 539: 'doormat, welcome mat',\n",
      " 540: 'drilling platform, offshore rig',\n",
      " 541: 'drum, membranophone, tympan',\n",
      " 542: 'drumstick',\n",
      " 543: 'dumbbell',\n",
      " 544: 'Dutch oven',\n",
      " 545: 'electric fan, blower',\n",
      " 546: 'electric guitar',\n",
      " 547: 'electric locomotive',\n",
      " 548: 'entertainment center',\n",
      " 549: 'envelope',\n",
      " 550: 'espresso maker',\n",
      " 551: 'face powder',\n",
      " 552: 'feather boa, boa',\n",
      " 553: 'file, file cabinet, filing cabinet',\n",
      " 554: 'fireboat',\n",
      " 555: 'fire engine, fire truck',\n",
      " 556: 'fire screen, fireguard',\n",
      " 557: 'flagpole, flagstaff',\n",
      " 558: 'flute, transverse flute',\n",
      " 559: 'folding chair',\n",
      " 560: 'football helmet',\n",
      " 561: 'forklift',\n",
      " 562: 'fountain',\n",
      " 563: 'fountain pen',\n",
      " 564: 'four-poster',\n",
      " 565: 'freight car',\n",
      " 566: 'French horn, horn',\n",
      " 567: 'frying pan, frypan, skillet',\n",
      " 568: 'fur coat',\n",
      " 569: 'garbage truck, dustcart',\n",
      " 570: 'gasmask, respirator, gas helmet',\n",
      " 571: 'gas pump, gasoline pump, petrol pump, island dispenser',\n",
      " 572: 'goblet',\n",
      " 573: 'go-kart',\n",
      " 574: 'golf ball',\n",
      " 575: 'golfcart, golf cart',\n",
      " 576: 'gondola',\n",
      " 577: 'gong, tam-tam',\n",
      " 578: 'gown',\n",
      " 579: 'grand piano, grand',\n",
      " 580: 'greenhouse, nursery, glasshouse',\n",
      " 581: 'grille, radiator grille',\n",
      " 582: 'grocery store, grocery, food market, market',\n",
      " 583: 'guillotine',\n",
      " 584: 'hair slide',\n",
      " 585: 'hair spray',\n",
      " 586: 'half track',\n",
      " 587: 'hammer',\n",
      " 588: 'hamper',\n",
      " 589: 'hand blower, blow dryer, blow drier, hair dryer, hair drier',\n",
      " 590: 'hand-held computer, hand-held microcomputer',\n",
      " 591: 'handkerchief, hankie, hanky, hankey',\n",
      " 592: 'hard disc, hard disk, fixed disk',\n",
      " 593: 'harmonica, mouth organ, harp, mouth harp',\n",
      " 594: 'harp',\n",
      " 595: 'harvester, reaper',\n",
      " 596: 'hatchet',\n",
      " 597: 'holster',\n",
      " 598: 'home theater, home theatre',\n",
      " 599: 'honeycomb',\n",
      " 600: 'hook, claw',\n",
      " 601: 'hoopskirt, crinoline',\n",
      " 602: 'horizontal bar, high bar',\n",
      " 603: 'horse cart, horse-cart',\n",
      " 604: 'hourglass',\n",
      " 605: 'iPod',\n",
      " 606: 'iron, smoothing iron',\n",
      " 607: \"jack-o'-lantern\",\n",
      " 608: 'jean, blue jean, denim',\n",
      " 609: 'jeep, landrover',\n",
      " 610: 'jersey, T-shirt, tee shirt',\n",
      " 611: 'jigsaw puzzle',\n",
      " 612: 'jinrikisha, ricksha, rickshaw',\n",
      " 613: 'joystick',\n",
      " 614: 'kimono',\n",
      " 615: 'knee pad',\n",
      " 616: 'knot',\n",
      " 617: 'lab coat, laboratory coat',\n",
      " 618: 'ladle',\n",
      " 619: 'lampshade, lamp shade',\n",
      " 620: 'laptop, laptop computer',\n",
      " 621: 'lawn mower, mower',\n",
      " 622: 'lens cap, lens cover',\n",
      " 623: 'letter opener, paper knife, paperknife',\n",
      " 624: 'library',\n",
      " 625: 'lifeboat',\n",
      " 626: 'lighter, light, igniter, ignitor',\n",
      " 627: 'limousine, limo',\n",
      " 628: 'liner, ocean liner',\n",
      " 629: 'lipstick, lip rouge',\n",
      " 630: 'Loafer',\n",
      " 631: 'lotion',\n",
      " 632: 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system',\n",
      " 633: \"loupe, jeweler's loupe\",\n",
      " 634: 'lumbermill, sawmill',\n",
      " 635: 'magnetic compass',\n",
      " 636: 'mailbag, postbag',\n",
      " 637: 'mailbox, letter box',\n",
      " 638: 'maillot',\n",
      " 639: 'maillot, tank suit',\n",
      " 640: 'manhole cover',\n",
      " 641: 'maraca',\n",
      " 642: 'marimba, xylophone',\n",
      " 643: 'mask',\n",
      " 644: 'matchstick',\n",
      " 645: 'maypole',\n",
      " 646: 'maze, labyrinth',\n",
      " 647: 'measuring cup',\n",
      " 648: 'medicine chest, medicine cabinet',\n",
      " 649: 'megalith, megalithic structure',\n",
      " 650: 'microphone, mike',\n",
      " 651: 'microwave, microwave oven',\n",
      " 652: 'military uniform',\n",
      " 653: 'milk can',\n",
      " 654: 'minibus',\n",
      " 655: 'miniskirt, mini',\n",
      " 656: 'minivan',\n",
      " 657: 'missile',\n",
      " 658: 'mitten',\n",
      " 659: 'mixing bowl',\n",
      " 660: 'mobile home, manufactured home',\n",
      " 661: 'Model T',\n",
      " 662: 'modem',\n",
      " 663: 'monastery',\n",
      " 664: 'monitor',\n",
      " 665: 'moped',\n",
      " 666: 'mortar',\n",
      " 667: 'mortarboard',\n",
      " 668: 'mosque',\n",
      " 669: 'mosquito net',\n",
      " 670: 'motor scooter, scooter',\n",
      " 671: 'mountain bike, all-terrain bike, off-roader',\n",
      " 672: 'mountain tent',\n",
      " 673: 'mouse, computer mouse',\n",
      " 674: 'mousetrap',\n",
      " 675: 'moving van',\n",
      " 676: 'muzzle',\n",
      " 677: 'nail',\n",
      " 678: 'neck brace',\n",
      " 679: 'necklace',\n",
      " 680: 'nipple',\n",
      " 681: 'notebook, notebook computer',\n",
      " 682: 'obelisk',\n",
      " 683: 'oboe, hautboy, hautbois',\n",
      " 684: 'ocarina, sweet potato',\n",
      " 685: 'odometer, hodometer, mileometer, milometer',\n",
      " 686: 'oil filter',\n",
      " 687: 'organ, pipe organ',\n",
      " 688: 'oscilloscope, scope, cathode-ray oscilloscope, CRO',\n",
      " 689: 'overskirt',\n",
      " 690: 'oxcart',\n",
      " 691: 'oxygen mask',\n",
      " 692: 'packet',\n",
      " 693: 'paddle, boat paddle',\n",
      " 694: 'paddlewheel, paddle wheel',\n",
      " 695: 'padlock',\n",
      " 696: 'paintbrush',\n",
      " 697: \"pajama, pyjama, pj's, jammies\",\n",
      " 698: 'palace',\n",
      " 699: 'panpipe, pandean pipe, syrinx',\n",
      " 700: 'paper towel',\n",
      " 701: 'parachute, chute',\n",
      " 702: 'parallel bars, bars',\n",
      " 703: 'park bench',\n",
      " 704: 'parking meter',\n",
      " 705: 'passenger car, coach, carriage',\n",
      " 706: 'patio, terrace',\n",
      " 707: 'pay-phone, pay-station',\n",
      " 708: 'pedestal, plinth, footstall',\n",
      " 709: 'pencil box, pencil case',\n",
      " 710: 'pencil sharpener',\n",
      " 711: 'perfume, essence',\n",
      " 712: 'Petri dish',\n",
      " 713: 'photocopier',\n",
      " 714: 'pick, plectrum, plectron',\n",
      " 715: 'pickelhaube',\n",
      " 716: 'picket fence, paling',\n",
      " 717: 'pickup, pickup truck',\n",
      " 718: 'pier',\n",
      " 719: 'piggy bank, penny bank',\n",
      " 720: 'pill bottle',\n",
      " 721: 'pillow',\n",
      " 722: 'ping-pong ball',\n",
      " 723: 'pinwheel',\n",
      " 724: 'pirate, pirate ship',\n",
      " 725: 'pitcher, ewer',\n",
      " 726: \"plane, carpenter's plane, woodworking plane\",\n",
      " 727: 'planetarium',\n",
      " 728: 'plastic bag',\n",
      " 729: 'plate rack',\n",
      " 730: 'plow, plough',\n",
      " 731: \"plunger, plumber's helper\",\n",
      " 732: 'Polaroid camera, Polaroid Land camera',\n",
      " 733: 'pole',\n",
      " 734: 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria',\n",
      " 735: 'poncho',\n",
      " 736: 'pool table, billiard table, snooker table',\n",
      " 737: 'pop bottle, soda bottle',\n",
      " 738: 'pot, flowerpot',\n",
      " 739: \"potter's wheel\",\n",
      " 740: 'power drill',\n",
      " 741: 'prayer rug, prayer mat',\n",
      " 742: 'printer',\n",
      " 743: 'prison, prison house',\n",
      " 744: 'projectile, missile',\n",
      " 745: 'projector',\n",
      " 746: 'puck, hockey puck',\n",
      " 747: 'punching bag, punch bag, punching ball, punchball',\n",
      " 748: 'purse',\n",
      " 749: 'quill, quill pen',\n",
      " 750: 'quilt, comforter, comfort, puff',\n",
      " 751: 'racer, race car, racing car',\n",
      " 752: 'racket, racquet',\n",
      " 753: 'radiator',\n",
      " 754: 'radio, wireless',\n",
      " 755: 'radio telescope, radio reflector',\n",
      " 756: 'rain barrel',\n",
      " 757: 'recreational vehicle, RV, R.V.',\n",
      " 758: 'reel',\n",
      " 759: 'reflex camera',\n",
      " 760: 'refrigerator, icebox',\n",
      " 761: 'remote control, remote',\n",
      " 762: 'restaurant, eating house, eating place, eatery',\n",
      " 763: 'revolver, six-gun, six-shooter',\n",
      " 764: 'rifle',\n",
      " 765: 'rocking chair, rocker',\n",
      " 766: 'rotisserie',\n",
      " 767: 'rubber eraser, rubber, pencil eraser',\n",
      " 768: 'rugby ball',\n",
      " 769: 'rule, ruler',\n",
      " 770: 'running shoe',\n",
      " 771: 'safe',\n",
      " 772: 'safety pin',\n",
      " 773: 'saltshaker, salt shaker',\n",
      " 774: 'sandal',\n",
      " 775: 'sarong',\n",
      " 776: 'sax, saxophone',\n",
      " 777: 'scabbard',\n",
      " 778: 'scale, weighing machine',\n",
      " 779: 'school bus',\n",
      " 780: 'schooner',\n",
      " 781: 'scoreboard',\n",
      " 782: 'screen, CRT screen',\n",
      " 783: 'screw',\n",
      " 784: 'screwdriver',\n",
      " 785: 'seat belt, seatbelt',\n",
      " 786: 'sewing machine',\n",
      " 787: 'shield, buckler',\n",
      " 788: 'shoe shop, shoe-shop, shoe store',\n",
      " 789: 'shoji',\n",
      " 790: 'shopping basket',\n",
      " 791: 'shopping cart',\n",
      " 792: 'shovel',\n",
      " 793: 'shower cap',\n",
      " 794: 'shower curtain',\n",
      " 795: 'ski',\n",
      " 796: 'ski mask',\n",
      " 797: 'sleeping bag',\n",
      " 798: 'slide rule, slipstick',\n",
      " 799: 'sliding door',\n",
      " 800: 'slot, one-armed bandit',\n",
      " 801: 'snorkel',\n",
      " 802: 'snowmobile',\n",
      " 803: 'snowplow, snowplough',\n",
      " 804: 'soap dispenser',\n",
      " 805: 'soccer ball',\n",
      " 806: 'sock',\n",
      " 807: 'solar dish, solar collector, solar furnace',\n",
      " 808: 'sombrero',\n",
      " 809: 'soup bowl',\n",
      " 810: 'space bar',\n",
      " 811: 'space heater',\n",
      " 812: 'space shuttle',\n",
      " 813: 'spatula',\n",
      " 814: 'speedboat',\n",
      " 815: \"spider web, spider's web\",\n",
      " 816: 'spindle',\n",
      " 817: 'sports car, sport car',\n",
      " 818: 'spotlight, spot',\n",
      " 819: 'stage',\n",
      " 820: 'steam locomotive',\n",
      " 821: 'steel arch bridge',\n",
      " 822: 'steel drum',\n",
      " 823: 'stethoscope',\n",
      " 824: 'stole',\n",
      " 825: 'stone wall',\n",
      " 826: 'stopwatch, stop watch',\n",
      " 827: 'stove',\n",
      " 828: 'strainer',\n",
      " 829: 'streetcar, tram, tramcar, trolley, trolley car',\n",
      " 830: 'stretcher',\n",
      " 831: 'studio couch, day bed',\n",
      " 832: 'stupa, tope',\n",
      " 833: 'submarine, pigboat, sub, U-boat',\n",
      " 834: 'suit, suit of clothes',\n",
      " 835: 'sundial',\n",
      " 836: 'sunglass',\n",
      " 837: 'sunglasses, dark glasses, shades',\n",
      " 838: 'sunscreen, sunblock, sun blocker',\n",
      " 839: 'suspension bridge',\n",
      " 840: 'swab, swob, mop',\n",
      " 841: 'sweatshirt',\n",
      " 842: 'swimming trunks, bathing trunks',\n",
      " 843: 'swing',\n",
      " 844: 'switch, electric switch, electrical switch',\n",
      " 845: 'syringe',\n",
      " 846: 'table lamp',\n",
      " 847: 'tank, army tank, armored combat vehicle, armoured combat vehicle',\n",
      " 848: 'tape player',\n",
      " 849: 'teapot',\n",
      " 850: 'teddy, teddy bear',\n",
      " 851: 'television, television system',\n",
      " 852: 'tennis ball',\n",
      " 853: 'thatch, thatched roof',\n",
      " 854: 'theater curtain, theatre curtain',\n",
      " 855: 'thimble',\n",
      " 856: 'thresher, thrasher, threshing machine',\n",
      " 857: 'throne',\n",
      " 858: 'tile roof',\n",
      " 859: 'toaster',\n",
      " 860: 'tobacco shop, tobacconist shop, tobacconist',\n",
      " 861: 'toilet seat',\n",
      " 862: 'torch',\n",
      " 863: 'totem pole',\n",
      " 864: 'tow truck, tow car, wrecker',\n",
      " 865: 'toyshop',\n",
      " 866: 'tractor',\n",
      " 867: 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi',\n",
      " 868: 'tray',\n",
      " 869: 'trench coat',\n",
      " 870: 'tricycle, trike, velocipede',\n",
      " 871: 'trimaran',\n",
      " 872: 'tripod',\n",
      " 873: 'triumphal arch',\n",
      " 874: 'trolleybus, trolley coach, trackless trolley',\n",
      " 875: 'trombone',\n",
      " 876: 'tub, vat',\n",
      " 877: 'turnstile',\n",
      " 878: 'typewriter keyboard',\n",
      " 879: 'umbrella',\n",
      " 880: 'unicycle, monocycle',\n",
      " 881: 'upright, upright piano',\n",
      " 882: 'vacuum, vacuum cleaner',\n",
      " 883: 'vase',\n",
      " 884: 'vault',\n",
      " 885: 'velvet',\n",
      " 886: 'vending machine',\n",
      " 887: 'vestment',\n",
      " 888: 'viaduct',\n",
      " 889: 'violin, fiddle',\n",
      " 890: 'volleyball',\n",
      " 891: 'waffle iron',\n",
      " 892: 'wall clock',\n",
      " 893: 'wallet, billfold, notecase, pocketbook',\n",
      " 894: 'wardrobe, closet, press',\n",
      " 895: 'warplane, military plane',\n",
      " 896: 'washbasin, handbasin, washbowl, lavabo, wash-hand basin',\n",
      " 897: 'washer, automatic washer, washing machine',\n",
      " 898: 'water bottle',\n",
      " 899: 'water jug',\n",
      " 900: 'water tower',\n",
      " 901: 'whiskey jug',\n",
      " 902: 'whistle',\n",
      " 903: 'wig',\n",
      " 904: 'window screen',\n",
      " 905: 'window shade',\n",
      " 906: 'Windsor tie',\n",
      " 907: 'wine bottle',\n",
      " 908: 'wing',\n",
      " 909: 'wok',\n",
      " 910: 'wooden spoon',\n",
      " 911: 'wool, woolen, woollen',\n",
      " 912: 'worm fence, snake fence, snake-rail fence, Virginia fence',\n",
      " 913: 'wreck',\n",
      " 914: 'yawl',\n",
      " 915: 'yurt',\n",
      " 916: 'web site, website, internet site, site',\n",
      " 917: 'comic book',\n",
      " 918: 'crossword puzzle, crossword',\n",
      " 919: 'street sign',\n",
      " 920: 'traffic light, traffic signal, stoplight',\n",
      " 921: 'book jacket, dust cover, dust jacket, dust wrapper',\n",
      " 922: 'menu',\n",
      " 923: 'plate',\n",
      " 924: 'guacamole',\n",
      " 925: 'consomme',\n",
      " 926: 'hot pot, hotpot',\n",
      " 927: 'trifle',\n",
      " 928: 'ice cream, icecream',\n",
      " 929: 'ice lolly, lolly, lollipop, popsicle',\n",
      " 930: 'French loaf',\n",
      " 931: 'bagel, beigel',\n",
      " 932: 'pretzel',\n",
      " 933: 'cheeseburger',\n",
      " 934: 'hotdog, hot dog, red hot',\n",
      " 935: 'mashed potato',\n",
      " 936: 'head cabbage',\n",
      " 937: 'broccoli',\n",
      " 938: 'cauliflower',\n",
      " 939: 'zucchini, courgette',\n",
      " 940: 'spaghetti squash',\n",
      " 941: 'acorn squash',\n",
      " 942: 'butternut squash',\n",
      " 943: 'cucumber, cuke',\n",
      " 944: 'artichoke, globe artichoke',\n",
      " 945: 'bell pepper',\n",
      " 946: 'cardoon',\n",
      " 947: 'mushroom',\n",
      " 948: 'Granny Smith',\n",
      " 949: 'strawberry',\n",
      " 950: 'orange',\n",
      " 951: 'lemon',\n",
      " 952: 'fig',\n",
      " 953: 'pineapple, ananas',\n",
      " 954: 'banana',\n",
      " 955: 'jackfruit, jak, jack',\n",
      " 956: 'custard apple',\n",
      " 957: 'pomegranate',\n",
      " 958: 'hay',\n",
      " 959: 'carbonara',\n",
      " 960: 'chocolate sauce, chocolate syrup',\n",
      " 961: 'dough',\n",
      " 962: 'meat loaf, meatloaf',\n",
      " 963: 'pizza, pizza pie',\n",
      " 964: 'potpie',\n",
      " 965: 'burrito',\n",
      " 966: 'red wine',\n",
      " 967: 'espresso',\n",
      " 968: 'cup',\n",
      " 969: 'eggnog',\n",
      " 970: 'alp',\n",
      " 971: 'bubble',\n",
      " 972: 'cliff, drop, drop-off',\n",
      " 973: 'coral reef',\n",
      " 974: 'geyser',\n",
      " 975: 'lakeside, lakeshore',\n",
      " 976: 'promontory, headland, head, foreland',\n",
      " 977: 'sandbar, sand bar',\n",
      " 978: 'seashore, coast, seacoast, sea-coast',\n",
      " 979: 'valley, vale',\n",
      " 980: 'volcano',\n",
      " 981: 'ballplayer, baseball player',\n",
      " 982: 'groom, bridegroom',\n",
      " 983: 'scuba diver',\n",
      " 984: 'rapeseed',\n",
      " 985: 'daisy',\n",
      " 986: \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\",\n",
      " 987: 'corn',\n",
      " 988: 'acorn',\n",
      " 989: 'hip, rose hip, rosehip',\n",
      " 990: 'buckeye, horse chestnut, conker',\n",
      " 991: 'coral fungus',\n",
      " 992: 'agaric',\n",
      " 993: 'gyromitra',\n",
      " 994: 'stinkhorn, carrion fungus',\n",
      " 995: 'earthstar',\n",
      " 996: 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa',\n",
      " 997: 'bolete',\n",
      " 998: 'ear, spike, capitulum',\n",
      " 999: 'toilet tissue, toilet paper, bathroom tissue'}\n",
      "59/2:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 380\n",
      "\n",
      "def prep_image(image):\n",
      "    res = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "    res = res.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "    return res\n",
      "\n",
      "def predict(image):\n",
      "    cropped_image = center_crop(image)\n",
      "    prediction = (model.predict(prep_image(image)), model.predict(prep_image(cropped_image)))\n",
      "    if np.max(prediction[0]>=prediction[1]):\n",
      "        return np.argmax(prediction[0], axis=1)[0]\n",
      "    else:\n",
      "        return np.argmax(prediction[1], axis=1)[1]\n",
      "    \n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "plt.imshow(image)\n",
      "\n",
      "print(class_dict[predict(image)])\n",
      "61/1:\n",
      "from tensorflow.keras.applications import EfficientNetB4\n",
      "from tensorflow.image import resize\n",
      "model = EfficientNetB4(weights='imagenet')\n",
      "\n",
      "def center_crop(img, new_width=None, new_height=None):        \n",
      "\n",
      "    width = img.shape[1]\n",
      "    height = img.shape[0]\n",
      "\n",
      "    if new_width is None:\n",
      "        new_width = min(width, height)\n",
      "\n",
      "    if new_height is None:\n",
      "        new_height = min(width, height)\n",
      "\n",
      "    left = int(np.ceil((width - new_width) / 2))\n",
      "    right = width - int(np.floor((width - new_width) / 2))\n",
      "\n",
      "    top = int(np.ceil((height - new_height) / 2))\n",
      "    bottom = height - int(np.floor((height - new_height) / 2))\n",
      "\n",
      "    if len(img.shape) == 2:\n",
      "        center_cropped_img = img[top:bottom, left:right]\n",
      "    else:\n",
      "        center_cropped_img = img[top:bottom, left:right, ...]\n",
      "\n",
      "    return center_cropped_img\n",
      "\n",
      "class_dict = {0: 'tench, Tinca tinca',\n",
      " 1: 'goldfish, Carassius auratus',\n",
      " 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n",
      " 3: 'tiger shark, Galeocerdo cuvieri',\n",
      " 4: 'hammerhead, hammerhead shark',\n",
      " 5: 'electric ray, crampfish, numbfish, torpedo',\n",
      " 6: 'stingray',\n",
      " 7: 'cock',\n",
      " 8: 'hen',\n",
      " 9: 'ostrich, Struthio camelus',\n",
      " 10: 'brambling, Fringilla montifringilla',\n",
      " 11: 'goldfinch, Carduelis carduelis',\n",
      " 12: 'house finch, linnet, Carpodacus mexicanus',\n",
      " 13: 'junco, snowbird',\n",
      " 14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea',\n",
      " 15: 'robin, American robin, Turdus migratorius',\n",
      " 16: 'bulbul',\n",
      " 17: 'jay',\n",
      " 18: 'magpie',\n",
      " 19: 'chickadee',\n",
      " 20: 'water ouzel, dipper',\n",
      " 21: 'kite',\n",
      " 22: 'bald eagle, American eagle, Haliaeetus leucocephalus',\n",
      " 23: 'vulture',\n",
      " 24: 'great grey owl, great gray owl, Strix nebulosa',\n",
      " 25: 'European fire salamander, Salamandra salamandra',\n",
      " 26: 'common newt, Triturus vulgaris',\n",
      " 27: 'eft',\n",
      " 28: 'spotted salamander, Ambystoma maculatum',\n",
      " 29: 'axolotl, mud puppy, Ambystoma mexicanum',\n",
      " 30: 'bullfrog, Rana catesbeiana',\n",
      " 31: 'tree frog, tree-frog',\n",
      " 32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui',\n",
      " 33: 'loggerhead, loggerhead turtle, Caretta caretta',\n",
      " 34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea',\n",
      " 35: 'mud turtle',\n",
      " 36: 'terrapin',\n",
      " 37: 'box turtle, box tortoise',\n",
      " 38: 'banded gecko',\n",
      " 39: 'common iguana, iguana, Iguana iguana',\n",
      " 40: 'American chameleon, anole, Anolis carolinensis',\n",
      " 41: 'whiptail, whiptail lizard',\n",
      " 42: 'agama',\n",
      " 43: 'frilled lizard, Chlamydosaurus kingi',\n",
      " 44: 'alligator lizard',\n",
      " 45: 'Gila monster, Heloderma suspectum',\n",
      " 46: 'green lizard, Lacerta viridis',\n",
      " 47: 'African chameleon, Chamaeleo chamaeleon',\n",
      " 48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis',\n",
      " 49: 'African crocodile, Nile crocodile, Crocodylus niloticus',\n",
      " 50: 'American alligator, Alligator mississipiensis',\n",
      " 51: 'triceratops',\n",
      " 52: 'thunder snake, worm snake, Carphophis amoenus',\n",
      " 53: 'ringneck snake, ring-necked snake, ring snake',\n",
      " 54: 'hognose snake, puff adder, sand viper',\n",
      " 55: 'green snake, grass snake',\n",
      " 56: 'king snake, kingsnake',\n",
      " 57: 'garter snake, grass snake',\n",
      " 58: 'water snake',\n",
      " 59: 'vine snake',\n",
      " 60: 'night snake, Hypsiglena torquata',\n",
      " 61: 'boa constrictor, Constrictor constrictor',\n",
      " 62: 'rock python, rock snake, Python sebae',\n",
      " 63: 'Indian cobra, Naja naja',\n",
      " 64: 'green mamba',\n",
      " 65: 'sea snake',\n",
      " 66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus',\n",
      " 67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus',\n",
      " 68: 'sidewinder, horned rattlesnake, Crotalus cerastes',\n",
      " 69: 'trilobite',\n",
      " 70: 'harvestman, daddy longlegs, Phalangium opilio',\n",
      " 71: 'scorpion',\n",
      " 72: 'black and gold garden spider, Argiope aurantia',\n",
      " 73: 'barn spider, Araneus cavaticus',\n",
      " 74: 'garden spider, Aranea diademata',\n",
      " 75: 'black widow, Latrodectus mactans',\n",
      " 76: 'tarantula',\n",
      " 77: 'wolf spider, hunting spider',\n",
      " 78: 'tick',\n",
      " 79: 'centipede',\n",
      " 80: 'black grouse',\n",
      " 81: 'ptarmigan',\n",
      " 82: 'ruffed grouse, partridge, Bonasa umbellus',\n",
      " 83: 'prairie chicken, prairie grouse, prairie fowl',\n",
      " 84: 'peacock',\n",
      " 85: 'quail',\n",
      " 86: 'partridge',\n",
      " 87: 'African grey, African gray, Psittacus erithacus',\n",
      " 88: 'macaw',\n",
      " 89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n",
      " 90: 'lorikeet',\n",
      " 91: 'coucal',\n",
      " 92: 'bee eater',\n",
      " 93: 'hornbill',\n",
      " 94: 'hummingbird',\n",
      " 95: 'jacamar',\n",
      " 96: 'toucan',\n",
      " 97: 'drake',\n",
      " 98: 'red-breasted merganser, Mergus serrator',\n",
      " 99: 'goose',\n",
      " 100: 'black swan, Cygnus atratus',\n",
      " 101: 'tusker',\n",
      " 102: 'echidna, spiny anteater, anteater',\n",
      " 103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n",
      " 104: 'wallaby, brush kangaroo',\n",
      " 105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n",
      " 106: 'wombat',\n",
      " 107: 'jellyfish',\n",
      " 108: 'sea anemone, anemone',\n",
      " 109: 'brain coral',\n",
      " 110: 'flatworm, platyhelminth',\n",
      " 111: 'nematode, nematode worm, roundworm',\n",
      " 112: 'conch',\n",
      " 113: 'snail',\n",
      " 114: 'slug',\n",
      " 115: 'sea slug, nudibranch',\n",
      " 116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore',\n",
      " 117: 'chambered nautilus, pearly nautilus, nautilus',\n",
      " 118: 'Dungeness crab, Cancer magister',\n",
      " 119: 'rock crab, Cancer irroratus',\n",
      " 120: 'fiddler crab',\n",
      " 121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica',\n",
      " 122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus',\n",
      " 123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish',\n",
      " 124: 'crayfish, crawfish, crawdad, crawdaddy',\n",
      " 125: 'hermit crab',\n",
      " 126: 'isopod',\n",
      " 127: 'white stork, Ciconia ciconia',\n",
      " 128: 'black stork, Ciconia nigra',\n",
      " 129: 'spoonbill',\n",
      " 130: 'flamingo',\n",
      " 131: 'little blue heron, Egretta caerulea',\n",
      " 132: 'American egret, great white heron, Egretta albus',\n",
      " 133: 'bittern',\n",
      " 134: 'crane',\n",
      " 135: 'limpkin, Aramus pictus',\n",
      " 136: 'European gallinule, Porphyrio porphyrio',\n",
      " 137: 'American coot, marsh hen, mud hen, water hen, Fulica americana',\n",
      " 138: 'bustard',\n",
      " 139: 'ruddy turnstone, Arenaria interpres',\n",
      " 140: 'red-backed sandpiper, dunlin, Erolia alpina',\n",
      " 141: 'redshank, Tringa totanus',\n",
      " 142: 'dowitcher',\n",
      " 143: 'oystercatcher, oyster catcher',\n",
      " 144: 'pelican',\n",
      " 145: 'king penguin, Aptenodytes patagonica',\n",
      " 146: 'albatross, mollymawk',\n",
      " 147: 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus',\n",
      " 148: 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca',\n",
      " 149: 'dugong, Dugong dugon',\n",
      " 150: 'sea lion',\n",
      " 151: 'Chihuahua',\n",
      " 152: 'Japanese spaniel',\n",
      " 153: 'Maltese dog, Maltese terrier, Maltese',\n",
      " 154: 'Pekinese, Pekingese, Peke',\n",
      " 155: 'Shih-Tzu',\n",
      " 156: 'Blenheim spaniel',\n",
      " 157: 'papillon',\n",
      " 158: 'toy terrier',\n",
      " 159: 'Rhodesian ridgeback',\n",
      " 160: 'Afghan hound, Afghan',\n",
      " 161: 'basset, basset hound',\n",
      " 162: 'beagle',\n",
      " 163: 'bloodhound, sleuthhound',\n",
      " 164: 'bluetick',\n",
      " 165: 'black-and-tan coonhound',\n",
      " 166: 'Walker hound, Walker foxhound',\n",
      " 167: 'English foxhound',\n",
      " 168: 'redbone',\n",
      " 169: 'borzoi, Russian wolfhound',\n",
      " 170: 'Irish wolfhound',\n",
      " 171: 'Italian greyhound',\n",
      " 172: 'whippet',\n",
      " 173: 'Ibizan hound, Ibizan Podenco',\n",
      " 174: 'Norwegian elkhound, elkhound',\n",
      " 175: 'otterhound, otter hound',\n",
      " 176: 'Saluki, gazelle hound',\n",
      " 177: 'Scottish deerhound, deerhound',\n",
      " 178: 'Weimaraner',\n",
      " 179: 'Staffordshire bullterrier, Staffordshire bull terrier',\n",
      " 180: 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier',\n",
      " 181: 'Bedlington terrier',\n",
      " 182: 'Border terrier',\n",
      " 183: 'Kerry blue terrier',\n",
      " 184: 'Irish terrier',\n",
      " 185: 'Norfolk terrier',\n",
      " 186: 'Norwich terrier',\n",
      " 187: 'Yorkshire terrier',\n",
      " 188: 'wire-haired fox terrier',\n",
      " 189: 'Lakeland terrier',\n",
      " 190: 'Sealyham terrier, Sealyham',\n",
      " 191: 'Airedale, Airedale terrier',\n",
      " 192: 'cairn, cairn terrier',\n",
      " 193: 'Australian terrier',\n",
      " 194: 'Dandie Dinmont, Dandie Dinmont terrier',\n",
      " 195: 'Boston bull, Boston terrier',\n",
      " 196: 'miniature schnauzer',\n",
      " 197: 'giant schnauzer',\n",
      " 198: 'standard schnauzer',\n",
      " 199: 'Scotch terrier, Scottish terrier, Scottie',\n",
      " 200: 'Tibetan terrier, chrysanthemum dog',\n",
      " 201: 'silky terrier, Sydney silky',\n",
      " 202: 'soft-coated wheaten terrier',\n",
      " 203: 'West Highland white terrier',\n",
      " 204: 'Lhasa, Lhasa apso',\n",
      " 205: 'flat-coated retriever',\n",
      " 206: 'curly-coated retriever',\n",
      " 207: 'golden retriever',\n",
      " 208: 'Labrador retriever',\n",
      " 209: 'Chesapeake Bay retriever',\n",
      " 210: 'German short-haired pointer',\n",
      " 211: 'vizsla, Hungarian pointer',\n",
      " 212: 'English setter',\n",
      " 213: 'Irish setter, red setter',\n",
      " 214: 'Gordon setter',\n",
      " 215: 'Brittany spaniel',\n",
      " 216: 'clumber, clumber spaniel',\n",
      " 217: 'English springer, English springer spaniel',\n",
      " 218: 'Welsh springer spaniel',\n",
      " 219: 'cocker spaniel, English cocker spaniel, cocker',\n",
      " 220: 'Sussex spaniel',\n",
      " 221: 'Irish water spaniel',\n",
      " 222: 'kuvasz',\n",
      " 223: 'schipperke',\n",
      " 224: 'groenendael',\n",
      " 225: 'malinois',\n",
      " 226: 'briard',\n",
      " 227: 'kelpie',\n",
      " 228: 'komondor',\n",
      " 229: 'Old English sheepdog, bobtail',\n",
      " 230: 'Shetland sheepdog, Shetland sheep dog, Shetland',\n",
      " 231: 'collie',\n",
      " 232: 'Border collie',\n",
      " 233: 'Bouvier des Flandres, Bouviers des Flandres',\n",
      " 234: 'Rottweiler',\n",
      " 235: 'German shepherd, German shepherd dog, German police dog, alsatian',\n",
      " 236: 'Doberman, Doberman pinscher',\n",
      " 237: 'miniature pinscher',\n",
      " 238: 'Greater Swiss Mountain dog',\n",
      " 239: 'Bernese mountain dog',\n",
      " 240: 'Appenzeller',\n",
      " 241: 'EntleBucher',\n",
      " 242: 'boxer',\n",
      " 243: 'bull mastiff',\n",
      " 244: 'Tibetan mastiff',\n",
      " 245: 'French bulldog',\n",
      " 246: 'Great Dane',\n",
      " 247: 'Saint Bernard, St Bernard',\n",
      " 248: 'Eskimo dog, husky',\n",
      " 249: 'malamute, malemute, Alaskan malamute',\n",
      " 250: 'Siberian husky',\n",
      " 251: 'dalmatian, coach dog, carriage dog',\n",
      " 252: 'affenpinscher, monkey pinscher, monkey dog',\n",
      " 253: 'basenji',\n",
      " 254: 'pug, pug-dog',\n",
      " 255: 'Leonberg',\n",
      " 256: 'Newfoundland, Newfoundland dog',\n",
      " 257: 'Great Pyrenees',\n",
      " 258: 'Samoyed, Samoyede',\n",
      " 259: 'Pomeranian',\n",
      " 260: 'chow, chow chow',\n",
      " 261: 'keeshond',\n",
      " 262: 'Brabancon griffon',\n",
      " 263: 'Pembroke, Pembroke Welsh corgi',\n",
      " 264: 'Cardigan, Cardigan Welsh corgi',\n",
      " 265: 'toy poodle',\n",
      " 266: 'miniature poodle',\n",
      " 267: 'standard poodle',\n",
      " 268: 'Mexican hairless',\n",
      " 269: 'timber wolf, grey wolf, gray wolf, Canis lupus',\n",
      " 270: 'white wolf, Arctic wolf, Canis lupus tundrarum',\n",
      " 271: 'red wolf, maned wolf, Canis rufus, Canis niger',\n",
      " 272: 'coyote, prairie wolf, brush wolf, Canis latrans',\n",
      " 273: 'dingo, warrigal, warragal, Canis dingo',\n",
      " 274: 'dhole, Cuon alpinus',\n",
      " 275: 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus',\n",
      " 276: 'hyena, hyaena',\n",
      " 277: 'red fox, Vulpes vulpes',\n",
      " 278: 'kit fox, Vulpes macrotis',\n",
      " 279: 'Arctic fox, white fox, Alopex lagopus',\n",
      " 280: 'grey fox, gray fox, Urocyon cinereoargenteus',\n",
      " 281: 'tabby, tabby cat',\n",
      " 282: 'tiger cat',\n",
      " 283: 'Persian cat',\n",
      " 284: 'Siamese cat, Siamese',\n",
      " 285: 'Egyptian cat',\n",
      " 286: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n",
      " 287: 'lynx, catamount',\n",
      " 288: 'leopard, Panthera pardus',\n",
      " 289: 'snow leopard, ounce, Panthera uncia',\n",
      " 290: 'jaguar, panther, Panthera onca, Felis onca',\n",
      " 291: 'lion, king of beasts, Panthera leo',\n",
      " 292: 'tiger, Panthera tigris',\n",
      " 293: 'cheetah, chetah, Acinonyx jubatus',\n",
      " 294: 'brown bear, bruin, Ursus arctos',\n",
      " 295: 'American black bear, black bear, Ursus americanus, Euarctos americanus',\n",
      " 296: 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus',\n",
      " 297: 'sloth bear, Melursus ursinus, Ursus ursinus',\n",
      " 298: 'mongoose',\n",
      " 299: 'meerkat, mierkat',\n",
      " 300: 'tiger beetle',\n",
      " 301: 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle',\n",
      " 302: 'ground beetle, carabid beetle',\n",
      " 303: 'long-horned beetle, longicorn, longicorn beetle',\n",
      " 304: 'leaf beetle, chrysomelid',\n",
      " 305: 'dung beetle',\n",
      " 306: 'rhinoceros beetle',\n",
      " 307: 'weevil',\n",
      " 308: 'fly',\n",
      " 309: 'bee',\n",
      " 310: 'ant, emmet, pismire',\n",
      " 311: 'grasshopper, hopper',\n",
      " 312: 'cricket',\n",
      " 313: 'walking stick, walkingstick, stick insect',\n",
      " 314: 'cockroach, roach',\n",
      " 315: 'mantis, mantid',\n",
      " 316: 'cicada, cicala',\n",
      " 317: 'leafhopper',\n",
      " 318: 'lacewing, lacewing fly',\n",
      " 319: \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\",\n",
      " 320: 'damselfly',\n",
      " 321: 'admiral',\n",
      " 322: 'ringlet, ringlet butterfly',\n",
      " 323: 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus',\n",
      " 324: 'cabbage butterfly',\n",
      " 325: 'sulphur butterfly, sulfur butterfly',\n",
      " 326: 'lycaenid, lycaenid butterfly',\n",
      " 327: 'starfish, sea star',\n",
      " 328: 'sea urchin',\n",
      " 329: 'sea cucumber, holothurian',\n",
      " 330: 'wood rabbit, cottontail, cottontail rabbit',\n",
      " 331: 'hare',\n",
      " 332: 'Angora, Angora rabbit',\n",
      " 333: 'hamster',\n",
      " 334: 'porcupine, hedgehog',\n",
      " 335: 'fox squirrel, eastern fox squirrel, Sciurus niger',\n",
      " 336: 'marmot',\n",
      " 337: 'beaver',\n",
      " 338: 'guinea pig, Cavia cobaya',\n",
      " 339: 'sorrel',\n",
      " 340: 'zebra',\n",
      " 341: 'hog, pig, grunter, squealer, Sus scrofa',\n",
      " 342: 'wild boar, boar, Sus scrofa',\n",
      " 343: 'warthog',\n",
      " 344: 'hippopotamus, hippo, river horse, Hippopotamus amphibius',\n",
      " 345: 'ox',\n",
      " 346: 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis',\n",
      " 347: 'bison',\n",
      " 348: 'ram, tup',\n",
      " 349: 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis',\n",
      " 350: 'ibex, Capra ibex',\n",
      " 351: 'hartebeest',\n",
      " 352: 'impala, Aepyceros melampus',\n",
      " 353: 'gazelle',\n",
      " 354: 'Arabian camel, dromedary, Camelus dromedarius',\n",
      " 355: 'llama',\n",
      " 356: 'weasel',\n",
      " 357: 'mink',\n",
      " 358: 'polecat, fitch, foulmart, foumart, Mustela putorius',\n",
      " 359: 'black-footed ferret, ferret, Mustela nigripes',\n",
      " 360: 'otter',\n",
      " 361: 'skunk, polecat, wood pussy',\n",
      " 362: 'badger',\n",
      " 363: 'armadillo',\n",
      " 364: 'three-toed sloth, ai, Bradypus tridactylus',\n",
      " 365: 'orangutan, orang, orangutang, Pongo pygmaeus',\n",
      " 366: 'gorilla, Gorilla gorilla',\n",
      " 367: 'chimpanzee, chimp, Pan troglodytes',\n",
      " 368: 'gibbon, Hylobates lar',\n",
      " 369: 'siamang, Hylobates syndactylus, Symphalangus syndactylus',\n",
      " 370: 'guenon, guenon monkey',\n",
      " 371: 'patas, hussar monkey, Erythrocebus patas',\n",
      " 372: 'baboon',\n",
      " 373: 'macaque',\n",
      " 374: 'langur',\n",
      " 375: 'colobus, colobus monkey',\n",
      " 376: 'proboscis monkey, Nasalis larvatus',\n",
      " 377: 'marmoset',\n",
      " 378: 'capuchin, ringtail, Cebus capucinus',\n",
      " 379: 'howler monkey, howler',\n",
      " 380: 'titi, titi monkey',\n",
      " 381: 'spider monkey, Ateles geoffroyi',\n",
      " 382: 'squirrel monkey, Saimiri sciureus',\n",
      " 383: 'Madagascar cat, ring-tailed lemur, Lemur catta',\n",
      " 384: 'indri, indris, Indri indri, Indri brevicaudatus',\n",
      " 385: 'Indian elephant, Elephas maximus',\n",
      " 386: 'African elephant, Loxodonta africana',\n",
      " 387: 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens',\n",
      " 388: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca',\n",
      " 389: 'barracouta, snoek',\n",
      " 390: 'eel',\n",
      " 391: 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch',\n",
      " 392: 'rock beauty, Holocanthus tricolor',\n",
      " 393: 'anemone fish',\n",
      " 394: 'sturgeon',\n",
      " 395: 'gar, garfish, garpike, billfish, Lepisosteus osseus',\n",
      " 396: 'lionfish',\n",
      " 397: 'puffer, pufferfish, blowfish, globefish',\n",
      " 398: 'abacus',\n",
      " 399: 'abaya',\n",
      " 400: \"academic gown, academic robe, judge's robe\",\n",
      " 401: 'accordion, piano accordion, squeeze box',\n",
      " 402: 'acoustic guitar',\n",
      " 403: 'aircraft carrier, carrier, flattop, attack aircraft carrier',\n",
      " 404: 'airliner',\n",
      " 405: 'airship, dirigible',\n",
      " 406: 'altar',\n",
      " 407: 'ambulance',\n",
      " 408: 'amphibian, amphibious vehicle',\n",
      " 409: 'analog clock',\n",
      " 410: 'apiary, bee house',\n",
      " 411: 'apron',\n",
      " 412: 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin',\n",
      " 413: 'assault rifle, assault gun',\n",
      " 414: 'backpack, back pack, knapsack, packsack, rucksack, haversack',\n",
      " 415: 'bakery, bakeshop, bakehouse',\n",
      " 416: 'balance beam, beam',\n",
      " 417: 'balloon',\n",
      " 418: 'ballpoint, ballpoint pen, ballpen, Biro',\n",
      " 419: 'Band Aid',\n",
      " 420: 'banjo',\n",
      " 421: 'bannister, banister, balustrade, balusters, handrail',\n",
      " 422: 'barbell',\n",
      " 423: 'barber chair',\n",
      " 424: 'barbershop',\n",
      " 425: 'barn',\n",
      " 426: 'barometer',\n",
      " 427: 'barrel, cask',\n",
      " 428: 'barrow, garden cart, lawn cart, wheelbarrow',\n",
      " 429: 'baseball',\n",
      " 430: 'basketball',\n",
      " 431: 'bassinet',\n",
      " 432: 'bassoon',\n",
      " 433: 'bathing cap, swimming cap',\n",
      " 434: 'bath towel',\n",
      " 435: 'bathtub, bathing tub, bath, tub',\n",
      " 436: 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon',\n",
      " 437: 'beacon, lighthouse, beacon light, pharos',\n",
      " 438: 'beaker',\n",
      " 439: 'bearskin, busby, shako',\n",
      " 440: 'beer bottle',\n",
      " 441: 'beer glass',\n",
      " 442: 'bell cote, bell cot',\n",
      " 443: 'bib',\n",
      " 444: 'bicycle-built-for-two, tandem bicycle, tandem',\n",
      " 445: 'bikini, two-piece',\n",
      " 446: 'binder, ring-binder',\n",
      " 447: 'binoculars, field glasses, opera glasses',\n",
      " 448: 'birdhouse',\n",
      " 449: 'boathouse',\n",
      " 450: 'bobsled, bobsleigh, bob',\n",
      " 451: 'bolo tie, bolo, bola tie, bola',\n",
      " 452: 'bonnet, poke bonnet',\n",
      " 453: 'bookcase',\n",
      " 454: 'bookshop, bookstore, bookstall',\n",
      " 455: 'bottlecap',\n",
      " 456: 'bow',\n",
      " 457: 'bow tie, bow-tie, bowtie',\n",
      " 458: 'brass, memorial tablet, plaque',\n",
      " 459: 'brassiere, bra, bandeau',\n",
      " 460: 'breakwater, groin, groyne, mole, bulwark, seawall, jetty',\n",
      " 461: 'breastplate, aegis, egis',\n",
      " 462: 'broom',\n",
      " 463: 'bucket, pail',\n",
      " 464: 'buckle',\n",
      " 465: 'bulletproof vest',\n",
      " 466: 'bullet train, bullet',\n",
      " 467: 'butcher shop, meat market',\n",
      " 468: 'cab, hack, taxi, taxicab',\n",
      " 469: 'caldron, cauldron',\n",
      " 470: 'candle, taper, wax light',\n",
      " 471: 'cannon',\n",
      " 472: 'canoe',\n",
      " 473: 'can opener, tin opener',\n",
      " 474: 'cardigan',\n",
      " 475: 'car mirror',\n",
      " 476: 'carousel, carrousel, merry-go-round, roundabout, whirligig',\n",
      " 477: \"carpenter's kit, tool kit\",\n",
      " 478: 'carton',\n",
      " 479: 'car wheel',\n",
      " 480: 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM',\n",
      " 481: 'cassette',\n",
      " 482: 'cassette player',\n",
      " 483: 'castle',\n",
      " 484: 'catamaran',\n",
      " 485: 'CD player',\n",
      " 486: 'cello, violoncello',\n",
      " 487: 'cellular telephone, cellular phone, cellphone, cell, mobile phone',\n",
      " 488: 'chain',\n",
      " 489: 'chainlink fence',\n",
      " 490: 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour',\n",
      " 491: 'chain saw, chainsaw',\n",
      " 492: 'chest',\n",
      " 493: 'chiffonier, commode',\n",
      " 494: 'chime, bell, gong',\n",
      " 495: 'china cabinet, china closet',\n",
      " 496: 'Christmas stocking',\n",
      " 497: 'church, church building',\n",
      " 498: 'cinema, movie theater, movie theatre, movie house, picture palace',\n",
      " 499: 'cleaver, meat cleaver, chopper',\n",
      " 500: 'cliff dwelling',\n",
      " 501: 'cloak',\n",
      " 502: 'clog, geta, patten, sabot',\n",
      " 503: 'cocktail shaker',\n",
      " 504: 'coffee mug',\n",
      " 505: 'coffeepot',\n",
      " 506: 'coil, spiral, volute, whorl, helix',\n",
      " 507: 'combination lock',\n",
      " 508: 'computer keyboard, keypad',\n",
      " 509: 'confectionery, confectionary, candy store',\n",
      " 510: 'container ship, containership, container vessel',\n",
      " 511: 'convertible',\n",
      " 512: 'corkscrew, bottle screw',\n",
      " 513: 'cornet, horn, trumpet, trump',\n",
      " 514: 'cowboy boot',\n",
      " 515: 'cowboy hat, ten-gallon hat',\n",
      " 516: 'cradle',\n",
      " 517: 'crane',\n",
      " 518: 'crash helmet',\n",
      " 519: 'crate',\n",
      " 520: 'crib, cot',\n",
      " 521: 'Crock Pot',\n",
      " 522: 'croquet ball',\n",
      " 523: 'crutch',\n",
      " 524: 'cuirass',\n",
      " 525: 'dam, dike, dyke',\n",
      " 526: 'desk',\n",
      " 527: 'desktop computer',\n",
      " 528: 'dial telephone, dial phone',\n",
      " 529: 'diaper, nappy, napkin',\n",
      " 530: 'digital clock',\n",
      " 531: 'digital watch',\n",
      " 532: 'dining table, board',\n",
      " 533: 'dishrag, dishcloth',\n",
      " 534: 'dishwasher, dish washer, dishwashing machine',\n",
      " 535: 'disk brake, disc brake',\n",
      " 536: 'dock, dockage, docking facility',\n",
      " 537: 'dogsled, dog sled, dog sleigh',\n",
      " 538: 'dome',\n",
      " 539: 'doormat, welcome mat',\n",
      " 540: 'drilling platform, offshore rig',\n",
      " 541: 'drum, membranophone, tympan',\n",
      " 542: 'drumstick',\n",
      " 543: 'dumbbell',\n",
      " 544: 'Dutch oven',\n",
      " 545: 'electric fan, blower',\n",
      " 546: 'electric guitar',\n",
      " 547: 'electric locomotive',\n",
      " 548: 'entertainment center',\n",
      " 549: 'envelope',\n",
      " 550: 'espresso maker',\n",
      " 551: 'face powder',\n",
      " 552: 'feather boa, boa',\n",
      " 553: 'file, file cabinet, filing cabinet',\n",
      " 554: 'fireboat',\n",
      " 555: 'fire engine, fire truck',\n",
      " 556: 'fire screen, fireguard',\n",
      " 557: 'flagpole, flagstaff',\n",
      " 558: 'flute, transverse flute',\n",
      " 559: 'folding chair',\n",
      " 560: 'football helmet',\n",
      " 561: 'forklift',\n",
      " 562: 'fountain',\n",
      " 563: 'fountain pen',\n",
      " 564: 'four-poster',\n",
      " 565: 'freight car',\n",
      " 566: 'French horn, horn',\n",
      " 567: 'frying pan, frypan, skillet',\n",
      " 568: 'fur coat',\n",
      " 569: 'garbage truck, dustcart',\n",
      " 570: 'gasmask, respirator, gas helmet',\n",
      " 571: 'gas pump, gasoline pump, petrol pump, island dispenser',\n",
      " 572: 'goblet',\n",
      " 573: 'go-kart',\n",
      " 574: 'golf ball',\n",
      " 575: 'golfcart, golf cart',\n",
      " 576: 'gondola',\n",
      " 577: 'gong, tam-tam',\n",
      " 578: 'gown',\n",
      " 579: 'grand piano, grand',\n",
      " 580: 'greenhouse, nursery, glasshouse',\n",
      " 581: 'grille, radiator grille',\n",
      " 582: 'grocery store, grocery, food market, market',\n",
      " 583: 'guillotine',\n",
      " 584: 'hair slide',\n",
      " 585: 'hair spray',\n",
      " 586: 'half track',\n",
      " 587: 'hammer',\n",
      " 588: 'hamper',\n",
      " 589: 'hand blower, blow dryer, blow drier, hair dryer, hair drier',\n",
      " 590: 'hand-held computer, hand-held microcomputer',\n",
      " 591: 'handkerchief, hankie, hanky, hankey',\n",
      " 592: 'hard disc, hard disk, fixed disk',\n",
      " 593: 'harmonica, mouth organ, harp, mouth harp',\n",
      " 594: 'harp',\n",
      " 595: 'harvester, reaper',\n",
      " 596: 'hatchet',\n",
      " 597: 'holster',\n",
      " 598: 'home theater, home theatre',\n",
      " 599: 'honeycomb',\n",
      " 600: 'hook, claw',\n",
      " 601: 'hoopskirt, crinoline',\n",
      " 602: 'horizontal bar, high bar',\n",
      " 603: 'horse cart, horse-cart',\n",
      " 604: 'hourglass',\n",
      " 605: 'iPod',\n",
      " 606: 'iron, smoothing iron',\n",
      " 607: \"jack-o'-lantern\",\n",
      " 608: 'jean, blue jean, denim',\n",
      " 609: 'jeep, landrover',\n",
      " 610: 'jersey, T-shirt, tee shirt',\n",
      " 611: 'jigsaw puzzle',\n",
      " 612: 'jinrikisha, ricksha, rickshaw',\n",
      " 613: 'joystick',\n",
      " 614: 'kimono',\n",
      " 615: 'knee pad',\n",
      " 616: 'knot',\n",
      " 617: 'lab coat, laboratory coat',\n",
      " 618: 'ladle',\n",
      " 619: 'lampshade, lamp shade',\n",
      " 620: 'laptop, laptop computer',\n",
      " 621: 'lawn mower, mower',\n",
      " 622: 'lens cap, lens cover',\n",
      " 623: 'letter opener, paper knife, paperknife',\n",
      " 624: 'library',\n",
      " 625: 'lifeboat',\n",
      " 626: 'lighter, light, igniter, ignitor',\n",
      " 627: 'limousine, limo',\n",
      " 628: 'liner, ocean liner',\n",
      " 629: 'lipstick, lip rouge',\n",
      " 630: 'Loafer',\n",
      " 631: 'lotion',\n",
      " 632: 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system',\n",
      " 633: \"loupe, jeweler's loupe\",\n",
      " 634: 'lumbermill, sawmill',\n",
      " 635: 'magnetic compass',\n",
      " 636: 'mailbag, postbag',\n",
      " 637: 'mailbox, letter box',\n",
      " 638: 'maillot',\n",
      " 639: 'maillot, tank suit',\n",
      " 640: 'manhole cover',\n",
      " 641: 'maraca',\n",
      " 642: 'marimba, xylophone',\n",
      " 643: 'mask',\n",
      " 644: 'matchstick',\n",
      " 645: 'maypole',\n",
      " 646: 'maze, labyrinth',\n",
      " 647: 'measuring cup',\n",
      " 648: 'medicine chest, medicine cabinet',\n",
      " 649: 'megalith, megalithic structure',\n",
      " 650: 'microphone, mike',\n",
      " 651: 'microwave, microwave oven',\n",
      " 652: 'military uniform',\n",
      " 653: 'milk can',\n",
      " 654: 'minibus',\n",
      " 655: 'miniskirt, mini',\n",
      " 656: 'minivan',\n",
      " 657: 'missile',\n",
      " 658: 'mitten',\n",
      " 659: 'mixing bowl',\n",
      " 660: 'mobile home, manufactured home',\n",
      " 661: 'Model T',\n",
      " 662: 'modem',\n",
      " 663: 'monastery',\n",
      " 664: 'monitor',\n",
      " 665: 'moped',\n",
      " 666: 'mortar',\n",
      " 667: 'mortarboard',\n",
      " 668: 'mosque',\n",
      " 669: 'mosquito net',\n",
      " 670: 'motor scooter, scooter',\n",
      " 671: 'mountain bike, all-terrain bike, off-roader',\n",
      " 672: 'mountain tent',\n",
      " 673: 'mouse, computer mouse',\n",
      " 674: 'mousetrap',\n",
      " 675: 'moving van',\n",
      " 676: 'muzzle',\n",
      " 677: 'nail',\n",
      " 678: 'neck brace',\n",
      " 679: 'necklace',\n",
      " 680: 'nipple',\n",
      " 681: 'notebook, notebook computer',\n",
      " 682: 'obelisk',\n",
      " 683: 'oboe, hautboy, hautbois',\n",
      " 684: 'ocarina, sweet potato',\n",
      " 685: 'odometer, hodometer, mileometer, milometer',\n",
      " 686: 'oil filter',\n",
      " 687: 'organ, pipe organ',\n",
      " 688: 'oscilloscope, scope, cathode-ray oscilloscope, CRO',\n",
      " 689: 'overskirt',\n",
      " 690: 'oxcart',\n",
      " 691: 'oxygen mask',\n",
      " 692: 'packet',\n",
      " 693: 'paddle, boat paddle',\n",
      " 694: 'paddlewheel, paddle wheel',\n",
      " 695: 'padlock',\n",
      " 696: 'paintbrush',\n",
      " 697: \"pajama, pyjama, pj's, jammies\",\n",
      " 698: 'palace',\n",
      " 699: 'panpipe, pandean pipe, syrinx',\n",
      " 700: 'paper towel',\n",
      " 701: 'parachute, chute',\n",
      " 702: 'parallel bars, bars',\n",
      " 703: 'park bench',\n",
      " 704: 'parking meter',\n",
      " 705: 'passenger car, coach, carriage',\n",
      " 706: 'patio, terrace',\n",
      " 707: 'pay-phone, pay-station',\n",
      " 708: 'pedestal, plinth, footstall',\n",
      " 709: 'pencil box, pencil case',\n",
      " 710: 'pencil sharpener',\n",
      " 711: 'perfume, essence',\n",
      " 712: 'Petri dish',\n",
      " 713: 'photocopier',\n",
      " 714: 'pick, plectrum, plectron',\n",
      " 715: 'pickelhaube',\n",
      " 716: 'picket fence, paling',\n",
      " 717: 'pickup, pickup truck',\n",
      " 718: 'pier',\n",
      " 719: 'piggy bank, penny bank',\n",
      " 720: 'pill bottle',\n",
      " 721: 'pillow',\n",
      " 722: 'ping-pong ball',\n",
      " 723: 'pinwheel',\n",
      " 724: 'pirate, pirate ship',\n",
      " 725: 'pitcher, ewer',\n",
      " 726: \"plane, carpenter's plane, woodworking plane\",\n",
      " 727: 'planetarium',\n",
      " 728: 'plastic bag',\n",
      " 729: 'plate rack',\n",
      " 730: 'plow, plough',\n",
      " 731: \"plunger, plumber's helper\",\n",
      " 732: 'Polaroid camera, Polaroid Land camera',\n",
      " 733: 'pole',\n",
      " 734: 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria',\n",
      " 735: 'poncho',\n",
      " 736: 'pool table, billiard table, snooker table',\n",
      " 737: 'pop bottle, soda bottle',\n",
      " 738: 'pot, flowerpot',\n",
      " 739: \"potter's wheel\",\n",
      " 740: 'power drill',\n",
      " 741: 'prayer rug, prayer mat',\n",
      " 742: 'printer',\n",
      " 743: 'prison, prison house',\n",
      " 744: 'projectile, missile',\n",
      " 745: 'projector',\n",
      " 746: 'puck, hockey puck',\n",
      " 747: 'punching bag, punch bag, punching ball, punchball',\n",
      " 748: 'purse',\n",
      " 749: 'quill, quill pen',\n",
      " 750: 'quilt, comforter, comfort, puff',\n",
      " 751: 'racer, race car, racing car',\n",
      " 752: 'racket, racquet',\n",
      " 753: 'radiator',\n",
      " 754: 'radio, wireless',\n",
      " 755: 'radio telescope, radio reflector',\n",
      " 756: 'rain barrel',\n",
      " 757: 'recreational vehicle, RV, R.V.',\n",
      " 758: 'reel',\n",
      " 759: 'reflex camera',\n",
      " 760: 'refrigerator, icebox',\n",
      " 761: 'remote control, remote',\n",
      " 762: 'restaurant, eating house, eating place, eatery',\n",
      " 763: 'revolver, six-gun, six-shooter',\n",
      " 764: 'rifle',\n",
      " 765: 'rocking chair, rocker',\n",
      " 766: 'rotisserie',\n",
      " 767: 'rubber eraser, rubber, pencil eraser',\n",
      " 768: 'rugby ball',\n",
      " 769: 'rule, ruler',\n",
      " 770: 'running shoe',\n",
      " 771: 'safe',\n",
      " 772: 'safety pin',\n",
      " 773: 'saltshaker, salt shaker',\n",
      " 774: 'sandal',\n",
      " 775: 'sarong',\n",
      " 776: 'sax, saxophone',\n",
      " 777: 'scabbard',\n",
      " 778: 'scale, weighing machine',\n",
      " 779: 'school bus',\n",
      " 780: 'schooner',\n",
      " 781: 'scoreboard',\n",
      " 782: 'screen, CRT screen',\n",
      " 783: 'screw',\n",
      " 784: 'screwdriver',\n",
      " 785: 'seat belt, seatbelt',\n",
      " 786: 'sewing machine',\n",
      " 787: 'shield, buckler',\n",
      " 788: 'shoe shop, shoe-shop, shoe store',\n",
      " 789: 'shoji',\n",
      " 790: 'shopping basket',\n",
      " 791: 'shopping cart',\n",
      " 792: 'shovel',\n",
      " 793: 'shower cap',\n",
      " 794: 'shower curtain',\n",
      " 795: 'ski',\n",
      " 796: 'ski mask',\n",
      " 797: 'sleeping bag',\n",
      " 798: 'slide rule, slipstick',\n",
      " 799: 'sliding door',\n",
      " 800: 'slot, one-armed bandit',\n",
      " 801: 'snorkel',\n",
      " 802: 'snowmobile',\n",
      " 803: 'snowplow, snowplough',\n",
      " 804: 'soap dispenser',\n",
      " 805: 'soccer ball',\n",
      " 806: 'sock',\n",
      " 807: 'solar dish, solar collector, solar furnace',\n",
      " 808: 'sombrero',\n",
      " 809: 'soup bowl',\n",
      " 810: 'space bar',\n",
      " 811: 'space heater',\n",
      " 812: 'space shuttle',\n",
      " 813: 'spatula',\n",
      " 814: 'speedboat',\n",
      " 815: \"spider web, spider's web\",\n",
      " 816: 'spindle',\n",
      " 817: 'sports car, sport car',\n",
      " 818: 'spotlight, spot',\n",
      " 819: 'stage',\n",
      " 820: 'steam locomotive',\n",
      " 821: 'steel arch bridge',\n",
      " 822: 'steel drum',\n",
      " 823: 'stethoscope',\n",
      " 824: 'stole',\n",
      " 825: 'stone wall',\n",
      " 826: 'stopwatch, stop watch',\n",
      " 827: 'stove',\n",
      " 828: 'strainer',\n",
      " 829: 'streetcar, tram, tramcar, trolley, trolley car',\n",
      " 830: 'stretcher',\n",
      " 831: 'studio couch, day bed',\n",
      " 832: 'stupa, tope',\n",
      " 833: 'submarine, pigboat, sub, U-boat',\n",
      " 834: 'suit, suit of clothes',\n",
      " 835: 'sundial',\n",
      " 836: 'sunglass',\n",
      " 837: 'sunglasses, dark glasses, shades',\n",
      " 838: 'sunscreen, sunblock, sun blocker',\n",
      " 839: 'suspension bridge',\n",
      " 840: 'swab, swob, mop',\n",
      " 841: 'sweatshirt',\n",
      " 842: 'swimming trunks, bathing trunks',\n",
      " 843: 'swing',\n",
      " 844: 'switch, electric switch, electrical switch',\n",
      " 845: 'syringe',\n",
      " 846: 'table lamp',\n",
      " 847: 'tank, army tank, armored combat vehicle, armoured combat vehicle',\n",
      " 848: 'tape player',\n",
      " 849: 'teapot',\n",
      " 850: 'teddy, teddy bear',\n",
      " 851: 'television, television system',\n",
      " 852: 'tennis ball',\n",
      " 853: 'thatch, thatched roof',\n",
      " 854: 'theater curtain, theatre curtain',\n",
      " 855: 'thimble',\n",
      " 856: 'thresher, thrasher, threshing machine',\n",
      " 857: 'throne',\n",
      " 858: 'tile roof',\n",
      " 859: 'toaster',\n",
      " 860: 'tobacco shop, tobacconist shop, tobacconist',\n",
      " 861: 'toilet seat',\n",
      " 862: 'torch',\n",
      " 863: 'totem pole',\n",
      " 864: 'tow truck, tow car, wrecker',\n",
      " 865: 'toyshop',\n",
      " 866: 'tractor',\n",
      " 867: 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi',\n",
      " 868: 'tray',\n",
      " 869: 'trench coat',\n",
      " 870: 'tricycle, trike, velocipede',\n",
      " 871: 'trimaran',\n",
      " 872: 'tripod',\n",
      " 873: 'triumphal arch',\n",
      " 874: 'trolleybus, trolley coach, trackless trolley',\n",
      " 875: 'trombone',\n",
      " 876: 'tub, vat',\n",
      " 877: 'turnstile',\n",
      " 878: 'typewriter keyboard',\n",
      " 879: 'umbrella',\n",
      " 880: 'unicycle, monocycle',\n",
      " 881: 'upright, upright piano',\n",
      " 882: 'vacuum, vacuum cleaner',\n",
      " 883: 'vase',\n",
      " 884: 'vault',\n",
      " 885: 'velvet',\n",
      " 886: 'vending machine',\n",
      " 887: 'vestment',\n",
      " 888: 'viaduct',\n",
      " 889: 'violin, fiddle',\n",
      " 890: 'volleyball',\n",
      " 891: 'waffle iron',\n",
      " 892: 'wall clock',\n",
      " 893: 'wallet, billfold, notecase, pocketbook',\n",
      " 894: 'wardrobe, closet, press',\n",
      " 895: 'warplane, military plane',\n",
      " 896: 'washbasin, handbasin, washbowl, lavabo, wash-hand basin',\n",
      " 897: 'washer, automatic washer, washing machine',\n",
      " 898: 'water bottle',\n",
      " 899: 'water jug',\n",
      " 900: 'water tower',\n",
      " 901: 'whiskey jug',\n",
      " 902: 'whistle',\n",
      " 903: 'wig',\n",
      " 904: 'window screen',\n",
      " 905: 'window shade',\n",
      " 906: 'Windsor tie',\n",
      " 907: 'wine bottle',\n",
      " 908: 'wing',\n",
      " 909: 'wok',\n",
      " 910: 'wooden spoon',\n",
      " 911: 'wool, woolen, woollen',\n",
      " 912: 'worm fence, snake fence, snake-rail fence, Virginia fence',\n",
      " 913: 'wreck',\n",
      " 914: 'yawl',\n",
      " 915: 'yurt',\n",
      " 916: 'web site, website, internet site, site',\n",
      " 917: 'comic book',\n",
      " 918: 'crossword puzzle, crossword',\n",
      " 919: 'street sign',\n",
      " 920: 'traffic light, traffic signal, stoplight',\n",
      " 921: 'book jacket, dust cover, dust jacket, dust wrapper',\n",
      " 922: 'menu',\n",
      " 923: 'plate',\n",
      " 924: 'guacamole',\n",
      " 925: 'consomme',\n",
      " 926: 'hot pot, hotpot',\n",
      " 927: 'trifle',\n",
      " 928: 'ice cream, icecream',\n",
      " 929: 'ice lolly, lolly, lollipop, popsicle',\n",
      " 930: 'French loaf',\n",
      " 931: 'bagel, beigel',\n",
      " 932: 'pretzel',\n",
      " 933: 'cheeseburger',\n",
      " 934: 'hotdog, hot dog, red hot',\n",
      " 935: 'mashed potato',\n",
      " 936: 'head cabbage',\n",
      " 937: 'broccoli',\n",
      " 938: 'cauliflower',\n",
      " 939: 'zucchini, courgette',\n",
      " 940: 'spaghetti squash',\n",
      " 941: 'acorn squash',\n",
      " 942: 'butternut squash',\n",
      " 943: 'cucumber, cuke',\n",
      " 944: 'artichoke, globe artichoke',\n",
      " 945: 'bell pepper',\n",
      " 946: 'cardoon',\n",
      " 947: 'mushroom',\n",
      " 948: 'Granny Smith',\n",
      " 949: 'strawberry',\n",
      " 950: 'orange',\n",
      " 951: 'lemon',\n",
      " 952: 'fig',\n",
      " 953: 'pineapple, ananas',\n",
      " 954: 'banana',\n",
      " 955: 'jackfruit, jak, jack',\n",
      " 956: 'custard apple',\n",
      " 957: 'pomegranate',\n",
      " 958: 'hay',\n",
      " 959: 'carbonara',\n",
      " 960: 'chocolate sauce, chocolate syrup',\n",
      " 961: 'dough',\n",
      " 962: 'meat loaf, meatloaf',\n",
      " 963: 'pizza, pizza pie',\n",
      " 964: 'potpie',\n",
      " 965: 'burrito',\n",
      " 966: 'red wine',\n",
      " 967: 'espresso',\n",
      " 968: 'cup',\n",
      " 969: 'eggnog',\n",
      " 970: 'alp',\n",
      " 971: 'bubble',\n",
      " 972: 'cliff, drop, drop-off',\n",
      " 973: 'coral reef',\n",
      " 974: 'geyser',\n",
      " 975: 'lakeside, lakeshore',\n",
      " 976: 'promontory, headland, head, foreland',\n",
      " 977: 'sandbar, sand bar',\n",
      " 978: 'seashore, coast, seacoast, sea-coast',\n",
      " 979: 'valley, vale',\n",
      " 980: 'volcano',\n",
      " 981: 'ballplayer, baseball player',\n",
      " 982: 'groom, bridegroom',\n",
      " 983: 'scuba diver',\n",
      " 984: 'rapeseed',\n",
      " 985: 'daisy',\n",
      " 986: \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\",\n",
      " 987: 'corn',\n",
      " 988: 'acorn',\n",
      " 989: 'hip, rose hip, rosehip',\n",
      " 990: 'buckeye, horse chestnut, conker',\n",
      " 991: 'coral fungus',\n",
      " 992: 'agaric',\n",
      " 993: 'gyromitra',\n",
      " 994: 'stinkhorn, carrion fungus',\n",
      " 995: 'earthstar',\n",
      " 996: 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa',\n",
      " 997: 'bolete',\n",
      " 998: 'ear, spike, capitulum',\n",
      " 999: 'toilet tissue, toilet paper, bathroom tissue'}\n",
      "61/2:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 380\n",
      "\n",
      "def prep_image(image):\n",
      "    res = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "    res = res.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "    return res\n",
      "\n",
      "def predict(image):\n",
      "    cropped_image = center_crop(image)\n",
      "    prediction = (model.predict(prep_image(image)), model.predict(prep_image(cropped_image)))\n",
      "    if np.max(prediction[0]>=prediction[1]):\n",
      "        return np.argmax(prediction[0], axis=1)[0]\n",
      "    else:\n",
      "        return np.argmax(prediction[1], axis=1)[1]\n",
      "    \n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "plt.imshow(image)\n",
      "\n",
      "print(class_dict[predict(image)])\n",
      "62/1:\n",
      "from tensorflow.keras.applications import EfficientNetB4\n",
      "from tensorflow.image import resize\n",
      "model = EfficientNetB4(weights='imagenet')\n",
      "\n",
      "def center_crop(img, new_width=None, new_height=None):        \n",
      "\n",
      "    width = img.shape[1]\n",
      "    height = img.shape[0]\n",
      "\n",
      "    if new_width is None:\n",
      "        new_width = min(width, height)\n",
      "\n",
      "    if new_height is None:\n",
      "        new_height = min(width, height)\n",
      "\n",
      "    left = int(np.ceil((width - new_width) / 2))\n",
      "    right = width - int(np.floor((width - new_width) / 2))\n",
      "\n",
      "    top = int(np.ceil((height - new_height) / 2))\n",
      "    bottom = height - int(np.floor((height - new_height) / 2))\n",
      "\n",
      "    if len(img.shape) == 2:\n",
      "        center_cropped_img = img[top:bottom, left:right]\n",
      "    else:\n",
      "        center_cropped_img = img[top:bottom, left:right, ...]\n",
      "\n",
      "    return center_cropped_img\n",
      "\n",
      "class_dict = {0: 'tench, Tinca tinca',\n",
      " 1: 'goldfish, Carassius auratus',\n",
      " 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n",
      " 3: 'tiger shark, Galeocerdo cuvieri',\n",
      " 4: 'hammerhead, hammerhead shark',\n",
      " 5: 'electric ray, crampfish, numbfish, torpedo',\n",
      " 6: 'stingray',\n",
      " 7: 'cock',\n",
      " 8: 'hen',\n",
      " 9: 'ostrich, Struthio camelus',\n",
      " 10: 'brambling, Fringilla montifringilla',\n",
      " 11: 'goldfinch, Carduelis carduelis',\n",
      " 12: 'house finch, linnet, Carpodacus mexicanus',\n",
      " 13: 'junco, snowbird',\n",
      " 14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea',\n",
      " 15: 'robin, American robin, Turdus migratorius',\n",
      " 16: 'bulbul',\n",
      " 17: 'jay',\n",
      " 18: 'magpie',\n",
      " 19: 'chickadee',\n",
      " 20: 'water ouzel, dipper',\n",
      " 21: 'kite',\n",
      " 22: 'bald eagle, American eagle, Haliaeetus leucocephalus',\n",
      " 23: 'vulture',\n",
      " 24: 'great grey owl, great gray owl, Strix nebulosa',\n",
      " 25: 'European fire salamander, Salamandra salamandra',\n",
      " 26: 'common newt, Triturus vulgaris',\n",
      " 27: 'eft',\n",
      " 28: 'spotted salamander, Ambystoma maculatum',\n",
      " 29: 'axolotl, mud puppy, Ambystoma mexicanum',\n",
      " 30: 'bullfrog, Rana catesbeiana',\n",
      " 31: 'tree frog, tree-frog',\n",
      " 32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui',\n",
      " 33: 'loggerhead, loggerhead turtle, Caretta caretta',\n",
      " 34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea',\n",
      " 35: 'mud turtle',\n",
      " 36: 'terrapin',\n",
      " 37: 'box turtle, box tortoise',\n",
      " 38: 'banded gecko',\n",
      " 39: 'common iguana, iguana, Iguana iguana',\n",
      " 40: 'American chameleon, anole, Anolis carolinensis',\n",
      " 41: 'whiptail, whiptail lizard',\n",
      " 42: 'agama',\n",
      " 43: 'frilled lizard, Chlamydosaurus kingi',\n",
      " 44: 'alligator lizard',\n",
      " 45: 'Gila monster, Heloderma suspectum',\n",
      " 46: 'green lizard, Lacerta viridis',\n",
      " 47: 'African chameleon, Chamaeleo chamaeleon',\n",
      " 48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis',\n",
      " 49: 'African crocodile, Nile crocodile, Crocodylus niloticus',\n",
      " 50: 'American alligator, Alligator mississipiensis',\n",
      " 51: 'triceratops',\n",
      " 52: 'thunder snake, worm snake, Carphophis amoenus',\n",
      " 53: 'ringneck snake, ring-necked snake, ring snake',\n",
      " 54: 'hognose snake, puff adder, sand viper',\n",
      " 55: 'green snake, grass snake',\n",
      " 56: 'king snake, kingsnake',\n",
      " 57: 'garter snake, grass snake',\n",
      " 58: 'water snake',\n",
      " 59: 'vine snake',\n",
      " 60: 'night snake, Hypsiglena torquata',\n",
      " 61: 'boa constrictor, Constrictor constrictor',\n",
      " 62: 'rock python, rock snake, Python sebae',\n",
      " 63: 'Indian cobra, Naja naja',\n",
      " 64: 'green mamba',\n",
      " 65: 'sea snake',\n",
      " 66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus',\n",
      " 67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus',\n",
      " 68: 'sidewinder, horned rattlesnake, Crotalus cerastes',\n",
      " 69: 'trilobite',\n",
      " 70: 'harvestman, daddy longlegs, Phalangium opilio',\n",
      " 71: 'scorpion',\n",
      " 72: 'black and gold garden spider, Argiope aurantia',\n",
      " 73: 'barn spider, Araneus cavaticus',\n",
      " 74: 'garden spider, Aranea diademata',\n",
      " 75: 'black widow, Latrodectus mactans',\n",
      " 76: 'tarantula',\n",
      " 77: 'wolf spider, hunting spider',\n",
      " 78: 'tick',\n",
      " 79: 'centipede',\n",
      " 80: 'black grouse',\n",
      " 81: 'ptarmigan',\n",
      " 82: 'ruffed grouse, partridge, Bonasa umbellus',\n",
      " 83: 'prairie chicken, prairie grouse, prairie fowl',\n",
      " 84: 'peacock',\n",
      " 85: 'quail',\n",
      " 86: 'partridge',\n",
      " 87: 'African grey, African gray, Psittacus erithacus',\n",
      " 88: 'macaw',\n",
      " 89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n",
      " 90: 'lorikeet',\n",
      " 91: 'coucal',\n",
      " 92: 'bee eater',\n",
      " 93: 'hornbill',\n",
      " 94: 'hummingbird',\n",
      " 95: 'jacamar',\n",
      " 96: 'toucan',\n",
      " 97: 'drake',\n",
      " 98: 'red-breasted merganser, Mergus serrator',\n",
      " 99: 'goose',\n",
      " 100: 'black swan, Cygnus atratus',\n",
      " 101: 'tusker',\n",
      " 102: 'echidna, spiny anteater, anteater',\n",
      " 103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n",
      " 104: 'wallaby, brush kangaroo',\n",
      " 105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n",
      " 106: 'wombat',\n",
      " 107: 'jellyfish',\n",
      " 108: 'sea anemone, anemone',\n",
      " 109: 'brain coral',\n",
      " 110: 'flatworm, platyhelminth',\n",
      " 111: 'nematode, nematode worm, roundworm',\n",
      " 112: 'conch',\n",
      " 113: 'snail',\n",
      " 114: 'slug',\n",
      " 115: 'sea slug, nudibranch',\n",
      " 116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore',\n",
      " 117: 'chambered nautilus, pearly nautilus, nautilus',\n",
      " 118: 'Dungeness crab, Cancer magister',\n",
      " 119: 'rock crab, Cancer irroratus',\n",
      " 120: 'fiddler crab',\n",
      " 121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica',\n",
      " 122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus',\n",
      " 123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish',\n",
      " 124: 'crayfish, crawfish, crawdad, crawdaddy',\n",
      " 125: 'hermit crab',\n",
      " 126: 'isopod',\n",
      " 127: 'white stork, Ciconia ciconia',\n",
      " 128: 'black stork, Ciconia nigra',\n",
      " 129: 'spoonbill',\n",
      " 130: 'flamingo',\n",
      " 131: 'little blue heron, Egretta caerulea',\n",
      " 132: 'American egret, great white heron, Egretta albus',\n",
      " 133: 'bittern',\n",
      " 134: 'crane',\n",
      " 135: 'limpkin, Aramus pictus',\n",
      " 136: 'European gallinule, Porphyrio porphyrio',\n",
      " 137: 'American coot, marsh hen, mud hen, water hen, Fulica americana',\n",
      " 138: 'bustard',\n",
      " 139: 'ruddy turnstone, Arenaria interpres',\n",
      " 140: 'red-backed sandpiper, dunlin, Erolia alpina',\n",
      " 141: 'redshank, Tringa totanus',\n",
      " 142: 'dowitcher',\n",
      " 143: 'oystercatcher, oyster catcher',\n",
      " 144: 'pelican',\n",
      " 145: 'king penguin, Aptenodytes patagonica',\n",
      " 146: 'albatross, mollymawk',\n",
      " 147: 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus',\n",
      " 148: 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca',\n",
      " 149: 'dugong, Dugong dugon',\n",
      " 150: 'sea lion',\n",
      " 151: 'Chihuahua',\n",
      " 152: 'Japanese spaniel',\n",
      " 153: 'Maltese dog, Maltese terrier, Maltese',\n",
      " 154: 'Pekinese, Pekingese, Peke',\n",
      " 155: 'Shih-Tzu',\n",
      " 156: 'Blenheim spaniel',\n",
      " 157: 'papillon',\n",
      " 158: 'toy terrier',\n",
      " 159: 'Rhodesian ridgeback',\n",
      " 160: 'Afghan hound, Afghan',\n",
      " 161: 'basset, basset hound',\n",
      " 162: 'beagle',\n",
      " 163: 'bloodhound, sleuthhound',\n",
      " 164: 'bluetick',\n",
      " 165: 'black-and-tan coonhound',\n",
      " 166: 'Walker hound, Walker foxhound',\n",
      " 167: 'English foxhound',\n",
      " 168: 'redbone',\n",
      " 169: 'borzoi, Russian wolfhound',\n",
      " 170: 'Irish wolfhound',\n",
      " 171: 'Italian greyhound',\n",
      " 172: 'whippet',\n",
      " 173: 'Ibizan hound, Ibizan Podenco',\n",
      " 174: 'Norwegian elkhound, elkhound',\n",
      " 175: 'otterhound, otter hound',\n",
      " 176: 'Saluki, gazelle hound',\n",
      " 177: 'Scottish deerhound, deerhound',\n",
      " 178: 'Weimaraner',\n",
      " 179: 'Staffordshire bullterrier, Staffordshire bull terrier',\n",
      " 180: 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier',\n",
      " 181: 'Bedlington terrier',\n",
      " 182: 'Border terrier',\n",
      " 183: 'Kerry blue terrier',\n",
      " 184: 'Irish terrier',\n",
      " 185: 'Norfolk terrier',\n",
      " 186: 'Norwich terrier',\n",
      " 187: 'Yorkshire terrier',\n",
      " 188: 'wire-haired fox terrier',\n",
      " 189: 'Lakeland terrier',\n",
      " 190: 'Sealyham terrier, Sealyham',\n",
      " 191: 'Airedale, Airedale terrier',\n",
      " 192: 'cairn, cairn terrier',\n",
      " 193: 'Australian terrier',\n",
      " 194: 'Dandie Dinmont, Dandie Dinmont terrier',\n",
      " 195: 'Boston bull, Boston terrier',\n",
      " 196: 'miniature schnauzer',\n",
      " 197: 'giant schnauzer',\n",
      " 198: 'standard schnauzer',\n",
      " 199: 'Scotch terrier, Scottish terrier, Scottie',\n",
      " 200: 'Tibetan terrier, chrysanthemum dog',\n",
      " 201: 'silky terrier, Sydney silky',\n",
      " 202: 'soft-coated wheaten terrier',\n",
      " 203: 'West Highland white terrier',\n",
      " 204: 'Lhasa, Lhasa apso',\n",
      " 205: 'flat-coated retriever',\n",
      " 206: 'curly-coated retriever',\n",
      " 207: 'golden retriever',\n",
      " 208: 'Labrador retriever',\n",
      " 209: 'Chesapeake Bay retriever',\n",
      " 210: 'German short-haired pointer',\n",
      " 211: 'vizsla, Hungarian pointer',\n",
      " 212: 'English setter',\n",
      " 213: 'Irish setter, red setter',\n",
      " 214: 'Gordon setter',\n",
      " 215: 'Brittany spaniel',\n",
      " 216: 'clumber, clumber spaniel',\n",
      " 217: 'English springer, English springer spaniel',\n",
      " 218: 'Welsh springer spaniel',\n",
      " 219: 'cocker spaniel, English cocker spaniel, cocker',\n",
      " 220: 'Sussex spaniel',\n",
      " 221: 'Irish water spaniel',\n",
      " 222: 'kuvasz',\n",
      " 223: 'schipperke',\n",
      " 224: 'groenendael',\n",
      " 225: 'malinois',\n",
      " 226: 'briard',\n",
      " 227: 'kelpie',\n",
      " 228: 'komondor',\n",
      " 229: 'Old English sheepdog, bobtail',\n",
      " 230: 'Shetland sheepdog, Shetland sheep dog, Shetland',\n",
      " 231: 'collie',\n",
      " 232: 'Border collie',\n",
      " 233: 'Bouvier des Flandres, Bouviers des Flandres',\n",
      " 234: 'Rottweiler',\n",
      " 235: 'German shepherd, German shepherd dog, German police dog, alsatian',\n",
      " 236: 'Doberman, Doberman pinscher',\n",
      " 237: 'miniature pinscher',\n",
      " 238: 'Greater Swiss Mountain dog',\n",
      " 239: 'Bernese mountain dog',\n",
      " 240: 'Appenzeller',\n",
      " 241: 'EntleBucher',\n",
      " 242: 'boxer',\n",
      " 243: 'bull mastiff',\n",
      " 244: 'Tibetan mastiff',\n",
      " 245: 'French bulldog',\n",
      " 246: 'Great Dane',\n",
      " 247: 'Saint Bernard, St Bernard',\n",
      " 248: 'Eskimo dog, husky',\n",
      " 249: 'malamute, malemute, Alaskan malamute',\n",
      " 250: 'Siberian husky',\n",
      " 251: 'dalmatian, coach dog, carriage dog',\n",
      " 252: 'affenpinscher, monkey pinscher, monkey dog',\n",
      " 253: 'basenji',\n",
      " 254: 'pug, pug-dog',\n",
      " 255: 'Leonberg',\n",
      " 256: 'Newfoundland, Newfoundland dog',\n",
      " 257: 'Great Pyrenees',\n",
      " 258: 'Samoyed, Samoyede',\n",
      " 259: 'Pomeranian',\n",
      " 260: 'chow, chow chow',\n",
      " 261: 'keeshond',\n",
      " 262: 'Brabancon griffon',\n",
      " 263: 'Pembroke, Pembroke Welsh corgi',\n",
      " 264: 'Cardigan, Cardigan Welsh corgi',\n",
      " 265: 'toy poodle',\n",
      " 266: 'miniature poodle',\n",
      " 267: 'standard poodle',\n",
      " 268: 'Mexican hairless',\n",
      " 269: 'timber wolf, grey wolf, gray wolf, Canis lupus',\n",
      " 270: 'white wolf, Arctic wolf, Canis lupus tundrarum',\n",
      " 271: 'red wolf, maned wolf, Canis rufus, Canis niger',\n",
      " 272: 'coyote, prairie wolf, brush wolf, Canis latrans',\n",
      " 273: 'dingo, warrigal, warragal, Canis dingo',\n",
      " 274: 'dhole, Cuon alpinus',\n",
      " 275: 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus',\n",
      " 276: 'hyena, hyaena',\n",
      " 277: 'red fox, Vulpes vulpes',\n",
      " 278: 'kit fox, Vulpes macrotis',\n",
      " 279: 'Arctic fox, white fox, Alopex lagopus',\n",
      " 280: 'grey fox, gray fox, Urocyon cinereoargenteus',\n",
      " 281: 'tabby, tabby cat',\n",
      " 282: 'tiger cat',\n",
      " 283: 'Persian cat',\n",
      " 284: 'Siamese cat, Siamese',\n",
      " 285: 'Egyptian cat',\n",
      " 286: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n",
      " 287: 'lynx, catamount',\n",
      " 288: 'leopard, Panthera pardus',\n",
      " 289: 'snow leopard, ounce, Panthera uncia',\n",
      " 290: 'jaguar, panther, Panthera onca, Felis onca',\n",
      " 291: 'lion, king of beasts, Panthera leo',\n",
      " 292: 'tiger, Panthera tigris',\n",
      " 293: 'cheetah, chetah, Acinonyx jubatus',\n",
      " 294: 'brown bear, bruin, Ursus arctos',\n",
      " 295: 'American black bear, black bear, Ursus americanus, Euarctos americanus',\n",
      " 296: 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus',\n",
      " 297: 'sloth bear, Melursus ursinus, Ursus ursinus',\n",
      " 298: 'mongoose',\n",
      " 299: 'meerkat, mierkat',\n",
      " 300: 'tiger beetle',\n",
      " 301: 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle',\n",
      " 302: 'ground beetle, carabid beetle',\n",
      " 303: 'long-horned beetle, longicorn, longicorn beetle',\n",
      " 304: 'leaf beetle, chrysomelid',\n",
      " 305: 'dung beetle',\n",
      " 306: 'rhinoceros beetle',\n",
      " 307: 'weevil',\n",
      " 308: 'fly',\n",
      " 309: 'bee',\n",
      " 310: 'ant, emmet, pismire',\n",
      " 311: 'grasshopper, hopper',\n",
      " 312: 'cricket',\n",
      " 313: 'walking stick, walkingstick, stick insect',\n",
      " 314: 'cockroach, roach',\n",
      " 315: 'mantis, mantid',\n",
      " 316: 'cicada, cicala',\n",
      " 317: 'leafhopper',\n",
      " 318: 'lacewing, lacewing fly',\n",
      " 319: \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\",\n",
      " 320: 'damselfly',\n",
      " 321: 'admiral',\n",
      " 322: 'ringlet, ringlet butterfly',\n",
      " 323: 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus',\n",
      " 324: 'cabbage butterfly',\n",
      " 325: 'sulphur butterfly, sulfur butterfly',\n",
      " 326: 'lycaenid, lycaenid butterfly',\n",
      " 327: 'starfish, sea star',\n",
      " 328: 'sea urchin',\n",
      " 329: 'sea cucumber, holothurian',\n",
      " 330: 'wood rabbit, cottontail, cottontail rabbit',\n",
      " 331: 'hare',\n",
      " 332: 'Angora, Angora rabbit',\n",
      " 333: 'hamster',\n",
      " 334: 'porcupine, hedgehog',\n",
      " 335: 'fox squirrel, eastern fox squirrel, Sciurus niger',\n",
      " 336: 'marmot',\n",
      " 337: 'beaver',\n",
      " 338: 'guinea pig, Cavia cobaya',\n",
      " 339: 'sorrel',\n",
      " 340: 'zebra',\n",
      " 341: 'hog, pig, grunter, squealer, Sus scrofa',\n",
      " 342: 'wild boar, boar, Sus scrofa',\n",
      " 343: 'warthog',\n",
      " 344: 'hippopotamus, hippo, river horse, Hippopotamus amphibius',\n",
      " 345: 'ox',\n",
      " 346: 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis',\n",
      " 347: 'bison',\n",
      " 348: 'ram, tup',\n",
      " 349: 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis',\n",
      " 350: 'ibex, Capra ibex',\n",
      " 351: 'hartebeest',\n",
      " 352: 'impala, Aepyceros melampus',\n",
      " 353: 'gazelle',\n",
      " 354: 'Arabian camel, dromedary, Camelus dromedarius',\n",
      " 355: 'llama',\n",
      " 356: 'weasel',\n",
      " 357: 'mink',\n",
      " 358: 'polecat, fitch, foulmart, foumart, Mustela putorius',\n",
      " 359: 'black-footed ferret, ferret, Mustela nigripes',\n",
      " 360: 'otter',\n",
      " 361: 'skunk, polecat, wood pussy',\n",
      " 362: 'badger',\n",
      " 363: 'armadillo',\n",
      " 364: 'three-toed sloth, ai, Bradypus tridactylus',\n",
      " 365: 'orangutan, orang, orangutang, Pongo pygmaeus',\n",
      " 366: 'gorilla, Gorilla gorilla',\n",
      " 367: 'chimpanzee, chimp, Pan troglodytes',\n",
      " 368: 'gibbon, Hylobates lar',\n",
      " 369: 'siamang, Hylobates syndactylus, Symphalangus syndactylus',\n",
      " 370: 'guenon, guenon monkey',\n",
      " 371: 'patas, hussar monkey, Erythrocebus patas',\n",
      " 372: 'baboon',\n",
      " 373: 'macaque',\n",
      " 374: 'langur',\n",
      " 375: 'colobus, colobus monkey',\n",
      " 376: 'proboscis monkey, Nasalis larvatus',\n",
      " 377: 'marmoset',\n",
      " 378: 'capuchin, ringtail, Cebus capucinus',\n",
      " 379: 'howler monkey, howler',\n",
      " 380: 'titi, titi monkey',\n",
      " 381: 'spider monkey, Ateles geoffroyi',\n",
      " 382: 'squirrel monkey, Saimiri sciureus',\n",
      " 383: 'Madagascar cat, ring-tailed lemur, Lemur catta',\n",
      " 384: 'indri, indris, Indri indri, Indri brevicaudatus',\n",
      " 385: 'Indian elephant, Elephas maximus',\n",
      " 386: 'African elephant, Loxodonta africana',\n",
      " 387: 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens',\n",
      " 388: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca',\n",
      " 389: 'barracouta, snoek',\n",
      " 390: 'eel',\n",
      " 391: 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch',\n",
      " 392: 'rock beauty, Holocanthus tricolor',\n",
      " 393: 'anemone fish',\n",
      " 394: 'sturgeon',\n",
      " 395: 'gar, garfish, garpike, billfish, Lepisosteus osseus',\n",
      " 396: 'lionfish',\n",
      " 397: 'puffer, pufferfish, blowfish, globefish',\n",
      " 398: 'abacus',\n",
      " 399: 'abaya',\n",
      " 400: \"academic gown, academic robe, judge's robe\",\n",
      " 401: 'accordion, piano accordion, squeeze box',\n",
      " 402: 'acoustic guitar',\n",
      " 403: 'aircraft carrier, carrier, flattop, attack aircraft carrier',\n",
      " 404: 'airliner',\n",
      " 405: 'airship, dirigible',\n",
      " 406: 'altar',\n",
      " 407: 'ambulance',\n",
      " 408: 'amphibian, amphibious vehicle',\n",
      " 409: 'analog clock',\n",
      " 410: 'apiary, bee house',\n",
      " 411: 'apron',\n",
      " 412: 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin',\n",
      " 413: 'assault rifle, assault gun',\n",
      " 414: 'backpack, back pack, knapsack, packsack, rucksack, haversack',\n",
      " 415: 'bakery, bakeshop, bakehouse',\n",
      " 416: 'balance beam, beam',\n",
      " 417: 'balloon',\n",
      " 418: 'ballpoint, ballpoint pen, ballpen, Biro',\n",
      " 419: 'Band Aid',\n",
      " 420: 'banjo',\n",
      " 421: 'bannister, banister, balustrade, balusters, handrail',\n",
      " 422: 'barbell',\n",
      " 423: 'barber chair',\n",
      " 424: 'barbershop',\n",
      " 425: 'barn',\n",
      " 426: 'barometer',\n",
      " 427: 'barrel, cask',\n",
      " 428: 'barrow, garden cart, lawn cart, wheelbarrow',\n",
      " 429: 'baseball',\n",
      " 430: 'basketball',\n",
      " 431: 'bassinet',\n",
      " 432: 'bassoon',\n",
      " 433: 'bathing cap, swimming cap',\n",
      " 434: 'bath towel',\n",
      " 435: 'bathtub, bathing tub, bath, tub',\n",
      " 436: 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon',\n",
      " 437: 'beacon, lighthouse, beacon light, pharos',\n",
      " 438: 'beaker',\n",
      " 439: 'bearskin, busby, shako',\n",
      " 440: 'beer bottle',\n",
      " 441: 'beer glass',\n",
      " 442: 'bell cote, bell cot',\n",
      " 443: 'bib',\n",
      " 444: 'bicycle-built-for-two, tandem bicycle, tandem',\n",
      " 445: 'bikini, two-piece',\n",
      " 446: 'binder, ring-binder',\n",
      " 447: 'binoculars, field glasses, opera glasses',\n",
      " 448: 'birdhouse',\n",
      " 449: 'boathouse',\n",
      " 450: 'bobsled, bobsleigh, bob',\n",
      " 451: 'bolo tie, bolo, bola tie, bola',\n",
      " 452: 'bonnet, poke bonnet',\n",
      " 453: 'bookcase',\n",
      " 454: 'bookshop, bookstore, bookstall',\n",
      " 455: 'bottlecap',\n",
      " 456: 'bow',\n",
      " 457: 'bow tie, bow-tie, bowtie',\n",
      " 458: 'brass, memorial tablet, plaque',\n",
      " 459: 'brassiere, bra, bandeau',\n",
      " 460: 'breakwater, groin, groyne, mole, bulwark, seawall, jetty',\n",
      " 461: 'breastplate, aegis, egis',\n",
      " 462: 'broom',\n",
      " 463: 'bucket, pail',\n",
      " 464: 'buckle',\n",
      " 465: 'bulletproof vest',\n",
      " 466: 'bullet train, bullet',\n",
      " 467: 'butcher shop, meat market',\n",
      " 468: 'cab, hack, taxi, taxicab',\n",
      " 469: 'caldron, cauldron',\n",
      " 470: 'candle, taper, wax light',\n",
      " 471: 'cannon',\n",
      " 472: 'canoe',\n",
      " 473: 'can opener, tin opener',\n",
      " 474: 'cardigan',\n",
      " 475: 'car mirror',\n",
      " 476: 'carousel, carrousel, merry-go-round, roundabout, whirligig',\n",
      " 477: \"carpenter's kit, tool kit\",\n",
      " 478: 'carton',\n",
      " 479: 'car wheel',\n",
      " 480: 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM',\n",
      " 481: 'cassette',\n",
      " 482: 'cassette player',\n",
      " 483: 'castle',\n",
      " 484: 'catamaran',\n",
      " 485: 'CD player',\n",
      " 486: 'cello, violoncello',\n",
      " 487: 'cellular telephone, cellular phone, cellphone, cell, mobile phone',\n",
      " 488: 'chain',\n",
      " 489: 'chainlink fence',\n",
      " 490: 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour',\n",
      " 491: 'chain saw, chainsaw',\n",
      " 492: 'chest',\n",
      " 493: 'chiffonier, commode',\n",
      " 494: 'chime, bell, gong',\n",
      " 495: 'china cabinet, china closet',\n",
      " 496: 'Christmas stocking',\n",
      " 497: 'church, church building',\n",
      " 498: 'cinema, movie theater, movie theatre, movie house, picture palace',\n",
      " 499: 'cleaver, meat cleaver, chopper',\n",
      " 500: 'cliff dwelling',\n",
      " 501: 'cloak',\n",
      " 502: 'clog, geta, patten, sabot',\n",
      " 503: 'cocktail shaker',\n",
      " 504: 'coffee mug',\n",
      " 505: 'coffeepot',\n",
      " 506: 'coil, spiral, volute, whorl, helix',\n",
      " 507: 'combination lock',\n",
      " 508: 'computer keyboard, keypad',\n",
      " 509: 'confectionery, confectionary, candy store',\n",
      " 510: 'container ship, containership, container vessel',\n",
      " 511: 'convertible',\n",
      " 512: 'corkscrew, bottle screw',\n",
      " 513: 'cornet, horn, trumpet, trump',\n",
      " 514: 'cowboy boot',\n",
      " 515: 'cowboy hat, ten-gallon hat',\n",
      " 516: 'cradle',\n",
      " 517: 'crane',\n",
      " 518: 'crash helmet',\n",
      " 519: 'crate',\n",
      " 520: 'crib, cot',\n",
      " 521: 'Crock Pot',\n",
      " 522: 'croquet ball',\n",
      " 523: 'crutch',\n",
      " 524: 'cuirass',\n",
      " 525: 'dam, dike, dyke',\n",
      " 526: 'desk',\n",
      " 527: 'desktop computer',\n",
      " 528: 'dial telephone, dial phone',\n",
      " 529: 'diaper, nappy, napkin',\n",
      " 530: 'digital clock',\n",
      " 531: 'digital watch',\n",
      " 532: 'dining table, board',\n",
      " 533: 'dishrag, dishcloth',\n",
      " 534: 'dishwasher, dish washer, dishwashing machine',\n",
      " 535: 'disk brake, disc brake',\n",
      " 536: 'dock, dockage, docking facility',\n",
      " 537: 'dogsled, dog sled, dog sleigh',\n",
      " 538: 'dome',\n",
      " 539: 'doormat, welcome mat',\n",
      " 540: 'drilling platform, offshore rig',\n",
      " 541: 'drum, membranophone, tympan',\n",
      " 542: 'drumstick',\n",
      " 543: 'dumbbell',\n",
      " 544: 'Dutch oven',\n",
      " 545: 'electric fan, blower',\n",
      " 546: 'electric guitar',\n",
      " 547: 'electric locomotive',\n",
      " 548: 'entertainment center',\n",
      " 549: 'envelope',\n",
      " 550: 'espresso maker',\n",
      " 551: 'face powder',\n",
      " 552: 'feather boa, boa',\n",
      " 553: 'file, file cabinet, filing cabinet',\n",
      " 554: 'fireboat',\n",
      " 555: 'fire engine, fire truck',\n",
      " 556: 'fire screen, fireguard',\n",
      " 557: 'flagpole, flagstaff',\n",
      " 558: 'flute, transverse flute',\n",
      " 559: 'folding chair',\n",
      " 560: 'football helmet',\n",
      " 561: 'forklift',\n",
      " 562: 'fountain',\n",
      " 563: 'fountain pen',\n",
      " 564: 'four-poster',\n",
      " 565: 'freight car',\n",
      " 566: 'French horn, horn',\n",
      " 567: 'frying pan, frypan, skillet',\n",
      " 568: 'fur coat',\n",
      " 569: 'garbage truck, dustcart',\n",
      " 570: 'gasmask, respirator, gas helmet',\n",
      " 571: 'gas pump, gasoline pump, petrol pump, island dispenser',\n",
      " 572: 'goblet',\n",
      " 573: 'go-kart',\n",
      " 574: 'golf ball',\n",
      " 575: 'golfcart, golf cart',\n",
      " 576: 'gondola',\n",
      " 577: 'gong, tam-tam',\n",
      " 578: 'gown',\n",
      " 579: 'grand piano, grand',\n",
      " 580: 'greenhouse, nursery, glasshouse',\n",
      " 581: 'grille, radiator grille',\n",
      " 582: 'grocery store, grocery, food market, market',\n",
      " 583: 'guillotine',\n",
      " 584: 'hair slide',\n",
      " 585: 'hair spray',\n",
      " 586: 'half track',\n",
      " 587: 'hammer',\n",
      " 588: 'hamper',\n",
      " 589: 'hand blower, blow dryer, blow drier, hair dryer, hair drier',\n",
      " 590: 'hand-held computer, hand-held microcomputer',\n",
      " 591: 'handkerchief, hankie, hanky, hankey',\n",
      " 592: 'hard disc, hard disk, fixed disk',\n",
      " 593: 'harmonica, mouth organ, harp, mouth harp',\n",
      " 594: 'harp',\n",
      " 595: 'harvester, reaper',\n",
      " 596: 'hatchet',\n",
      " 597: 'holster',\n",
      " 598: 'home theater, home theatre',\n",
      " 599: 'honeycomb',\n",
      " 600: 'hook, claw',\n",
      " 601: 'hoopskirt, crinoline',\n",
      " 602: 'horizontal bar, high bar',\n",
      " 603: 'horse cart, horse-cart',\n",
      " 604: 'hourglass',\n",
      " 605: 'iPod',\n",
      " 606: 'iron, smoothing iron',\n",
      " 607: \"jack-o'-lantern\",\n",
      " 608: 'jean, blue jean, denim',\n",
      " 609: 'jeep, landrover',\n",
      " 610: 'jersey, T-shirt, tee shirt',\n",
      " 611: 'jigsaw puzzle',\n",
      " 612: 'jinrikisha, ricksha, rickshaw',\n",
      " 613: 'joystick',\n",
      " 614: 'kimono',\n",
      " 615: 'knee pad',\n",
      " 616: 'knot',\n",
      " 617: 'lab coat, laboratory coat',\n",
      " 618: 'ladle',\n",
      " 619: 'lampshade, lamp shade',\n",
      " 620: 'laptop, laptop computer',\n",
      " 621: 'lawn mower, mower',\n",
      " 622: 'lens cap, lens cover',\n",
      " 623: 'letter opener, paper knife, paperknife',\n",
      " 624: 'library',\n",
      " 625: 'lifeboat',\n",
      " 626: 'lighter, light, igniter, ignitor',\n",
      " 627: 'limousine, limo',\n",
      " 628: 'liner, ocean liner',\n",
      " 629: 'lipstick, lip rouge',\n",
      " 630: 'Loafer',\n",
      " 631: 'lotion',\n",
      " 632: 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system',\n",
      " 633: \"loupe, jeweler's loupe\",\n",
      " 634: 'lumbermill, sawmill',\n",
      " 635: 'magnetic compass',\n",
      " 636: 'mailbag, postbag',\n",
      " 637: 'mailbox, letter box',\n",
      " 638: 'maillot',\n",
      " 639: 'maillot, tank suit',\n",
      " 640: 'manhole cover',\n",
      " 641: 'maraca',\n",
      " 642: 'marimba, xylophone',\n",
      " 643: 'mask',\n",
      " 644: 'matchstick',\n",
      " 645: 'maypole',\n",
      " 646: 'maze, labyrinth',\n",
      " 647: 'measuring cup',\n",
      " 648: 'medicine chest, medicine cabinet',\n",
      " 649: 'megalith, megalithic structure',\n",
      " 650: 'microphone, mike',\n",
      " 651: 'microwave, microwave oven',\n",
      " 652: 'military uniform',\n",
      " 653: 'milk can',\n",
      " 654: 'minibus',\n",
      " 655: 'miniskirt, mini',\n",
      " 656: 'minivan',\n",
      " 657: 'missile',\n",
      " 658: 'mitten',\n",
      " 659: 'mixing bowl',\n",
      " 660: 'mobile home, manufactured home',\n",
      " 661: 'Model T',\n",
      " 662: 'modem',\n",
      " 663: 'monastery',\n",
      " 664: 'monitor',\n",
      " 665: 'moped',\n",
      " 666: 'mortar',\n",
      " 667: 'mortarboard',\n",
      " 668: 'mosque',\n",
      " 669: 'mosquito net',\n",
      " 670: 'motor scooter, scooter',\n",
      " 671: 'mountain bike, all-terrain bike, off-roader',\n",
      " 672: 'mountain tent',\n",
      " 673: 'mouse, computer mouse',\n",
      " 674: 'mousetrap',\n",
      " 675: 'moving van',\n",
      " 676: 'muzzle',\n",
      " 677: 'nail',\n",
      " 678: 'neck brace',\n",
      " 679: 'necklace',\n",
      " 680: 'nipple',\n",
      " 681: 'notebook, notebook computer',\n",
      " 682: 'obelisk',\n",
      " 683: 'oboe, hautboy, hautbois',\n",
      " 684: 'ocarina, sweet potato',\n",
      " 685: 'odometer, hodometer, mileometer, milometer',\n",
      " 686: 'oil filter',\n",
      " 687: 'organ, pipe organ',\n",
      " 688: 'oscilloscope, scope, cathode-ray oscilloscope, CRO',\n",
      " 689: 'overskirt',\n",
      " 690: 'oxcart',\n",
      " 691: 'oxygen mask',\n",
      " 692: 'packet',\n",
      " 693: 'paddle, boat paddle',\n",
      " 694: 'paddlewheel, paddle wheel',\n",
      " 695: 'padlock',\n",
      " 696: 'paintbrush',\n",
      " 697: \"pajama, pyjama, pj's, jammies\",\n",
      " 698: 'palace',\n",
      " 699: 'panpipe, pandean pipe, syrinx',\n",
      " 700: 'paper towel',\n",
      " 701: 'parachute, chute',\n",
      " 702: 'parallel bars, bars',\n",
      " 703: 'park bench',\n",
      " 704: 'parking meter',\n",
      " 705: 'passenger car, coach, carriage',\n",
      " 706: 'patio, terrace',\n",
      " 707: 'pay-phone, pay-station',\n",
      " 708: 'pedestal, plinth, footstall',\n",
      " 709: 'pencil box, pencil case',\n",
      " 710: 'pencil sharpener',\n",
      " 711: 'perfume, essence',\n",
      " 712: 'Petri dish',\n",
      " 713: 'photocopier',\n",
      " 714: 'pick, plectrum, plectron',\n",
      " 715: 'pickelhaube',\n",
      " 716: 'picket fence, paling',\n",
      " 717: 'pickup, pickup truck',\n",
      " 718: 'pier',\n",
      " 719: 'piggy bank, penny bank',\n",
      " 720: 'pill bottle',\n",
      " 721: 'pillow',\n",
      " 722: 'ping-pong ball',\n",
      " 723: 'pinwheel',\n",
      " 724: 'pirate, pirate ship',\n",
      " 725: 'pitcher, ewer',\n",
      " 726: \"plane, carpenter's plane, woodworking plane\",\n",
      " 727: 'planetarium',\n",
      " 728: 'plastic bag',\n",
      " 729: 'plate rack',\n",
      " 730: 'plow, plough',\n",
      " 731: \"plunger, plumber's helper\",\n",
      " 732: 'Polaroid camera, Polaroid Land camera',\n",
      " 733: 'pole',\n",
      " 734: 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria',\n",
      " 735: 'poncho',\n",
      " 736: 'pool table, billiard table, snooker table',\n",
      " 737: 'pop bottle, soda bottle',\n",
      " 738: 'pot, flowerpot',\n",
      " 739: \"potter's wheel\",\n",
      " 740: 'power drill',\n",
      " 741: 'prayer rug, prayer mat',\n",
      " 742: 'printer',\n",
      " 743: 'prison, prison house',\n",
      " 744: 'projectile, missile',\n",
      " 745: 'projector',\n",
      " 746: 'puck, hockey puck',\n",
      " 747: 'punching bag, punch bag, punching ball, punchball',\n",
      " 748: 'purse',\n",
      " 749: 'quill, quill pen',\n",
      " 750: 'quilt, comforter, comfort, puff',\n",
      " 751: 'racer, race car, racing car',\n",
      " 752: 'racket, racquet',\n",
      " 753: 'radiator',\n",
      " 754: 'radio, wireless',\n",
      " 755: 'radio telescope, radio reflector',\n",
      " 756: 'rain barrel',\n",
      " 757: 'recreational vehicle, RV, R.V.',\n",
      " 758: 'reel',\n",
      " 759: 'reflex camera',\n",
      " 760: 'refrigerator, icebox',\n",
      " 761: 'remote control, remote',\n",
      " 762: 'restaurant, eating house, eating place, eatery',\n",
      " 763: 'revolver, six-gun, six-shooter',\n",
      " 764: 'rifle',\n",
      " 765: 'rocking chair, rocker',\n",
      " 766: 'rotisserie',\n",
      " 767: 'rubber eraser, rubber, pencil eraser',\n",
      " 768: 'rugby ball',\n",
      " 769: 'rule, ruler',\n",
      " 770: 'running shoe',\n",
      " 771: 'safe',\n",
      " 772: 'safety pin',\n",
      " 773: 'saltshaker, salt shaker',\n",
      " 774: 'sandal',\n",
      " 775: 'sarong',\n",
      " 776: 'sax, saxophone',\n",
      " 777: 'scabbard',\n",
      " 778: 'scale, weighing machine',\n",
      " 779: 'school bus',\n",
      " 780: 'schooner',\n",
      " 781: 'scoreboard',\n",
      " 782: 'screen, CRT screen',\n",
      " 783: 'screw',\n",
      " 784: 'screwdriver',\n",
      " 785: 'seat belt, seatbelt',\n",
      " 786: 'sewing machine',\n",
      " 787: 'shield, buckler',\n",
      " 788: 'shoe shop, shoe-shop, shoe store',\n",
      " 789: 'shoji',\n",
      " 790: 'shopping basket',\n",
      " 791: 'shopping cart',\n",
      " 792: 'shovel',\n",
      " 793: 'shower cap',\n",
      " 794: 'shower curtain',\n",
      " 795: 'ski',\n",
      " 796: 'ski mask',\n",
      " 797: 'sleeping bag',\n",
      " 798: 'slide rule, slipstick',\n",
      " 799: 'sliding door',\n",
      " 800: 'slot, one-armed bandit',\n",
      " 801: 'snorkel',\n",
      " 802: 'snowmobile',\n",
      " 803: 'snowplow, snowplough',\n",
      " 804: 'soap dispenser',\n",
      " 805: 'soccer ball',\n",
      " 806: 'sock',\n",
      " 807: 'solar dish, solar collector, solar furnace',\n",
      " 808: 'sombrero',\n",
      " 809: 'soup bowl',\n",
      " 810: 'space bar',\n",
      " 811: 'space heater',\n",
      " 812: 'space shuttle',\n",
      " 813: 'spatula',\n",
      " 814: 'speedboat',\n",
      " 815: \"spider web, spider's web\",\n",
      " 816: 'spindle',\n",
      " 817: 'sports car, sport car',\n",
      " 818: 'spotlight, spot',\n",
      " 819: 'stage',\n",
      " 820: 'steam locomotive',\n",
      " 821: 'steel arch bridge',\n",
      " 822: 'steel drum',\n",
      " 823: 'stethoscope',\n",
      " 824: 'stole',\n",
      " 825: 'stone wall',\n",
      " 826: 'stopwatch, stop watch',\n",
      " 827: 'stove',\n",
      " 828: 'strainer',\n",
      " 829: 'streetcar, tram, tramcar, trolley, trolley car',\n",
      " 830: 'stretcher',\n",
      " 831: 'studio couch, day bed',\n",
      " 832: 'stupa, tope',\n",
      " 833: 'submarine, pigboat, sub, U-boat',\n",
      " 834: 'suit, suit of clothes',\n",
      " 835: 'sundial',\n",
      " 836: 'sunglass',\n",
      " 837: 'sunglasses, dark glasses, shades',\n",
      " 838: 'sunscreen, sunblock, sun blocker',\n",
      " 839: 'suspension bridge',\n",
      " 840: 'swab, swob, mop',\n",
      " 841: 'sweatshirt',\n",
      " 842: 'swimming trunks, bathing trunks',\n",
      " 843: 'swing',\n",
      " 844: 'switch, electric switch, electrical switch',\n",
      " 845: 'syringe',\n",
      " 846: 'table lamp',\n",
      " 847: 'tank, army tank, armored combat vehicle, armoured combat vehicle',\n",
      " 848: 'tape player',\n",
      " 849: 'teapot',\n",
      " 850: 'teddy, teddy bear',\n",
      " 851: 'television, television system',\n",
      " 852: 'tennis ball',\n",
      " 853: 'thatch, thatched roof',\n",
      " 854: 'theater curtain, theatre curtain',\n",
      " 855: 'thimble',\n",
      " 856: 'thresher, thrasher, threshing machine',\n",
      " 857: 'throne',\n",
      " 858: 'tile roof',\n",
      " 859: 'toaster',\n",
      " 860: 'tobacco shop, tobacconist shop, tobacconist',\n",
      " 861: 'toilet seat',\n",
      " 862: 'torch',\n",
      " 863: 'totem pole',\n",
      " 864: 'tow truck, tow car, wrecker',\n",
      " 865: 'toyshop',\n",
      " 866: 'tractor',\n",
      " 867: 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi',\n",
      " 868: 'tray',\n",
      " 869: 'trench coat',\n",
      " 870: 'tricycle, trike, velocipede',\n",
      " 871: 'trimaran',\n",
      " 872: 'tripod',\n",
      " 873: 'triumphal arch',\n",
      " 874: 'trolleybus, trolley coach, trackless trolley',\n",
      " 875: 'trombone',\n",
      " 876: 'tub, vat',\n",
      " 877: 'turnstile',\n",
      " 878: 'typewriter keyboard',\n",
      " 879: 'umbrella',\n",
      " 880: 'unicycle, monocycle',\n",
      " 881: 'upright, upright piano',\n",
      " 882: 'vacuum, vacuum cleaner',\n",
      " 883: 'vase',\n",
      " 884: 'vault',\n",
      " 885: 'velvet',\n",
      " 886: 'vending machine',\n",
      " 887: 'vestment',\n",
      " 888: 'viaduct',\n",
      " 889: 'violin, fiddle',\n",
      " 890: 'volleyball',\n",
      " 891: 'waffle iron',\n",
      " 892: 'wall clock',\n",
      " 893: 'wallet, billfold, notecase, pocketbook',\n",
      " 894: 'wardrobe, closet, press',\n",
      " 895: 'warplane, military plane',\n",
      " 896: 'washbasin, handbasin, washbowl, lavabo, wash-hand basin',\n",
      " 897: 'washer, automatic washer, washing machine',\n",
      " 898: 'water bottle',\n",
      " 899: 'water jug',\n",
      " 900: 'water tower',\n",
      " 901: 'whiskey jug',\n",
      " 902: 'whistle',\n",
      " 903: 'wig',\n",
      " 904: 'window screen',\n",
      " 905: 'window shade',\n",
      " 906: 'Windsor tie',\n",
      " 907: 'wine bottle',\n",
      " 908: 'wing',\n",
      " 909: 'wok',\n",
      " 910: 'wooden spoon',\n",
      " 911: 'wool, woolen, woollen',\n",
      " 912: 'worm fence, snake fence, snake-rail fence, Virginia fence',\n",
      " 913: 'wreck',\n",
      " 914: 'yawl',\n",
      " 915: 'yurt',\n",
      " 916: 'web site, website, internet site, site',\n",
      " 917: 'comic book',\n",
      " 918: 'crossword puzzle, crossword',\n",
      " 919: 'street sign',\n",
      " 920: 'traffic light, traffic signal, stoplight',\n",
      " 921: 'book jacket, dust cover, dust jacket, dust wrapper',\n",
      " 922: 'menu',\n",
      " 923: 'plate',\n",
      " 924: 'guacamole',\n",
      " 925: 'consomme',\n",
      " 926: 'hot pot, hotpot',\n",
      " 927: 'trifle',\n",
      " 928: 'ice cream, icecream',\n",
      " 929: 'ice lolly, lolly, lollipop, popsicle',\n",
      " 930: 'French loaf',\n",
      " 931: 'bagel, beigel',\n",
      " 932: 'pretzel',\n",
      " 933: 'cheeseburger',\n",
      " 934: 'hotdog, hot dog, red hot',\n",
      " 935: 'mashed potato',\n",
      " 936: 'head cabbage',\n",
      " 937: 'broccoli',\n",
      " 938: 'cauliflower',\n",
      " 939: 'zucchini, courgette',\n",
      " 940: 'spaghetti squash',\n",
      " 941: 'acorn squash',\n",
      " 942: 'butternut squash',\n",
      " 943: 'cucumber, cuke',\n",
      " 944: 'artichoke, globe artichoke',\n",
      " 945: 'bell pepper',\n",
      " 946: 'cardoon',\n",
      " 947: 'mushroom',\n",
      " 948: 'Granny Smith',\n",
      " 949: 'strawberry',\n",
      " 950: 'orange',\n",
      " 951: 'lemon',\n",
      " 952: 'fig',\n",
      " 953: 'pineapple, ananas',\n",
      " 954: 'banana',\n",
      " 955: 'jackfruit, jak, jack',\n",
      " 956: 'custard apple',\n",
      " 957: 'pomegranate',\n",
      " 958: 'hay',\n",
      " 959: 'carbonara',\n",
      " 960: 'chocolate sauce, chocolate syrup',\n",
      " 961: 'dough',\n",
      " 962: 'meat loaf, meatloaf',\n",
      " 963: 'pizza, pizza pie',\n",
      " 964: 'potpie',\n",
      " 965: 'burrito',\n",
      " 966: 'red wine',\n",
      " 967: 'espresso',\n",
      " 968: 'cup',\n",
      " 969: 'eggnog',\n",
      " 970: 'alp',\n",
      " 971: 'bubble',\n",
      " 972: 'cliff, drop, drop-off',\n",
      " 973: 'coral reef',\n",
      " 974: 'geyser',\n",
      " 975: 'lakeside, lakeshore',\n",
      " 976: 'promontory, headland, head, foreland',\n",
      " 977: 'sandbar, sand bar',\n",
      " 978: 'seashore, coast, seacoast, sea-coast',\n",
      " 979: 'valley, vale',\n",
      " 980: 'volcano',\n",
      " 981: 'ballplayer, baseball player',\n",
      " 982: 'groom, bridegroom',\n",
      " 983: 'scuba diver',\n",
      " 984: 'rapeseed',\n",
      " 985: 'daisy',\n",
      " 986: \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\",\n",
      " 987: 'corn',\n",
      " 988: 'acorn',\n",
      " 989: 'hip, rose hip, rosehip',\n",
      " 990: 'buckeye, horse chestnut, conker',\n",
      " 991: 'coral fungus',\n",
      " 992: 'agaric',\n",
      " 993: 'gyromitra',\n",
      " 994: 'stinkhorn, carrion fungus',\n",
      " 995: 'earthstar',\n",
      " 996: 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa',\n",
      " 997: 'bolete',\n",
      " 998: 'ear, spike, capitulum',\n",
      " 999: 'toilet tissue, toilet paper, bathroom tissue'}\n",
      "62/2:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 224\n",
      "\n",
      "def prep_image(image):\n",
      "    res = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "    res = res.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "    return res\n",
      "\n",
      "def predict(image):\n",
      "    cropped_image = center_crop(image)\n",
      "    prediction = (model.predict(prep_image(image)), model.predict(prep_image(cropped_image)))\n",
      "    if np.max(prediction[0]>=prediction[1]):\n",
      "        return np.argmax(prediction[0], axis=1)[0]\n",
      "    else:\n",
      "        return np.argmax(prediction[1], axis=1)[1]\n",
      "    \n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "plt.imshow(image)\n",
      "\n",
      "print(class_dict[predict(image)])\n",
      "62/3:\n",
      "from tensorflow.keras.applications import EfficientNetB0\n",
      "from tensorflow.image import resize\n",
      "model = EfficientNetB0(weights='imagenet')\n",
      "\n",
      "def center_crop(img, new_width=None, new_height=None):        \n",
      "\n",
      "    width = img.shape[1]\n",
      "    height = img.shape[0]\n",
      "\n",
      "    if new_width is None:\n",
      "        new_width = min(width, height)\n",
      "\n",
      "    if new_height is None:\n",
      "        new_height = min(width, height)\n",
      "\n",
      "    left = int(np.ceil((width - new_width) / 2))\n",
      "    right = width - int(np.floor((width - new_width) / 2))\n",
      "\n",
      "    top = int(np.ceil((height - new_height) / 2))\n",
      "    bottom = height - int(np.floor((height - new_height) / 2))\n",
      "\n",
      "    if len(img.shape) == 2:\n",
      "        center_cropped_img = img[top:bottom, left:right]\n",
      "    else:\n",
      "        center_cropped_img = img[top:bottom, left:right, ...]\n",
      "\n",
      "    return center_cropped_img\n",
      "\n",
      "class_dict = {0: 'tench, Tinca tinca',\n",
      " 1: 'goldfish, Carassius auratus',\n",
      " 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n",
      " 3: 'tiger shark, Galeocerdo cuvieri',\n",
      " 4: 'hammerhead, hammerhead shark',\n",
      " 5: 'electric ray, crampfish, numbfish, torpedo',\n",
      " 6: 'stingray',\n",
      " 7: 'cock',\n",
      " 8: 'hen',\n",
      " 9: 'ostrich, Struthio camelus',\n",
      " 10: 'brambling, Fringilla montifringilla',\n",
      " 11: 'goldfinch, Carduelis carduelis',\n",
      " 12: 'house finch, linnet, Carpodacus mexicanus',\n",
      " 13: 'junco, snowbird',\n",
      " 14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea',\n",
      " 15: 'robin, American robin, Turdus migratorius',\n",
      " 16: 'bulbul',\n",
      " 17: 'jay',\n",
      " 18: 'magpie',\n",
      " 19: 'chickadee',\n",
      " 20: 'water ouzel, dipper',\n",
      " 21: 'kite',\n",
      " 22: 'bald eagle, American eagle, Haliaeetus leucocephalus',\n",
      " 23: 'vulture',\n",
      " 24: 'great grey owl, great gray owl, Strix nebulosa',\n",
      " 25: 'European fire salamander, Salamandra salamandra',\n",
      " 26: 'common newt, Triturus vulgaris',\n",
      " 27: 'eft',\n",
      " 28: 'spotted salamander, Ambystoma maculatum',\n",
      " 29: 'axolotl, mud puppy, Ambystoma mexicanum',\n",
      " 30: 'bullfrog, Rana catesbeiana',\n",
      " 31: 'tree frog, tree-frog',\n",
      " 32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui',\n",
      " 33: 'loggerhead, loggerhead turtle, Caretta caretta',\n",
      " 34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea',\n",
      " 35: 'mud turtle',\n",
      " 36: 'terrapin',\n",
      " 37: 'box turtle, box tortoise',\n",
      " 38: 'banded gecko',\n",
      " 39: 'common iguana, iguana, Iguana iguana',\n",
      " 40: 'American chameleon, anole, Anolis carolinensis',\n",
      " 41: 'whiptail, whiptail lizard',\n",
      " 42: 'agama',\n",
      " 43: 'frilled lizard, Chlamydosaurus kingi',\n",
      " 44: 'alligator lizard',\n",
      " 45: 'Gila monster, Heloderma suspectum',\n",
      " 46: 'green lizard, Lacerta viridis',\n",
      " 47: 'African chameleon, Chamaeleo chamaeleon',\n",
      " 48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis',\n",
      " 49: 'African crocodile, Nile crocodile, Crocodylus niloticus',\n",
      " 50: 'American alligator, Alligator mississipiensis',\n",
      " 51: 'triceratops',\n",
      " 52: 'thunder snake, worm snake, Carphophis amoenus',\n",
      " 53: 'ringneck snake, ring-necked snake, ring snake',\n",
      " 54: 'hognose snake, puff adder, sand viper',\n",
      " 55: 'green snake, grass snake',\n",
      " 56: 'king snake, kingsnake',\n",
      " 57: 'garter snake, grass snake',\n",
      " 58: 'water snake',\n",
      " 59: 'vine snake',\n",
      " 60: 'night snake, Hypsiglena torquata',\n",
      " 61: 'boa constrictor, Constrictor constrictor',\n",
      " 62: 'rock python, rock snake, Python sebae',\n",
      " 63: 'Indian cobra, Naja naja',\n",
      " 64: 'green mamba',\n",
      " 65: 'sea snake',\n",
      " 66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus',\n",
      " 67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus',\n",
      " 68: 'sidewinder, horned rattlesnake, Crotalus cerastes',\n",
      " 69: 'trilobite',\n",
      " 70: 'harvestman, daddy longlegs, Phalangium opilio',\n",
      " 71: 'scorpion',\n",
      " 72: 'black and gold garden spider, Argiope aurantia',\n",
      " 73: 'barn spider, Araneus cavaticus',\n",
      " 74: 'garden spider, Aranea diademata',\n",
      " 75: 'black widow, Latrodectus mactans',\n",
      " 76: 'tarantula',\n",
      " 77: 'wolf spider, hunting spider',\n",
      " 78: 'tick',\n",
      " 79: 'centipede',\n",
      " 80: 'black grouse',\n",
      " 81: 'ptarmigan',\n",
      " 82: 'ruffed grouse, partridge, Bonasa umbellus',\n",
      " 83: 'prairie chicken, prairie grouse, prairie fowl',\n",
      " 84: 'peacock',\n",
      " 85: 'quail',\n",
      " 86: 'partridge',\n",
      " 87: 'African grey, African gray, Psittacus erithacus',\n",
      " 88: 'macaw',\n",
      " 89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n",
      " 90: 'lorikeet',\n",
      " 91: 'coucal',\n",
      " 92: 'bee eater',\n",
      " 93: 'hornbill',\n",
      " 94: 'hummingbird',\n",
      " 95: 'jacamar',\n",
      " 96: 'toucan',\n",
      " 97: 'drake',\n",
      " 98: 'red-breasted merganser, Mergus serrator',\n",
      " 99: 'goose',\n",
      " 100: 'black swan, Cygnus atratus',\n",
      " 101: 'tusker',\n",
      " 102: 'echidna, spiny anteater, anteater',\n",
      " 103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n",
      " 104: 'wallaby, brush kangaroo',\n",
      " 105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n",
      " 106: 'wombat',\n",
      " 107: 'jellyfish',\n",
      " 108: 'sea anemone, anemone',\n",
      " 109: 'brain coral',\n",
      " 110: 'flatworm, platyhelminth',\n",
      " 111: 'nematode, nematode worm, roundworm',\n",
      " 112: 'conch',\n",
      " 113: 'snail',\n",
      " 114: 'slug',\n",
      " 115: 'sea slug, nudibranch',\n",
      " 116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore',\n",
      " 117: 'chambered nautilus, pearly nautilus, nautilus',\n",
      " 118: 'Dungeness crab, Cancer magister',\n",
      " 119: 'rock crab, Cancer irroratus',\n",
      " 120: 'fiddler crab',\n",
      " 121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica',\n",
      " 122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus',\n",
      " 123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish',\n",
      " 124: 'crayfish, crawfish, crawdad, crawdaddy',\n",
      " 125: 'hermit crab',\n",
      " 126: 'isopod',\n",
      " 127: 'white stork, Ciconia ciconia',\n",
      " 128: 'black stork, Ciconia nigra',\n",
      " 129: 'spoonbill',\n",
      " 130: 'flamingo',\n",
      " 131: 'little blue heron, Egretta caerulea',\n",
      " 132: 'American egret, great white heron, Egretta albus',\n",
      " 133: 'bittern',\n",
      " 134: 'crane',\n",
      " 135: 'limpkin, Aramus pictus',\n",
      " 136: 'European gallinule, Porphyrio porphyrio',\n",
      " 137: 'American coot, marsh hen, mud hen, water hen, Fulica americana',\n",
      " 138: 'bustard',\n",
      " 139: 'ruddy turnstone, Arenaria interpres',\n",
      " 140: 'red-backed sandpiper, dunlin, Erolia alpina',\n",
      " 141: 'redshank, Tringa totanus',\n",
      " 142: 'dowitcher',\n",
      " 143: 'oystercatcher, oyster catcher',\n",
      " 144: 'pelican',\n",
      " 145: 'king penguin, Aptenodytes patagonica',\n",
      " 146: 'albatross, mollymawk',\n",
      " 147: 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus',\n",
      " 148: 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca',\n",
      " 149: 'dugong, Dugong dugon',\n",
      " 150: 'sea lion',\n",
      " 151: 'Chihuahua',\n",
      " 152: 'Japanese spaniel',\n",
      " 153: 'Maltese dog, Maltese terrier, Maltese',\n",
      " 154: 'Pekinese, Pekingese, Peke',\n",
      " 155: 'Shih-Tzu',\n",
      " 156: 'Blenheim spaniel',\n",
      " 157: 'papillon',\n",
      " 158: 'toy terrier',\n",
      " 159: 'Rhodesian ridgeback',\n",
      " 160: 'Afghan hound, Afghan',\n",
      " 161: 'basset, basset hound',\n",
      " 162: 'beagle',\n",
      " 163: 'bloodhound, sleuthhound',\n",
      " 164: 'bluetick',\n",
      " 165: 'black-and-tan coonhound',\n",
      " 166: 'Walker hound, Walker foxhound',\n",
      " 167: 'English foxhound',\n",
      " 168: 'redbone',\n",
      " 169: 'borzoi, Russian wolfhound',\n",
      " 170: 'Irish wolfhound',\n",
      " 171: 'Italian greyhound',\n",
      " 172: 'whippet',\n",
      " 173: 'Ibizan hound, Ibizan Podenco',\n",
      " 174: 'Norwegian elkhound, elkhound',\n",
      " 175: 'otterhound, otter hound',\n",
      " 176: 'Saluki, gazelle hound',\n",
      " 177: 'Scottish deerhound, deerhound',\n",
      " 178: 'Weimaraner',\n",
      " 179: 'Staffordshire bullterrier, Staffordshire bull terrier',\n",
      " 180: 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier',\n",
      " 181: 'Bedlington terrier',\n",
      " 182: 'Border terrier',\n",
      " 183: 'Kerry blue terrier',\n",
      " 184: 'Irish terrier',\n",
      " 185: 'Norfolk terrier',\n",
      " 186: 'Norwich terrier',\n",
      " 187: 'Yorkshire terrier',\n",
      " 188: 'wire-haired fox terrier',\n",
      " 189: 'Lakeland terrier',\n",
      " 190: 'Sealyham terrier, Sealyham',\n",
      " 191: 'Airedale, Airedale terrier',\n",
      " 192: 'cairn, cairn terrier',\n",
      " 193: 'Australian terrier',\n",
      " 194: 'Dandie Dinmont, Dandie Dinmont terrier',\n",
      " 195: 'Boston bull, Boston terrier',\n",
      " 196: 'miniature schnauzer',\n",
      " 197: 'giant schnauzer',\n",
      " 198: 'standard schnauzer',\n",
      " 199: 'Scotch terrier, Scottish terrier, Scottie',\n",
      " 200: 'Tibetan terrier, chrysanthemum dog',\n",
      " 201: 'silky terrier, Sydney silky',\n",
      " 202: 'soft-coated wheaten terrier',\n",
      " 203: 'West Highland white terrier',\n",
      " 204: 'Lhasa, Lhasa apso',\n",
      " 205: 'flat-coated retriever',\n",
      " 206: 'curly-coated retriever',\n",
      " 207: 'golden retriever',\n",
      " 208: 'Labrador retriever',\n",
      " 209: 'Chesapeake Bay retriever',\n",
      " 210: 'German short-haired pointer',\n",
      " 211: 'vizsla, Hungarian pointer',\n",
      " 212: 'English setter',\n",
      " 213: 'Irish setter, red setter',\n",
      " 214: 'Gordon setter',\n",
      " 215: 'Brittany spaniel',\n",
      " 216: 'clumber, clumber spaniel',\n",
      " 217: 'English springer, English springer spaniel',\n",
      " 218: 'Welsh springer spaniel',\n",
      " 219: 'cocker spaniel, English cocker spaniel, cocker',\n",
      " 220: 'Sussex spaniel',\n",
      " 221: 'Irish water spaniel',\n",
      " 222: 'kuvasz',\n",
      " 223: 'schipperke',\n",
      " 224: 'groenendael',\n",
      " 225: 'malinois',\n",
      " 226: 'briard',\n",
      " 227: 'kelpie',\n",
      " 228: 'komondor',\n",
      " 229: 'Old English sheepdog, bobtail',\n",
      " 230: 'Shetland sheepdog, Shetland sheep dog, Shetland',\n",
      " 231: 'collie',\n",
      " 232: 'Border collie',\n",
      " 233: 'Bouvier des Flandres, Bouviers des Flandres',\n",
      " 234: 'Rottweiler',\n",
      " 235: 'German shepherd, German shepherd dog, German police dog, alsatian',\n",
      " 236: 'Doberman, Doberman pinscher',\n",
      " 237: 'miniature pinscher',\n",
      " 238: 'Greater Swiss Mountain dog',\n",
      " 239: 'Bernese mountain dog',\n",
      " 240: 'Appenzeller',\n",
      " 241: 'EntleBucher',\n",
      " 242: 'boxer',\n",
      " 243: 'bull mastiff',\n",
      " 244: 'Tibetan mastiff',\n",
      " 245: 'French bulldog',\n",
      " 246: 'Great Dane',\n",
      " 247: 'Saint Bernard, St Bernard',\n",
      " 248: 'Eskimo dog, husky',\n",
      " 249: 'malamute, malemute, Alaskan malamute',\n",
      " 250: 'Siberian husky',\n",
      " 251: 'dalmatian, coach dog, carriage dog',\n",
      " 252: 'affenpinscher, monkey pinscher, monkey dog',\n",
      " 253: 'basenji',\n",
      " 254: 'pug, pug-dog',\n",
      " 255: 'Leonberg',\n",
      " 256: 'Newfoundland, Newfoundland dog',\n",
      " 257: 'Great Pyrenees',\n",
      " 258: 'Samoyed, Samoyede',\n",
      " 259: 'Pomeranian',\n",
      " 260: 'chow, chow chow',\n",
      " 261: 'keeshond',\n",
      " 262: 'Brabancon griffon',\n",
      " 263: 'Pembroke, Pembroke Welsh corgi',\n",
      " 264: 'Cardigan, Cardigan Welsh corgi',\n",
      " 265: 'toy poodle',\n",
      " 266: 'miniature poodle',\n",
      " 267: 'standard poodle',\n",
      " 268: 'Mexican hairless',\n",
      " 269: 'timber wolf, grey wolf, gray wolf, Canis lupus',\n",
      " 270: 'white wolf, Arctic wolf, Canis lupus tundrarum',\n",
      " 271: 'red wolf, maned wolf, Canis rufus, Canis niger',\n",
      " 272: 'coyote, prairie wolf, brush wolf, Canis latrans',\n",
      " 273: 'dingo, warrigal, warragal, Canis dingo',\n",
      " 274: 'dhole, Cuon alpinus',\n",
      " 275: 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus',\n",
      " 276: 'hyena, hyaena',\n",
      " 277: 'red fox, Vulpes vulpes',\n",
      " 278: 'kit fox, Vulpes macrotis',\n",
      " 279: 'Arctic fox, white fox, Alopex lagopus',\n",
      " 280: 'grey fox, gray fox, Urocyon cinereoargenteus',\n",
      " 281: 'tabby, tabby cat',\n",
      " 282: 'tiger cat',\n",
      " 283: 'Persian cat',\n",
      " 284: 'Siamese cat, Siamese',\n",
      " 285: 'Egyptian cat',\n",
      " 286: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n",
      " 287: 'lynx, catamount',\n",
      " 288: 'leopard, Panthera pardus',\n",
      " 289: 'snow leopard, ounce, Panthera uncia',\n",
      " 290: 'jaguar, panther, Panthera onca, Felis onca',\n",
      " 291: 'lion, king of beasts, Panthera leo',\n",
      " 292: 'tiger, Panthera tigris',\n",
      " 293: 'cheetah, chetah, Acinonyx jubatus',\n",
      " 294: 'brown bear, bruin, Ursus arctos',\n",
      " 295: 'American black bear, black bear, Ursus americanus, Euarctos americanus',\n",
      " 296: 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus',\n",
      " 297: 'sloth bear, Melursus ursinus, Ursus ursinus',\n",
      " 298: 'mongoose',\n",
      " 299: 'meerkat, mierkat',\n",
      " 300: 'tiger beetle',\n",
      " 301: 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle',\n",
      " 302: 'ground beetle, carabid beetle',\n",
      " 303: 'long-horned beetle, longicorn, longicorn beetle',\n",
      " 304: 'leaf beetle, chrysomelid',\n",
      " 305: 'dung beetle',\n",
      " 306: 'rhinoceros beetle',\n",
      " 307: 'weevil',\n",
      " 308: 'fly',\n",
      " 309: 'bee',\n",
      " 310: 'ant, emmet, pismire',\n",
      " 311: 'grasshopper, hopper',\n",
      " 312: 'cricket',\n",
      " 313: 'walking stick, walkingstick, stick insect',\n",
      " 314: 'cockroach, roach',\n",
      " 315: 'mantis, mantid',\n",
      " 316: 'cicada, cicala',\n",
      " 317: 'leafhopper',\n",
      " 318: 'lacewing, lacewing fly',\n",
      " 319: \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\",\n",
      " 320: 'damselfly',\n",
      " 321: 'admiral',\n",
      " 322: 'ringlet, ringlet butterfly',\n",
      " 323: 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus',\n",
      " 324: 'cabbage butterfly',\n",
      " 325: 'sulphur butterfly, sulfur butterfly',\n",
      " 326: 'lycaenid, lycaenid butterfly',\n",
      " 327: 'starfish, sea star',\n",
      " 328: 'sea urchin',\n",
      " 329: 'sea cucumber, holothurian',\n",
      " 330: 'wood rabbit, cottontail, cottontail rabbit',\n",
      " 331: 'hare',\n",
      " 332: 'Angora, Angora rabbit',\n",
      " 333: 'hamster',\n",
      " 334: 'porcupine, hedgehog',\n",
      " 335: 'fox squirrel, eastern fox squirrel, Sciurus niger',\n",
      " 336: 'marmot',\n",
      " 337: 'beaver',\n",
      " 338: 'guinea pig, Cavia cobaya',\n",
      " 339: 'sorrel',\n",
      " 340: 'zebra',\n",
      " 341: 'hog, pig, grunter, squealer, Sus scrofa',\n",
      " 342: 'wild boar, boar, Sus scrofa',\n",
      " 343: 'warthog',\n",
      " 344: 'hippopotamus, hippo, river horse, Hippopotamus amphibius',\n",
      " 345: 'ox',\n",
      " 346: 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis',\n",
      " 347: 'bison',\n",
      " 348: 'ram, tup',\n",
      " 349: 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis',\n",
      " 350: 'ibex, Capra ibex',\n",
      " 351: 'hartebeest',\n",
      " 352: 'impala, Aepyceros melampus',\n",
      " 353: 'gazelle',\n",
      " 354: 'Arabian camel, dromedary, Camelus dromedarius',\n",
      " 355: 'llama',\n",
      " 356: 'weasel',\n",
      " 357: 'mink',\n",
      " 358: 'polecat, fitch, foulmart, foumart, Mustela putorius',\n",
      " 359: 'black-footed ferret, ferret, Mustela nigripes',\n",
      " 360: 'otter',\n",
      " 361: 'skunk, polecat, wood pussy',\n",
      " 362: 'badger',\n",
      " 363: 'armadillo',\n",
      " 364: 'three-toed sloth, ai, Bradypus tridactylus',\n",
      " 365: 'orangutan, orang, orangutang, Pongo pygmaeus',\n",
      " 366: 'gorilla, Gorilla gorilla',\n",
      " 367: 'chimpanzee, chimp, Pan troglodytes',\n",
      " 368: 'gibbon, Hylobates lar',\n",
      " 369: 'siamang, Hylobates syndactylus, Symphalangus syndactylus',\n",
      " 370: 'guenon, guenon monkey',\n",
      " 371: 'patas, hussar monkey, Erythrocebus patas',\n",
      " 372: 'baboon',\n",
      " 373: 'macaque',\n",
      " 374: 'langur',\n",
      " 375: 'colobus, colobus monkey',\n",
      " 376: 'proboscis monkey, Nasalis larvatus',\n",
      " 377: 'marmoset',\n",
      " 378: 'capuchin, ringtail, Cebus capucinus',\n",
      " 379: 'howler monkey, howler',\n",
      " 380: 'titi, titi monkey',\n",
      " 381: 'spider monkey, Ateles geoffroyi',\n",
      " 382: 'squirrel monkey, Saimiri sciureus',\n",
      " 383: 'Madagascar cat, ring-tailed lemur, Lemur catta',\n",
      " 384: 'indri, indris, Indri indri, Indri brevicaudatus',\n",
      " 385: 'Indian elephant, Elephas maximus',\n",
      " 386: 'African elephant, Loxodonta africana',\n",
      " 387: 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens',\n",
      " 388: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca',\n",
      " 389: 'barracouta, snoek',\n",
      " 390: 'eel',\n",
      " 391: 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch',\n",
      " 392: 'rock beauty, Holocanthus tricolor',\n",
      " 393: 'anemone fish',\n",
      " 394: 'sturgeon',\n",
      " 395: 'gar, garfish, garpike, billfish, Lepisosteus osseus',\n",
      " 396: 'lionfish',\n",
      " 397: 'puffer, pufferfish, blowfish, globefish',\n",
      " 398: 'abacus',\n",
      " 399: 'abaya',\n",
      " 400: \"academic gown, academic robe, judge's robe\",\n",
      " 401: 'accordion, piano accordion, squeeze box',\n",
      " 402: 'acoustic guitar',\n",
      " 403: 'aircraft carrier, carrier, flattop, attack aircraft carrier',\n",
      " 404: 'airliner',\n",
      " 405: 'airship, dirigible',\n",
      " 406: 'altar',\n",
      " 407: 'ambulance',\n",
      " 408: 'amphibian, amphibious vehicle',\n",
      " 409: 'analog clock',\n",
      " 410: 'apiary, bee house',\n",
      " 411: 'apron',\n",
      " 412: 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin',\n",
      " 413: 'assault rifle, assault gun',\n",
      " 414: 'backpack, back pack, knapsack, packsack, rucksack, haversack',\n",
      " 415: 'bakery, bakeshop, bakehouse',\n",
      " 416: 'balance beam, beam',\n",
      " 417: 'balloon',\n",
      " 418: 'ballpoint, ballpoint pen, ballpen, Biro',\n",
      " 419: 'Band Aid',\n",
      " 420: 'banjo',\n",
      " 421: 'bannister, banister, balustrade, balusters, handrail',\n",
      " 422: 'barbell',\n",
      " 423: 'barber chair',\n",
      " 424: 'barbershop',\n",
      " 425: 'barn',\n",
      " 426: 'barometer',\n",
      " 427: 'barrel, cask',\n",
      " 428: 'barrow, garden cart, lawn cart, wheelbarrow',\n",
      " 429: 'baseball',\n",
      " 430: 'basketball',\n",
      " 431: 'bassinet',\n",
      " 432: 'bassoon',\n",
      " 433: 'bathing cap, swimming cap',\n",
      " 434: 'bath towel',\n",
      " 435: 'bathtub, bathing tub, bath, tub',\n",
      " 436: 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon',\n",
      " 437: 'beacon, lighthouse, beacon light, pharos',\n",
      " 438: 'beaker',\n",
      " 439: 'bearskin, busby, shako',\n",
      " 440: 'beer bottle',\n",
      " 441: 'beer glass',\n",
      " 442: 'bell cote, bell cot',\n",
      " 443: 'bib',\n",
      " 444: 'bicycle-built-for-two, tandem bicycle, tandem',\n",
      " 445: 'bikini, two-piece',\n",
      " 446: 'binder, ring-binder',\n",
      " 447: 'binoculars, field glasses, opera glasses',\n",
      " 448: 'birdhouse',\n",
      " 449: 'boathouse',\n",
      " 450: 'bobsled, bobsleigh, bob',\n",
      " 451: 'bolo tie, bolo, bola tie, bola',\n",
      " 452: 'bonnet, poke bonnet',\n",
      " 453: 'bookcase',\n",
      " 454: 'bookshop, bookstore, bookstall',\n",
      " 455: 'bottlecap',\n",
      " 456: 'bow',\n",
      " 457: 'bow tie, bow-tie, bowtie',\n",
      " 458: 'brass, memorial tablet, plaque',\n",
      " 459: 'brassiere, bra, bandeau',\n",
      " 460: 'breakwater, groin, groyne, mole, bulwark, seawall, jetty',\n",
      " 461: 'breastplate, aegis, egis',\n",
      " 462: 'broom',\n",
      " 463: 'bucket, pail',\n",
      " 464: 'buckle',\n",
      " 465: 'bulletproof vest',\n",
      " 466: 'bullet train, bullet',\n",
      " 467: 'butcher shop, meat market',\n",
      " 468: 'cab, hack, taxi, taxicab',\n",
      " 469: 'caldron, cauldron',\n",
      " 470: 'candle, taper, wax light',\n",
      " 471: 'cannon',\n",
      " 472: 'canoe',\n",
      " 473: 'can opener, tin opener',\n",
      " 474: 'cardigan',\n",
      " 475: 'car mirror',\n",
      " 476: 'carousel, carrousel, merry-go-round, roundabout, whirligig',\n",
      " 477: \"carpenter's kit, tool kit\",\n",
      " 478: 'carton',\n",
      " 479: 'car wheel',\n",
      " 480: 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM',\n",
      " 481: 'cassette',\n",
      " 482: 'cassette player',\n",
      " 483: 'castle',\n",
      " 484: 'catamaran',\n",
      " 485: 'CD player',\n",
      " 486: 'cello, violoncello',\n",
      " 487: 'cellular telephone, cellular phone, cellphone, cell, mobile phone',\n",
      " 488: 'chain',\n",
      " 489: 'chainlink fence',\n",
      " 490: 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour',\n",
      " 491: 'chain saw, chainsaw',\n",
      " 492: 'chest',\n",
      " 493: 'chiffonier, commode',\n",
      " 494: 'chime, bell, gong',\n",
      " 495: 'china cabinet, china closet',\n",
      " 496: 'Christmas stocking',\n",
      " 497: 'church, church building',\n",
      " 498: 'cinema, movie theater, movie theatre, movie house, picture palace',\n",
      " 499: 'cleaver, meat cleaver, chopper',\n",
      " 500: 'cliff dwelling',\n",
      " 501: 'cloak',\n",
      " 502: 'clog, geta, patten, sabot',\n",
      " 503: 'cocktail shaker',\n",
      " 504: 'coffee mug',\n",
      " 505: 'coffeepot',\n",
      " 506: 'coil, spiral, volute, whorl, helix',\n",
      " 507: 'combination lock',\n",
      " 508: 'computer keyboard, keypad',\n",
      " 509: 'confectionery, confectionary, candy store',\n",
      " 510: 'container ship, containership, container vessel',\n",
      " 511: 'convertible',\n",
      " 512: 'corkscrew, bottle screw',\n",
      " 513: 'cornet, horn, trumpet, trump',\n",
      " 514: 'cowboy boot',\n",
      " 515: 'cowboy hat, ten-gallon hat',\n",
      " 516: 'cradle',\n",
      " 517: 'crane',\n",
      " 518: 'crash helmet',\n",
      " 519: 'crate',\n",
      " 520: 'crib, cot',\n",
      " 521: 'Crock Pot',\n",
      " 522: 'croquet ball',\n",
      " 523: 'crutch',\n",
      " 524: 'cuirass',\n",
      " 525: 'dam, dike, dyke',\n",
      " 526: 'desk',\n",
      " 527: 'desktop computer',\n",
      " 528: 'dial telephone, dial phone',\n",
      " 529: 'diaper, nappy, napkin',\n",
      " 530: 'digital clock',\n",
      " 531: 'digital watch',\n",
      " 532: 'dining table, board',\n",
      " 533: 'dishrag, dishcloth',\n",
      " 534: 'dishwasher, dish washer, dishwashing machine',\n",
      " 535: 'disk brake, disc brake',\n",
      " 536: 'dock, dockage, docking facility',\n",
      " 537: 'dogsled, dog sled, dog sleigh',\n",
      " 538: 'dome',\n",
      " 539: 'doormat, welcome mat',\n",
      " 540: 'drilling platform, offshore rig',\n",
      " 541: 'drum, membranophone, tympan',\n",
      " 542: 'drumstick',\n",
      " 543: 'dumbbell',\n",
      " 544: 'Dutch oven',\n",
      " 545: 'electric fan, blower',\n",
      " 546: 'electric guitar',\n",
      " 547: 'electric locomotive',\n",
      " 548: 'entertainment center',\n",
      " 549: 'envelope',\n",
      " 550: 'espresso maker',\n",
      " 551: 'face powder',\n",
      " 552: 'feather boa, boa',\n",
      " 553: 'file, file cabinet, filing cabinet',\n",
      " 554: 'fireboat',\n",
      " 555: 'fire engine, fire truck',\n",
      " 556: 'fire screen, fireguard',\n",
      " 557: 'flagpole, flagstaff',\n",
      " 558: 'flute, transverse flute',\n",
      " 559: 'folding chair',\n",
      " 560: 'football helmet',\n",
      " 561: 'forklift',\n",
      " 562: 'fountain',\n",
      " 563: 'fountain pen',\n",
      " 564: 'four-poster',\n",
      " 565: 'freight car',\n",
      " 566: 'French horn, horn',\n",
      " 567: 'frying pan, frypan, skillet',\n",
      " 568: 'fur coat',\n",
      " 569: 'garbage truck, dustcart',\n",
      " 570: 'gasmask, respirator, gas helmet',\n",
      " 571: 'gas pump, gasoline pump, petrol pump, island dispenser',\n",
      " 572: 'goblet',\n",
      " 573: 'go-kart',\n",
      " 574: 'golf ball',\n",
      " 575: 'golfcart, golf cart',\n",
      " 576: 'gondola',\n",
      " 577: 'gong, tam-tam',\n",
      " 578: 'gown',\n",
      " 579: 'grand piano, grand',\n",
      " 580: 'greenhouse, nursery, glasshouse',\n",
      " 581: 'grille, radiator grille',\n",
      " 582: 'grocery store, grocery, food market, market',\n",
      " 583: 'guillotine',\n",
      " 584: 'hair slide',\n",
      " 585: 'hair spray',\n",
      " 586: 'half track',\n",
      " 587: 'hammer',\n",
      " 588: 'hamper',\n",
      " 589: 'hand blower, blow dryer, blow drier, hair dryer, hair drier',\n",
      " 590: 'hand-held computer, hand-held microcomputer',\n",
      " 591: 'handkerchief, hankie, hanky, hankey',\n",
      " 592: 'hard disc, hard disk, fixed disk',\n",
      " 593: 'harmonica, mouth organ, harp, mouth harp',\n",
      " 594: 'harp',\n",
      " 595: 'harvester, reaper',\n",
      " 596: 'hatchet',\n",
      " 597: 'holster',\n",
      " 598: 'home theater, home theatre',\n",
      " 599: 'honeycomb',\n",
      " 600: 'hook, claw',\n",
      " 601: 'hoopskirt, crinoline',\n",
      " 602: 'horizontal bar, high bar',\n",
      " 603: 'horse cart, horse-cart',\n",
      " 604: 'hourglass',\n",
      " 605: 'iPod',\n",
      " 606: 'iron, smoothing iron',\n",
      " 607: \"jack-o'-lantern\",\n",
      " 608: 'jean, blue jean, denim',\n",
      " 609: 'jeep, landrover',\n",
      " 610: 'jersey, T-shirt, tee shirt',\n",
      " 611: 'jigsaw puzzle',\n",
      " 612: 'jinrikisha, ricksha, rickshaw',\n",
      " 613: 'joystick',\n",
      " 614: 'kimono',\n",
      " 615: 'knee pad',\n",
      " 616: 'knot',\n",
      " 617: 'lab coat, laboratory coat',\n",
      " 618: 'ladle',\n",
      " 619: 'lampshade, lamp shade',\n",
      " 620: 'laptop, laptop computer',\n",
      " 621: 'lawn mower, mower',\n",
      " 622: 'lens cap, lens cover',\n",
      " 623: 'letter opener, paper knife, paperknife',\n",
      " 624: 'library',\n",
      " 625: 'lifeboat',\n",
      " 626: 'lighter, light, igniter, ignitor',\n",
      " 627: 'limousine, limo',\n",
      " 628: 'liner, ocean liner',\n",
      " 629: 'lipstick, lip rouge',\n",
      " 630: 'Loafer',\n",
      " 631: 'lotion',\n",
      " 632: 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system',\n",
      " 633: \"loupe, jeweler's loupe\",\n",
      " 634: 'lumbermill, sawmill',\n",
      " 635: 'magnetic compass',\n",
      " 636: 'mailbag, postbag',\n",
      " 637: 'mailbox, letter box',\n",
      " 638: 'maillot',\n",
      " 639: 'maillot, tank suit',\n",
      " 640: 'manhole cover',\n",
      " 641: 'maraca',\n",
      " 642: 'marimba, xylophone',\n",
      " 643: 'mask',\n",
      " 644: 'matchstick',\n",
      " 645: 'maypole',\n",
      " 646: 'maze, labyrinth',\n",
      " 647: 'measuring cup',\n",
      " 648: 'medicine chest, medicine cabinet',\n",
      " 649: 'megalith, megalithic structure',\n",
      " 650: 'microphone, mike',\n",
      " 651: 'microwave, microwave oven',\n",
      " 652: 'military uniform',\n",
      " 653: 'milk can',\n",
      " 654: 'minibus',\n",
      " 655: 'miniskirt, mini',\n",
      " 656: 'minivan',\n",
      " 657: 'missile',\n",
      " 658: 'mitten',\n",
      " 659: 'mixing bowl',\n",
      " 660: 'mobile home, manufactured home',\n",
      " 661: 'Model T',\n",
      " 662: 'modem',\n",
      " 663: 'monastery',\n",
      " 664: 'monitor',\n",
      " 665: 'moped',\n",
      " 666: 'mortar',\n",
      " 667: 'mortarboard',\n",
      " 668: 'mosque',\n",
      " 669: 'mosquito net',\n",
      " 670: 'motor scooter, scooter',\n",
      " 671: 'mountain bike, all-terrain bike, off-roader',\n",
      " 672: 'mountain tent',\n",
      " 673: 'mouse, computer mouse',\n",
      " 674: 'mousetrap',\n",
      " 675: 'moving van',\n",
      " 676: 'muzzle',\n",
      " 677: 'nail',\n",
      " 678: 'neck brace',\n",
      " 679: 'necklace',\n",
      " 680: 'nipple',\n",
      " 681: 'notebook, notebook computer',\n",
      " 682: 'obelisk',\n",
      " 683: 'oboe, hautboy, hautbois',\n",
      " 684: 'ocarina, sweet potato',\n",
      " 685: 'odometer, hodometer, mileometer, milometer',\n",
      " 686: 'oil filter',\n",
      " 687: 'organ, pipe organ',\n",
      " 688: 'oscilloscope, scope, cathode-ray oscilloscope, CRO',\n",
      " 689: 'overskirt',\n",
      " 690: 'oxcart',\n",
      " 691: 'oxygen mask',\n",
      " 692: 'packet',\n",
      " 693: 'paddle, boat paddle',\n",
      " 694: 'paddlewheel, paddle wheel',\n",
      " 695: 'padlock',\n",
      " 696: 'paintbrush',\n",
      " 697: \"pajama, pyjama, pj's, jammies\",\n",
      " 698: 'palace',\n",
      " 699: 'panpipe, pandean pipe, syrinx',\n",
      " 700: 'paper towel',\n",
      " 701: 'parachute, chute',\n",
      " 702: 'parallel bars, bars',\n",
      " 703: 'park bench',\n",
      " 704: 'parking meter',\n",
      " 705: 'passenger car, coach, carriage',\n",
      " 706: 'patio, terrace',\n",
      " 707: 'pay-phone, pay-station',\n",
      " 708: 'pedestal, plinth, footstall',\n",
      " 709: 'pencil box, pencil case',\n",
      " 710: 'pencil sharpener',\n",
      " 711: 'perfume, essence',\n",
      " 712: 'Petri dish',\n",
      " 713: 'photocopier',\n",
      " 714: 'pick, plectrum, plectron',\n",
      " 715: 'pickelhaube',\n",
      " 716: 'picket fence, paling',\n",
      " 717: 'pickup, pickup truck',\n",
      " 718: 'pier',\n",
      " 719: 'piggy bank, penny bank',\n",
      " 720: 'pill bottle',\n",
      " 721: 'pillow',\n",
      " 722: 'ping-pong ball',\n",
      " 723: 'pinwheel',\n",
      " 724: 'pirate, pirate ship',\n",
      " 725: 'pitcher, ewer',\n",
      " 726: \"plane, carpenter's plane, woodworking plane\",\n",
      " 727: 'planetarium',\n",
      " 728: 'plastic bag',\n",
      " 729: 'plate rack',\n",
      " 730: 'plow, plough',\n",
      " 731: \"plunger, plumber's helper\",\n",
      " 732: 'Polaroid camera, Polaroid Land camera',\n",
      " 733: 'pole',\n",
      " 734: 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria',\n",
      " 735: 'poncho',\n",
      " 736: 'pool table, billiard table, snooker table',\n",
      " 737: 'pop bottle, soda bottle',\n",
      " 738: 'pot, flowerpot',\n",
      " 739: \"potter's wheel\",\n",
      " 740: 'power drill',\n",
      " 741: 'prayer rug, prayer mat',\n",
      " 742: 'printer',\n",
      " 743: 'prison, prison house',\n",
      " 744: 'projectile, missile',\n",
      " 745: 'projector',\n",
      " 746: 'puck, hockey puck',\n",
      " 747: 'punching bag, punch bag, punching ball, punchball',\n",
      " 748: 'purse',\n",
      " 749: 'quill, quill pen',\n",
      " 750: 'quilt, comforter, comfort, puff',\n",
      " 751: 'racer, race car, racing car',\n",
      " 752: 'racket, racquet',\n",
      " 753: 'radiator',\n",
      " 754: 'radio, wireless',\n",
      " 755: 'radio telescope, radio reflector',\n",
      " 756: 'rain barrel',\n",
      " 757: 'recreational vehicle, RV, R.V.',\n",
      " 758: 'reel',\n",
      " 759: 'reflex camera',\n",
      " 760: 'refrigerator, icebox',\n",
      " 761: 'remote control, remote',\n",
      " 762: 'restaurant, eating house, eating place, eatery',\n",
      " 763: 'revolver, six-gun, six-shooter',\n",
      " 764: 'rifle',\n",
      " 765: 'rocking chair, rocker',\n",
      " 766: 'rotisserie',\n",
      " 767: 'rubber eraser, rubber, pencil eraser',\n",
      " 768: 'rugby ball',\n",
      " 769: 'rule, ruler',\n",
      " 770: 'running shoe',\n",
      " 771: 'safe',\n",
      " 772: 'safety pin',\n",
      " 773: 'saltshaker, salt shaker',\n",
      " 774: 'sandal',\n",
      " 775: 'sarong',\n",
      " 776: 'sax, saxophone',\n",
      " 777: 'scabbard',\n",
      " 778: 'scale, weighing machine',\n",
      " 779: 'school bus',\n",
      " 780: 'schooner',\n",
      " 781: 'scoreboard',\n",
      " 782: 'screen, CRT screen',\n",
      " 783: 'screw',\n",
      " 784: 'screwdriver',\n",
      " 785: 'seat belt, seatbelt',\n",
      " 786: 'sewing machine',\n",
      " 787: 'shield, buckler',\n",
      " 788: 'shoe shop, shoe-shop, shoe store',\n",
      " 789: 'shoji',\n",
      " 790: 'shopping basket',\n",
      " 791: 'shopping cart',\n",
      " 792: 'shovel',\n",
      " 793: 'shower cap',\n",
      " 794: 'shower curtain',\n",
      " 795: 'ski',\n",
      " 796: 'ski mask',\n",
      " 797: 'sleeping bag',\n",
      " 798: 'slide rule, slipstick',\n",
      " 799: 'sliding door',\n",
      " 800: 'slot, one-armed bandit',\n",
      " 801: 'snorkel',\n",
      " 802: 'snowmobile',\n",
      " 803: 'snowplow, snowplough',\n",
      " 804: 'soap dispenser',\n",
      " 805: 'soccer ball',\n",
      " 806: 'sock',\n",
      " 807: 'solar dish, solar collector, solar furnace',\n",
      " 808: 'sombrero',\n",
      " 809: 'soup bowl',\n",
      " 810: 'space bar',\n",
      " 811: 'space heater',\n",
      " 812: 'space shuttle',\n",
      " 813: 'spatula',\n",
      " 814: 'speedboat',\n",
      " 815: \"spider web, spider's web\",\n",
      " 816: 'spindle',\n",
      " 817: 'sports car, sport car',\n",
      " 818: 'spotlight, spot',\n",
      " 819: 'stage',\n",
      " 820: 'steam locomotive',\n",
      " 821: 'steel arch bridge',\n",
      " 822: 'steel drum',\n",
      " 823: 'stethoscope',\n",
      " 824: 'stole',\n",
      " 825: 'stone wall',\n",
      " 826: 'stopwatch, stop watch',\n",
      " 827: 'stove',\n",
      " 828: 'strainer',\n",
      " 829: 'streetcar, tram, tramcar, trolley, trolley car',\n",
      " 830: 'stretcher',\n",
      " 831: 'studio couch, day bed',\n",
      " 832: 'stupa, tope',\n",
      " 833: 'submarine, pigboat, sub, U-boat',\n",
      " 834: 'suit, suit of clothes',\n",
      " 835: 'sundial',\n",
      " 836: 'sunglass',\n",
      " 837: 'sunglasses, dark glasses, shades',\n",
      " 838: 'sunscreen, sunblock, sun blocker',\n",
      " 839: 'suspension bridge',\n",
      " 840: 'swab, swob, mop',\n",
      " 841: 'sweatshirt',\n",
      " 842: 'swimming trunks, bathing trunks',\n",
      " 843: 'swing',\n",
      " 844: 'switch, electric switch, electrical switch',\n",
      " 845: 'syringe',\n",
      " 846: 'table lamp',\n",
      " 847: 'tank, army tank, armored combat vehicle, armoured combat vehicle',\n",
      " 848: 'tape player',\n",
      " 849: 'teapot',\n",
      " 850: 'teddy, teddy bear',\n",
      " 851: 'television, television system',\n",
      " 852: 'tennis ball',\n",
      " 853: 'thatch, thatched roof',\n",
      " 854: 'theater curtain, theatre curtain',\n",
      " 855: 'thimble',\n",
      " 856: 'thresher, thrasher, threshing machine',\n",
      " 857: 'throne',\n",
      " 858: 'tile roof',\n",
      " 859: 'toaster',\n",
      " 860: 'tobacco shop, tobacconist shop, tobacconist',\n",
      " 861: 'toilet seat',\n",
      " 862: 'torch',\n",
      " 863: 'totem pole',\n",
      " 864: 'tow truck, tow car, wrecker',\n",
      " 865: 'toyshop',\n",
      " 866: 'tractor',\n",
      " 867: 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi',\n",
      " 868: 'tray',\n",
      " 869: 'trench coat',\n",
      " 870: 'tricycle, trike, velocipede',\n",
      " 871: 'trimaran',\n",
      " 872: 'tripod',\n",
      " 873: 'triumphal arch',\n",
      " 874: 'trolleybus, trolley coach, trackless trolley',\n",
      " 875: 'trombone',\n",
      " 876: 'tub, vat',\n",
      " 877: 'turnstile',\n",
      " 878: 'typewriter keyboard',\n",
      " 879: 'umbrella',\n",
      " 880: 'unicycle, monocycle',\n",
      " 881: 'upright, upright piano',\n",
      " 882: 'vacuum, vacuum cleaner',\n",
      " 883: 'vase',\n",
      " 884: 'vault',\n",
      " 885: 'velvet',\n",
      " 886: 'vending machine',\n",
      " 887: 'vestment',\n",
      " 888: 'viaduct',\n",
      " 889: 'violin, fiddle',\n",
      " 890: 'volleyball',\n",
      " 891: 'waffle iron',\n",
      " 892: 'wall clock',\n",
      " 893: 'wallet, billfold, notecase, pocketbook',\n",
      " 894: 'wardrobe, closet, press',\n",
      " 895: 'warplane, military plane',\n",
      " 896: 'washbasin, handbasin, washbowl, lavabo, wash-hand basin',\n",
      " 897: 'washer, automatic washer, washing machine',\n",
      " 898: 'water bottle',\n",
      " 899: 'water jug',\n",
      " 900: 'water tower',\n",
      " 901: 'whiskey jug',\n",
      " 902: 'whistle',\n",
      " 903: 'wig',\n",
      " 904: 'window screen',\n",
      " 905: 'window shade',\n",
      " 906: 'Windsor tie',\n",
      " 907: 'wine bottle',\n",
      " 908: 'wing',\n",
      " 909: 'wok',\n",
      " 910: 'wooden spoon',\n",
      " 911: 'wool, woolen, woollen',\n",
      " 912: 'worm fence, snake fence, snake-rail fence, Virginia fence',\n",
      " 913: 'wreck',\n",
      " 914: 'yawl',\n",
      " 915: 'yurt',\n",
      " 916: 'web site, website, internet site, site',\n",
      " 917: 'comic book',\n",
      " 918: 'crossword puzzle, crossword',\n",
      " 919: 'street sign',\n",
      " 920: 'traffic light, traffic signal, stoplight',\n",
      " 921: 'book jacket, dust cover, dust jacket, dust wrapper',\n",
      " 922: 'menu',\n",
      " 923: 'plate',\n",
      " 924: 'guacamole',\n",
      " 925: 'consomme',\n",
      " 926: 'hot pot, hotpot',\n",
      " 927: 'trifle',\n",
      " 928: 'ice cream, icecream',\n",
      " 929: 'ice lolly, lolly, lollipop, popsicle',\n",
      " 930: 'French loaf',\n",
      " 931: 'bagel, beigel',\n",
      " 932: 'pretzel',\n",
      " 933: 'cheeseburger',\n",
      " 934: 'hotdog, hot dog, red hot',\n",
      " 935: 'mashed potato',\n",
      " 936: 'head cabbage',\n",
      " 937: 'broccoli',\n",
      " 938: 'cauliflower',\n",
      " 939: 'zucchini, courgette',\n",
      " 940: 'spaghetti squash',\n",
      " 941: 'acorn squash',\n",
      " 942: 'butternut squash',\n",
      " 943: 'cucumber, cuke',\n",
      " 944: 'artichoke, globe artichoke',\n",
      " 945: 'bell pepper',\n",
      " 946: 'cardoon',\n",
      " 947: 'mushroom',\n",
      " 948: 'Granny Smith',\n",
      " 949: 'strawberry',\n",
      " 950: 'orange',\n",
      " 951: 'lemon',\n",
      " 952: 'fig',\n",
      " 953: 'pineapple, ananas',\n",
      " 954: 'banana',\n",
      " 955: 'jackfruit, jak, jack',\n",
      " 956: 'custard apple',\n",
      " 957: 'pomegranate',\n",
      " 958: 'hay',\n",
      " 959: 'carbonara',\n",
      " 960: 'chocolate sauce, chocolate syrup',\n",
      " 961: 'dough',\n",
      " 962: 'meat loaf, meatloaf',\n",
      " 963: 'pizza, pizza pie',\n",
      " 964: 'potpie',\n",
      " 965: 'burrito',\n",
      " 966: 'red wine',\n",
      " 967: 'espresso',\n",
      " 968: 'cup',\n",
      " 969: 'eggnog',\n",
      " 970: 'alp',\n",
      " 971: 'bubble',\n",
      " 972: 'cliff, drop, drop-off',\n",
      " 973: 'coral reef',\n",
      " 974: 'geyser',\n",
      " 975: 'lakeside, lakeshore',\n",
      " 976: 'promontory, headland, head, foreland',\n",
      " 977: 'sandbar, sand bar',\n",
      " 978: 'seashore, coast, seacoast, sea-coast',\n",
      " 979: 'valley, vale',\n",
      " 980: 'volcano',\n",
      " 981: 'ballplayer, baseball player',\n",
      " 982: 'groom, bridegroom',\n",
      " 983: 'scuba diver',\n",
      " 984: 'rapeseed',\n",
      " 985: 'daisy',\n",
      " 986: \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\",\n",
      " 987: 'corn',\n",
      " 988: 'acorn',\n",
      " 989: 'hip, rose hip, rosehip',\n",
      " 990: 'buckeye, horse chestnut, conker',\n",
      " 991: 'coral fungus',\n",
      " 992: 'agaric',\n",
      " 993: 'gyromitra',\n",
      " 994: 'stinkhorn, carrion fungus',\n",
      " 995: 'earthstar',\n",
      " 996: 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa',\n",
      " 997: 'bolete',\n",
      " 998: 'ear, spike, capitulum',\n",
      " 999: 'toilet tissue, toilet paper, bathroom tissue'}\n",
      "62/4:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 224\n",
      "\n",
      "def prep_image(image):\n",
      "    res = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "    res = res.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "    return res\n",
      "\n",
      "def predict(image):\n",
      "    cropped_image = center_crop(image)\n",
      "    prediction = (model.predict(prep_image(image)), model.predict(prep_image(cropped_image)))\n",
      "    if np.max(prediction[0]>=prediction[1]):\n",
      "        return np.argmax(prediction[0], axis=1)[0]\n",
      "    else:\n",
      "        return np.argmax(prediction[1], axis=1)[1]\n",
      "    \n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "plt.imshow(image)\n",
      "\n",
      "print(class_dict[predict(image)])\n",
      "63/1:\n",
      "from tensorflow.keras.applications import EfficientNetB0\n",
      "from tensorflow.image import resize\n",
      "model = EfficientNetB0(weights='imagenet')\n",
      "\n",
      "def center_crop(img, new_width=None, new_height=None):        \n",
      "\n",
      "    width = img.shape[1]\n",
      "    height = img.shape[0]\n",
      "\n",
      "    if new_width is None:\n",
      "        new_width = min(width, height)\n",
      "\n",
      "    if new_height is None:\n",
      "        new_height = min(width, height)\n",
      "\n",
      "    left = int(np.ceil((width - new_width) / 2))\n",
      "    right = width - int(np.floor((width - new_width) / 2))\n",
      "\n",
      "    top = int(np.ceil((height - new_height) / 2))\n",
      "    bottom = height - int(np.floor((height - new_height) / 2))\n",
      "\n",
      "    if len(img.shape) == 2:\n",
      "        center_cropped_img = img[top:bottom, left:right]\n",
      "    else:\n",
      "        center_cropped_img = img[top:bottom, left:right, ...]\n",
      "\n",
      "    return center_cropped_img\n",
      "\n",
      "class_dict = {0: 'tench, Tinca tinca',\n",
      " 1: 'goldfish, Carassius auratus',\n",
      " 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n",
      " 3: 'tiger shark, Galeocerdo cuvieri',\n",
      " 4: 'hammerhead, hammerhead shark',\n",
      " 5: 'electric ray, crampfish, numbfish, torpedo',\n",
      " 6: 'stingray',\n",
      " 7: 'cock',\n",
      " 8: 'hen',\n",
      " 9: 'ostrich, Struthio camelus',\n",
      " 10: 'brambling, Fringilla montifringilla',\n",
      " 11: 'goldfinch, Carduelis carduelis',\n",
      " 12: 'house finch, linnet, Carpodacus mexicanus',\n",
      " 13: 'junco, snowbird',\n",
      " 14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea',\n",
      " 15: 'robin, American robin, Turdus migratorius',\n",
      " 16: 'bulbul',\n",
      " 17: 'jay',\n",
      " 18: 'magpie',\n",
      " 19: 'chickadee',\n",
      " 20: 'water ouzel, dipper',\n",
      " 21: 'kite',\n",
      " 22: 'bald eagle, American eagle, Haliaeetus leucocephalus',\n",
      " 23: 'vulture',\n",
      " 24: 'great grey owl, great gray owl, Strix nebulosa',\n",
      " 25: 'European fire salamander, Salamandra salamandra',\n",
      " 26: 'common newt, Triturus vulgaris',\n",
      " 27: 'eft',\n",
      " 28: 'spotted salamander, Ambystoma maculatum',\n",
      " 29: 'axolotl, mud puppy, Ambystoma mexicanum',\n",
      " 30: 'bullfrog, Rana catesbeiana',\n",
      " 31: 'tree frog, tree-frog',\n",
      " 32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui',\n",
      " 33: 'loggerhead, loggerhead turtle, Caretta caretta',\n",
      " 34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea',\n",
      " 35: 'mud turtle',\n",
      " 36: 'terrapin',\n",
      " 37: 'box turtle, box tortoise',\n",
      " 38: 'banded gecko',\n",
      " 39: 'common iguana, iguana, Iguana iguana',\n",
      " 40: 'American chameleon, anole, Anolis carolinensis',\n",
      " 41: 'whiptail, whiptail lizard',\n",
      " 42: 'agama',\n",
      " 43: 'frilled lizard, Chlamydosaurus kingi',\n",
      " 44: 'alligator lizard',\n",
      " 45: 'Gila monster, Heloderma suspectum',\n",
      " 46: 'green lizard, Lacerta viridis',\n",
      " 47: 'African chameleon, Chamaeleo chamaeleon',\n",
      " 48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis',\n",
      " 49: 'African crocodile, Nile crocodile, Crocodylus niloticus',\n",
      " 50: 'American alligator, Alligator mississipiensis',\n",
      " 51: 'triceratops',\n",
      " 52: 'thunder snake, worm snake, Carphophis amoenus',\n",
      " 53: 'ringneck snake, ring-necked snake, ring snake',\n",
      " 54: 'hognose snake, puff adder, sand viper',\n",
      " 55: 'green snake, grass snake',\n",
      " 56: 'king snake, kingsnake',\n",
      " 57: 'garter snake, grass snake',\n",
      " 58: 'water snake',\n",
      " 59: 'vine snake',\n",
      " 60: 'night snake, Hypsiglena torquata',\n",
      " 61: 'boa constrictor, Constrictor constrictor',\n",
      " 62: 'rock python, rock snake, Python sebae',\n",
      " 63: 'Indian cobra, Naja naja',\n",
      " 64: 'green mamba',\n",
      " 65: 'sea snake',\n",
      " 66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus',\n",
      " 67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus',\n",
      " 68: 'sidewinder, horned rattlesnake, Crotalus cerastes',\n",
      " 69: 'trilobite',\n",
      " 70: 'harvestman, daddy longlegs, Phalangium opilio',\n",
      " 71: 'scorpion',\n",
      " 72: 'black and gold garden spider, Argiope aurantia',\n",
      " 73: 'barn spider, Araneus cavaticus',\n",
      " 74: 'garden spider, Aranea diademata',\n",
      " 75: 'black widow, Latrodectus mactans',\n",
      " 76: 'tarantula',\n",
      " 77: 'wolf spider, hunting spider',\n",
      " 78: 'tick',\n",
      " 79: 'centipede',\n",
      " 80: 'black grouse',\n",
      " 81: 'ptarmigan',\n",
      " 82: 'ruffed grouse, partridge, Bonasa umbellus',\n",
      " 83: 'prairie chicken, prairie grouse, prairie fowl',\n",
      " 84: 'peacock',\n",
      " 85: 'quail',\n",
      " 86: 'partridge',\n",
      " 87: 'African grey, African gray, Psittacus erithacus',\n",
      " 88: 'macaw',\n",
      " 89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n",
      " 90: 'lorikeet',\n",
      " 91: 'coucal',\n",
      " 92: 'bee eater',\n",
      " 93: 'hornbill',\n",
      " 94: 'hummingbird',\n",
      " 95: 'jacamar',\n",
      " 96: 'toucan',\n",
      " 97: 'drake',\n",
      " 98: 'red-breasted merganser, Mergus serrator',\n",
      " 99: 'goose',\n",
      " 100: 'black swan, Cygnus atratus',\n",
      " 101: 'tusker',\n",
      " 102: 'echidna, spiny anteater, anteater',\n",
      " 103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n",
      " 104: 'wallaby, brush kangaroo',\n",
      " 105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n",
      " 106: 'wombat',\n",
      " 107: 'jellyfish',\n",
      " 108: 'sea anemone, anemone',\n",
      " 109: 'brain coral',\n",
      " 110: 'flatworm, platyhelminth',\n",
      " 111: 'nematode, nematode worm, roundworm',\n",
      " 112: 'conch',\n",
      " 113: 'snail',\n",
      " 114: 'slug',\n",
      " 115: 'sea slug, nudibranch',\n",
      " 116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore',\n",
      " 117: 'chambered nautilus, pearly nautilus, nautilus',\n",
      " 118: 'Dungeness crab, Cancer magister',\n",
      " 119: 'rock crab, Cancer irroratus',\n",
      " 120: 'fiddler crab',\n",
      " 121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica',\n",
      " 122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus',\n",
      " 123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish',\n",
      " 124: 'crayfish, crawfish, crawdad, crawdaddy',\n",
      " 125: 'hermit crab',\n",
      " 126: 'isopod',\n",
      " 127: 'white stork, Ciconia ciconia',\n",
      " 128: 'black stork, Ciconia nigra',\n",
      " 129: 'spoonbill',\n",
      " 130: 'flamingo',\n",
      " 131: 'little blue heron, Egretta caerulea',\n",
      " 132: 'American egret, great white heron, Egretta albus',\n",
      " 133: 'bittern',\n",
      " 134: 'crane',\n",
      " 135: 'limpkin, Aramus pictus',\n",
      " 136: 'European gallinule, Porphyrio porphyrio',\n",
      " 137: 'American coot, marsh hen, mud hen, water hen, Fulica americana',\n",
      " 138: 'bustard',\n",
      " 139: 'ruddy turnstone, Arenaria interpres',\n",
      " 140: 'red-backed sandpiper, dunlin, Erolia alpina',\n",
      " 141: 'redshank, Tringa totanus',\n",
      " 142: 'dowitcher',\n",
      " 143: 'oystercatcher, oyster catcher',\n",
      " 144: 'pelican',\n",
      " 145: 'king penguin, Aptenodytes patagonica',\n",
      " 146: 'albatross, mollymawk',\n",
      " 147: 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus',\n",
      " 148: 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca',\n",
      " 149: 'dugong, Dugong dugon',\n",
      " 150: 'sea lion',\n",
      " 151: 'Chihuahua',\n",
      " 152: 'Japanese spaniel',\n",
      " 153: 'Maltese dog, Maltese terrier, Maltese',\n",
      " 154: 'Pekinese, Pekingese, Peke',\n",
      " 155: 'Shih-Tzu',\n",
      " 156: 'Blenheim spaniel',\n",
      " 157: 'papillon',\n",
      " 158: 'toy terrier',\n",
      " 159: 'Rhodesian ridgeback',\n",
      " 160: 'Afghan hound, Afghan',\n",
      " 161: 'basset, basset hound',\n",
      " 162: 'beagle',\n",
      " 163: 'bloodhound, sleuthhound',\n",
      " 164: 'bluetick',\n",
      " 165: 'black-and-tan coonhound',\n",
      " 166: 'Walker hound, Walker foxhound',\n",
      " 167: 'English foxhound',\n",
      " 168: 'redbone',\n",
      " 169: 'borzoi, Russian wolfhound',\n",
      " 170: 'Irish wolfhound',\n",
      " 171: 'Italian greyhound',\n",
      " 172: 'whippet',\n",
      " 173: 'Ibizan hound, Ibizan Podenco',\n",
      " 174: 'Norwegian elkhound, elkhound',\n",
      " 175: 'otterhound, otter hound',\n",
      " 176: 'Saluki, gazelle hound',\n",
      " 177: 'Scottish deerhound, deerhound',\n",
      " 178: 'Weimaraner',\n",
      " 179: 'Staffordshire bullterrier, Staffordshire bull terrier',\n",
      " 180: 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier',\n",
      " 181: 'Bedlington terrier',\n",
      " 182: 'Border terrier',\n",
      " 183: 'Kerry blue terrier',\n",
      " 184: 'Irish terrier',\n",
      " 185: 'Norfolk terrier',\n",
      " 186: 'Norwich terrier',\n",
      " 187: 'Yorkshire terrier',\n",
      " 188: 'wire-haired fox terrier',\n",
      " 189: 'Lakeland terrier',\n",
      " 190: 'Sealyham terrier, Sealyham',\n",
      " 191: 'Airedale, Airedale terrier',\n",
      " 192: 'cairn, cairn terrier',\n",
      " 193: 'Australian terrier',\n",
      " 194: 'Dandie Dinmont, Dandie Dinmont terrier',\n",
      " 195: 'Boston bull, Boston terrier',\n",
      " 196: 'miniature schnauzer',\n",
      " 197: 'giant schnauzer',\n",
      " 198: 'standard schnauzer',\n",
      " 199: 'Scotch terrier, Scottish terrier, Scottie',\n",
      " 200: 'Tibetan terrier, chrysanthemum dog',\n",
      " 201: 'silky terrier, Sydney silky',\n",
      " 202: 'soft-coated wheaten terrier',\n",
      " 203: 'West Highland white terrier',\n",
      " 204: 'Lhasa, Lhasa apso',\n",
      " 205: 'flat-coated retriever',\n",
      " 206: 'curly-coated retriever',\n",
      " 207: 'golden retriever',\n",
      " 208: 'Labrador retriever',\n",
      " 209: 'Chesapeake Bay retriever',\n",
      " 210: 'German short-haired pointer',\n",
      " 211: 'vizsla, Hungarian pointer',\n",
      " 212: 'English setter',\n",
      " 213: 'Irish setter, red setter',\n",
      " 214: 'Gordon setter',\n",
      " 215: 'Brittany spaniel',\n",
      " 216: 'clumber, clumber spaniel',\n",
      " 217: 'English springer, English springer spaniel',\n",
      " 218: 'Welsh springer spaniel',\n",
      " 219: 'cocker spaniel, English cocker spaniel, cocker',\n",
      " 220: 'Sussex spaniel',\n",
      " 221: 'Irish water spaniel',\n",
      " 222: 'kuvasz',\n",
      " 223: 'schipperke',\n",
      " 224: 'groenendael',\n",
      " 225: 'malinois',\n",
      " 226: 'briard',\n",
      " 227: 'kelpie',\n",
      " 228: 'komondor',\n",
      " 229: 'Old English sheepdog, bobtail',\n",
      " 230: 'Shetland sheepdog, Shetland sheep dog, Shetland',\n",
      " 231: 'collie',\n",
      " 232: 'Border collie',\n",
      " 233: 'Bouvier des Flandres, Bouviers des Flandres',\n",
      " 234: 'Rottweiler',\n",
      " 235: 'German shepherd, German shepherd dog, German police dog, alsatian',\n",
      " 236: 'Doberman, Doberman pinscher',\n",
      " 237: 'miniature pinscher',\n",
      " 238: 'Greater Swiss Mountain dog',\n",
      " 239: 'Bernese mountain dog',\n",
      " 240: 'Appenzeller',\n",
      " 241: 'EntleBucher',\n",
      " 242: 'boxer',\n",
      " 243: 'bull mastiff',\n",
      " 244: 'Tibetan mastiff',\n",
      " 245: 'French bulldog',\n",
      " 246: 'Great Dane',\n",
      " 247: 'Saint Bernard, St Bernard',\n",
      " 248: 'Eskimo dog, husky',\n",
      " 249: 'malamute, malemute, Alaskan malamute',\n",
      " 250: 'Siberian husky',\n",
      " 251: 'dalmatian, coach dog, carriage dog',\n",
      " 252: 'affenpinscher, monkey pinscher, monkey dog',\n",
      " 253: 'basenji',\n",
      " 254: 'pug, pug-dog',\n",
      " 255: 'Leonberg',\n",
      " 256: 'Newfoundland, Newfoundland dog',\n",
      " 257: 'Great Pyrenees',\n",
      " 258: 'Samoyed, Samoyede',\n",
      " 259: 'Pomeranian',\n",
      " 260: 'chow, chow chow',\n",
      " 261: 'keeshond',\n",
      " 262: 'Brabancon griffon',\n",
      " 263: 'Pembroke, Pembroke Welsh corgi',\n",
      " 264: 'Cardigan, Cardigan Welsh corgi',\n",
      " 265: 'toy poodle',\n",
      " 266: 'miniature poodle',\n",
      " 267: 'standard poodle',\n",
      " 268: 'Mexican hairless',\n",
      " 269: 'timber wolf, grey wolf, gray wolf, Canis lupus',\n",
      " 270: 'white wolf, Arctic wolf, Canis lupus tundrarum',\n",
      " 271: 'red wolf, maned wolf, Canis rufus, Canis niger',\n",
      " 272: 'coyote, prairie wolf, brush wolf, Canis latrans',\n",
      " 273: 'dingo, warrigal, warragal, Canis dingo',\n",
      " 274: 'dhole, Cuon alpinus',\n",
      " 275: 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus',\n",
      " 276: 'hyena, hyaena',\n",
      " 277: 'red fox, Vulpes vulpes',\n",
      " 278: 'kit fox, Vulpes macrotis',\n",
      " 279: 'Arctic fox, white fox, Alopex lagopus',\n",
      " 280: 'grey fox, gray fox, Urocyon cinereoargenteus',\n",
      " 281: 'tabby, tabby cat',\n",
      " 282: 'tiger cat',\n",
      " 283: 'Persian cat',\n",
      " 284: 'Siamese cat, Siamese',\n",
      " 285: 'Egyptian cat',\n",
      " 286: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n",
      " 287: 'lynx, catamount',\n",
      " 288: 'leopard, Panthera pardus',\n",
      " 289: 'snow leopard, ounce, Panthera uncia',\n",
      " 290: 'jaguar, panther, Panthera onca, Felis onca',\n",
      " 291: 'lion, king of beasts, Panthera leo',\n",
      " 292: 'tiger, Panthera tigris',\n",
      " 293: 'cheetah, chetah, Acinonyx jubatus',\n",
      " 294: 'brown bear, bruin, Ursus arctos',\n",
      " 295: 'American black bear, black bear, Ursus americanus, Euarctos americanus',\n",
      " 296: 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus',\n",
      " 297: 'sloth bear, Melursus ursinus, Ursus ursinus',\n",
      " 298: 'mongoose',\n",
      " 299: 'meerkat, mierkat',\n",
      " 300: 'tiger beetle',\n",
      " 301: 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle',\n",
      " 302: 'ground beetle, carabid beetle',\n",
      " 303: 'long-horned beetle, longicorn, longicorn beetle',\n",
      " 304: 'leaf beetle, chrysomelid',\n",
      " 305: 'dung beetle',\n",
      " 306: 'rhinoceros beetle',\n",
      " 307: 'weevil',\n",
      " 308: 'fly',\n",
      " 309: 'bee',\n",
      " 310: 'ant, emmet, pismire',\n",
      " 311: 'grasshopper, hopper',\n",
      " 312: 'cricket',\n",
      " 313: 'walking stick, walkingstick, stick insect',\n",
      " 314: 'cockroach, roach',\n",
      " 315: 'mantis, mantid',\n",
      " 316: 'cicada, cicala',\n",
      " 317: 'leafhopper',\n",
      " 318: 'lacewing, lacewing fly',\n",
      " 319: \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\",\n",
      " 320: 'damselfly',\n",
      " 321: 'admiral',\n",
      " 322: 'ringlet, ringlet butterfly',\n",
      " 323: 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus',\n",
      " 324: 'cabbage butterfly',\n",
      " 325: 'sulphur butterfly, sulfur butterfly',\n",
      " 326: 'lycaenid, lycaenid butterfly',\n",
      " 327: 'starfish, sea star',\n",
      " 328: 'sea urchin',\n",
      " 329: 'sea cucumber, holothurian',\n",
      " 330: 'wood rabbit, cottontail, cottontail rabbit',\n",
      " 331: 'hare',\n",
      " 332: 'Angora, Angora rabbit',\n",
      " 333: 'hamster',\n",
      " 334: 'porcupine, hedgehog',\n",
      " 335: 'fox squirrel, eastern fox squirrel, Sciurus niger',\n",
      " 336: 'marmot',\n",
      " 337: 'beaver',\n",
      " 338: 'guinea pig, Cavia cobaya',\n",
      " 339: 'sorrel',\n",
      " 340: 'zebra',\n",
      " 341: 'hog, pig, grunter, squealer, Sus scrofa',\n",
      " 342: 'wild boar, boar, Sus scrofa',\n",
      " 343: 'warthog',\n",
      " 344: 'hippopotamus, hippo, river horse, Hippopotamus amphibius',\n",
      " 345: 'ox',\n",
      " 346: 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis',\n",
      " 347: 'bison',\n",
      " 348: 'ram, tup',\n",
      " 349: 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis',\n",
      " 350: 'ibex, Capra ibex',\n",
      " 351: 'hartebeest',\n",
      " 352: 'impala, Aepyceros melampus',\n",
      " 353: 'gazelle',\n",
      " 354: 'Arabian camel, dromedary, Camelus dromedarius',\n",
      " 355: 'llama',\n",
      " 356: 'weasel',\n",
      " 357: 'mink',\n",
      " 358: 'polecat, fitch, foulmart, foumart, Mustela putorius',\n",
      " 359: 'black-footed ferret, ferret, Mustela nigripes',\n",
      " 360: 'otter',\n",
      " 361: 'skunk, polecat, wood pussy',\n",
      " 362: 'badger',\n",
      " 363: 'armadillo',\n",
      " 364: 'three-toed sloth, ai, Bradypus tridactylus',\n",
      " 365: 'orangutan, orang, orangutang, Pongo pygmaeus',\n",
      " 366: 'gorilla, Gorilla gorilla',\n",
      " 367: 'chimpanzee, chimp, Pan troglodytes',\n",
      " 368: 'gibbon, Hylobates lar',\n",
      " 369: 'siamang, Hylobates syndactylus, Symphalangus syndactylus',\n",
      " 370: 'guenon, guenon monkey',\n",
      " 371: 'patas, hussar monkey, Erythrocebus patas',\n",
      " 372: 'baboon',\n",
      " 373: 'macaque',\n",
      " 374: 'langur',\n",
      " 375: 'colobus, colobus monkey',\n",
      " 376: 'proboscis monkey, Nasalis larvatus',\n",
      " 377: 'marmoset',\n",
      " 378: 'capuchin, ringtail, Cebus capucinus',\n",
      " 379: 'howler monkey, howler',\n",
      " 380: 'titi, titi monkey',\n",
      " 381: 'spider monkey, Ateles geoffroyi',\n",
      " 382: 'squirrel monkey, Saimiri sciureus',\n",
      " 383: 'Madagascar cat, ring-tailed lemur, Lemur catta',\n",
      " 384: 'indri, indris, Indri indri, Indri brevicaudatus',\n",
      " 385: 'Indian elephant, Elephas maximus',\n",
      " 386: 'African elephant, Loxodonta africana',\n",
      " 387: 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens',\n",
      " 388: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca',\n",
      " 389: 'barracouta, snoek',\n",
      " 390: 'eel',\n",
      " 391: 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch',\n",
      " 392: 'rock beauty, Holocanthus tricolor',\n",
      " 393: 'anemone fish',\n",
      " 394: 'sturgeon',\n",
      " 395: 'gar, garfish, garpike, billfish, Lepisosteus osseus',\n",
      " 396: 'lionfish',\n",
      " 397: 'puffer, pufferfish, blowfish, globefish',\n",
      " 398: 'abacus',\n",
      " 399: 'abaya',\n",
      " 400: \"academic gown, academic robe, judge's robe\",\n",
      " 401: 'accordion, piano accordion, squeeze box',\n",
      " 402: 'acoustic guitar',\n",
      " 403: 'aircraft carrier, carrier, flattop, attack aircraft carrier',\n",
      " 404: 'airliner',\n",
      " 405: 'airship, dirigible',\n",
      " 406: 'altar',\n",
      " 407: 'ambulance',\n",
      " 408: 'amphibian, amphibious vehicle',\n",
      " 409: 'analog clock',\n",
      " 410: 'apiary, bee house',\n",
      " 411: 'apron',\n",
      " 412: 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin',\n",
      " 413: 'assault rifle, assault gun',\n",
      " 414: 'backpack, back pack, knapsack, packsack, rucksack, haversack',\n",
      " 415: 'bakery, bakeshop, bakehouse',\n",
      " 416: 'balance beam, beam',\n",
      " 417: 'balloon',\n",
      " 418: 'ballpoint, ballpoint pen, ballpen, Biro',\n",
      " 419: 'Band Aid',\n",
      " 420: 'banjo',\n",
      " 421: 'bannister, banister, balustrade, balusters, handrail',\n",
      " 422: 'barbell',\n",
      " 423: 'barber chair',\n",
      " 424: 'barbershop',\n",
      " 425: 'barn',\n",
      " 426: 'barometer',\n",
      " 427: 'barrel, cask',\n",
      " 428: 'barrow, garden cart, lawn cart, wheelbarrow',\n",
      " 429: 'baseball',\n",
      " 430: 'basketball',\n",
      " 431: 'bassinet',\n",
      " 432: 'bassoon',\n",
      " 433: 'bathing cap, swimming cap',\n",
      " 434: 'bath towel',\n",
      " 435: 'bathtub, bathing tub, bath, tub',\n",
      " 436: 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon',\n",
      " 437: 'beacon, lighthouse, beacon light, pharos',\n",
      " 438: 'beaker',\n",
      " 439: 'bearskin, busby, shako',\n",
      " 440: 'beer bottle',\n",
      " 441: 'beer glass',\n",
      " 442: 'bell cote, bell cot',\n",
      " 443: 'bib',\n",
      " 444: 'bicycle-built-for-two, tandem bicycle, tandem',\n",
      " 445: 'bikini, two-piece',\n",
      " 446: 'binder, ring-binder',\n",
      " 447: 'binoculars, field glasses, opera glasses',\n",
      " 448: 'birdhouse',\n",
      " 449: 'boathouse',\n",
      " 450: 'bobsled, bobsleigh, bob',\n",
      " 451: 'bolo tie, bolo, bola tie, bola',\n",
      " 452: 'bonnet, poke bonnet',\n",
      " 453: 'bookcase',\n",
      " 454: 'bookshop, bookstore, bookstall',\n",
      " 455: 'bottlecap',\n",
      " 456: 'bow',\n",
      " 457: 'bow tie, bow-tie, bowtie',\n",
      " 458: 'brass, memorial tablet, plaque',\n",
      " 459: 'brassiere, bra, bandeau',\n",
      " 460: 'breakwater, groin, groyne, mole, bulwark, seawall, jetty',\n",
      " 461: 'breastplate, aegis, egis',\n",
      " 462: 'broom',\n",
      " 463: 'bucket, pail',\n",
      " 464: 'buckle',\n",
      " 465: 'bulletproof vest',\n",
      " 466: 'bullet train, bullet',\n",
      " 467: 'butcher shop, meat market',\n",
      " 468: 'cab, hack, taxi, taxicab',\n",
      " 469: 'caldron, cauldron',\n",
      " 470: 'candle, taper, wax light',\n",
      " 471: 'cannon',\n",
      " 472: 'canoe',\n",
      " 473: 'can opener, tin opener',\n",
      " 474: 'cardigan',\n",
      " 475: 'car mirror',\n",
      " 476: 'carousel, carrousel, merry-go-round, roundabout, whirligig',\n",
      " 477: \"carpenter's kit, tool kit\",\n",
      " 478: 'carton',\n",
      " 479: 'car wheel',\n",
      " 480: 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM',\n",
      " 481: 'cassette',\n",
      " 482: 'cassette player',\n",
      " 483: 'castle',\n",
      " 484: 'catamaran',\n",
      " 485: 'CD player',\n",
      " 486: 'cello, violoncello',\n",
      " 487: 'cellular telephone, cellular phone, cellphone, cell, mobile phone',\n",
      " 488: 'chain',\n",
      " 489: 'chainlink fence',\n",
      " 490: 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour',\n",
      " 491: 'chain saw, chainsaw',\n",
      " 492: 'chest',\n",
      " 493: 'chiffonier, commode',\n",
      " 494: 'chime, bell, gong',\n",
      " 495: 'china cabinet, china closet',\n",
      " 496: 'Christmas stocking',\n",
      " 497: 'church, church building',\n",
      " 498: 'cinema, movie theater, movie theatre, movie house, picture palace',\n",
      " 499: 'cleaver, meat cleaver, chopper',\n",
      " 500: 'cliff dwelling',\n",
      " 501: 'cloak',\n",
      " 502: 'clog, geta, patten, sabot',\n",
      " 503: 'cocktail shaker',\n",
      " 504: 'coffee mug',\n",
      " 505: 'coffeepot',\n",
      " 506: 'coil, spiral, volute, whorl, helix',\n",
      " 507: 'combination lock',\n",
      " 508: 'computer keyboard, keypad',\n",
      " 509: 'confectionery, confectionary, candy store',\n",
      " 510: 'container ship, containership, container vessel',\n",
      " 511: 'convertible',\n",
      " 512: 'corkscrew, bottle screw',\n",
      " 513: 'cornet, horn, trumpet, trump',\n",
      " 514: 'cowboy boot',\n",
      " 515: 'cowboy hat, ten-gallon hat',\n",
      " 516: 'cradle',\n",
      " 517: 'crane',\n",
      " 518: 'crash helmet',\n",
      " 519: 'crate',\n",
      " 520: 'crib, cot',\n",
      " 521: 'Crock Pot',\n",
      " 522: 'croquet ball',\n",
      " 523: 'crutch',\n",
      " 524: 'cuirass',\n",
      " 525: 'dam, dike, dyke',\n",
      " 526: 'desk',\n",
      " 527: 'desktop computer',\n",
      " 528: 'dial telephone, dial phone',\n",
      " 529: 'diaper, nappy, napkin',\n",
      " 530: 'digital clock',\n",
      " 531: 'digital watch',\n",
      " 532: 'dining table, board',\n",
      " 533: 'dishrag, dishcloth',\n",
      " 534: 'dishwasher, dish washer, dishwashing machine',\n",
      " 535: 'disk brake, disc brake',\n",
      " 536: 'dock, dockage, docking facility',\n",
      " 537: 'dogsled, dog sled, dog sleigh',\n",
      " 538: 'dome',\n",
      " 539: 'doormat, welcome mat',\n",
      " 540: 'drilling platform, offshore rig',\n",
      " 541: 'drum, membranophone, tympan',\n",
      " 542: 'drumstick',\n",
      " 543: 'dumbbell',\n",
      " 544: 'Dutch oven',\n",
      " 545: 'electric fan, blower',\n",
      " 546: 'electric guitar',\n",
      " 547: 'electric locomotive',\n",
      " 548: 'entertainment center',\n",
      " 549: 'envelope',\n",
      " 550: 'espresso maker',\n",
      " 551: 'face powder',\n",
      " 552: 'feather boa, boa',\n",
      " 553: 'file, file cabinet, filing cabinet',\n",
      " 554: 'fireboat',\n",
      " 555: 'fire engine, fire truck',\n",
      " 556: 'fire screen, fireguard',\n",
      " 557: 'flagpole, flagstaff',\n",
      " 558: 'flute, transverse flute',\n",
      " 559: 'folding chair',\n",
      " 560: 'football helmet',\n",
      " 561: 'forklift',\n",
      " 562: 'fountain',\n",
      " 563: 'fountain pen',\n",
      " 564: 'four-poster',\n",
      " 565: 'freight car',\n",
      " 566: 'French horn, horn',\n",
      " 567: 'frying pan, frypan, skillet',\n",
      " 568: 'fur coat',\n",
      " 569: 'garbage truck, dustcart',\n",
      " 570: 'gasmask, respirator, gas helmet',\n",
      " 571: 'gas pump, gasoline pump, petrol pump, island dispenser',\n",
      " 572: 'goblet',\n",
      " 573: 'go-kart',\n",
      " 574: 'golf ball',\n",
      " 575: 'golfcart, golf cart',\n",
      " 576: 'gondola',\n",
      " 577: 'gong, tam-tam',\n",
      " 578: 'gown',\n",
      " 579: 'grand piano, grand',\n",
      " 580: 'greenhouse, nursery, glasshouse',\n",
      " 581: 'grille, radiator grille',\n",
      " 582: 'grocery store, grocery, food market, market',\n",
      " 583: 'guillotine',\n",
      " 584: 'hair slide',\n",
      " 585: 'hair spray',\n",
      " 586: 'half track',\n",
      " 587: 'hammer',\n",
      " 588: 'hamper',\n",
      " 589: 'hand blower, blow dryer, blow drier, hair dryer, hair drier',\n",
      " 590: 'hand-held computer, hand-held microcomputer',\n",
      " 591: 'handkerchief, hankie, hanky, hankey',\n",
      " 592: 'hard disc, hard disk, fixed disk',\n",
      " 593: 'harmonica, mouth organ, harp, mouth harp',\n",
      " 594: 'harp',\n",
      " 595: 'harvester, reaper',\n",
      " 596: 'hatchet',\n",
      " 597: 'holster',\n",
      " 598: 'home theater, home theatre',\n",
      " 599: 'honeycomb',\n",
      " 600: 'hook, claw',\n",
      " 601: 'hoopskirt, crinoline',\n",
      " 602: 'horizontal bar, high bar',\n",
      " 603: 'horse cart, horse-cart',\n",
      " 604: 'hourglass',\n",
      " 605: 'iPod',\n",
      " 606: 'iron, smoothing iron',\n",
      " 607: \"jack-o'-lantern\",\n",
      " 608: 'jean, blue jean, denim',\n",
      " 609: 'jeep, landrover',\n",
      " 610: 'jersey, T-shirt, tee shirt',\n",
      " 611: 'jigsaw puzzle',\n",
      " 612: 'jinrikisha, ricksha, rickshaw',\n",
      " 613: 'joystick',\n",
      " 614: 'kimono',\n",
      " 615: 'knee pad',\n",
      " 616: 'knot',\n",
      " 617: 'lab coat, laboratory coat',\n",
      " 618: 'ladle',\n",
      " 619: 'lampshade, lamp shade',\n",
      " 620: 'laptop, laptop computer',\n",
      " 621: 'lawn mower, mower',\n",
      " 622: 'lens cap, lens cover',\n",
      " 623: 'letter opener, paper knife, paperknife',\n",
      " 624: 'library',\n",
      " 625: 'lifeboat',\n",
      " 626: 'lighter, light, igniter, ignitor',\n",
      " 627: 'limousine, limo',\n",
      " 628: 'liner, ocean liner',\n",
      " 629: 'lipstick, lip rouge',\n",
      " 630: 'Loafer',\n",
      " 631: 'lotion',\n",
      " 632: 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system',\n",
      " 633: \"loupe, jeweler's loupe\",\n",
      " 634: 'lumbermill, sawmill',\n",
      " 635: 'magnetic compass',\n",
      " 636: 'mailbag, postbag',\n",
      " 637: 'mailbox, letter box',\n",
      " 638: 'maillot',\n",
      " 639: 'maillot, tank suit',\n",
      " 640: 'manhole cover',\n",
      " 641: 'maraca',\n",
      " 642: 'marimba, xylophone',\n",
      " 643: 'mask',\n",
      " 644: 'matchstick',\n",
      " 645: 'maypole',\n",
      " 646: 'maze, labyrinth',\n",
      " 647: 'measuring cup',\n",
      " 648: 'medicine chest, medicine cabinet',\n",
      " 649: 'megalith, megalithic structure',\n",
      " 650: 'microphone, mike',\n",
      " 651: 'microwave, microwave oven',\n",
      " 652: 'military uniform',\n",
      " 653: 'milk can',\n",
      " 654: 'minibus',\n",
      " 655: 'miniskirt, mini',\n",
      " 656: 'minivan',\n",
      " 657: 'missile',\n",
      " 658: 'mitten',\n",
      " 659: 'mixing bowl',\n",
      " 660: 'mobile home, manufactured home',\n",
      " 661: 'Model T',\n",
      " 662: 'modem',\n",
      " 663: 'monastery',\n",
      " 664: 'monitor',\n",
      " 665: 'moped',\n",
      " 666: 'mortar',\n",
      " 667: 'mortarboard',\n",
      " 668: 'mosque',\n",
      " 669: 'mosquito net',\n",
      " 670: 'motor scooter, scooter',\n",
      " 671: 'mountain bike, all-terrain bike, off-roader',\n",
      " 672: 'mountain tent',\n",
      " 673: 'mouse, computer mouse',\n",
      " 674: 'mousetrap',\n",
      " 675: 'moving van',\n",
      " 676: 'muzzle',\n",
      " 677: 'nail',\n",
      " 678: 'neck brace',\n",
      " 679: 'necklace',\n",
      " 680: 'nipple',\n",
      " 681: 'notebook, notebook computer',\n",
      " 682: 'obelisk',\n",
      " 683: 'oboe, hautboy, hautbois',\n",
      " 684: 'ocarina, sweet potato',\n",
      " 685: 'odometer, hodometer, mileometer, milometer',\n",
      " 686: 'oil filter',\n",
      " 687: 'organ, pipe organ',\n",
      " 688: 'oscilloscope, scope, cathode-ray oscilloscope, CRO',\n",
      " 689: 'overskirt',\n",
      " 690: 'oxcart',\n",
      " 691: 'oxygen mask',\n",
      " 692: 'packet',\n",
      " 693: 'paddle, boat paddle',\n",
      " 694: 'paddlewheel, paddle wheel',\n",
      " 695: 'padlock',\n",
      " 696: 'paintbrush',\n",
      " 697: \"pajama, pyjama, pj's, jammies\",\n",
      " 698: 'palace',\n",
      " 699: 'panpipe, pandean pipe, syrinx',\n",
      " 700: 'paper towel',\n",
      " 701: 'parachute, chute',\n",
      " 702: 'parallel bars, bars',\n",
      " 703: 'park bench',\n",
      " 704: 'parking meter',\n",
      " 705: 'passenger car, coach, carriage',\n",
      " 706: 'patio, terrace',\n",
      " 707: 'pay-phone, pay-station',\n",
      " 708: 'pedestal, plinth, footstall',\n",
      " 709: 'pencil box, pencil case',\n",
      " 710: 'pencil sharpener',\n",
      " 711: 'perfume, essence',\n",
      " 712: 'Petri dish',\n",
      " 713: 'photocopier',\n",
      " 714: 'pick, plectrum, plectron',\n",
      " 715: 'pickelhaube',\n",
      " 716: 'picket fence, paling',\n",
      " 717: 'pickup, pickup truck',\n",
      " 718: 'pier',\n",
      " 719: 'piggy bank, penny bank',\n",
      " 720: 'pill bottle',\n",
      " 721: 'pillow',\n",
      " 722: 'ping-pong ball',\n",
      " 723: 'pinwheel',\n",
      " 724: 'pirate, pirate ship',\n",
      " 725: 'pitcher, ewer',\n",
      " 726: \"plane, carpenter's plane, woodworking plane\",\n",
      " 727: 'planetarium',\n",
      " 728: 'plastic bag',\n",
      " 729: 'plate rack',\n",
      " 730: 'plow, plough',\n",
      " 731: \"plunger, plumber's helper\",\n",
      " 732: 'Polaroid camera, Polaroid Land camera',\n",
      " 733: 'pole',\n",
      " 734: 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria',\n",
      " 735: 'poncho',\n",
      " 736: 'pool table, billiard table, snooker table',\n",
      " 737: 'pop bottle, soda bottle',\n",
      " 738: 'pot, flowerpot',\n",
      " 739: \"potter's wheel\",\n",
      " 740: 'power drill',\n",
      " 741: 'prayer rug, prayer mat',\n",
      " 742: 'printer',\n",
      " 743: 'prison, prison house',\n",
      " 744: 'projectile, missile',\n",
      " 745: 'projector',\n",
      " 746: 'puck, hockey puck',\n",
      " 747: 'punching bag, punch bag, punching ball, punchball',\n",
      " 748: 'purse',\n",
      " 749: 'quill, quill pen',\n",
      " 750: 'quilt, comforter, comfort, puff',\n",
      " 751: 'racer, race car, racing car',\n",
      " 752: 'racket, racquet',\n",
      " 753: 'radiator',\n",
      " 754: 'radio, wireless',\n",
      " 755: 'radio telescope, radio reflector',\n",
      " 756: 'rain barrel',\n",
      " 757: 'recreational vehicle, RV, R.V.',\n",
      " 758: 'reel',\n",
      " 759: 'reflex camera',\n",
      " 760: 'refrigerator, icebox',\n",
      " 761: 'remote control, remote',\n",
      " 762: 'restaurant, eating house, eating place, eatery',\n",
      " 763: 'revolver, six-gun, six-shooter',\n",
      " 764: 'rifle',\n",
      " 765: 'rocking chair, rocker',\n",
      " 766: 'rotisserie',\n",
      " 767: 'rubber eraser, rubber, pencil eraser',\n",
      " 768: 'rugby ball',\n",
      " 769: 'rule, ruler',\n",
      " 770: 'running shoe',\n",
      " 771: 'safe',\n",
      " 772: 'safety pin',\n",
      " 773: 'saltshaker, salt shaker',\n",
      " 774: 'sandal',\n",
      " 775: 'sarong',\n",
      " 776: 'sax, saxophone',\n",
      " 777: 'scabbard',\n",
      " 778: 'scale, weighing machine',\n",
      " 779: 'school bus',\n",
      " 780: 'schooner',\n",
      " 781: 'scoreboard',\n",
      " 782: 'screen, CRT screen',\n",
      " 783: 'screw',\n",
      " 784: 'screwdriver',\n",
      " 785: 'seat belt, seatbelt',\n",
      " 786: 'sewing machine',\n",
      " 787: 'shield, buckler',\n",
      " 788: 'shoe shop, shoe-shop, shoe store',\n",
      " 789: 'shoji',\n",
      " 790: 'shopping basket',\n",
      " 791: 'shopping cart',\n",
      " 792: 'shovel',\n",
      " 793: 'shower cap',\n",
      " 794: 'shower curtain',\n",
      " 795: 'ski',\n",
      " 796: 'ski mask',\n",
      " 797: 'sleeping bag',\n",
      " 798: 'slide rule, slipstick',\n",
      " 799: 'sliding door',\n",
      " 800: 'slot, one-armed bandit',\n",
      " 801: 'snorkel',\n",
      " 802: 'snowmobile',\n",
      " 803: 'snowplow, snowplough',\n",
      " 804: 'soap dispenser',\n",
      " 805: 'soccer ball',\n",
      " 806: 'sock',\n",
      " 807: 'solar dish, solar collector, solar furnace',\n",
      " 808: 'sombrero',\n",
      " 809: 'soup bowl',\n",
      " 810: 'space bar',\n",
      " 811: 'space heater',\n",
      " 812: 'space shuttle',\n",
      " 813: 'spatula',\n",
      " 814: 'speedboat',\n",
      " 815: \"spider web, spider's web\",\n",
      " 816: 'spindle',\n",
      " 817: 'sports car, sport car',\n",
      " 818: 'spotlight, spot',\n",
      " 819: 'stage',\n",
      " 820: 'steam locomotive',\n",
      " 821: 'steel arch bridge',\n",
      " 822: 'steel drum',\n",
      " 823: 'stethoscope',\n",
      " 824: 'stole',\n",
      " 825: 'stone wall',\n",
      " 826: 'stopwatch, stop watch',\n",
      " 827: 'stove',\n",
      " 828: 'strainer',\n",
      " 829: 'streetcar, tram, tramcar, trolley, trolley car',\n",
      " 830: 'stretcher',\n",
      " 831: 'studio couch, day bed',\n",
      " 832: 'stupa, tope',\n",
      " 833: 'submarine, pigboat, sub, U-boat',\n",
      " 834: 'suit, suit of clothes',\n",
      " 835: 'sundial',\n",
      " 836: 'sunglass',\n",
      " 837: 'sunglasses, dark glasses, shades',\n",
      " 838: 'sunscreen, sunblock, sun blocker',\n",
      " 839: 'suspension bridge',\n",
      " 840: 'swab, swob, mop',\n",
      " 841: 'sweatshirt',\n",
      " 842: 'swimming trunks, bathing trunks',\n",
      " 843: 'swing',\n",
      " 844: 'switch, electric switch, electrical switch',\n",
      " 845: 'syringe',\n",
      " 846: 'table lamp',\n",
      " 847: 'tank, army tank, armored combat vehicle, armoured combat vehicle',\n",
      " 848: 'tape player',\n",
      " 849: 'teapot',\n",
      " 850: 'teddy, teddy bear',\n",
      " 851: 'television, television system',\n",
      " 852: 'tennis ball',\n",
      " 853: 'thatch, thatched roof',\n",
      " 854: 'theater curtain, theatre curtain',\n",
      " 855: 'thimble',\n",
      " 856: 'thresher, thrasher, threshing machine',\n",
      " 857: 'throne',\n",
      " 858: 'tile roof',\n",
      " 859: 'toaster',\n",
      " 860: 'tobacco shop, tobacconist shop, tobacconist',\n",
      " 861: 'toilet seat',\n",
      " 862: 'torch',\n",
      " 863: 'totem pole',\n",
      " 864: 'tow truck, tow car, wrecker',\n",
      " 865: 'toyshop',\n",
      " 866: 'tractor',\n",
      " 867: 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi',\n",
      " 868: 'tray',\n",
      " 869: 'trench coat',\n",
      " 870: 'tricycle, trike, velocipede',\n",
      " 871: 'trimaran',\n",
      " 872: 'tripod',\n",
      " 873: 'triumphal arch',\n",
      " 874: 'trolleybus, trolley coach, trackless trolley',\n",
      " 875: 'trombone',\n",
      " 876: 'tub, vat',\n",
      " 877: 'turnstile',\n",
      " 878: 'typewriter keyboard',\n",
      " 879: 'umbrella',\n",
      " 880: 'unicycle, monocycle',\n",
      " 881: 'upright, upright piano',\n",
      " 882: 'vacuum, vacuum cleaner',\n",
      " 883: 'vase',\n",
      " 884: 'vault',\n",
      " 885: 'velvet',\n",
      " 886: 'vending machine',\n",
      " 887: 'vestment',\n",
      " 888: 'viaduct',\n",
      " 889: 'violin, fiddle',\n",
      " 890: 'volleyball',\n",
      " 891: 'waffle iron',\n",
      " 892: 'wall clock',\n",
      " 893: 'wallet, billfold, notecase, pocketbook',\n",
      " 894: 'wardrobe, closet, press',\n",
      " 895: 'warplane, military plane',\n",
      " 896: 'washbasin, handbasin, washbowl, lavabo, wash-hand basin',\n",
      " 897: 'washer, automatic washer, washing machine',\n",
      " 898: 'water bottle',\n",
      " 899: 'water jug',\n",
      " 900: 'water tower',\n",
      " 901: 'whiskey jug',\n",
      " 902: 'whistle',\n",
      " 903: 'wig',\n",
      " 904: 'window screen',\n",
      " 905: 'window shade',\n",
      " 906: 'Windsor tie',\n",
      " 907: 'wine bottle',\n",
      " 908: 'wing',\n",
      " 909: 'wok',\n",
      " 910: 'wooden spoon',\n",
      " 911: 'wool, woolen, woollen',\n",
      " 912: 'worm fence, snake fence, snake-rail fence, Virginia fence',\n",
      " 913: 'wreck',\n",
      " 914: 'yawl',\n",
      " 915: 'yurt',\n",
      " 916: 'web site, website, internet site, site',\n",
      " 917: 'comic book',\n",
      " 918: 'crossword puzzle, crossword',\n",
      " 919: 'street sign',\n",
      " 920: 'traffic light, traffic signal, stoplight',\n",
      " 921: 'book jacket, dust cover, dust jacket, dust wrapper',\n",
      " 922: 'menu',\n",
      " 923: 'plate',\n",
      " 924: 'guacamole',\n",
      " 925: 'consomme',\n",
      " 926: 'hot pot, hotpot',\n",
      " 927: 'trifle',\n",
      " 928: 'ice cream, icecream',\n",
      " 929: 'ice lolly, lolly, lollipop, popsicle',\n",
      " 930: 'French loaf',\n",
      " 931: 'bagel, beigel',\n",
      " 932: 'pretzel',\n",
      " 933: 'cheeseburger',\n",
      " 934: 'hotdog, hot dog, red hot',\n",
      " 935: 'mashed potato',\n",
      " 936: 'head cabbage',\n",
      " 937: 'broccoli',\n",
      " 938: 'cauliflower',\n",
      " 939: 'zucchini, courgette',\n",
      " 940: 'spaghetti squash',\n",
      " 941: 'acorn squash',\n",
      " 942: 'butternut squash',\n",
      " 943: 'cucumber, cuke',\n",
      " 944: 'artichoke, globe artichoke',\n",
      " 945: 'bell pepper',\n",
      " 946: 'cardoon',\n",
      " 947: 'mushroom',\n",
      " 948: 'Granny Smith',\n",
      " 949: 'strawberry',\n",
      " 950: 'orange',\n",
      " 951: 'lemon',\n",
      " 952: 'fig',\n",
      " 953: 'pineapple, ananas',\n",
      " 954: 'banana',\n",
      " 955: 'jackfruit, jak, jack',\n",
      " 956: 'custard apple',\n",
      " 957: 'pomegranate',\n",
      " 958: 'hay',\n",
      " 959: 'carbonara',\n",
      " 960: 'chocolate sauce, chocolate syrup',\n",
      " 961: 'dough',\n",
      " 962: 'meat loaf, meatloaf',\n",
      " 963: 'pizza, pizza pie',\n",
      " 964: 'potpie',\n",
      " 965: 'burrito',\n",
      " 966: 'red wine',\n",
      " 967: 'espresso',\n",
      " 968: 'cup',\n",
      " 969: 'eggnog',\n",
      " 970: 'alp',\n",
      " 971: 'bubble',\n",
      " 972: 'cliff, drop, drop-off',\n",
      " 973: 'coral reef',\n",
      " 974: 'geyser',\n",
      " 975: 'lakeside, lakeshore',\n",
      " 976: 'promontory, headland, head, foreland',\n",
      " 977: 'sandbar, sand bar',\n",
      " 978: 'seashore, coast, seacoast, sea-coast',\n",
      " 979: 'valley, vale',\n",
      " 980: 'volcano',\n",
      " 981: 'ballplayer, baseball player',\n",
      " 982: 'groom, bridegroom',\n",
      " 983: 'scuba diver',\n",
      " 984: 'rapeseed',\n",
      " 985: 'daisy',\n",
      " 986: \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\",\n",
      " 987: 'corn',\n",
      " 988: 'acorn',\n",
      " 989: 'hip, rose hip, rosehip',\n",
      " 990: 'buckeye, horse chestnut, conker',\n",
      " 991: 'coral fungus',\n",
      " 992: 'agaric',\n",
      " 993: 'gyromitra',\n",
      " 994: 'stinkhorn, carrion fungus',\n",
      " 995: 'earthstar',\n",
      " 996: 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa',\n",
      " 997: 'bolete',\n",
      " 998: 'ear, spike, capitulum',\n",
      " 999: 'toilet tissue, toilet paper, bathroom tissue'}\n",
      "63/2:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 224\n",
      "\n",
      "def prep_image(image):\n",
      "    res = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "    res = res.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "    return res\n",
      "\n",
      "def predict(image):\n",
      "    cropped_image = center_crop(image)\n",
      "    prediction = (model.predict(prep_image(image)), model.predict(prep_image(cropped_image)))\n",
      "    if np.max(prediction[0]>=prediction[1]):\n",
      "        return np.argmax(prediction[0], axis=1)[0]\n",
      "    else:\n",
      "        return np.argmax(prediction[1], axis=1)[1]\n",
      "    \n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "plt.imshow(image)\n",
      "\n",
      "print(class_dict[predict(image)])\n",
      "65/1:\n",
      "from tensorflow.keras.applications import EfficientNetB4\n",
      "from tensorflow.image import resize\n",
      "model = EfficientNetB4(weights='imagenet')\n",
      "\n",
      "def center_crop(img, new_width=None, new_height=None):        \n",
      "\n",
      "    width = img.shape[1]\n",
      "    height = img.shape[0]\n",
      "\n",
      "    if new_width is None:\n",
      "        new_width = min(width, height)\n",
      "\n",
      "    if new_height is None:\n",
      "        new_height = min(width, height)\n",
      "\n",
      "    left = int(np.ceil((width - new_width) / 2))\n",
      "    right = width - int(np.floor((width - new_width) / 2))\n",
      "\n",
      "    top = int(np.ceil((height - new_height) / 2))\n",
      "    bottom = height - int(np.floor((height - new_height) / 2))\n",
      "\n",
      "    if len(img.shape) == 2:\n",
      "        center_cropped_img = img[top:bottom, left:right]\n",
      "    else:\n",
      "        center_cropped_img = img[top:bottom, left:right, ...]\n",
      "\n",
      "    return center_cropped_img\n",
      "\n",
      "class_dict = {0: 'tench, Tinca tinca',\n",
      " 1: 'goldfish, Carassius auratus',\n",
      " 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n",
      " 3: 'tiger shark, Galeocerdo cuvieri',\n",
      " 4: 'hammerhead, hammerhead shark',\n",
      " 5: 'electric ray, crampfish, numbfish, torpedo',\n",
      " 6: 'stingray',\n",
      " 7: 'cock',\n",
      " 8: 'hen',\n",
      " 9: 'ostrich, Struthio camelus',\n",
      " 10: 'brambling, Fringilla montifringilla',\n",
      " 11: 'goldfinch, Carduelis carduelis',\n",
      " 12: 'house finch, linnet, Carpodacus mexicanus',\n",
      " 13: 'junco, snowbird',\n",
      " 14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea',\n",
      " 15: 'robin, American robin, Turdus migratorius',\n",
      " 16: 'bulbul',\n",
      " 17: 'jay',\n",
      " 18: 'magpie',\n",
      " 19: 'chickadee',\n",
      " 20: 'water ouzel, dipper',\n",
      " 21: 'kite',\n",
      " 22: 'bald eagle, American eagle, Haliaeetus leucocephalus',\n",
      " 23: 'vulture',\n",
      " 24: 'great grey owl, great gray owl, Strix nebulosa',\n",
      " 25: 'European fire salamander, Salamandra salamandra',\n",
      " 26: 'common newt, Triturus vulgaris',\n",
      " 27: 'eft',\n",
      " 28: 'spotted salamander, Ambystoma maculatum',\n",
      " 29: 'axolotl, mud puppy, Ambystoma mexicanum',\n",
      " 30: 'bullfrog, Rana catesbeiana',\n",
      " 31: 'tree frog, tree-frog',\n",
      " 32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui',\n",
      " 33: 'loggerhead, loggerhead turtle, Caretta caretta',\n",
      " 34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea',\n",
      " 35: 'mud turtle',\n",
      " 36: 'terrapin',\n",
      " 37: 'box turtle, box tortoise',\n",
      " 38: 'banded gecko',\n",
      " 39: 'common iguana, iguana, Iguana iguana',\n",
      " 40: 'American chameleon, anole, Anolis carolinensis',\n",
      " 41: 'whiptail, whiptail lizard',\n",
      " 42: 'agama',\n",
      " 43: 'frilled lizard, Chlamydosaurus kingi',\n",
      " 44: 'alligator lizard',\n",
      " 45: 'Gila monster, Heloderma suspectum',\n",
      " 46: 'green lizard, Lacerta viridis',\n",
      " 47: 'African chameleon, Chamaeleo chamaeleon',\n",
      " 48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis',\n",
      " 49: 'African crocodile, Nile crocodile, Crocodylus niloticus',\n",
      " 50: 'American alligator, Alligator mississipiensis',\n",
      " 51: 'triceratops',\n",
      " 52: 'thunder snake, worm snake, Carphophis amoenus',\n",
      " 53: 'ringneck snake, ring-necked snake, ring snake',\n",
      " 54: 'hognose snake, puff adder, sand viper',\n",
      " 55: 'green snake, grass snake',\n",
      " 56: 'king snake, kingsnake',\n",
      " 57: 'garter snake, grass snake',\n",
      " 58: 'water snake',\n",
      " 59: 'vine snake',\n",
      " 60: 'night snake, Hypsiglena torquata',\n",
      " 61: 'boa constrictor, Constrictor constrictor',\n",
      " 62: 'rock python, rock snake, Python sebae',\n",
      " 63: 'Indian cobra, Naja naja',\n",
      " 64: 'green mamba',\n",
      " 65: 'sea snake',\n",
      " 66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus',\n",
      " 67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus',\n",
      " 68: 'sidewinder, horned rattlesnake, Crotalus cerastes',\n",
      " 69: 'trilobite',\n",
      " 70: 'harvestman, daddy longlegs, Phalangium opilio',\n",
      " 71: 'scorpion',\n",
      " 72: 'black and gold garden spider, Argiope aurantia',\n",
      " 73: 'barn spider, Araneus cavaticus',\n",
      " 74: 'garden spider, Aranea diademata',\n",
      " 75: 'black widow, Latrodectus mactans',\n",
      " 76: 'tarantula',\n",
      " 77: 'wolf spider, hunting spider',\n",
      " 78: 'tick',\n",
      " 79: 'centipede',\n",
      " 80: 'black grouse',\n",
      " 81: 'ptarmigan',\n",
      " 82: 'ruffed grouse, partridge, Bonasa umbellus',\n",
      " 83: 'prairie chicken, prairie grouse, prairie fowl',\n",
      " 84: 'peacock',\n",
      " 85: 'quail',\n",
      " 86: 'partridge',\n",
      " 87: 'African grey, African gray, Psittacus erithacus',\n",
      " 88: 'macaw',\n",
      " 89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n",
      " 90: 'lorikeet',\n",
      " 91: 'coucal',\n",
      " 92: 'bee eater',\n",
      " 93: 'hornbill',\n",
      " 94: 'hummingbird',\n",
      " 95: 'jacamar',\n",
      " 96: 'toucan',\n",
      " 97: 'drake',\n",
      " 98: 'red-breasted merganser, Mergus serrator',\n",
      " 99: 'goose',\n",
      " 100: 'black swan, Cygnus atratus',\n",
      " 101: 'tusker',\n",
      " 102: 'echidna, spiny anteater, anteater',\n",
      " 103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n",
      " 104: 'wallaby, brush kangaroo',\n",
      " 105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n",
      " 106: 'wombat',\n",
      " 107: 'jellyfish',\n",
      " 108: 'sea anemone, anemone',\n",
      " 109: 'brain coral',\n",
      " 110: 'flatworm, platyhelminth',\n",
      " 111: 'nematode, nematode worm, roundworm',\n",
      " 112: 'conch',\n",
      " 113: 'snail',\n",
      " 114: 'slug',\n",
      " 115: 'sea slug, nudibranch',\n",
      " 116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore',\n",
      " 117: 'chambered nautilus, pearly nautilus, nautilus',\n",
      " 118: 'Dungeness crab, Cancer magister',\n",
      " 119: 'rock crab, Cancer irroratus',\n",
      " 120: 'fiddler crab',\n",
      " 121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica',\n",
      " 122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus',\n",
      " 123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish',\n",
      " 124: 'crayfish, crawfish, crawdad, crawdaddy',\n",
      " 125: 'hermit crab',\n",
      " 126: 'isopod',\n",
      " 127: 'white stork, Ciconia ciconia',\n",
      " 128: 'black stork, Ciconia nigra',\n",
      " 129: 'spoonbill',\n",
      " 130: 'flamingo',\n",
      " 131: 'little blue heron, Egretta caerulea',\n",
      " 132: 'American egret, great white heron, Egretta albus',\n",
      " 133: 'bittern',\n",
      " 134: 'crane',\n",
      " 135: 'limpkin, Aramus pictus',\n",
      " 136: 'European gallinule, Porphyrio porphyrio',\n",
      " 137: 'American coot, marsh hen, mud hen, water hen, Fulica americana',\n",
      " 138: 'bustard',\n",
      " 139: 'ruddy turnstone, Arenaria interpres',\n",
      " 140: 'red-backed sandpiper, dunlin, Erolia alpina',\n",
      " 141: 'redshank, Tringa totanus',\n",
      " 142: 'dowitcher',\n",
      " 143: 'oystercatcher, oyster catcher',\n",
      " 144: 'pelican',\n",
      " 145: 'king penguin, Aptenodytes patagonica',\n",
      " 146: 'albatross, mollymawk',\n",
      " 147: 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus',\n",
      " 148: 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca',\n",
      " 149: 'dugong, Dugong dugon',\n",
      " 150: 'sea lion',\n",
      " 151: 'Chihuahua',\n",
      " 152: 'Japanese spaniel',\n",
      " 153: 'Maltese dog, Maltese terrier, Maltese',\n",
      " 154: 'Pekinese, Pekingese, Peke',\n",
      " 155: 'Shih-Tzu',\n",
      " 156: 'Blenheim spaniel',\n",
      " 157: 'papillon',\n",
      " 158: 'toy terrier',\n",
      " 159: 'Rhodesian ridgeback',\n",
      " 160: 'Afghan hound, Afghan',\n",
      " 161: 'basset, basset hound',\n",
      " 162: 'beagle',\n",
      " 163: 'bloodhound, sleuthhound',\n",
      " 164: 'bluetick',\n",
      " 165: 'black-and-tan coonhound',\n",
      " 166: 'Walker hound, Walker foxhound',\n",
      " 167: 'English foxhound',\n",
      " 168: 'redbone',\n",
      " 169: 'borzoi, Russian wolfhound',\n",
      " 170: 'Irish wolfhound',\n",
      " 171: 'Italian greyhound',\n",
      " 172: 'whippet',\n",
      " 173: 'Ibizan hound, Ibizan Podenco',\n",
      " 174: 'Norwegian elkhound, elkhound',\n",
      " 175: 'otterhound, otter hound',\n",
      " 176: 'Saluki, gazelle hound',\n",
      " 177: 'Scottish deerhound, deerhound',\n",
      " 178: 'Weimaraner',\n",
      " 179: 'Staffordshire bullterrier, Staffordshire bull terrier',\n",
      " 180: 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier',\n",
      " 181: 'Bedlington terrier',\n",
      " 182: 'Border terrier',\n",
      " 183: 'Kerry blue terrier',\n",
      " 184: 'Irish terrier',\n",
      " 185: 'Norfolk terrier',\n",
      " 186: 'Norwich terrier',\n",
      " 187: 'Yorkshire terrier',\n",
      " 188: 'wire-haired fox terrier',\n",
      " 189: 'Lakeland terrier',\n",
      " 190: 'Sealyham terrier, Sealyham',\n",
      " 191: 'Airedale, Airedale terrier',\n",
      " 192: 'cairn, cairn terrier',\n",
      " 193: 'Australian terrier',\n",
      " 194: 'Dandie Dinmont, Dandie Dinmont terrier',\n",
      " 195: 'Boston bull, Boston terrier',\n",
      " 196: 'miniature schnauzer',\n",
      " 197: 'giant schnauzer',\n",
      " 198: 'standard schnauzer',\n",
      " 199: 'Scotch terrier, Scottish terrier, Scottie',\n",
      " 200: 'Tibetan terrier, chrysanthemum dog',\n",
      " 201: 'silky terrier, Sydney silky',\n",
      " 202: 'soft-coated wheaten terrier',\n",
      " 203: 'West Highland white terrier',\n",
      " 204: 'Lhasa, Lhasa apso',\n",
      " 205: 'flat-coated retriever',\n",
      " 206: 'curly-coated retriever',\n",
      " 207: 'golden retriever',\n",
      " 208: 'Labrador retriever',\n",
      " 209: 'Chesapeake Bay retriever',\n",
      " 210: 'German short-haired pointer',\n",
      " 211: 'vizsla, Hungarian pointer',\n",
      " 212: 'English setter',\n",
      " 213: 'Irish setter, red setter',\n",
      " 214: 'Gordon setter',\n",
      " 215: 'Brittany spaniel',\n",
      " 216: 'clumber, clumber spaniel',\n",
      " 217: 'English springer, English springer spaniel',\n",
      " 218: 'Welsh springer spaniel',\n",
      " 219: 'cocker spaniel, English cocker spaniel, cocker',\n",
      " 220: 'Sussex spaniel',\n",
      " 221: 'Irish water spaniel',\n",
      " 222: 'kuvasz',\n",
      " 223: 'schipperke',\n",
      " 224: 'groenendael',\n",
      " 225: 'malinois',\n",
      " 226: 'briard',\n",
      " 227: 'kelpie',\n",
      " 228: 'komondor',\n",
      " 229: 'Old English sheepdog, bobtail',\n",
      " 230: 'Shetland sheepdog, Shetland sheep dog, Shetland',\n",
      " 231: 'collie',\n",
      " 232: 'Border collie',\n",
      " 233: 'Bouvier des Flandres, Bouviers des Flandres',\n",
      " 234: 'Rottweiler',\n",
      " 235: 'German shepherd, German shepherd dog, German police dog, alsatian',\n",
      " 236: 'Doberman, Doberman pinscher',\n",
      " 237: 'miniature pinscher',\n",
      " 238: 'Greater Swiss Mountain dog',\n",
      " 239: 'Bernese mountain dog',\n",
      " 240: 'Appenzeller',\n",
      " 241: 'EntleBucher',\n",
      " 242: 'boxer',\n",
      " 243: 'bull mastiff',\n",
      " 244: 'Tibetan mastiff',\n",
      " 245: 'French bulldog',\n",
      " 246: 'Great Dane',\n",
      " 247: 'Saint Bernard, St Bernard',\n",
      " 248: 'Eskimo dog, husky',\n",
      " 249: 'malamute, malemute, Alaskan malamute',\n",
      " 250: 'Siberian husky',\n",
      " 251: 'dalmatian, coach dog, carriage dog',\n",
      " 252: 'affenpinscher, monkey pinscher, monkey dog',\n",
      " 253: 'basenji',\n",
      " 254: 'pug, pug-dog',\n",
      " 255: 'Leonberg',\n",
      " 256: 'Newfoundland, Newfoundland dog',\n",
      " 257: 'Great Pyrenees',\n",
      " 258: 'Samoyed, Samoyede',\n",
      " 259: 'Pomeranian',\n",
      " 260: 'chow, chow chow',\n",
      " 261: 'keeshond',\n",
      " 262: 'Brabancon griffon',\n",
      " 263: 'Pembroke, Pembroke Welsh corgi',\n",
      " 264: 'Cardigan, Cardigan Welsh corgi',\n",
      " 265: 'toy poodle',\n",
      " 266: 'miniature poodle',\n",
      " 267: 'standard poodle',\n",
      " 268: 'Mexican hairless',\n",
      " 269: 'timber wolf, grey wolf, gray wolf, Canis lupus',\n",
      " 270: 'white wolf, Arctic wolf, Canis lupus tundrarum',\n",
      " 271: 'red wolf, maned wolf, Canis rufus, Canis niger',\n",
      " 272: 'coyote, prairie wolf, brush wolf, Canis latrans',\n",
      " 273: 'dingo, warrigal, warragal, Canis dingo',\n",
      " 274: 'dhole, Cuon alpinus',\n",
      " 275: 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus',\n",
      " 276: 'hyena, hyaena',\n",
      " 277: 'red fox, Vulpes vulpes',\n",
      " 278: 'kit fox, Vulpes macrotis',\n",
      " 279: 'Arctic fox, white fox, Alopex lagopus',\n",
      " 280: 'grey fox, gray fox, Urocyon cinereoargenteus',\n",
      " 281: 'tabby, tabby cat',\n",
      " 282: 'tiger cat',\n",
      " 283: 'Persian cat',\n",
      " 284: 'Siamese cat, Siamese',\n",
      " 285: 'Egyptian cat',\n",
      " 286: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n",
      " 287: 'lynx, catamount',\n",
      " 288: 'leopard, Panthera pardus',\n",
      " 289: 'snow leopard, ounce, Panthera uncia',\n",
      " 290: 'jaguar, panther, Panthera onca, Felis onca',\n",
      " 291: 'lion, king of beasts, Panthera leo',\n",
      " 292: 'tiger, Panthera tigris',\n",
      " 293: 'cheetah, chetah, Acinonyx jubatus',\n",
      " 294: 'brown bear, bruin, Ursus arctos',\n",
      " 295: 'American black bear, black bear, Ursus americanus, Euarctos americanus',\n",
      " 296: 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus',\n",
      " 297: 'sloth bear, Melursus ursinus, Ursus ursinus',\n",
      " 298: 'mongoose',\n",
      " 299: 'meerkat, mierkat',\n",
      " 300: 'tiger beetle',\n",
      " 301: 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle',\n",
      " 302: 'ground beetle, carabid beetle',\n",
      " 303: 'long-horned beetle, longicorn, longicorn beetle',\n",
      " 304: 'leaf beetle, chrysomelid',\n",
      " 305: 'dung beetle',\n",
      " 306: 'rhinoceros beetle',\n",
      " 307: 'weevil',\n",
      " 308: 'fly',\n",
      " 309: 'bee',\n",
      " 310: 'ant, emmet, pismire',\n",
      " 311: 'grasshopper, hopper',\n",
      " 312: 'cricket',\n",
      " 313: 'walking stick, walkingstick, stick insect',\n",
      " 314: 'cockroach, roach',\n",
      " 315: 'mantis, mantid',\n",
      " 316: 'cicada, cicala',\n",
      " 317: 'leafhopper',\n",
      " 318: 'lacewing, lacewing fly',\n",
      " 319: \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\",\n",
      " 320: 'damselfly',\n",
      " 321: 'admiral',\n",
      " 322: 'ringlet, ringlet butterfly',\n",
      " 323: 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus',\n",
      " 324: 'cabbage butterfly',\n",
      " 325: 'sulphur butterfly, sulfur butterfly',\n",
      " 326: 'lycaenid, lycaenid butterfly',\n",
      " 327: 'starfish, sea star',\n",
      " 328: 'sea urchin',\n",
      " 329: 'sea cucumber, holothurian',\n",
      " 330: 'wood rabbit, cottontail, cottontail rabbit',\n",
      " 331: 'hare',\n",
      " 332: 'Angora, Angora rabbit',\n",
      " 333: 'hamster',\n",
      " 334: 'porcupine, hedgehog',\n",
      " 335: 'fox squirrel, eastern fox squirrel, Sciurus niger',\n",
      " 336: 'marmot',\n",
      " 337: 'beaver',\n",
      " 338: 'guinea pig, Cavia cobaya',\n",
      " 339: 'sorrel',\n",
      " 340: 'zebra',\n",
      " 341: 'hog, pig, grunter, squealer, Sus scrofa',\n",
      " 342: 'wild boar, boar, Sus scrofa',\n",
      " 343: 'warthog',\n",
      " 344: 'hippopotamus, hippo, river horse, Hippopotamus amphibius',\n",
      " 345: 'ox',\n",
      " 346: 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis',\n",
      " 347: 'bison',\n",
      " 348: 'ram, tup',\n",
      " 349: 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis',\n",
      " 350: 'ibex, Capra ibex',\n",
      " 351: 'hartebeest',\n",
      " 352: 'impala, Aepyceros melampus',\n",
      " 353: 'gazelle',\n",
      " 354: 'Arabian camel, dromedary, Camelus dromedarius',\n",
      " 355: 'llama',\n",
      " 356: 'weasel',\n",
      " 357: 'mink',\n",
      " 358: 'polecat, fitch, foulmart, foumart, Mustela putorius',\n",
      " 359: 'black-footed ferret, ferret, Mustela nigripes',\n",
      " 360: 'otter',\n",
      " 361: 'skunk, polecat, wood pussy',\n",
      " 362: 'badger',\n",
      " 363: 'armadillo',\n",
      " 364: 'three-toed sloth, ai, Bradypus tridactylus',\n",
      " 365: 'orangutan, orang, orangutang, Pongo pygmaeus',\n",
      " 366: 'gorilla, Gorilla gorilla',\n",
      " 367: 'chimpanzee, chimp, Pan troglodytes',\n",
      " 368: 'gibbon, Hylobates lar',\n",
      " 369: 'siamang, Hylobates syndactylus, Symphalangus syndactylus',\n",
      " 370: 'guenon, guenon monkey',\n",
      " 371: 'patas, hussar monkey, Erythrocebus patas',\n",
      " 372: 'baboon',\n",
      " 373: 'macaque',\n",
      " 374: 'langur',\n",
      " 375: 'colobus, colobus monkey',\n",
      " 376: 'proboscis monkey, Nasalis larvatus',\n",
      " 377: 'marmoset',\n",
      " 378: 'capuchin, ringtail, Cebus capucinus',\n",
      " 379: 'howler monkey, howler',\n",
      " 380: 'titi, titi monkey',\n",
      " 381: 'spider monkey, Ateles geoffroyi',\n",
      " 382: 'squirrel monkey, Saimiri sciureus',\n",
      " 383: 'Madagascar cat, ring-tailed lemur, Lemur catta',\n",
      " 384: 'indri, indris, Indri indri, Indri brevicaudatus',\n",
      " 385: 'Indian elephant, Elephas maximus',\n",
      " 386: 'African elephant, Loxodonta africana',\n",
      " 387: 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens',\n",
      " 388: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca',\n",
      " 389: 'barracouta, snoek',\n",
      " 390: 'eel',\n",
      " 391: 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch',\n",
      " 392: 'rock beauty, Holocanthus tricolor',\n",
      " 393: 'anemone fish',\n",
      " 394: 'sturgeon',\n",
      " 395: 'gar, garfish, garpike, billfish, Lepisosteus osseus',\n",
      " 396: 'lionfish',\n",
      " 397: 'puffer, pufferfish, blowfish, globefish',\n",
      " 398: 'abacus',\n",
      " 399: 'abaya',\n",
      " 400: \"academic gown, academic robe, judge's robe\",\n",
      " 401: 'accordion, piano accordion, squeeze box',\n",
      " 402: 'acoustic guitar',\n",
      " 403: 'aircraft carrier, carrier, flattop, attack aircraft carrier',\n",
      " 404: 'airliner',\n",
      " 405: 'airship, dirigible',\n",
      " 406: 'altar',\n",
      " 407: 'ambulance',\n",
      " 408: 'amphibian, amphibious vehicle',\n",
      " 409: 'analog clock',\n",
      " 410: 'apiary, bee house',\n",
      " 411: 'apron',\n",
      " 412: 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin',\n",
      " 413: 'assault rifle, assault gun',\n",
      " 414: 'backpack, back pack, knapsack, packsack, rucksack, haversack',\n",
      " 415: 'bakery, bakeshop, bakehouse',\n",
      " 416: 'balance beam, beam',\n",
      " 417: 'balloon',\n",
      " 418: 'ballpoint, ballpoint pen, ballpen, Biro',\n",
      " 419: 'Band Aid',\n",
      " 420: 'banjo',\n",
      " 421: 'bannister, banister, balustrade, balusters, handrail',\n",
      " 422: 'barbell',\n",
      " 423: 'barber chair',\n",
      " 424: 'barbershop',\n",
      " 425: 'barn',\n",
      " 426: 'barometer',\n",
      " 427: 'barrel, cask',\n",
      " 428: 'barrow, garden cart, lawn cart, wheelbarrow',\n",
      " 429: 'baseball',\n",
      " 430: 'basketball',\n",
      " 431: 'bassinet',\n",
      " 432: 'bassoon',\n",
      " 433: 'bathing cap, swimming cap',\n",
      " 434: 'bath towel',\n",
      " 435: 'bathtub, bathing tub, bath, tub',\n",
      " 436: 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon',\n",
      " 437: 'beacon, lighthouse, beacon light, pharos',\n",
      " 438: 'beaker',\n",
      " 439: 'bearskin, busby, shako',\n",
      " 440: 'beer bottle',\n",
      " 441: 'beer glass',\n",
      " 442: 'bell cote, bell cot',\n",
      " 443: 'bib',\n",
      " 444: 'bicycle-built-for-two, tandem bicycle, tandem',\n",
      " 445: 'bikini, two-piece',\n",
      " 446: 'binder, ring-binder',\n",
      " 447: 'binoculars, field glasses, opera glasses',\n",
      " 448: 'birdhouse',\n",
      " 449: 'boathouse',\n",
      " 450: 'bobsled, bobsleigh, bob',\n",
      " 451: 'bolo tie, bolo, bola tie, bola',\n",
      " 452: 'bonnet, poke bonnet',\n",
      " 453: 'bookcase',\n",
      " 454: 'bookshop, bookstore, bookstall',\n",
      " 455: 'bottlecap',\n",
      " 456: 'bow',\n",
      " 457: 'bow tie, bow-tie, bowtie',\n",
      " 458: 'brass, memorial tablet, plaque',\n",
      " 459: 'brassiere, bra, bandeau',\n",
      " 460: 'breakwater, groin, groyne, mole, bulwark, seawall, jetty',\n",
      " 461: 'breastplate, aegis, egis',\n",
      " 462: 'broom',\n",
      " 463: 'bucket, pail',\n",
      " 464: 'buckle',\n",
      " 465: 'bulletproof vest',\n",
      " 466: 'bullet train, bullet',\n",
      " 467: 'butcher shop, meat market',\n",
      " 468: 'cab, hack, taxi, taxicab',\n",
      " 469: 'caldron, cauldron',\n",
      " 470: 'candle, taper, wax light',\n",
      " 471: 'cannon',\n",
      " 472: 'canoe',\n",
      " 473: 'can opener, tin opener',\n",
      " 474: 'cardigan',\n",
      " 475: 'car mirror',\n",
      " 476: 'carousel, carrousel, merry-go-round, roundabout, whirligig',\n",
      " 477: \"carpenter's kit, tool kit\",\n",
      " 478: 'carton',\n",
      " 479: 'car wheel',\n",
      " 480: 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM',\n",
      " 481: 'cassette',\n",
      " 482: 'cassette player',\n",
      " 483: 'castle',\n",
      " 484: 'catamaran',\n",
      " 485: 'CD player',\n",
      " 486: 'cello, violoncello',\n",
      " 487: 'cellular telephone, cellular phone, cellphone, cell, mobile phone',\n",
      " 488: 'chain',\n",
      " 489: 'chainlink fence',\n",
      " 490: 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour',\n",
      " 491: 'chain saw, chainsaw',\n",
      " 492: 'chest',\n",
      " 493: 'chiffonier, commode',\n",
      " 494: 'chime, bell, gong',\n",
      " 495: 'china cabinet, china closet',\n",
      " 496: 'Christmas stocking',\n",
      " 497: 'church, church building',\n",
      " 498: 'cinema, movie theater, movie theatre, movie house, picture palace',\n",
      " 499: 'cleaver, meat cleaver, chopper',\n",
      " 500: 'cliff dwelling',\n",
      " 501: 'cloak',\n",
      " 502: 'clog, geta, patten, sabot',\n",
      " 503: 'cocktail shaker',\n",
      " 504: 'coffee mug',\n",
      " 505: 'coffeepot',\n",
      " 506: 'coil, spiral, volute, whorl, helix',\n",
      " 507: 'combination lock',\n",
      " 508: 'computer keyboard, keypad',\n",
      " 509: 'confectionery, confectionary, candy store',\n",
      " 510: 'container ship, containership, container vessel',\n",
      " 511: 'convertible',\n",
      " 512: 'corkscrew, bottle screw',\n",
      " 513: 'cornet, horn, trumpet, trump',\n",
      " 514: 'cowboy boot',\n",
      " 515: 'cowboy hat, ten-gallon hat',\n",
      " 516: 'cradle',\n",
      " 517: 'crane',\n",
      " 518: 'crash helmet',\n",
      " 519: 'crate',\n",
      " 520: 'crib, cot',\n",
      " 521: 'Crock Pot',\n",
      " 522: 'croquet ball',\n",
      " 523: 'crutch',\n",
      " 524: 'cuirass',\n",
      " 525: 'dam, dike, dyke',\n",
      " 526: 'desk',\n",
      " 527: 'desktop computer',\n",
      " 528: 'dial telephone, dial phone',\n",
      " 529: 'diaper, nappy, napkin',\n",
      " 530: 'digital clock',\n",
      " 531: 'digital watch',\n",
      " 532: 'dining table, board',\n",
      " 533: 'dishrag, dishcloth',\n",
      " 534: 'dishwasher, dish washer, dishwashing machine',\n",
      " 535: 'disk brake, disc brake',\n",
      " 536: 'dock, dockage, docking facility',\n",
      " 537: 'dogsled, dog sled, dog sleigh',\n",
      " 538: 'dome',\n",
      " 539: 'doormat, welcome mat',\n",
      " 540: 'drilling platform, offshore rig',\n",
      " 541: 'drum, membranophone, tympan',\n",
      " 542: 'drumstick',\n",
      " 543: 'dumbbell',\n",
      " 544: 'Dutch oven',\n",
      " 545: 'electric fan, blower',\n",
      " 546: 'electric guitar',\n",
      " 547: 'electric locomotive',\n",
      " 548: 'entertainment center',\n",
      " 549: 'envelope',\n",
      " 550: 'espresso maker',\n",
      " 551: 'face powder',\n",
      " 552: 'feather boa, boa',\n",
      " 553: 'file, file cabinet, filing cabinet',\n",
      " 554: 'fireboat',\n",
      " 555: 'fire engine, fire truck',\n",
      " 556: 'fire screen, fireguard',\n",
      " 557: 'flagpole, flagstaff',\n",
      " 558: 'flute, transverse flute',\n",
      " 559: 'folding chair',\n",
      " 560: 'football helmet',\n",
      " 561: 'forklift',\n",
      " 562: 'fountain',\n",
      " 563: 'fountain pen',\n",
      " 564: 'four-poster',\n",
      " 565: 'freight car',\n",
      " 566: 'French horn, horn',\n",
      " 567: 'frying pan, frypan, skillet',\n",
      " 568: 'fur coat',\n",
      " 569: 'garbage truck, dustcart',\n",
      " 570: 'gasmask, respirator, gas helmet',\n",
      " 571: 'gas pump, gasoline pump, petrol pump, island dispenser',\n",
      " 572: 'goblet',\n",
      " 573: 'go-kart',\n",
      " 574: 'golf ball',\n",
      " 575: 'golfcart, golf cart',\n",
      " 576: 'gondola',\n",
      " 577: 'gong, tam-tam',\n",
      " 578: 'gown',\n",
      " 579: 'grand piano, grand',\n",
      " 580: 'greenhouse, nursery, glasshouse',\n",
      " 581: 'grille, radiator grille',\n",
      " 582: 'grocery store, grocery, food market, market',\n",
      " 583: 'guillotine',\n",
      " 584: 'hair slide',\n",
      " 585: 'hair spray',\n",
      " 586: 'half track',\n",
      " 587: 'hammer',\n",
      " 588: 'hamper',\n",
      " 589: 'hand blower, blow dryer, blow drier, hair dryer, hair drier',\n",
      " 590: 'hand-held computer, hand-held microcomputer',\n",
      " 591: 'handkerchief, hankie, hanky, hankey',\n",
      " 592: 'hard disc, hard disk, fixed disk',\n",
      " 593: 'harmonica, mouth organ, harp, mouth harp',\n",
      " 594: 'harp',\n",
      " 595: 'harvester, reaper',\n",
      " 596: 'hatchet',\n",
      " 597: 'holster',\n",
      " 598: 'home theater, home theatre',\n",
      " 599: 'honeycomb',\n",
      " 600: 'hook, claw',\n",
      " 601: 'hoopskirt, crinoline',\n",
      " 602: 'horizontal bar, high bar',\n",
      " 603: 'horse cart, horse-cart',\n",
      " 604: 'hourglass',\n",
      " 605: 'iPod',\n",
      " 606: 'iron, smoothing iron',\n",
      " 607: \"jack-o'-lantern\",\n",
      " 608: 'jean, blue jean, denim',\n",
      " 609: 'jeep, landrover',\n",
      " 610: 'jersey, T-shirt, tee shirt',\n",
      " 611: 'jigsaw puzzle',\n",
      " 612: 'jinrikisha, ricksha, rickshaw',\n",
      " 613: 'joystick',\n",
      " 614: 'kimono',\n",
      " 615: 'knee pad',\n",
      " 616: 'knot',\n",
      " 617: 'lab coat, laboratory coat',\n",
      " 618: 'ladle',\n",
      " 619: 'lampshade, lamp shade',\n",
      " 620: 'laptop, laptop computer',\n",
      " 621: 'lawn mower, mower',\n",
      " 622: 'lens cap, lens cover',\n",
      " 623: 'letter opener, paper knife, paperknife',\n",
      " 624: 'library',\n",
      " 625: 'lifeboat',\n",
      " 626: 'lighter, light, igniter, ignitor',\n",
      " 627: 'limousine, limo',\n",
      " 628: 'liner, ocean liner',\n",
      " 629: 'lipstick, lip rouge',\n",
      " 630: 'Loafer',\n",
      " 631: 'lotion',\n",
      " 632: 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system',\n",
      " 633: \"loupe, jeweler's loupe\",\n",
      " 634: 'lumbermill, sawmill',\n",
      " 635: 'magnetic compass',\n",
      " 636: 'mailbag, postbag',\n",
      " 637: 'mailbox, letter box',\n",
      " 638: 'maillot',\n",
      " 639: 'maillot, tank suit',\n",
      " 640: 'manhole cover',\n",
      " 641: 'maraca',\n",
      " 642: 'marimba, xylophone',\n",
      " 643: 'mask',\n",
      " 644: 'matchstick',\n",
      " 645: 'maypole',\n",
      " 646: 'maze, labyrinth',\n",
      " 647: 'measuring cup',\n",
      " 648: 'medicine chest, medicine cabinet',\n",
      " 649: 'megalith, megalithic structure',\n",
      " 650: 'microphone, mike',\n",
      " 651: 'microwave, microwave oven',\n",
      " 652: 'military uniform',\n",
      " 653: 'milk can',\n",
      " 654: 'minibus',\n",
      " 655: 'miniskirt, mini',\n",
      " 656: 'minivan',\n",
      " 657: 'missile',\n",
      " 658: 'mitten',\n",
      " 659: 'mixing bowl',\n",
      " 660: 'mobile home, manufactured home',\n",
      " 661: 'Model T',\n",
      " 662: 'modem',\n",
      " 663: 'monastery',\n",
      " 664: 'monitor',\n",
      " 665: 'moped',\n",
      " 666: 'mortar',\n",
      " 667: 'mortarboard',\n",
      " 668: 'mosque',\n",
      " 669: 'mosquito net',\n",
      " 670: 'motor scooter, scooter',\n",
      " 671: 'mountain bike, all-terrain bike, off-roader',\n",
      " 672: 'mountain tent',\n",
      " 673: 'mouse, computer mouse',\n",
      " 674: 'mousetrap',\n",
      " 675: 'moving van',\n",
      " 676: 'muzzle',\n",
      " 677: 'nail',\n",
      " 678: 'neck brace',\n",
      " 679: 'necklace',\n",
      " 680: 'nipple',\n",
      " 681: 'notebook, notebook computer',\n",
      " 682: 'obelisk',\n",
      " 683: 'oboe, hautboy, hautbois',\n",
      " 684: 'ocarina, sweet potato',\n",
      " 685: 'odometer, hodometer, mileometer, milometer',\n",
      " 686: 'oil filter',\n",
      " 687: 'organ, pipe organ',\n",
      " 688: 'oscilloscope, scope, cathode-ray oscilloscope, CRO',\n",
      " 689: 'overskirt',\n",
      " 690: 'oxcart',\n",
      " 691: 'oxygen mask',\n",
      " 692: 'packet',\n",
      " 693: 'paddle, boat paddle',\n",
      " 694: 'paddlewheel, paddle wheel',\n",
      " 695: 'padlock',\n",
      " 696: 'paintbrush',\n",
      " 697: \"pajama, pyjama, pj's, jammies\",\n",
      " 698: 'palace',\n",
      " 699: 'panpipe, pandean pipe, syrinx',\n",
      " 700: 'paper towel',\n",
      " 701: 'parachute, chute',\n",
      " 702: 'parallel bars, bars',\n",
      " 703: 'park bench',\n",
      " 704: 'parking meter',\n",
      " 705: 'passenger car, coach, carriage',\n",
      " 706: 'patio, terrace',\n",
      " 707: 'pay-phone, pay-station',\n",
      " 708: 'pedestal, plinth, footstall',\n",
      " 709: 'pencil box, pencil case',\n",
      " 710: 'pencil sharpener',\n",
      " 711: 'perfume, essence',\n",
      " 712: 'Petri dish',\n",
      " 713: 'photocopier',\n",
      " 714: 'pick, plectrum, plectron',\n",
      " 715: 'pickelhaube',\n",
      " 716: 'picket fence, paling',\n",
      " 717: 'pickup, pickup truck',\n",
      " 718: 'pier',\n",
      " 719: 'piggy bank, penny bank',\n",
      " 720: 'pill bottle',\n",
      " 721: 'pillow',\n",
      " 722: 'ping-pong ball',\n",
      " 723: 'pinwheel',\n",
      " 724: 'pirate, pirate ship',\n",
      " 725: 'pitcher, ewer',\n",
      " 726: \"plane, carpenter's plane, woodworking plane\",\n",
      " 727: 'planetarium',\n",
      " 728: 'plastic bag',\n",
      " 729: 'plate rack',\n",
      " 730: 'plow, plough',\n",
      " 731: \"plunger, plumber's helper\",\n",
      " 732: 'Polaroid camera, Polaroid Land camera',\n",
      " 733: 'pole',\n",
      " 734: 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria',\n",
      " 735: 'poncho',\n",
      " 736: 'pool table, billiard table, snooker table',\n",
      " 737: 'pop bottle, soda bottle',\n",
      " 738: 'pot, flowerpot',\n",
      " 739: \"potter's wheel\",\n",
      " 740: 'power drill',\n",
      " 741: 'prayer rug, prayer mat',\n",
      " 742: 'printer',\n",
      " 743: 'prison, prison house',\n",
      " 744: 'projectile, missile',\n",
      " 745: 'projector',\n",
      " 746: 'puck, hockey puck',\n",
      " 747: 'punching bag, punch bag, punching ball, punchball',\n",
      " 748: 'purse',\n",
      " 749: 'quill, quill pen',\n",
      " 750: 'quilt, comforter, comfort, puff',\n",
      " 751: 'racer, race car, racing car',\n",
      " 752: 'racket, racquet',\n",
      " 753: 'radiator',\n",
      " 754: 'radio, wireless',\n",
      " 755: 'radio telescope, radio reflector',\n",
      " 756: 'rain barrel',\n",
      " 757: 'recreational vehicle, RV, R.V.',\n",
      " 758: 'reel',\n",
      " 759: 'reflex camera',\n",
      " 760: 'refrigerator, icebox',\n",
      " 761: 'remote control, remote',\n",
      " 762: 'restaurant, eating house, eating place, eatery',\n",
      " 763: 'revolver, six-gun, six-shooter',\n",
      " 764: 'rifle',\n",
      " 765: 'rocking chair, rocker',\n",
      " 766: 'rotisserie',\n",
      " 767: 'rubber eraser, rubber, pencil eraser',\n",
      " 768: 'rugby ball',\n",
      " 769: 'rule, ruler',\n",
      " 770: 'running shoe',\n",
      " 771: 'safe',\n",
      " 772: 'safety pin',\n",
      " 773: 'saltshaker, salt shaker',\n",
      " 774: 'sandal',\n",
      " 775: 'sarong',\n",
      " 776: 'sax, saxophone',\n",
      " 777: 'scabbard',\n",
      " 778: 'scale, weighing machine',\n",
      " 779: 'school bus',\n",
      " 780: 'schooner',\n",
      " 781: 'scoreboard',\n",
      " 782: 'screen, CRT screen',\n",
      " 783: 'screw',\n",
      " 784: 'screwdriver',\n",
      " 785: 'seat belt, seatbelt',\n",
      " 786: 'sewing machine',\n",
      " 787: 'shield, buckler',\n",
      " 788: 'shoe shop, shoe-shop, shoe store',\n",
      " 789: 'shoji',\n",
      " 790: 'shopping basket',\n",
      " 791: 'shopping cart',\n",
      " 792: 'shovel',\n",
      " 793: 'shower cap',\n",
      " 794: 'shower curtain',\n",
      " 795: 'ski',\n",
      " 796: 'ski mask',\n",
      " 797: 'sleeping bag',\n",
      " 798: 'slide rule, slipstick',\n",
      " 799: 'sliding door',\n",
      " 800: 'slot, one-armed bandit',\n",
      " 801: 'snorkel',\n",
      " 802: 'snowmobile',\n",
      " 803: 'snowplow, snowplough',\n",
      " 804: 'soap dispenser',\n",
      " 805: 'soccer ball',\n",
      " 806: 'sock',\n",
      " 807: 'solar dish, solar collector, solar furnace',\n",
      " 808: 'sombrero',\n",
      " 809: 'soup bowl',\n",
      " 810: 'space bar',\n",
      " 811: 'space heater',\n",
      " 812: 'space shuttle',\n",
      " 813: 'spatula',\n",
      " 814: 'speedboat',\n",
      " 815: \"spider web, spider's web\",\n",
      " 816: 'spindle',\n",
      " 817: 'sports car, sport car',\n",
      " 818: 'spotlight, spot',\n",
      " 819: 'stage',\n",
      " 820: 'steam locomotive',\n",
      " 821: 'steel arch bridge',\n",
      " 822: 'steel drum',\n",
      " 823: 'stethoscope',\n",
      " 824: 'stole',\n",
      " 825: 'stone wall',\n",
      " 826: 'stopwatch, stop watch',\n",
      " 827: 'stove',\n",
      " 828: 'strainer',\n",
      " 829: 'streetcar, tram, tramcar, trolley, trolley car',\n",
      " 830: 'stretcher',\n",
      " 831: 'studio couch, day bed',\n",
      " 832: 'stupa, tope',\n",
      " 833: 'submarine, pigboat, sub, U-boat',\n",
      " 834: 'suit, suit of clothes',\n",
      " 835: 'sundial',\n",
      " 836: 'sunglass',\n",
      " 837: 'sunglasses, dark glasses, shades',\n",
      " 838: 'sunscreen, sunblock, sun blocker',\n",
      " 839: 'suspension bridge',\n",
      " 840: 'swab, swob, mop',\n",
      " 841: 'sweatshirt',\n",
      " 842: 'swimming trunks, bathing trunks',\n",
      " 843: 'swing',\n",
      " 844: 'switch, electric switch, electrical switch',\n",
      " 845: 'syringe',\n",
      " 846: 'table lamp',\n",
      " 847: 'tank, army tank, armored combat vehicle, armoured combat vehicle',\n",
      " 848: 'tape player',\n",
      " 849: 'teapot',\n",
      " 850: 'teddy, teddy bear',\n",
      " 851: 'television, television system',\n",
      " 852: 'tennis ball',\n",
      " 853: 'thatch, thatched roof',\n",
      " 854: 'theater curtain, theatre curtain',\n",
      " 855: 'thimble',\n",
      " 856: 'thresher, thrasher, threshing machine',\n",
      " 857: 'throne',\n",
      " 858: 'tile roof',\n",
      " 859: 'toaster',\n",
      " 860: 'tobacco shop, tobacconist shop, tobacconist',\n",
      " 861: 'toilet seat',\n",
      " 862: 'torch',\n",
      " 863: 'totem pole',\n",
      " 864: 'tow truck, tow car, wrecker',\n",
      " 865: 'toyshop',\n",
      " 866: 'tractor',\n",
      " 867: 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi',\n",
      " 868: 'tray',\n",
      " 869: 'trench coat',\n",
      " 870: 'tricycle, trike, velocipede',\n",
      " 871: 'trimaran',\n",
      " 872: 'tripod',\n",
      " 873: 'triumphal arch',\n",
      " 874: 'trolleybus, trolley coach, trackless trolley',\n",
      " 875: 'trombone',\n",
      " 876: 'tub, vat',\n",
      " 877: 'turnstile',\n",
      " 878: 'typewriter keyboard',\n",
      " 879: 'umbrella',\n",
      " 880: 'unicycle, monocycle',\n",
      " 881: 'upright, upright piano',\n",
      " 882: 'vacuum, vacuum cleaner',\n",
      " 883: 'vase',\n",
      " 884: 'vault',\n",
      " 885: 'velvet',\n",
      " 886: 'vending machine',\n",
      " 887: 'vestment',\n",
      " 888: 'viaduct',\n",
      " 889: 'violin, fiddle',\n",
      " 890: 'volleyball',\n",
      " 891: 'waffle iron',\n",
      " 892: 'wall clock',\n",
      " 893: 'wallet, billfold, notecase, pocketbook',\n",
      " 894: 'wardrobe, closet, press',\n",
      " 895: 'warplane, military plane',\n",
      " 896: 'washbasin, handbasin, washbowl, lavabo, wash-hand basin',\n",
      " 897: 'washer, automatic washer, washing machine',\n",
      " 898: 'water bottle',\n",
      " 899: 'water jug',\n",
      " 900: 'water tower',\n",
      " 901: 'whiskey jug',\n",
      " 902: 'whistle',\n",
      " 903: 'wig',\n",
      " 904: 'window screen',\n",
      " 905: 'window shade',\n",
      " 906: 'Windsor tie',\n",
      " 907: 'wine bottle',\n",
      " 908: 'wing',\n",
      " 909: 'wok',\n",
      " 910: 'wooden spoon',\n",
      " 911: 'wool, woolen, woollen',\n",
      " 912: 'worm fence, snake fence, snake-rail fence, Virginia fence',\n",
      " 913: 'wreck',\n",
      " 914: 'yawl',\n",
      " 915: 'yurt',\n",
      " 916: 'web site, website, internet site, site',\n",
      " 917: 'comic book',\n",
      " 918: 'crossword puzzle, crossword',\n",
      " 919: 'street sign',\n",
      " 920: 'traffic light, traffic signal, stoplight',\n",
      " 921: 'book jacket, dust cover, dust jacket, dust wrapper',\n",
      " 922: 'menu',\n",
      " 923: 'plate',\n",
      " 924: 'guacamole',\n",
      " 925: 'consomme',\n",
      " 926: 'hot pot, hotpot',\n",
      " 927: 'trifle',\n",
      " 928: 'ice cream, icecream',\n",
      " 929: 'ice lolly, lolly, lollipop, popsicle',\n",
      " 930: 'French loaf',\n",
      " 931: 'bagel, beigel',\n",
      " 932: 'pretzel',\n",
      " 933: 'cheeseburger',\n",
      " 934: 'hotdog, hot dog, red hot',\n",
      " 935: 'mashed potato',\n",
      " 936: 'head cabbage',\n",
      " 937: 'broccoli',\n",
      " 938: 'cauliflower',\n",
      " 939: 'zucchini, courgette',\n",
      " 940: 'spaghetti squash',\n",
      " 941: 'acorn squash',\n",
      " 942: 'butternut squash',\n",
      " 943: 'cucumber, cuke',\n",
      " 944: 'artichoke, globe artichoke',\n",
      " 945: 'bell pepper',\n",
      " 946: 'cardoon',\n",
      " 947: 'mushroom',\n",
      " 948: 'Granny Smith',\n",
      " 949: 'strawberry',\n",
      " 950: 'orange',\n",
      " 951: 'lemon',\n",
      " 952: 'fig',\n",
      " 953: 'pineapple, ananas',\n",
      " 954: 'banana',\n",
      " 955: 'jackfruit, jak, jack',\n",
      " 956: 'custard apple',\n",
      " 957: 'pomegranate',\n",
      " 958: 'hay',\n",
      " 959: 'carbonara',\n",
      " 960: 'chocolate sauce, chocolate syrup',\n",
      " 961: 'dough',\n",
      " 962: 'meat loaf, meatloaf',\n",
      " 963: 'pizza, pizza pie',\n",
      " 964: 'potpie',\n",
      " 965: 'burrito',\n",
      " 966: 'red wine',\n",
      " 967: 'espresso',\n",
      " 968: 'cup',\n",
      " 969: 'eggnog',\n",
      " 970: 'alp',\n",
      " 971: 'bubble',\n",
      " 972: 'cliff, drop, drop-off',\n",
      " 973: 'coral reef',\n",
      " 974: 'geyser',\n",
      " 975: 'lakeside, lakeshore',\n",
      " 976: 'promontory, headland, head, foreland',\n",
      " 977: 'sandbar, sand bar',\n",
      " 978: 'seashore, coast, seacoast, sea-coast',\n",
      " 979: 'valley, vale',\n",
      " 980: 'volcano',\n",
      " 981: 'ballplayer, baseball player',\n",
      " 982: 'groom, bridegroom',\n",
      " 983: 'scuba diver',\n",
      " 984: 'rapeseed',\n",
      " 985: 'daisy',\n",
      " 986: \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\",\n",
      " 987: 'corn',\n",
      " 988: 'acorn',\n",
      " 989: 'hip, rose hip, rosehip',\n",
      " 990: 'buckeye, horse chestnut, conker',\n",
      " 991: 'coral fungus',\n",
      " 992: 'agaric',\n",
      " 993: 'gyromitra',\n",
      " 994: 'stinkhorn, carrion fungus',\n",
      " 995: 'earthstar',\n",
      " 996: 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa',\n",
      " 997: 'bolete',\n",
      " 998: 'ear, spike, capitulum',\n",
      " 999: 'toilet tissue, toilet paper, bathroom tissue'}\n",
      "65/2:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 330\n",
      "\n",
      "def prep_image(image):\n",
      "    res = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "    res = res.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "    return res\n",
      "\n",
      "def predict(image):\n",
      "    cropped_image = center_crop(image)\n",
      "    prediction = (model.predict(prep_image(image)), model.predict(prep_image(cropped_image)))\n",
      "    if np.max(prediction[0]>=prediction[1]):\n",
      "        return {'class': np.argmax(prediction[0], axis=1)[0]. 'all_prob': prediction[0]}\n",
      "    else:\n",
      "        return  {'class': np.argmax(prediction[1], axis=1)[1]. 'all_prob': prediction[1]}\n",
      "    \n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "plt.imshow(image)\n",
      "\n",
      "print(class_dict[predict(image)])\n",
      "65/3:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 330\n",
      "\n",
      "def prep_image(image):\n",
      "    res = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "    res = res.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "    return res\n",
      "\n",
      "def predict(image):\n",
      "    cropped_image = center_crop(image)\n",
      "    prediction = (model.predict(prep_image(image)), model.predict(prep_image(cropped_image)))\n",
      "    if np.max(prediction[0]>=prediction[1]):\n",
      "        return {'class': np.argmax(prediction[0], axis=1)[0], 'all_prob': prediction[0]}\n",
      "    else:\n",
      "        return  {'class': np.argmax(prediction[1], axis=1)[1], 'all_prob': prediction[1]}\n",
      "    \n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "plt.imshow(image)\n",
      "\n",
      "print(class_dict[predict(image)])\n",
      "65/4:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 380\n",
      "\n",
      "def prep_image(image):\n",
      "    res = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "    res = res.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "    return res\n",
      "\n",
      "def predict(image):\n",
      "    cropped_image = center_crop(image)\n",
      "    prediction = (model.predict(prep_image(image)), model.predict(prep_image(cropped_image)))\n",
      "    if np.max(prediction[0]>=prediction[1]):\n",
      "        return {'class': np.argmax(prediction[0], axis=1)[0], 'all_prob': prediction[0]}\n",
      "    else:\n",
      "        return  {'class': np.argmax(prediction[1], axis=1)[1], 'all_prob': prediction[1]}\n",
      "    \n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "plt.imshow(image)\n",
      "\n",
      "print(class_dict[predict(image)])\n",
      "67/1:\n",
      "from tensorflow.keras.applications import EfficientNetB4\n",
      "from tensorflow.image import resize\n",
      "model = EfficientNetB4(weights='imagenet')\n",
      "\n",
      "def center_crop(img, new_width=None, new_height=None):        \n",
      "\n",
      "    width = img.shape[1]\n",
      "    height = img.shape[0]\n",
      "\n",
      "    if new_width is None:\n",
      "        new_width = min(width, height)\n",
      "\n",
      "    if new_height is None:\n",
      "        new_height = min(width, height)\n",
      "\n",
      "    left = int(np.ceil((width - new_width) / 2))\n",
      "    right = width - int(np.floor((width - new_width) / 2))\n",
      "\n",
      "    top = int(np.ceil((height - new_height) / 2))\n",
      "    bottom = height - int(np.floor((height - new_height) / 2))\n",
      "\n",
      "    if len(img.shape) == 2:\n",
      "        center_cropped_img = img[top:bottom, left:right]\n",
      "    else:\n",
      "        center_cropped_img = img[top:bottom, left:right, ...]\n",
      "\n",
      "    return center_cropped_img\n",
      "\n",
      "class_dict = {0: 'tench, Tinca tinca',\n",
      " 1: 'goldfish, Carassius auratus',\n",
      " 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n",
      " 3: 'tiger shark, Galeocerdo cuvieri',\n",
      " 4: 'hammerhead, hammerhead shark',\n",
      " 5: 'electric ray, crampfish, numbfish, torpedo',\n",
      " 6: 'stingray',\n",
      " 7: 'cock',\n",
      " 8: 'hen',\n",
      " 9: 'ostrich, Struthio camelus',\n",
      " 10: 'brambling, Fringilla montifringilla',\n",
      " 11: 'goldfinch, Carduelis carduelis',\n",
      " 12: 'house finch, linnet, Carpodacus mexicanus',\n",
      " 13: 'junco, snowbird',\n",
      " 14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea',\n",
      " 15: 'robin, American robin, Turdus migratorius',\n",
      " 16: 'bulbul',\n",
      " 17: 'jay',\n",
      " 18: 'magpie',\n",
      " 19: 'chickadee',\n",
      " 20: 'water ouzel, dipper',\n",
      " 21: 'kite',\n",
      " 22: 'bald eagle, American eagle, Haliaeetus leucocephalus',\n",
      " 23: 'vulture',\n",
      " 24: 'great grey owl, great gray owl, Strix nebulosa',\n",
      " 25: 'European fire salamander, Salamandra salamandra',\n",
      " 26: 'common newt, Triturus vulgaris',\n",
      " 27: 'eft',\n",
      " 28: 'spotted salamander, Ambystoma maculatum',\n",
      " 29: 'axolotl, mud puppy, Ambystoma mexicanum',\n",
      " 30: 'bullfrog, Rana catesbeiana',\n",
      " 31: 'tree frog, tree-frog',\n",
      " 32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui',\n",
      " 33: 'loggerhead, loggerhead turtle, Caretta caretta',\n",
      " 34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea',\n",
      " 35: 'mud turtle',\n",
      " 36: 'terrapin',\n",
      " 37: 'box turtle, box tortoise',\n",
      " 38: 'banded gecko',\n",
      " 39: 'common iguana, iguana, Iguana iguana',\n",
      " 40: 'American chameleon, anole, Anolis carolinensis',\n",
      " 41: 'whiptail, whiptail lizard',\n",
      " 42: 'agama',\n",
      " 43: 'frilled lizard, Chlamydosaurus kingi',\n",
      " 44: 'alligator lizard',\n",
      " 45: 'Gila monster, Heloderma suspectum',\n",
      " 46: 'green lizard, Lacerta viridis',\n",
      " 47: 'African chameleon, Chamaeleo chamaeleon',\n",
      " 48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis',\n",
      " 49: 'African crocodile, Nile crocodile, Crocodylus niloticus',\n",
      " 50: 'American alligator, Alligator mississipiensis',\n",
      " 51: 'triceratops',\n",
      " 52: 'thunder snake, worm snake, Carphophis amoenus',\n",
      " 53: 'ringneck snake, ring-necked snake, ring snake',\n",
      " 54: 'hognose snake, puff adder, sand viper',\n",
      " 55: 'green snake, grass snake',\n",
      " 56: 'king snake, kingsnake',\n",
      " 57: 'garter snake, grass snake',\n",
      " 58: 'water snake',\n",
      " 59: 'vine snake',\n",
      " 60: 'night snake, Hypsiglena torquata',\n",
      " 61: 'boa constrictor, Constrictor constrictor',\n",
      " 62: 'rock python, rock snake, Python sebae',\n",
      " 63: 'Indian cobra, Naja naja',\n",
      " 64: 'green mamba',\n",
      " 65: 'sea snake',\n",
      " 66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus',\n",
      " 67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus',\n",
      " 68: 'sidewinder, horned rattlesnake, Crotalus cerastes',\n",
      " 69: 'trilobite',\n",
      " 70: 'harvestman, daddy longlegs, Phalangium opilio',\n",
      " 71: 'scorpion',\n",
      " 72: 'black and gold garden spider, Argiope aurantia',\n",
      " 73: 'barn spider, Araneus cavaticus',\n",
      " 74: 'garden spider, Aranea diademata',\n",
      " 75: 'black widow, Latrodectus mactans',\n",
      " 76: 'tarantula',\n",
      " 77: 'wolf spider, hunting spider',\n",
      " 78: 'tick',\n",
      " 79: 'centipede',\n",
      " 80: 'black grouse',\n",
      " 81: 'ptarmigan',\n",
      " 82: 'ruffed grouse, partridge, Bonasa umbellus',\n",
      " 83: 'prairie chicken, prairie grouse, prairie fowl',\n",
      " 84: 'peacock',\n",
      " 85: 'quail',\n",
      " 86: 'partridge',\n",
      " 87: 'African grey, African gray, Psittacus erithacus',\n",
      " 88: 'macaw',\n",
      " 89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n",
      " 90: 'lorikeet',\n",
      " 91: 'coucal',\n",
      " 92: 'bee eater',\n",
      " 93: 'hornbill',\n",
      " 94: 'hummingbird',\n",
      " 95: 'jacamar',\n",
      " 96: 'toucan',\n",
      " 97: 'drake',\n",
      " 98: 'red-breasted merganser, Mergus serrator',\n",
      " 99: 'goose',\n",
      " 100: 'black swan, Cygnus atratus',\n",
      " 101: 'tusker',\n",
      " 102: 'echidna, spiny anteater, anteater',\n",
      " 103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n",
      " 104: 'wallaby, brush kangaroo',\n",
      " 105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n",
      " 106: 'wombat',\n",
      " 107: 'jellyfish',\n",
      " 108: 'sea anemone, anemone',\n",
      " 109: 'brain coral',\n",
      " 110: 'flatworm, platyhelminth',\n",
      " 111: 'nematode, nematode worm, roundworm',\n",
      " 112: 'conch',\n",
      " 113: 'snail',\n",
      " 114: 'slug',\n",
      " 115: 'sea slug, nudibranch',\n",
      " 116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore',\n",
      " 117: 'chambered nautilus, pearly nautilus, nautilus',\n",
      " 118: 'Dungeness crab, Cancer magister',\n",
      " 119: 'rock crab, Cancer irroratus',\n",
      " 120: 'fiddler crab',\n",
      " 121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica',\n",
      " 122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus',\n",
      " 123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish',\n",
      " 124: 'crayfish, crawfish, crawdad, crawdaddy',\n",
      " 125: 'hermit crab',\n",
      " 126: 'isopod',\n",
      " 127: 'white stork, Ciconia ciconia',\n",
      " 128: 'black stork, Ciconia nigra',\n",
      " 129: 'spoonbill',\n",
      " 130: 'flamingo',\n",
      " 131: 'little blue heron, Egretta caerulea',\n",
      " 132: 'American egret, great white heron, Egretta albus',\n",
      " 133: 'bittern',\n",
      " 134: 'crane',\n",
      " 135: 'limpkin, Aramus pictus',\n",
      " 136: 'European gallinule, Porphyrio porphyrio',\n",
      " 137: 'American coot, marsh hen, mud hen, water hen, Fulica americana',\n",
      " 138: 'bustard',\n",
      " 139: 'ruddy turnstone, Arenaria interpres',\n",
      " 140: 'red-backed sandpiper, dunlin, Erolia alpina',\n",
      " 141: 'redshank, Tringa totanus',\n",
      " 142: 'dowitcher',\n",
      " 143: 'oystercatcher, oyster catcher',\n",
      " 144: 'pelican',\n",
      " 145: 'king penguin, Aptenodytes patagonica',\n",
      " 146: 'albatross, mollymawk',\n",
      " 147: 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus',\n",
      " 148: 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca',\n",
      " 149: 'dugong, Dugong dugon',\n",
      " 150: 'sea lion',\n",
      " 151: 'Chihuahua',\n",
      " 152: 'Japanese spaniel',\n",
      " 153: 'Maltese dog, Maltese terrier, Maltese',\n",
      " 154: 'Pekinese, Pekingese, Peke',\n",
      " 155: 'Shih-Tzu',\n",
      " 156: 'Blenheim spaniel',\n",
      " 157: 'papillon',\n",
      " 158: 'toy terrier',\n",
      " 159: 'Rhodesian ridgeback',\n",
      " 160: 'Afghan hound, Afghan',\n",
      " 161: 'basset, basset hound',\n",
      " 162: 'beagle',\n",
      " 163: 'bloodhound, sleuthhound',\n",
      " 164: 'bluetick',\n",
      " 165: 'black-and-tan coonhound',\n",
      " 166: 'Walker hound, Walker foxhound',\n",
      " 167: 'English foxhound',\n",
      " 168: 'redbone',\n",
      " 169: 'borzoi, Russian wolfhound',\n",
      " 170: 'Irish wolfhound',\n",
      " 171: 'Italian greyhound',\n",
      " 172: 'whippet',\n",
      " 173: 'Ibizan hound, Ibizan Podenco',\n",
      " 174: 'Norwegian elkhound, elkhound',\n",
      " 175: 'otterhound, otter hound',\n",
      " 176: 'Saluki, gazelle hound',\n",
      " 177: 'Scottish deerhound, deerhound',\n",
      " 178: 'Weimaraner',\n",
      " 179: 'Staffordshire bullterrier, Staffordshire bull terrier',\n",
      " 180: 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier',\n",
      " 181: 'Bedlington terrier',\n",
      " 182: 'Border terrier',\n",
      " 183: 'Kerry blue terrier',\n",
      " 184: 'Irish terrier',\n",
      " 185: 'Norfolk terrier',\n",
      " 186: 'Norwich terrier',\n",
      " 187: 'Yorkshire terrier',\n",
      " 188: 'wire-haired fox terrier',\n",
      " 189: 'Lakeland terrier',\n",
      " 190: 'Sealyham terrier, Sealyham',\n",
      " 191: 'Airedale, Airedale terrier',\n",
      " 192: 'cairn, cairn terrier',\n",
      " 193: 'Australian terrier',\n",
      " 194: 'Dandie Dinmont, Dandie Dinmont terrier',\n",
      " 195: 'Boston bull, Boston terrier',\n",
      " 196: 'miniature schnauzer',\n",
      " 197: 'giant schnauzer',\n",
      " 198: 'standard schnauzer',\n",
      " 199: 'Scotch terrier, Scottish terrier, Scottie',\n",
      " 200: 'Tibetan terrier, chrysanthemum dog',\n",
      " 201: 'silky terrier, Sydney silky',\n",
      " 202: 'soft-coated wheaten terrier',\n",
      " 203: 'West Highland white terrier',\n",
      " 204: 'Lhasa, Lhasa apso',\n",
      " 205: 'flat-coated retriever',\n",
      " 206: 'curly-coated retriever',\n",
      " 207: 'golden retriever',\n",
      " 208: 'Labrador retriever',\n",
      " 209: 'Chesapeake Bay retriever',\n",
      " 210: 'German short-haired pointer',\n",
      " 211: 'vizsla, Hungarian pointer',\n",
      " 212: 'English setter',\n",
      " 213: 'Irish setter, red setter',\n",
      " 214: 'Gordon setter',\n",
      " 215: 'Brittany spaniel',\n",
      " 216: 'clumber, clumber spaniel',\n",
      " 217: 'English springer, English springer spaniel',\n",
      " 218: 'Welsh springer spaniel',\n",
      " 219: 'cocker spaniel, English cocker spaniel, cocker',\n",
      " 220: 'Sussex spaniel',\n",
      " 221: 'Irish water spaniel',\n",
      " 222: 'kuvasz',\n",
      " 223: 'schipperke',\n",
      " 224: 'groenendael',\n",
      " 225: 'malinois',\n",
      " 226: 'briard',\n",
      " 227: 'kelpie',\n",
      " 228: 'komondor',\n",
      " 229: 'Old English sheepdog, bobtail',\n",
      " 230: 'Shetland sheepdog, Shetland sheep dog, Shetland',\n",
      " 231: 'collie',\n",
      " 232: 'Border collie',\n",
      " 233: 'Bouvier des Flandres, Bouviers des Flandres',\n",
      " 234: 'Rottweiler',\n",
      " 235: 'German shepherd, German shepherd dog, German police dog, alsatian',\n",
      " 236: 'Doberman, Doberman pinscher',\n",
      " 237: 'miniature pinscher',\n",
      " 238: 'Greater Swiss Mountain dog',\n",
      " 239: 'Bernese mountain dog',\n",
      " 240: 'Appenzeller',\n",
      " 241: 'EntleBucher',\n",
      " 242: 'boxer',\n",
      " 243: 'bull mastiff',\n",
      " 244: 'Tibetan mastiff',\n",
      " 245: 'French bulldog',\n",
      " 246: 'Great Dane',\n",
      " 247: 'Saint Bernard, St Bernard',\n",
      " 248: 'Eskimo dog, husky',\n",
      " 249: 'malamute, malemute, Alaskan malamute',\n",
      " 250: 'Siberian husky',\n",
      " 251: 'dalmatian, coach dog, carriage dog',\n",
      " 252: 'affenpinscher, monkey pinscher, monkey dog',\n",
      " 253: 'basenji',\n",
      " 254: 'pug, pug-dog',\n",
      " 255: 'Leonberg',\n",
      " 256: 'Newfoundland, Newfoundland dog',\n",
      " 257: 'Great Pyrenees',\n",
      " 258: 'Samoyed, Samoyede',\n",
      " 259: 'Pomeranian',\n",
      " 260: 'chow, chow chow',\n",
      " 261: 'keeshond',\n",
      " 262: 'Brabancon griffon',\n",
      " 263: 'Pembroke, Pembroke Welsh corgi',\n",
      " 264: 'Cardigan, Cardigan Welsh corgi',\n",
      " 265: 'toy poodle',\n",
      " 266: 'miniature poodle',\n",
      " 267: 'standard poodle',\n",
      " 268: 'Mexican hairless',\n",
      " 269: 'timber wolf, grey wolf, gray wolf, Canis lupus',\n",
      " 270: 'white wolf, Arctic wolf, Canis lupus tundrarum',\n",
      " 271: 'red wolf, maned wolf, Canis rufus, Canis niger',\n",
      " 272: 'coyote, prairie wolf, brush wolf, Canis latrans',\n",
      " 273: 'dingo, warrigal, warragal, Canis dingo',\n",
      " 274: 'dhole, Cuon alpinus',\n",
      " 275: 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus',\n",
      " 276: 'hyena, hyaena',\n",
      " 277: 'red fox, Vulpes vulpes',\n",
      " 278: 'kit fox, Vulpes macrotis',\n",
      " 279: 'Arctic fox, white fox, Alopex lagopus',\n",
      " 280: 'grey fox, gray fox, Urocyon cinereoargenteus',\n",
      " 281: 'tabby, tabby cat',\n",
      " 282: 'tiger cat',\n",
      " 283: 'Persian cat',\n",
      " 284: 'Siamese cat, Siamese',\n",
      " 285: 'Egyptian cat',\n",
      " 286: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n",
      " 287: 'lynx, catamount',\n",
      " 288: 'leopard, Panthera pardus',\n",
      " 289: 'snow leopard, ounce, Panthera uncia',\n",
      " 290: 'jaguar, panther, Panthera onca, Felis onca',\n",
      " 291: 'lion, king of beasts, Panthera leo',\n",
      " 292: 'tiger, Panthera tigris',\n",
      " 293: 'cheetah, chetah, Acinonyx jubatus',\n",
      " 294: 'brown bear, bruin, Ursus arctos',\n",
      " 295: 'American black bear, black bear, Ursus americanus, Euarctos americanus',\n",
      " 296: 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus',\n",
      " 297: 'sloth bear, Melursus ursinus, Ursus ursinus',\n",
      " 298: 'mongoose',\n",
      " 299: 'meerkat, mierkat',\n",
      " 300: 'tiger beetle',\n",
      " 301: 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle',\n",
      " 302: 'ground beetle, carabid beetle',\n",
      " 303: 'long-horned beetle, longicorn, longicorn beetle',\n",
      " 304: 'leaf beetle, chrysomelid',\n",
      " 305: 'dung beetle',\n",
      " 306: 'rhinoceros beetle',\n",
      " 307: 'weevil',\n",
      " 308: 'fly',\n",
      " 309: 'bee',\n",
      " 310: 'ant, emmet, pismire',\n",
      " 311: 'grasshopper, hopper',\n",
      " 312: 'cricket',\n",
      " 313: 'walking stick, walkingstick, stick insect',\n",
      " 314: 'cockroach, roach',\n",
      " 315: 'mantis, mantid',\n",
      " 316: 'cicada, cicala',\n",
      " 317: 'leafhopper',\n",
      " 318: 'lacewing, lacewing fly',\n",
      " 319: \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\",\n",
      " 320: 'damselfly',\n",
      " 321: 'admiral',\n",
      " 322: 'ringlet, ringlet butterfly',\n",
      " 323: 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus',\n",
      " 324: 'cabbage butterfly',\n",
      " 325: 'sulphur butterfly, sulfur butterfly',\n",
      " 326: 'lycaenid, lycaenid butterfly',\n",
      " 327: 'starfish, sea star',\n",
      " 328: 'sea urchin',\n",
      " 329: 'sea cucumber, holothurian',\n",
      " 330: 'wood rabbit, cottontail, cottontail rabbit',\n",
      " 331: 'hare',\n",
      " 332: 'Angora, Angora rabbit',\n",
      " 333: 'hamster',\n",
      " 334: 'porcupine, hedgehog',\n",
      " 335: 'fox squirrel, eastern fox squirrel, Sciurus niger',\n",
      " 336: 'marmot',\n",
      " 337: 'beaver',\n",
      " 338: 'guinea pig, Cavia cobaya',\n",
      " 339: 'sorrel',\n",
      " 340: 'zebra',\n",
      " 341: 'hog, pig, grunter, squealer, Sus scrofa',\n",
      " 342: 'wild boar, boar, Sus scrofa',\n",
      " 343: 'warthog',\n",
      " 344: 'hippopotamus, hippo, river horse, Hippopotamus amphibius',\n",
      " 345: 'ox',\n",
      " 346: 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis',\n",
      " 347: 'bison',\n",
      " 348: 'ram, tup',\n",
      " 349: 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis',\n",
      " 350: 'ibex, Capra ibex',\n",
      " 351: 'hartebeest',\n",
      " 352: 'impala, Aepyceros melampus',\n",
      " 353: 'gazelle',\n",
      " 354: 'Arabian camel, dromedary, Camelus dromedarius',\n",
      " 355: 'llama',\n",
      " 356: 'weasel',\n",
      " 357: 'mink',\n",
      " 358: 'polecat, fitch, foulmart, foumart, Mustela putorius',\n",
      " 359: 'black-footed ferret, ferret, Mustela nigripes',\n",
      " 360: 'otter',\n",
      " 361: 'skunk, polecat, wood pussy',\n",
      " 362: 'badger',\n",
      " 363: 'armadillo',\n",
      " 364: 'three-toed sloth, ai, Bradypus tridactylus',\n",
      " 365: 'orangutan, orang, orangutang, Pongo pygmaeus',\n",
      " 366: 'gorilla, Gorilla gorilla',\n",
      " 367: 'chimpanzee, chimp, Pan troglodytes',\n",
      " 368: 'gibbon, Hylobates lar',\n",
      " 369: 'siamang, Hylobates syndactylus, Symphalangus syndactylus',\n",
      " 370: 'guenon, guenon monkey',\n",
      " 371: 'patas, hussar monkey, Erythrocebus patas',\n",
      " 372: 'baboon',\n",
      " 373: 'macaque',\n",
      " 374: 'langur',\n",
      " 375: 'colobus, colobus monkey',\n",
      " 376: 'proboscis monkey, Nasalis larvatus',\n",
      " 377: 'marmoset',\n",
      " 378: 'capuchin, ringtail, Cebus capucinus',\n",
      " 379: 'howler monkey, howler',\n",
      " 380: 'titi, titi monkey',\n",
      " 381: 'spider monkey, Ateles geoffroyi',\n",
      " 382: 'squirrel monkey, Saimiri sciureus',\n",
      " 383: 'Madagascar cat, ring-tailed lemur, Lemur catta',\n",
      " 384: 'indri, indris, Indri indri, Indri brevicaudatus',\n",
      " 385: 'Indian elephant, Elephas maximus',\n",
      " 386: 'African elephant, Loxodonta africana',\n",
      " 387: 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens',\n",
      " 388: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca',\n",
      " 389: 'barracouta, snoek',\n",
      " 390: 'eel',\n",
      " 391: 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch',\n",
      " 392: 'rock beauty, Holocanthus tricolor',\n",
      " 393: 'anemone fish',\n",
      " 394: 'sturgeon',\n",
      " 395: 'gar, garfish, garpike, billfish, Lepisosteus osseus',\n",
      " 396: 'lionfish',\n",
      " 397: 'puffer, pufferfish, blowfish, globefish',\n",
      " 398: 'abacus',\n",
      " 399: 'abaya',\n",
      " 400: \"academic gown, academic robe, judge's robe\",\n",
      " 401: 'accordion, piano accordion, squeeze box',\n",
      " 402: 'acoustic guitar',\n",
      " 403: 'aircraft carrier, carrier, flattop, attack aircraft carrier',\n",
      " 404: 'airliner',\n",
      " 405: 'airship, dirigible',\n",
      " 406: 'altar',\n",
      " 407: 'ambulance',\n",
      " 408: 'amphibian, amphibious vehicle',\n",
      " 409: 'analog clock',\n",
      " 410: 'apiary, bee house',\n",
      " 411: 'apron',\n",
      " 412: 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin',\n",
      " 413: 'assault rifle, assault gun',\n",
      " 414: 'backpack, back pack, knapsack, packsack, rucksack, haversack',\n",
      " 415: 'bakery, bakeshop, bakehouse',\n",
      " 416: 'balance beam, beam',\n",
      " 417: 'balloon',\n",
      " 418: 'ballpoint, ballpoint pen, ballpen, Biro',\n",
      " 419: 'Band Aid',\n",
      " 420: 'banjo',\n",
      " 421: 'bannister, banister, balustrade, balusters, handrail',\n",
      " 422: 'barbell',\n",
      " 423: 'barber chair',\n",
      " 424: 'barbershop',\n",
      " 425: 'barn',\n",
      " 426: 'barometer',\n",
      " 427: 'barrel, cask',\n",
      " 428: 'barrow, garden cart, lawn cart, wheelbarrow',\n",
      " 429: 'baseball',\n",
      " 430: 'basketball',\n",
      " 431: 'bassinet',\n",
      " 432: 'bassoon',\n",
      " 433: 'bathing cap, swimming cap',\n",
      " 434: 'bath towel',\n",
      " 435: 'bathtub, bathing tub, bath, tub',\n",
      " 436: 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon',\n",
      " 437: 'beacon, lighthouse, beacon light, pharos',\n",
      " 438: 'beaker',\n",
      " 439: 'bearskin, busby, shako',\n",
      " 440: 'beer bottle',\n",
      " 441: 'beer glass',\n",
      " 442: 'bell cote, bell cot',\n",
      " 443: 'bib',\n",
      " 444: 'bicycle-built-for-two, tandem bicycle, tandem',\n",
      " 445: 'bikini, two-piece',\n",
      " 446: 'binder, ring-binder',\n",
      " 447: 'binoculars, field glasses, opera glasses',\n",
      " 448: 'birdhouse',\n",
      " 449: 'boathouse',\n",
      " 450: 'bobsled, bobsleigh, bob',\n",
      " 451: 'bolo tie, bolo, bola tie, bola',\n",
      " 452: 'bonnet, poke bonnet',\n",
      " 453: 'bookcase',\n",
      " 454: 'bookshop, bookstore, bookstall',\n",
      " 455: 'bottlecap',\n",
      " 456: 'bow',\n",
      " 457: 'bow tie, bow-tie, bowtie',\n",
      " 458: 'brass, memorial tablet, plaque',\n",
      " 459: 'brassiere, bra, bandeau',\n",
      " 460: 'breakwater, groin, groyne, mole, bulwark, seawall, jetty',\n",
      " 461: 'breastplate, aegis, egis',\n",
      " 462: 'broom',\n",
      " 463: 'bucket, pail',\n",
      " 464: 'buckle',\n",
      " 465: 'bulletproof vest',\n",
      " 466: 'bullet train, bullet',\n",
      " 467: 'butcher shop, meat market',\n",
      " 468: 'cab, hack, taxi, taxicab',\n",
      " 469: 'caldron, cauldron',\n",
      " 470: 'candle, taper, wax light',\n",
      " 471: 'cannon',\n",
      " 472: 'canoe',\n",
      " 473: 'can opener, tin opener',\n",
      " 474: 'cardigan',\n",
      " 475: 'car mirror',\n",
      " 476: 'carousel, carrousel, merry-go-round, roundabout, whirligig',\n",
      " 477: \"carpenter's kit, tool kit\",\n",
      " 478: 'carton',\n",
      " 479: 'car wheel',\n",
      " 480: 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM',\n",
      " 481: 'cassette',\n",
      " 482: 'cassette player',\n",
      " 483: 'castle',\n",
      " 484: 'catamaran',\n",
      " 485: 'CD player',\n",
      " 486: 'cello, violoncello',\n",
      " 487: 'cellular telephone, cellular phone, cellphone, cell, mobile phone',\n",
      " 488: 'chain',\n",
      " 489: 'chainlink fence',\n",
      " 490: 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour',\n",
      " 491: 'chain saw, chainsaw',\n",
      " 492: 'chest',\n",
      " 493: 'chiffonier, commode',\n",
      " 494: 'chime, bell, gong',\n",
      " 495: 'china cabinet, china closet',\n",
      " 496: 'Christmas stocking',\n",
      " 497: 'church, church building',\n",
      " 498: 'cinema, movie theater, movie theatre, movie house, picture palace',\n",
      " 499: 'cleaver, meat cleaver, chopper',\n",
      " 500: 'cliff dwelling',\n",
      " 501: 'cloak',\n",
      " 502: 'clog, geta, patten, sabot',\n",
      " 503: 'cocktail shaker',\n",
      " 504: 'coffee mug',\n",
      " 505: 'coffeepot',\n",
      " 506: 'coil, spiral, volute, whorl, helix',\n",
      " 507: 'combination lock',\n",
      " 508: 'computer keyboard, keypad',\n",
      " 509: 'confectionery, confectionary, candy store',\n",
      " 510: 'container ship, containership, container vessel',\n",
      " 511: 'convertible',\n",
      " 512: 'corkscrew, bottle screw',\n",
      " 513: 'cornet, horn, trumpet, trump',\n",
      " 514: 'cowboy boot',\n",
      " 515: 'cowboy hat, ten-gallon hat',\n",
      " 516: 'cradle',\n",
      " 517: 'crane',\n",
      " 518: 'crash helmet',\n",
      " 519: 'crate',\n",
      " 520: 'crib, cot',\n",
      " 521: 'Crock Pot',\n",
      " 522: 'croquet ball',\n",
      " 523: 'crutch',\n",
      " 524: 'cuirass',\n",
      " 525: 'dam, dike, dyke',\n",
      " 526: 'desk',\n",
      " 527: 'desktop computer',\n",
      " 528: 'dial telephone, dial phone',\n",
      " 529: 'diaper, nappy, napkin',\n",
      " 530: 'digital clock',\n",
      " 531: 'digital watch',\n",
      " 532: 'dining table, board',\n",
      " 533: 'dishrag, dishcloth',\n",
      " 534: 'dishwasher, dish washer, dishwashing machine',\n",
      " 535: 'disk brake, disc brake',\n",
      " 536: 'dock, dockage, docking facility',\n",
      " 537: 'dogsled, dog sled, dog sleigh',\n",
      " 538: 'dome',\n",
      " 539: 'doormat, welcome mat',\n",
      " 540: 'drilling platform, offshore rig',\n",
      " 541: 'drum, membranophone, tympan',\n",
      " 542: 'drumstick',\n",
      " 543: 'dumbbell',\n",
      " 544: 'Dutch oven',\n",
      " 545: 'electric fan, blower',\n",
      " 546: 'electric guitar',\n",
      " 547: 'electric locomotive',\n",
      " 548: 'entertainment center',\n",
      " 549: 'envelope',\n",
      " 550: 'espresso maker',\n",
      " 551: 'face powder',\n",
      " 552: 'feather boa, boa',\n",
      " 553: 'file, file cabinet, filing cabinet',\n",
      " 554: 'fireboat',\n",
      " 555: 'fire engine, fire truck',\n",
      " 556: 'fire screen, fireguard',\n",
      " 557: 'flagpole, flagstaff',\n",
      " 558: 'flute, transverse flute',\n",
      " 559: 'folding chair',\n",
      " 560: 'football helmet',\n",
      " 561: 'forklift',\n",
      " 562: 'fountain',\n",
      " 563: 'fountain pen',\n",
      " 564: 'four-poster',\n",
      " 565: 'freight car',\n",
      " 566: 'French horn, horn',\n",
      " 567: 'frying pan, frypan, skillet',\n",
      " 568: 'fur coat',\n",
      " 569: 'garbage truck, dustcart',\n",
      " 570: 'gasmask, respirator, gas helmet',\n",
      " 571: 'gas pump, gasoline pump, petrol pump, island dispenser',\n",
      " 572: 'goblet',\n",
      " 573: 'go-kart',\n",
      " 574: 'golf ball',\n",
      " 575: 'golfcart, golf cart',\n",
      " 576: 'gondola',\n",
      " 577: 'gong, tam-tam',\n",
      " 578: 'gown',\n",
      " 579: 'grand piano, grand',\n",
      " 580: 'greenhouse, nursery, glasshouse',\n",
      " 581: 'grille, radiator grille',\n",
      " 582: 'grocery store, grocery, food market, market',\n",
      " 583: 'guillotine',\n",
      " 584: 'hair slide',\n",
      " 585: 'hair spray',\n",
      " 586: 'half track',\n",
      " 587: 'hammer',\n",
      " 588: 'hamper',\n",
      " 589: 'hand blower, blow dryer, blow drier, hair dryer, hair drier',\n",
      " 590: 'hand-held computer, hand-held microcomputer',\n",
      " 591: 'handkerchief, hankie, hanky, hankey',\n",
      " 592: 'hard disc, hard disk, fixed disk',\n",
      " 593: 'harmonica, mouth organ, harp, mouth harp',\n",
      " 594: 'harp',\n",
      " 595: 'harvester, reaper',\n",
      " 596: 'hatchet',\n",
      " 597: 'holster',\n",
      " 598: 'home theater, home theatre',\n",
      " 599: 'honeycomb',\n",
      " 600: 'hook, claw',\n",
      " 601: 'hoopskirt, crinoline',\n",
      " 602: 'horizontal bar, high bar',\n",
      " 603: 'horse cart, horse-cart',\n",
      " 604: 'hourglass',\n",
      " 605: 'iPod',\n",
      " 606: 'iron, smoothing iron',\n",
      " 607: \"jack-o'-lantern\",\n",
      " 608: 'jean, blue jean, denim',\n",
      " 609: 'jeep, landrover',\n",
      " 610: 'jersey, T-shirt, tee shirt',\n",
      " 611: 'jigsaw puzzle',\n",
      " 612: 'jinrikisha, ricksha, rickshaw',\n",
      " 613: 'joystick',\n",
      " 614: 'kimono',\n",
      " 615: 'knee pad',\n",
      " 616: 'knot',\n",
      " 617: 'lab coat, laboratory coat',\n",
      " 618: 'ladle',\n",
      " 619: 'lampshade, lamp shade',\n",
      " 620: 'laptop, laptop computer',\n",
      " 621: 'lawn mower, mower',\n",
      " 622: 'lens cap, lens cover',\n",
      " 623: 'letter opener, paper knife, paperknife',\n",
      " 624: 'library',\n",
      " 625: 'lifeboat',\n",
      " 626: 'lighter, light, igniter, ignitor',\n",
      " 627: 'limousine, limo',\n",
      " 628: 'liner, ocean liner',\n",
      " 629: 'lipstick, lip rouge',\n",
      " 630: 'Loafer',\n",
      " 631: 'lotion',\n",
      " 632: 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system',\n",
      " 633: \"loupe, jeweler's loupe\",\n",
      " 634: 'lumbermill, sawmill',\n",
      " 635: 'magnetic compass',\n",
      " 636: 'mailbag, postbag',\n",
      " 637: 'mailbox, letter box',\n",
      " 638: 'maillot',\n",
      " 639: 'maillot, tank suit',\n",
      " 640: 'manhole cover',\n",
      " 641: 'maraca',\n",
      " 642: 'marimba, xylophone',\n",
      " 643: 'mask',\n",
      " 644: 'matchstick',\n",
      " 645: 'maypole',\n",
      " 646: 'maze, labyrinth',\n",
      " 647: 'measuring cup',\n",
      " 648: 'medicine chest, medicine cabinet',\n",
      " 649: 'megalith, megalithic structure',\n",
      " 650: 'microphone, mike',\n",
      " 651: 'microwave, microwave oven',\n",
      " 652: 'military uniform',\n",
      " 653: 'milk can',\n",
      " 654: 'minibus',\n",
      " 655: 'miniskirt, mini',\n",
      " 656: 'minivan',\n",
      " 657: 'missile',\n",
      " 658: 'mitten',\n",
      " 659: 'mixing bowl',\n",
      " 660: 'mobile home, manufactured home',\n",
      " 661: 'Model T',\n",
      " 662: 'modem',\n",
      " 663: 'monastery',\n",
      " 664: 'monitor',\n",
      " 665: 'moped',\n",
      " 666: 'mortar',\n",
      " 667: 'mortarboard',\n",
      " 668: 'mosque',\n",
      " 669: 'mosquito net',\n",
      " 670: 'motor scooter, scooter',\n",
      " 671: 'mountain bike, all-terrain bike, off-roader',\n",
      " 672: 'mountain tent',\n",
      " 673: 'mouse, computer mouse',\n",
      " 674: 'mousetrap',\n",
      " 675: 'moving van',\n",
      " 676: 'muzzle',\n",
      " 677: 'nail',\n",
      " 678: 'neck brace',\n",
      " 679: 'necklace',\n",
      " 680: 'nipple',\n",
      " 681: 'notebook, notebook computer',\n",
      " 682: 'obelisk',\n",
      " 683: 'oboe, hautboy, hautbois',\n",
      " 684: 'ocarina, sweet potato',\n",
      " 685: 'odometer, hodometer, mileometer, milometer',\n",
      " 686: 'oil filter',\n",
      " 687: 'organ, pipe organ',\n",
      " 688: 'oscilloscope, scope, cathode-ray oscilloscope, CRO',\n",
      " 689: 'overskirt',\n",
      " 690: 'oxcart',\n",
      " 691: 'oxygen mask',\n",
      " 692: 'packet',\n",
      " 693: 'paddle, boat paddle',\n",
      " 694: 'paddlewheel, paddle wheel',\n",
      " 695: 'padlock',\n",
      " 696: 'paintbrush',\n",
      " 697: \"pajama, pyjama, pj's, jammies\",\n",
      " 698: 'palace',\n",
      " 699: 'panpipe, pandean pipe, syrinx',\n",
      " 700: 'paper towel',\n",
      " 701: 'parachute, chute',\n",
      " 702: 'parallel bars, bars',\n",
      " 703: 'park bench',\n",
      " 704: 'parking meter',\n",
      " 705: 'passenger car, coach, carriage',\n",
      " 706: 'patio, terrace',\n",
      " 707: 'pay-phone, pay-station',\n",
      " 708: 'pedestal, plinth, footstall',\n",
      " 709: 'pencil box, pencil case',\n",
      " 710: 'pencil sharpener',\n",
      " 711: 'perfume, essence',\n",
      " 712: 'Petri dish',\n",
      " 713: 'photocopier',\n",
      " 714: 'pick, plectrum, plectron',\n",
      " 715: 'pickelhaube',\n",
      " 716: 'picket fence, paling',\n",
      " 717: 'pickup, pickup truck',\n",
      " 718: 'pier',\n",
      " 719: 'piggy bank, penny bank',\n",
      " 720: 'pill bottle',\n",
      " 721: 'pillow',\n",
      " 722: 'ping-pong ball',\n",
      " 723: 'pinwheel',\n",
      " 724: 'pirate, pirate ship',\n",
      " 725: 'pitcher, ewer',\n",
      " 726: \"plane, carpenter's plane, woodworking plane\",\n",
      " 727: 'planetarium',\n",
      " 728: 'plastic bag',\n",
      " 729: 'plate rack',\n",
      " 730: 'plow, plough',\n",
      " 731: \"plunger, plumber's helper\",\n",
      " 732: 'Polaroid camera, Polaroid Land camera',\n",
      " 733: 'pole',\n",
      " 734: 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria',\n",
      " 735: 'poncho',\n",
      " 736: 'pool table, billiard table, snooker table',\n",
      " 737: 'pop bottle, soda bottle',\n",
      " 738: 'pot, flowerpot',\n",
      " 739: \"potter's wheel\",\n",
      " 740: 'power drill',\n",
      " 741: 'prayer rug, prayer mat',\n",
      " 742: 'printer',\n",
      " 743: 'prison, prison house',\n",
      " 744: 'projectile, missile',\n",
      " 745: 'projector',\n",
      " 746: 'puck, hockey puck',\n",
      " 747: 'punching bag, punch bag, punching ball, punchball',\n",
      " 748: 'purse',\n",
      " 749: 'quill, quill pen',\n",
      " 750: 'quilt, comforter, comfort, puff',\n",
      " 751: 'racer, race car, racing car',\n",
      " 752: 'racket, racquet',\n",
      " 753: 'radiator',\n",
      " 754: 'radio, wireless',\n",
      " 755: 'radio telescope, radio reflector',\n",
      " 756: 'rain barrel',\n",
      " 757: 'recreational vehicle, RV, R.V.',\n",
      " 758: 'reel',\n",
      " 759: 'reflex camera',\n",
      " 760: 'refrigerator, icebox',\n",
      " 761: 'remote control, remote',\n",
      " 762: 'restaurant, eating house, eating place, eatery',\n",
      " 763: 'revolver, six-gun, six-shooter',\n",
      " 764: 'rifle',\n",
      " 765: 'rocking chair, rocker',\n",
      " 766: 'rotisserie',\n",
      " 767: 'rubber eraser, rubber, pencil eraser',\n",
      " 768: 'rugby ball',\n",
      " 769: 'rule, ruler',\n",
      " 770: 'running shoe',\n",
      " 771: 'safe',\n",
      " 772: 'safety pin',\n",
      " 773: 'saltshaker, salt shaker',\n",
      " 774: 'sandal',\n",
      " 775: 'sarong',\n",
      " 776: 'sax, saxophone',\n",
      " 777: 'scabbard',\n",
      " 778: 'scale, weighing machine',\n",
      " 779: 'school bus',\n",
      " 780: 'schooner',\n",
      " 781: 'scoreboard',\n",
      " 782: 'screen, CRT screen',\n",
      " 783: 'screw',\n",
      " 784: 'screwdriver',\n",
      " 785: 'seat belt, seatbelt',\n",
      " 786: 'sewing machine',\n",
      " 787: 'shield, buckler',\n",
      " 788: 'shoe shop, shoe-shop, shoe store',\n",
      " 789: 'shoji',\n",
      " 790: 'shopping basket',\n",
      " 791: 'shopping cart',\n",
      " 792: 'shovel',\n",
      " 793: 'shower cap',\n",
      " 794: 'shower curtain',\n",
      " 795: 'ski',\n",
      " 796: 'ski mask',\n",
      " 797: 'sleeping bag',\n",
      " 798: 'slide rule, slipstick',\n",
      " 799: 'sliding door',\n",
      " 800: 'slot, one-armed bandit',\n",
      " 801: 'snorkel',\n",
      " 802: 'snowmobile',\n",
      " 803: 'snowplow, snowplough',\n",
      " 804: 'soap dispenser',\n",
      " 805: 'soccer ball',\n",
      " 806: 'sock',\n",
      " 807: 'solar dish, solar collector, solar furnace',\n",
      " 808: 'sombrero',\n",
      " 809: 'soup bowl',\n",
      " 810: 'space bar',\n",
      " 811: 'space heater',\n",
      " 812: 'space shuttle',\n",
      " 813: 'spatula',\n",
      " 814: 'speedboat',\n",
      " 815: \"spider web, spider's web\",\n",
      " 816: 'spindle',\n",
      " 817: 'sports car, sport car',\n",
      " 818: 'spotlight, spot',\n",
      " 819: 'stage',\n",
      " 820: 'steam locomotive',\n",
      " 821: 'steel arch bridge',\n",
      " 822: 'steel drum',\n",
      " 823: 'stethoscope',\n",
      " 824: 'stole',\n",
      " 825: 'stone wall',\n",
      " 826: 'stopwatch, stop watch',\n",
      " 827: 'stove',\n",
      " 828: 'strainer',\n",
      " 829: 'streetcar, tram, tramcar, trolley, trolley car',\n",
      " 830: 'stretcher',\n",
      " 831: 'studio couch, day bed',\n",
      " 832: 'stupa, tope',\n",
      " 833: 'submarine, pigboat, sub, U-boat',\n",
      " 834: 'suit, suit of clothes',\n",
      " 835: 'sundial',\n",
      " 836: 'sunglass',\n",
      " 837: 'sunglasses, dark glasses, shades',\n",
      " 838: 'sunscreen, sunblock, sun blocker',\n",
      " 839: 'suspension bridge',\n",
      " 840: 'swab, swob, mop',\n",
      " 841: 'sweatshirt',\n",
      " 842: 'swimming trunks, bathing trunks',\n",
      " 843: 'swing',\n",
      " 844: 'switch, electric switch, electrical switch',\n",
      " 845: 'syringe',\n",
      " 846: 'table lamp',\n",
      " 847: 'tank, army tank, armored combat vehicle, armoured combat vehicle',\n",
      " 848: 'tape player',\n",
      " 849: 'teapot',\n",
      " 850: 'teddy, teddy bear',\n",
      " 851: 'television, television system',\n",
      " 852: 'tennis ball',\n",
      " 853: 'thatch, thatched roof',\n",
      " 854: 'theater curtain, theatre curtain',\n",
      " 855: 'thimble',\n",
      " 856: 'thresher, thrasher, threshing machine',\n",
      " 857: 'throne',\n",
      " 858: 'tile roof',\n",
      " 859: 'toaster',\n",
      " 860: 'tobacco shop, tobacconist shop, tobacconist',\n",
      " 861: 'toilet seat',\n",
      " 862: 'torch',\n",
      " 863: 'totem pole',\n",
      " 864: 'tow truck, tow car, wrecker',\n",
      " 865: 'toyshop',\n",
      " 866: 'tractor',\n",
      " 867: 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi',\n",
      " 868: 'tray',\n",
      " 869: 'trench coat',\n",
      " 870: 'tricycle, trike, velocipede',\n",
      " 871: 'trimaran',\n",
      " 872: 'tripod',\n",
      " 873: 'triumphal arch',\n",
      " 874: 'trolleybus, trolley coach, trackless trolley',\n",
      " 875: 'trombone',\n",
      " 876: 'tub, vat',\n",
      " 877: 'turnstile',\n",
      " 878: 'typewriter keyboard',\n",
      " 879: 'umbrella',\n",
      " 880: 'unicycle, monocycle',\n",
      " 881: 'upright, upright piano',\n",
      " 882: 'vacuum, vacuum cleaner',\n",
      " 883: 'vase',\n",
      " 884: 'vault',\n",
      " 885: 'velvet',\n",
      " 886: 'vending machine',\n",
      " 887: 'vestment',\n",
      " 888: 'viaduct',\n",
      " 889: 'violin, fiddle',\n",
      " 890: 'volleyball',\n",
      " 891: 'waffle iron',\n",
      " 892: 'wall clock',\n",
      " 893: 'wallet, billfold, notecase, pocketbook',\n",
      " 894: 'wardrobe, closet, press',\n",
      " 895: 'warplane, military plane',\n",
      " 896: 'washbasin, handbasin, washbowl, lavabo, wash-hand basin',\n",
      " 897: 'washer, automatic washer, washing machine',\n",
      " 898: 'water bottle',\n",
      " 899: 'water jug',\n",
      " 900: 'water tower',\n",
      " 901: 'whiskey jug',\n",
      " 902: 'whistle',\n",
      " 903: 'wig',\n",
      " 904: 'window screen',\n",
      " 905: 'window shade',\n",
      " 906: 'Windsor tie',\n",
      " 907: 'wine bottle',\n",
      " 908: 'wing',\n",
      " 909: 'wok',\n",
      " 910: 'wooden spoon',\n",
      " 911: 'wool, woolen, woollen',\n",
      " 912: 'worm fence, snake fence, snake-rail fence, Virginia fence',\n",
      " 913: 'wreck',\n",
      " 914: 'yawl',\n",
      " 915: 'yurt',\n",
      " 916: 'web site, website, internet site, site',\n",
      " 917: 'comic book',\n",
      " 918: 'crossword puzzle, crossword',\n",
      " 919: 'street sign',\n",
      " 920: 'traffic light, traffic signal, stoplight',\n",
      " 921: 'book jacket, dust cover, dust jacket, dust wrapper',\n",
      " 922: 'menu',\n",
      " 923: 'plate',\n",
      " 924: 'guacamole',\n",
      " 925: 'consomme',\n",
      " 926: 'hot pot, hotpot',\n",
      " 927: 'trifle',\n",
      " 928: 'ice cream, icecream',\n",
      " 929: 'ice lolly, lolly, lollipop, popsicle',\n",
      " 930: 'French loaf',\n",
      " 931: 'bagel, beigel',\n",
      " 932: 'pretzel',\n",
      " 933: 'cheeseburger',\n",
      " 934: 'hotdog, hot dog, red hot',\n",
      " 935: 'mashed potato',\n",
      " 936: 'head cabbage',\n",
      " 937: 'broccoli',\n",
      " 938: 'cauliflower',\n",
      " 939: 'zucchini, courgette',\n",
      " 940: 'spaghetti squash',\n",
      " 941: 'acorn squash',\n",
      " 942: 'butternut squash',\n",
      " 943: 'cucumber, cuke',\n",
      " 944: 'artichoke, globe artichoke',\n",
      " 945: 'bell pepper',\n",
      " 946: 'cardoon',\n",
      " 947: 'mushroom',\n",
      " 948: 'Granny Smith',\n",
      " 949: 'strawberry',\n",
      " 950: 'orange',\n",
      " 951: 'lemon',\n",
      " 952: 'fig',\n",
      " 953: 'pineapple, ananas',\n",
      " 954: 'banana',\n",
      " 955: 'jackfruit, jak, jack',\n",
      " 956: 'custard apple',\n",
      " 957: 'pomegranate',\n",
      " 958: 'hay',\n",
      " 959: 'carbonara',\n",
      " 960: 'chocolate sauce, chocolate syrup',\n",
      " 961: 'dough',\n",
      " 962: 'meat loaf, meatloaf',\n",
      " 963: 'pizza, pizza pie',\n",
      " 964: 'potpie',\n",
      " 965: 'burrito',\n",
      " 966: 'red wine',\n",
      " 967: 'espresso',\n",
      " 968: 'cup',\n",
      " 969: 'eggnog',\n",
      " 970: 'alp',\n",
      " 971: 'bubble',\n",
      " 972: 'cliff, drop, drop-off',\n",
      " 973: 'coral reef',\n",
      " 974: 'geyser',\n",
      " 975: 'lakeside, lakeshore',\n",
      " 976: 'promontory, headland, head, foreland',\n",
      " 977: 'sandbar, sand bar',\n",
      " 978: 'seashore, coast, seacoast, sea-coast',\n",
      " 979: 'valley, vale',\n",
      " 980: 'volcano',\n",
      " 981: 'ballplayer, baseball player',\n",
      " 982: 'groom, bridegroom',\n",
      " 983: 'scuba diver',\n",
      " 984: 'rapeseed',\n",
      " 985: 'daisy',\n",
      " 986: \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\",\n",
      " 987: 'corn',\n",
      " 988: 'acorn',\n",
      " 989: 'hip, rose hip, rosehip',\n",
      " 990: 'buckeye, horse chestnut, conker',\n",
      " 991: 'coral fungus',\n",
      " 992: 'agaric',\n",
      " 993: 'gyromitra',\n",
      " 994: 'stinkhorn, carrion fungus',\n",
      " 995: 'earthstar',\n",
      " 996: 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa',\n",
      " 997: 'bolete',\n",
      " 998: 'ear, spike, capitulum',\n",
      " 999: 'toilet tissue, toilet paper, bathroom tissue'}\n",
      "68/1:\n",
      "from tensorflow.keras.applications import EfficientNetB4\n",
      "from tensorflow.image import resize\n",
      "model = EfficientNetB4(weights='imagenet')\n",
      "\n",
      "def center_crop(img, new_width=None, new_height=None):        \n",
      "\n",
      "    width = img.shape[1]\n",
      "    height = img.shape[0]\n",
      "\n",
      "    if new_width is None:\n",
      "        new_width = min(width, height)\n",
      "\n",
      "    if new_height is None:\n",
      "        new_height = min(width, height)\n",
      "\n",
      "    left = int(np.ceil((width - new_width) / 2))\n",
      "    right = width - int(np.floor((width - new_width) / 2))\n",
      "\n",
      "    top = int(np.ceil((height - new_height) / 2))\n",
      "    bottom = height - int(np.floor((height - new_height) / 2))\n",
      "\n",
      "    if len(img.shape) == 2:\n",
      "        center_cropped_img = img[top:bottom, left:right]\n",
      "    else:\n",
      "        center_cropped_img = img[top:bottom, left:right, ...]\n",
      "\n",
      "    return center_cropped_img\n",
      "\n",
      "class_dict = {0: 'tench, Tinca tinca',\n",
      " 1: 'goldfish, Carassius auratus',\n",
      " 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n",
      " 3: 'tiger shark, Galeocerdo cuvieri',\n",
      " 4: 'hammerhead, hammerhead shark',\n",
      " 5: 'electric ray, crampfish, numbfish, torpedo',\n",
      " 6: 'stingray',\n",
      " 7: 'cock',\n",
      " 8: 'hen',\n",
      " 9: 'ostrich, Struthio camelus',\n",
      " 10: 'brambling, Fringilla montifringilla',\n",
      " 11: 'goldfinch, Carduelis carduelis',\n",
      " 12: 'house finch, linnet, Carpodacus mexicanus',\n",
      " 13: 'junco, snowbird',\n",
      " 14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea',\n",
      " 15: 'robin, American robin, Turdus migratorius',\n",
      " 16: 'bulbul',\n",
      " 17: 'jay',\n",
      " 18: 'magpie',\n",
      " 19: 'chickadee',\n",
      " 20: 'water ouzel, dipper',\n",
      " 21: 'kite',\n",
      " 22: 'bald eagle, American eagle, Haliaeetus leucocephalus',\n",
      " 23: 'vulture',\n",
      " 24: 'great grey owl, great gray owl, Strix nebulosa',\n",
      " 25: 'European fire salamander, Salamandra salamandra',\n",
      " 26: 'common newt, Triturus vulgaris',\n",
      " 27: 'eft',\n",
      " 28: 'spotted salamander, Ambystoma maculatum',\n",
      " 29: 'axolotl, mud puppy, Ambystoma mexicanum',\n",
      " 30: 'bullfrog, Rana catesbeiana',\n",
      " 31: 'tree frog, tree-frog',\n",
      " 32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui',\n",
      " 33: 'loggerhead, loggerhead turtle, Caretta caretta',\n",
      " 34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea',\n",
      " 35: 'mud turtle',\n",
      " 36: 'terrapin',\n",
      " 37: 'box turtle, box tortoise',\n",
      " 38: 'banded gecko',\n",
      " 39: 'common iguana, iguana, Iguana iguana',\n",
      " 40: 'American chameleon, anole, Anolis carolinensis',\n",
      " 41: 'whiptail, whiptail lizard',\n",
      " 42: 'agama',\n",
      " 43: 'frilled lizard, Chlamydosaurus kingi',\n",
      " 44: 'alligator lizard',\n",
      " 45: 'Gila monster, Heloderma suspectum',\n",
      " 46: 'green lizard, Lacerta viridis',\n",
      " 47: 'African chameleon, Chamaeleo chamaeleon',\n",
      " 48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis',\n",
      " 49: 'African crocodile, Nile crocodile, Crocodylus niloticus',\n",
      " 50: 'American alligator, Alligator mississipiensis',\n",
      " 51: 'triceratops',\n",
      " 52: 'thunder snake, worm snake, Carphophis amoenus',\n",
      " 53: 'ringneck snake, ring-necked snake, ring snake',\n",
      " 54: 'hognose snake, puff adder, sand viper',\n",
      " 55: 'green snake, grass snake',\n",
      " 56: 'king snake, kingsnake',\n",
      " 57: 'garter snake, grass snake',\n",
      " 58: 'water snake',\n",
      " 59: 'vine snake',\n",
      " 60: 'night snake, Hypsiglena torquata',\n",
      " 61: 'boa constrictor, Constrictor constrictor',\n",
      " 62: 'rock python, rock snake, Python sebae',\n",
      " 63: 'Indian cobra, Naja naja',\n",
      " 64: 'green mamba',\n",
      " 65: 'sea snake',\n",
      " 66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus',\n",
      " 67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus',\n",
      " 68: 'sidewinder, horned rattlesnake, Crotalus cerastes',\n",
      " 69: 'trilobite',\n",
      " 70: 'harvestman, daddy longlegs, Phalangium opilio',\n",
      " 71: 'scorpion',\n",
      " 72: 'black and gold garden spider, Argiope aurantia',\n",
      " 73: 'barn spider, Araneus cavaticus',\n",
      " 74: 'garden spider, Aranea diademata',\n",
      " 75: 'black widow, Latrodectus mactans',\n",
      " 76: 'tarantula',\n",
      " 77: 'wolf spider, hunting spider',\n",
      " 78: 'tick',\n",
      " 79: 'centipede',\n",
      " 80: 'black grouse',\n",
      " 81: 'ptarmigan',\n",
      " 82: 'ruffed grouse, partridge, Bonasa umbellus',\n",
      " 83: 'prairie chicken, prairie grouse, prairie fowl',\n",
      " 84: 'peacock',\n",
      " 85: 'quail',\n",
      " 86: 'partridge',\n",
      " 87: 'African grey, African gray, Psittacus erithacus',\n",
      " 88: 'macaw',\n",
      " 89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n",
      " 90: 'lorikeet',\n",
      " 91: 'coucal',\n",
      " 92: 'bee eater',\n",
      " 93: 'hornbill',\n",
      " 94: 'hummingbird',\n",
      " 95: 'jacamar',\n",
      " 96: 'toucan',\n",
      " 97: 'drake',\n",
      " 98: 'red-breasted merganser, Mergus serrator',\n",
      " 99: 'goose',\n",
      " 100: 'black swan, Cygnus atratus',\n",
      " 101: 'tusker',\n",
      " 102: 'echidna, spiny anteater, anteater',\n",
      " 103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n",
      " 104: 'wallaby, brush kangaroo',\n",
      " 105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n",
      " 106: 'wombat',\n",
      " 107: 'jellyfish',\n",
      " 108: 'sea anemone, anemone',\n",
      " 109: 'brain coral',\n",
      " 110: 'flatworm, platyhelminth',\n",
      " 111: 'nematode, nematode worm, roundworm',\n",
      " 112: 'conch',\n",
      " 113: 'snail',\n",
      " 114: 'slug',\n",
      " 115: 'sea slug, nudibranch',\n",
      " 116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore',\n",
      " 117: 'chambered nautilus, pearly nautilus, nautilus',\n",
      " 118: 'Dungeness crab, Cancer magister',\n",
      " 119: 'rock crab, Cancer irroratus',\n",
      " 120: 'fiddler crab',\n",
      " 121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica',\n",
      " 122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus',\n",
      " 123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish',\n",
      " 124: 'crayfish, crawfish, crawdad, crawdaddy',\n",
      " 125: 'hermit crab',\n",
      " 126: 'isopod',\n",
      " 127: 'white stork, Ciconia ciconia',\n",
      " 128: 'black stork, Ciconia nigra',\n",
      " 129: 'spoonbill',\n",
      " 130: 'flamingo',\n",
      " 131: 'little blue heron, Egretta caerulea',\n",
      " 132: 'American egret, great white heron, Egretta albus',\n",
      " 133: 'bittern',\n",
      " 134: 'crane',\n",
      " 135: 'limpkin, Aramus pictus',\n",
      " 136: 'European gallinule, Porphyrio porphyrio',\n",
      " 137: 'American coot, marsh hen, mud hen, water hen, Fulica americana',\n",
      " 138: 'bustard',\n",
      " 139: 'ruddy turnstone, Arenaria interpres',\n",
      " 140: 'red-backed sandpiper, dunlin, Erolia alpina',\n",
      " 141: 'redshank, Tringa totanus',\n",
      " 142: 'dowitcher',\n",
      " 143: 'oystercatcher, oyster catcher',\n",
      " 144: 'pelican',\n",
      " 145: 'king penguin, Aptenodytes patagonica',\n",
      " 146: 'albatross, mollymawk',\n",
      " 147: 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus',\n",
      " 148: 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca',\n",
      " 149: 'dugong, Dugong dugon',\n",
      " 150: 'sea lion',\n",
      " 151: 'Chihuahua',\n",
      " 152: 'Japanese spaniel',\n",
      " 153: 'Maltese dog, Maltese terrier, Maltese',\n",
      " 154: 'Pekinese, Pekingese, Peke',\n",
      " 155: 'Shih-Tzu',\n",
      " 156: 'Blenheim spaniel',\n",
      " 157: 'papillon',\n",
      " 158: 'toy terrier',\n",
      " 159: 'Rhodesian ridgeback',\n",
      " 160: 'Afghan hound, Afghan',\n",
      " 161: 'basset, basset hound',\n",
      " 162: 'beagle',\n",
      " 163: 'bloodhound, sleuthhound',\n",
      " 164: 'bluetick',\n",
      " 165: 'black-and-tan coonhound',\n",
      " 166: 'Walker hound, Walker foxhound',\n",
      " 167: 'English foxhound',\n",
      " 168: 'redbone',\n",
      " 169: 'borzoi, Russian wolfhound',\n",
      " 170: 'Irish wolfhound',\n",
      " 171: 'Italian greyhound',\n",
      " 172: 'whippet',\n",
      " 173: 'Ibizan hound, Ibizan Podenco',\n",
      " 174: 'Norwegian elkhound, elkhound',\n",
      " 175: 'otterhound, otter hound',\n",
      " 176: 'Saluki, gazelle hound',\n",
      " 177: 'Scottish deerhound, deerhound',\n",
      " 178: 'Weimaraner',\n",
      " 179: 'Staffordshire bullterrier, Staffordshire bull terrier',\n",
      " 180: 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier',\n",
      " 181: 'Bedlington terrier',\n",
      " 182: 'Border terrier',\n",
      " 183: 'Kerry blue terrier',\n",
      " 184: 'Irish terrier',\n",
      " 185: 'Norfolk terrier',\n",
      " 186: 'Norwich terrier',\n",
      " 187: 'Yorkshire terrier',\n",
      " 188: 'wire-haired fox terrier',\n",
      " 189: 'Lakeland terrier',\n",
      " 190: 'Sealyham terrier, Sealyham',\n",
      " 191: 'Airedale, Airedale terrier',\n",
      " 192: 'cairn, cairn terrier',\n",
      " 193: 'Australian terrier',\n",
      " 194: 'Dandie Dinmont, Dandie Dinmont terrier',\n",
      " 195: 'Boston bull, Boston terrier',\n",
      " 196: 'miniature schnauzer',\n",
      " 197: 'giant schnauzer',\n",
      " 198: 'standard schnauzer',\n",
      " 199: 'Scotch terrier, Scottish terrier, Scottie',\n",
      " 200: 'Tibetan terrier, chrysanthemum dog',\n",
      " 201: 'silky terrier, Sydney silky',\n",
      " 202: 'soft-coated wheaten terrier',\n",
      " 203: 'West Highland white terrier',\n",
      " 204: 'Lhasa, Lhasa apso',\n",
      " 205: 'flat-coated retriever',\n",
      " 206: 'curly-coated retriever',\n",
      " 207: 'golden retriever',\n",
      " 208: 'Labrador retriever',\n",
      " 209: 'Chesapeake Bay retriever',\n",
      " 210: 'German short-haired pointer',\n",
      " 211: 'vizsla, Hungarian pointer',\n",
      " 212: 'English setter',\n",
      " 213: 'Irish setter, red setter',\n",
      " 214: 'Gordon setter',\n",
      " 215: 'Brittany spaniel',\n",
      " 216: 'clumber, clumber spaniel',\n",
      " 217: 'English springer, English springer spaniel',\n",
      " 218: 'Welsh springer spaniel',\n",
      " 219: 'cocker spaniel, English cocker spaniel, cocker',\n",
      " 220: 'Sussex spaniel',\n",
      " 221: 'Irish water spaniel',\n",
      " 222: 'kuvasz',\n",
      " 223: 'schipperke',\n",
      " 224: 'groenendael',\n",
      " 225: 'malinois',\n",
      " 226: 'briard',\n",
      " 227: 'kelpie',\n",
      " 228: 'komondor',\n",
      " 229: 'Old English sheepdog, bobtail',\n",
      " 230: 'Shetland sheepdog, Shetland sheep dog, Shetland',\n",
      " 231: 'collie',\n",
      " 232: 'Border collie',\n",
      " 233: 'Bouvier des Flandres, Bouviers des Flandres',\n",
      " 234: 'Rottweiler',\n",
      " 235: 'German shepherd, German shepherd dog, German police dog, alsatian',\n",
      " 236: 'Doberman, Doberman pinscher',\n",
      " 237: 'miniature pinscher',\n",
      " 238: 'Greater Swiss Mountain dog',\n",
      " 239: 'Bernese mountain dog',\n",
      " 240: 'Appenzeller',\n",
      " 241: 'EntleBucher',\n",
      " 242: 'boxer',\n",
      " 243: 'bull mastiff',\n",
      " 244: 'Tibetan mastiff',\n",
      " 245: 'French bulldog',\n",
      " 246: 'Great Dane',\n",
      " 247: 'Saint Bernard, St Bernard',\n",
      " 248: 'Eskimo dog, husky',\n",
      " 249: 'malamute, malemute, Alaskan malamute',\n",
      " 250: 'Siberian husky',\n",
      " 251: 'dalmatian, coach dog, carriage dog',\n",
      " 252: 'affenpinscher, monkey pinscher, monkey dog',\n",
      " 253: 'basenji',\n",
      " 254: 'pug, pug-dog',\n",
      " 255: 'Leonberg',\n",
      " 256: 'Newfoundland, Newfoundland dog',\n",
      " 257: 'Great Pyrenees',\n",
      " 258: 'Samoyed, Samoyede',\n",
      " 259: 'Pomeranian',\n",
      " 260: 'chow, chow chow',\n",
      " 261: 'keeshond',\n",
      " 262: 'Brabancon griffon',\n",
      " 263: 'Pembroke, Pembroke Welsh corgi',\n",
      " 264: 'Cardigan, Cardigan Welsh corgi',\n",
      " 265: 'toy poodle',\n",
      " 266: 'miniature poodle',\n",
      " 267: 'standard poodle',\n",
      " 268: 'Mexican hairless',\n",
      " 269: 'timber wolf, grey wolf, gray wolf, Canis lupus',\n",
      " 270: 'white wolf, Arctic wolf, Canis lupus tundrarum',\n",
      " 271: 'red wolf, maned wolf, Canis rufus, Canis niger',\n",
      " 272: 'coyote, prairie wolf, brush wolf, Canis latrans',\n",
      " 273: 'dingo, warrigal, warragal, Canis dingo',\n",
      " 274: 'dhole, Cuon alpinus',\n",
      " 275: 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus',\n",
      " 276: 'hyena, hyaena',\n",
      " 277: 'red fox, Vulpes vulpes',\n",
      " 278: 'kit fox, Vulpes macrotis',\n",
      " 279: 'Arctic fox, white fox, Alopex lagopus',\n",
      " 280: 'grey fox, gray fox, Urocyon cinereoargenteus',\n",
      " 281: 'tabby, tabby cat',\n",
      " 282: 'tiger cat',\n",
      " 283: 'Persian cat',\n",
      " 284: 'Siamese cat, Siamese',\n",
      " 285: 'Egyptian cat',\n",
      " 286: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n",
      " 287: 'lynx, catamount',\n",
      " 288: 'leopard, Panthera pardus',\n",
      " 289: 'snow leopard, ounce, Panthera uncia',\n",
      " 290: 'jaguar, panther, Panthera onca, Felis onca',\n",
      " 291: 'lion, king of beasts, Panthera leo',\n",
      " 292: 'tiger, Panthera tigris',\n",
      " 293: 'cheetah, chetah, Acinonyx jubatus',\n",
      " 294: 'brown bear, bruin, Ursus arctos',\n",
      " 295: 'American black bear, black bear, Ursus americanus, Euarctos americanus',\n",
      " 296: 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus',\n",
      " 297: 'sloth bear, Melursus ursinus, Ursus ursinus',\n",
      " 298: 'mongoose',\n",
      " 299: 'meerkat, mierkat',\n",
      " 300: 'tiger beetle',\n",
      " 301: 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle',\n",
      " 302: 'ground beetle, carabid beetle',\n",
      " 303: 'long-horned beetle, longicorn, longicorn beetle',\n",
      " 304: 'leaf beetle, chrysomelid',\n",
      " 305: 'dung beetle',\n",
      " 306: 'rhinoceros beetle',\n",
      " 307: 'weevil',\n",
      " 308: 'fly',\n",
      " 309: 'bee',\n",
      " 310: 'ant, emmet, pismire',\n",
      " 311: 'grasshopper, hopper',\n",
      " 312: 'cricket',\n",
      " 313: 'walking stick, walkingstick, stick insect',\n",
      " 314: 'cockroach, roach',\n",
      " 315: 'mantis, mantid',\n",
      " 316: 'cicada, cicala',\n",
      " 317: 'leafhopper',\n",
      " 318: 'lacewing, lacewing fly',\n",
      " 319: \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\",\n",
      " 320: 'damselfly',\n",
      " 321: 'admiral',\n",
      " 322: 'ringlet, ringlet butterfly',\n",
      " 323: 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus',\n",
      " 324: 'cabbage butterfly',\n",
      " 325: 'sulphur butterfly, sulfur butterfly',\n",
      " 326: 'lycaenid, lycaenid butterfly',\n",
      " 327: 'starfish, sea star',\n",
      " 328: 'sea urchin',\n",
      " 329: 'sea cucumber, holothurian',\n",
      " 330: 'wood rabbit, cottontail, cottontail rabbit',\n",
      " 331: 'hare',\n",
      " 332: 'Angora, Angora rabbit',\n",
      " 333: 'hamster',\n",
      " 334: 'porcupine, hedgehog',\n",
      " 335: 'fox squirrel, eastern fox squirrel, Sciurus niger',\n",
      " 336: 'marmot',\n",
      " 337: 'beaver',\n",
      " 338: 'guinea pig, Cavia cobaya',\n",
      " 339: 'sorrel',\n",
      " 340: 'zebra',\n",
      " 341: 'hog, pig, grunter, squealer, Sus scrofa',\n",
      " 342: 'wild boar, boar, Sus scrofa',\n",
      " 343: 'warthog',\n",
      " 344: 'hippopotamus, hippo, river horse, Hippopotamus amphibius',\n",
      " 345: 'ox',\n",
      " 346: 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis',\n",
      " 347: 'bison',\n",
      " 348: 'ram, tup',\n",
      " 349: 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis',\n",
      " 350: 'ibex, Capra ibex',\n",
      " 351: 'hartebeest',\n",
      " 352: 'impala, Aepyceros melampus',\n",
      " 353: 'gazelle',\n",
      " 354: 'Arabian camel, dromedary, Camelus dromedarius',\n",
      " 355: 'llama',\n",
      " 356: 'weasel',\n",
      " 357: 'mink',\n",
      " 358: 'polecat, fitch, foulmart, foumart, Mustela putorius',\n",
      " 359: 'black-footed ferret, ferret, Mustela nigripes',\n",
      " 360: 'otter',\n",
      " 361: 'skunk, polecat, wood pussy',\n",
      " 362: 'badger',\n",
      " 363: 'armadillo',\n",
      " 364: 'three-toed sloth, ai, Bradypus tridactylus',\n",
      " 365: 'orangutan, orang, orangutang, Pongo pygmaeus',\n",
      " 366: 'gorilla, Gorilla gorilla',\n",
      " 367: 'chimpanzee, chimp, Pan troglodytes',\n",
      " 368: 'gibbon, Hylobates lar',\n",
      " 369: 'siamang, Hylobates syndactylus, Symphalangus syndactylus',\n",
      " 370: 'guenon, guenon monkey',\n",
      " 371: 'patas, hussar monkey, Erythrocebus patas',\n",
      " 372: 'baboon',\n",
      " 373: 'macaque',\n",
      " 374: 'langur',\n",
      " 375: 'colobus, colobus monkey',\n",
      " 376: 'proboscis monkey, Nasalis larvatus',\n",
      " 377: 'marmoset',\n",
      " 378: 'capuchin, ringtail, Cebus capucinus',\n",
      " 379: 'howler monkey, howler',\n",
      " 380: 'titi, titi monkey',\n",
      " 381: 'spider monkey, Ateles geoffroyi',\n",
      " 382: 'squirrel monkey, Saimiri sciureus',\n",
      " 383: 'Madagascar cat, ring-tailed lemur, Lemur catta',\n",
      " 384: 'indri, indris, Indri indri, Indri brevicaudatus',\n",
      " 385: 'Indian elephant, Elephas maximus',\n",
      " 386: 'African elephant, Loxodonta africana',\n",
      " 387: 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens',\n",
      " 388: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca',\n",
      " 389: 'barracouta, snoek',\n",
      " 390: 'eel',\n",
      " 391: 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch',\n",
      " 392: 'rock beauty, Holocanthus tricolor',\n",
      " 393: 'anemone fish',\n",
      " 394: 'sturgeon',\n",
      " 395: 'gar, garfish, garpike, billfish, Lepisosteus osseus',\n",
      " 396: 'lionfish',\n",
      " 397: 'puffer, pufferfish, blowfish, globefish',\n",
      " 398: 'abacus',\n",
      " 399: 'abaya',\n",
      " 400: \"academic gown, academic robe, judge's robe\",\n",
      " 401: 'accordion, piano accordion, squeeze box',\n",
      " 402: 'acoustic guitar',\n",
      " 403: 'aircraft carrier, carrier, flattop, attack aircraft carrier',\n",
      " 404: 'airliner',\n",
      " 405: 'airship, dirigible',\n",
      " 406: 'altar',\n",
      " 407: 'ambulance',\n",
      " 408: 'amphibian, amphibious vehicle',\n",
      " 409: 'analog clock',\n",
      " 410: 'apiary, bee house',\n",
      " 411: 'apron',\n",
      " 412: 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin',\n",
      " 413: 'assault rifle, assault gun',\n",
      " 414: 'backpack, back pack, knapsack, packsack, rucksack, haversack',\n",
      " 415: 'bakery, bakeshop, bakehouse',\n",
      " 416: 'balance beam, beam',\n",
      " 417: 'balloon',\n",
      " 418: 'ballpoint, ballpoint pen, ballpen, Biro',\n",
      " 419: 'Band Aid',\n",
      " 420: 'banjo',\n",
      " 421: 'bannister, banister, balustrade, balusters, handrail',\n",
      " 422: 'barbell',\n",
      " 423: 'barber chair',\n",
      " 424: 'barbershop',\n",
      " 425: 'barn',\n",
      " 426: 'barometer',\n",
      " 427: 'barrel, cask',\n",
      " 428: 'barrow, garden cart, lawn cart, wheelbarrow',\n",
      " 429: 'baseball',\n",
      " 430: 'basketball',\n",
      " 431: 'bassinet',\n",
      " 432: 'bassoon',\n",
      " 433: 'bathing cap, swimming cap',\n",
      " 434: 'bath towel',\n",
      " 435: 'bathtub, bathing tub, bath, tub',\n",
      " 436: 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon',\n",
      " 437: 'beacon, lighthouse, beacon light, pharos',\n",
      " 438: 'beaker',\n",
      " 439: 'bearskin, busby, shako',\n",
      " 440: 'beer bottle',\n",
      " 441: 'beer glass',\n",
      " 442: 'bell cote, bell cot',\n",
      " 443: 'bib',\n",
      " 444: 'bicycle-built-for-two, tandem bicycle, tandem',\n",
      " 445: 'bikini, two-piece',\n",
      " 446: 'binder, ring-binder',\n",
      " 447: 'binoculars, field glasses, opera glasses',\n",
      " 448: 'birdhouse',\n",
      " 449: 'boathouse',\n",
      " 450: 'bobsled, bobsleigh, bob',\n",
      " 451: 'bolo tie, bolo, bola tie, bola',\n",
      " 452: 'bonnet, poke bonnet',\n",
      " 453: 'bookcase',\n",
      " 454: 'bookshop, bookstore, bookstall',\n",
      " 455: 'bottlecap',\n",
      " 456: 'bow',\n",
      " 457: 'bow tie, bow-tie, bowtie',\n",
      " 458: 'brass, memorial tablet, plaque',\n",
      " 459: 'brassiere, bra, bandeau',\n",
      " 460: 'breakwater, groin, groyne, mole, bulwark, seawall, jetty',\n",
      " 461: 'breastplate, aegis, egis',\n",
      " 462: 'broom',\n",
      " 463: 'bucket, pail',\n",
      " 464: 'buckle',\n",
      " 465: 'bulletproof vest',\n",
      " 466: 'bullet train, bullet',\n",
      " 467: 'butcher shop, meat market',\n",
      " 468: 'cab, hack, taxi, taxicab',\n",
      " 469: 'caldron, cauldron',\n",
      " 470: 'candle, taper, wax light',\n",
      " 471: 'cannon',\n",
      " 472: 'canoe',\n",
      " 473: 'can opener, tin opener',\n",
      " 474: 'cardigan',\n",
      " 475: 'car mirror',\n",
      " 476: 'carousel, carrousel, merry-go-round, roundabout, whirligig',\n",
      " 477: \"carpenter's kit, tool kit\",\n",
      " 478: 'carton',\n",
      " 479: 'car wheel',\n",
      " 480: 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM',\n",
      " 481: 'cassette',\n",
      " 482: 'cassette player',\n",
      " 483: 'castle',\n",
      " 484: 'catamaran',\n",
      " 485: 'CD player',\n",
      " 486: 'cello, violoncello',\n",
      " 487: 'cellular telephone, cellular phone, cellphone, cell, mobile phone',\n",
      " 488: 'chain',\n",
      " 489: 'chainlink fence',\n",
      " 490: 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour',\n",
      " 491: 'chain saw, chainsaw',\n",
      " 492: 'chest',\n",
      " 493: 'chiffonier, commode',\n",
      " 494: 'chime, bell, gong',\n",
      " 495: 'china cabinet, china closet',\n",
      " 496: 'Christmas stocking',\n",
      " 497: 'church, church building',\n",
      " 498: 'cinema, movie theater, movie theatre, movie house, picture palace',\n",
      " 499: 'cleaver, meat cleaver, chopper',\n",
      " 500: 'cliff dwelling',\n",
      " 501: 'cloak',\n",
      " 502: 'clog, geta, patten, sabot',\n",
      " 503: 'cocktail shaker',\n",
      " 504: 'coffee mug',\n",
      " 505: 'coffeepot',\n",
      " 506: 'coil, spiral, volute, whorl, helix',\n",
      " 507: 'combination lock',\n",
      " 508: 'computer keyboard, keypad',\n",
      " 509: 'confectionery, confectionary, candy store',\n",
      " 510: 'container ship, containership, container vessel',\n",
      " 511: 'convertible',\n",
      " 512: 'corkscrew, bottle screw',\n",
      " 513: 'cornet, horn, trumpet, trump',\n",
      " 514: 'cowboy boot',\n",
      " 515: 'cowboy hat, ten-gallon hat',\n",
      " 516: 'cradle',\n",
      " 517: 'crane',\n",
      " 518: 'crash helmet',\n",
      " 519: 'crate',\n",
      " 520: 'crib, cot',\n",
      " 521: 'Crock Pot',\n",
      " 522: 'croquet ball',\n",
      " 523: 'crutch',\n",
      " 524: 'cuirass',\n",
      " 525: 'dam, dike, dyke',\n",
      " 526: 'desk',\n",
      " 527: 'desktop computer',\n",
      " 528: 'dial telephone, dial phone',\n",
      " 529: 'diaper, nappy, napkin',\n",
      " 530: 'digital clock',\n",
      " 531: 'digital watch',\n",
      " 532: 'dining table, board',\n",
      " 533: 'dishrag, dishcloth',\n",
      " 534: 'dishwasher, dish washer, dishwashing machine',\n",
      " 535: 'disk brake, disc brake',\n",
      " 536: 'dock, dockage, docking facility',\n",
      " 537: 'dogsled, dog sled, dog sleigh',\n",
      " 538: 'dome',\n",
      " 539: 'doormat, welcome mat',\n",
      " 540: 'drilling platform, offshore rig',\n",
      " 541: 'drum, membranophone, tympan',\n",
      " 542: 'drumstick',\n",
      " 543: 'dumbbell',\n",
      " 544: 'Dutch oven',\n",
      " 545: 'electric fan, blower',\n",
      " 546: 'electric guitar',\n",
      " 547: 'electric locomotive',\n",
      " 548: 'entertainment center',\n",
      " 549: 'envelope',\n",
      " 550: 'espresso maker',\n",
      " 551: 'face powder',\n",
      " 552: 'feather boa, boa',\n",
      " 553: 'file, file cabinet, filing cabinet',\n",
      " 554: 'fireboat',\n",
      " 555: 'fire engine, fire truck',\n",
      " 556: 'fire screen, fireguard',\n",
      " 557: 'flagpole, flagstaff',\n",
      " 558: 'flute, transverse flute',\n",
      " 559: 'folding chair',\n",
      " 560: 'football helmet',\n",
      " 561: 'forklift',\n",
      " 562: 'fountain',\n",
      " 563: 'fountain pen',\n",
      " 564: 'four-poster',\n",
      " 565: 'freight car',\n",
      " 566: 'French horn, horn',\n",
      " 567: 'frying pan, frypan, skillet',\n",
      " 568: 'fur coat',\n",
      " 569: 'garbage truck, dustcart',\n",
      " 570: 'gasmask, respirator, gas helmet',\n",
      " 571: 'gas pump, gasoline pump, petrol pump, island dispenser',\n",
      " 572: 'goblet',\n",
      " 573: 'go-kart',\n",
      " 574: 'golf ball',\n",
      " 575: 'golfcart, golf cart',\n",
      " 576: 'gondola',\n",
      " 577: 'gong, tam-tam',\n",
      " 578: 'gown',\n",
      " 579: 'grand piano, grand',\n",
      " 580: 'greenhouse, nursery, glasshouse',\n",
      " 581: 'grille, radiator grille',\n",
      " 582: 'grocery store, grocery, food market, market',\n",
      " 583: 'guillotine',\n",
      " 584: 'hair slide',\n",
      " 585: 'hair spray',\n",
      " 586: 'half track',\n",
      " 587: 'hammer',\n",
      " 588: 'hamper',\n",
      " 589: 'hand blower, blow dryer, blow drier, hair dryer, hair drier',\n",
      " 590: 'hand-held computer, hand-held microcomputer',\n",
      " 591: 'handkerchief, hankie, hanky, hankey',\n",
      " 592: 'hard disc, hard disk, fixed disk',\n",
      " 593: 'harmonica, mouth organ, harp, mouth harp',\n",
      " 594: 'harp',\n",
      " 595: 'harvester, reaper',\n",
      " 596: 'hatchet',\n",
      " 597: 'holster',\n",
      " 598: 'home theater, home theatre',\n",
      " 599: 'honeycomb',\n",
      " 600: 'hook, claw',\n",
      " 601: 'hoopskirt, crinoline',\n",
      " 602: 'horizontal bar, high bar',\n",
      " 603: 'horse cart, horse-cart',\n",
      " 604: 'hourglass',\n",
      " 605: 'iPod',\n",
      " 606: 'iron, smoothing iron',\n",
      " 607: \"jack-o'-lantern\",\n",
      " 608: 'jean, blue jean, denim',\n",
      " 609: 'jeep, landrover',\n",
      " 610: 'jersey, T-shirt, tee shirt',\n",
      " 611: 'jigsaw puzzle',\n",
      " 612: 'jinrikisha, ricksha, rickshaw',\n",
      " 613: 'joystick',\n",
      " 614: 'kimono',\n",
      " 615: 'knee pad',\n",
      " 616: 'knot',\n",
      " 617: 'lab coat, laboratory coat',\n",
      " 618: 'ladle',\n",
      " 619: 'lampshade, lamp shade',\n",
      " 620: 'laptop, laptop computer',\n",
      " 621: 'lawn mower, mower',\n",
      " 622: 'lens cap, lens cover',\n",
      " 623: 'letter opener, paper knife, paperknife',\n",
      " 624: 'library',\n",
      " 625: 'lifeboat',\n",
      " 626: 'lighter, light, igniter, ignitor',\n",
      " 627: 'limousine, limo',\n",
      " 628: 'liner, ocean liner',\n",
      " 629: 'lipstick, lip rouge',\n",
      " 630: 'Loafer',\n",
      " 631: 'lotion',\n",
      " 632: 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system',\n",
      " 633: \"loupe, jeweler's loupe\",\n",
      " 634: 'lumbermill, sawmill',\n",
      " 635: 'magnetic compass',\n",
      " 636: 'mailbag, postbag',\n",
      " 637: 'mailbox, letter box',\n",
      " 638: 'maillot',\n",
      " 639: 'maillot, tank suit',\n",
      " 640: 'manhole cover',\n",
      " 641: 'maraca',\n",
      " 642: 'marimba, xylophone',\n",
      " 643: 'mask',\n",
      " 644: 'matchstick',\n",
      " 645: 'maypole',\n",
      " 646: 'maze, labyrinth',\n",
      " 647: 'measuring cup',\n",
      " 648: 'medicine chest, medicine cabinet',\n",
      " 649: 'megalith, megalithic structure',\n",
      " 650: 'microphone, mike',\n",
      " 651: 'microwave, microwave oven',\n",
      " 652: 'military uniform',\n",
      " 653: 'milk can',\n",
      " 654: 'minibus',\n",
      " 655: 'miniskirt, mini',\n",
      " 656: 'minivan',\n",
      " 657: 'missile',\n",
      " 658: 'mitten',\n",
      " 659: 'mixing bowl',\n",
      " 660: 'mobile home, manufactured home',\n",
      " 661: 'Model T',\n",
      " 662: 'modem',\n",
      " 663: 'monastery',\n",
      " 664: 'monitor',\n",
      " 665: 'moped',\n",
      " 666: 'mortar',\n",
      " 667: 'mortarboard',\n",
      " 668: 'mosque',\n",
      " 669: 'mosquito net',\n",
      " 670: 'motor scooter, scooter',\n",
      " 671: 'mountain bike, all-terrain bike, off-roader',\n",
      " 672: 'mountain tent',\n",
      " 673: 'mouse, computer mouse',\n",
      " 674: 'mousetrap',\n",
      " 675: 'moving van',\n",
      " 676: 'muzzle',\n",
      " 677: 'nail',\n",
      " 678: 'neck brace',\n",
      " 679: 'necklace',\n",
      " 680: 'nipple',\n",
      " 681: 'notebook, notebook computer',\n",
      " 682: 'obelisk',\n",
      " 683: 'oboe, hautboy, hautbois',\n",
      " 684: 'ocarina, sweet potato',\n",
      " 685: 'odometer, hodometer, mileometer, milometer',\n",
      " 686: 'oil filter',\n",
      " 687: 'organ, pipe organ',\n",
      " 688: 'oscilloscope, scope, cathode-ray oscilloscope, CRO',\n",
      " 689: 'overskirt',\n",
      " 690: 'oxcart',\n",
      " 691: 'oxygen mask',\n",
      " 692: 'packet',\n",
      " 693: 'paddle, boat paddle',\n",
      " 694: 'paddlewheel, paddle wheel',\n",
      " 695: 'padlock',\n",
      " 696: 'paintbrush',\n",
      " 697: \"pajama, pyjama, pj's, jammies\",\n",
      " 698: 'palace',\n",
      " 699: 'panpipe, pandean pipe, syrinx',\n",
      " 700: 'paper towel',\n",
      " 701: 'parachute, chute',\n",
      " 702: 'parallel bars, bars',\n",
      " 703: 'park bench',\n",
      " 704: 'parking meter',\n",
      " 705: 'passenger car, coach, carriage',\n",
      " 706: 'patio, terrace',\n",
      " 707: 'pay-phone, pay-station',\n",
      " 708: 'pedestal, plinth, footstall',\n",
      " 709: 'pencil box, pencil case',\n",
      " 710: 'pencil sharpener',\n",
      " 711: 'perfume, essence',\n",
      " 712: 'Petri dish',\n",
      " 713: 'photocopier',\n",
      " 714: 'pick, plectrum, plectron',\n",
      " 715: 'pickelhaube',\n",
      " 716: 'picket fence, paling',\n",
      " 717: 'pickup, pickup truck',\n",
      " 718: 'pier',\n",
      " 719: 'piggy bank, penny bank',\n",
      " 720: 'pill bottle',\n",
      " 721: 'pillow',\n",
      " 722: 'ping-pong ball',\n",
      " 723: 'pinwheel',\n",
      " 724: 'pirate, pirate ship',\n",
      " 725: 'pitcher, ewer',\n",
      " 726: \"plane, carpenter's plane, woodworking plane\",\n",
      " 727: 'planetarium',\n",
      " 728: 'plastic bag',\n",
      " 729: 'plate rack',\n",
      " 730: 'plow, plough',\n",
      " 731: \"plunger, plumber's helper\",\n",
      " 732: 'Polaroid camera, Polaroid Land camera',\n",
      " 733: 'pole',\n",
      " 734: 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria',\n",
      " 735: 'poncho',\n",
      " 736: 'pool table, billiard table, snooker table',\n",
      " 737: 'pop bottle, soda bottle',\n",
      " 738: 'pot, flowerpot',\n",
      " 739: \"potter's wheel\",\n",
      " 740: 'power drill',\n",
      " 741: 'prayer rug, prayer mat',\n",
      " 742: 'printer',\n",
      " 743: 'prison, prison house',\n",
      " 744: 'projectile, missile',\n",
      " 745: 'projector',\n",
      " 746: 'puck, hockey puck',\n",
      " 747: 'punching bag, punch bag, punching ball, punchball',\n",
      " 748: 'purse',\n",
      " 749: 'quill, quill pen',\n",
      " 750: 'quilt, comforter, comfort, puff',\n",
      " 751: 'racer, race car, racing car',\n",
      " 752: 'racket, racquet',\n",
      " 753: 'radiator',\n",
      " 754: 'radio, wireless',\n",
      " 755: 'radio telescope, radio reflector',\n",
      " 756: 'rain barrel',\n",
      " 757: 'recreational vehicle, RV, R.V.',\n",
      " 758: 'reel',\n",
      " 759: 'reflex camera',\n",
      " 760: 'refrigerator, icebox',\n",
      " 761: 'remote control, remote',\n",
      " 762: 'restaurant, eating house, eating place, eatery',\n",
      " 763: 'revolver, six-gun, six-shooter',\n",
      " 764: 'rifle',\n",
      " 765: 'rocking chair, rocker',\n",
      " 766: 'rotisserie',\n",
      " 767: 'rubber eraser, rubber, pencil eraser',\n",
      " 768: 'rugby ball',\n",
      " 769: 'rule, ruler',\n",
      " 770: 'running shoe',\n",
      " 771: 'safe',\n",
      " 772: 'safety pin',\n",
      " 773: 'saltshaker, salt shaker',\n",
      " 774: 'sandal',\n",
      " 775: 'sarong',\n",
      " 776: 'sax, saxophone',\n",
      " 777: 'scabbard',\n",
      " 778: 'scale, weighing machine',\n",
      " 779: 'school bus',\n",
      " 780: 'schooner',\n",
      " 781: 'scoreboard',\n",
      " 782: 'screen, CRT screen',\n",
      " 783: 'screw',\n",
      " 784: 'screwdriver',\n",
      " 785: 'seat belt, seatbelt',\n",
      " 786: 'sewing machine',\n",
      " 787: 'shield, buckler',\n",
      " 788: 'shoe shop, shoe-shop, shoe store',\n",
      " 789: 'shoji',\n",
      " 790: 'shopping basket',\n",
      " 791: 'shopping cart',\n",
      " 792: 'shovel',\n",
      " 793: 'shower cap',\n",
      " 794: 'shower curtain',\n",
      " 795: 'ski',\n",
      " 796: 'ski mask',\n",
      " 797: 'sleeping bag',\n",
      " 798: 'slide rule, slipstick',\n",
      " 799: 'sliding door',\n",
      " 800: 'slot, one-armed bandit',\n",
      " 801: 'snorkel',\n",
      " 802: 'snowmobile',\n",
      " 803: 'snowplow, snowplough',\n",
      " 804: 'soap dispenser',\n",
      " 805: 'soccer ball',\n",
      " 806: 'sock',\n",
      " 807: 'solar dish, solar collector, solar furnace',\n",
      " 808: 'sombrero',\n",
      " 809: 'soup bowl',\n",
      " 810: 'space bar',\n",
      " 811: 'space heater',\n",
      " 812: 'space shuttle',\n",
      " 813: 'spatula',\n",
      " 814: 'speedboat',\n",
      " 815: \"spider web, spider's web\",\n",
      " 816: 'spindle',\n",
      " 817: 'sports car, sport car',\n",
      " 818: 'spotlight, spot',\n",
      " 819: 'stage',\n",
      " 820: 'steam locomotive',\n",
      " 821: 'steel arch bridge',\n",
      " 822: 'steel drum',\n",
      " 823: 'stethoscope',\n",
      " 824: 'stole',\n",
      " 825: 'stone wall',\n",
      " 826: 'stopwatch, stop watch',\n",
      " 827: 'stove',\n",
      " 828: 'strainer',\n",
      " 829: 'streetcar, tram, tramcar, trolley, trolley car',\n",
      " 830: 'stretcher',\n",
      " 831: 'studio couch, day bed',\n",
      " 832: 'stupa, tope',\n",
      " 833: 'submarine, pigboat, sub, U-boat',\n",
      " 834: 'suit, suit of clothes',\n",
      " 835: 'sundial',\n",
      " 836: 'sunglass',\n",
      " 837: 'sunglasses, dark glasses, shades',\n",
      " 838: 'sunscreen, sunblock, sun blocker',\n",
      " 839: 'suspension bridge',\n",
      " 840: 'swab, swob, mop',\n",
      " 841: 'sweatshirt',\n",
      " 842: 'swimming trunks, bathing trunks',\n",
      " 843: 'swing',\n",
      " 844: 'switch, electric switch, electrical switch',\n",
      " 845: 'syringe',\n",
      " 846: 'table lamp',\n",
      " 847: 'tank, army tank, armored combat vehicle, armoured combat vehicle',\n",
      " 848: 'tape player',\n",
      " 849: 'teapot',\n",
      " 850: 'teddy, teddy bear',\n",
      " 851: 'television, television system',\n",
      " 852: 'tennis ball',\n",
      " 853: 'thatch, thatched roof',\n",
      " 854: 'theater curtain, theatre curtain',\n",
      " 855: 'thimble',\n",
      " 856: 'thresher, thrasher, threshing machine',\n",
      " 857: 'throne',\n",
      " 858: 'tile roof',\n",
      " 859: 'toaster',\n",
      " 860: 'tobacco shop, tobacconist shop, tobacconist',\n",
      " 861: 'toilet seat',\n",
      " 862: 'torch',\n",
      " 863: 'totem pole',\n",
      " 864: 'tow truck, tow car, wrecker',\n",
      " 865: 'toyshop',\n",
      " 866: 'tractor',\n",
      " 867: 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi',\n",
      " 868: 'tray',\n",
      " 869: 'trench coat',\n",
      " 870: 'tricycle, trike, velocipede',\n",
      " 871: 'trimaran',\n",
      " 872: 'tripod',\n",
      " 873: 'triumphal arch',\n",
      " 874: 'trolleybus, trolley coach, trackless trolley',\n",
      " 875: 'trombone',\n",
      " 876: 'tub, vat',\n",
      " 877: 'turnstile',\n",
      " 878: 'typewriter keyboard',\n",
      " 879: 'umbrella',\n",
      " 880: 'unicycle, monocycle',\n",
      " 881: 'upright, upright piano',\n",
      " 882: 'vacuum, vacuum cleaner',\n",
      " 883: 'vase',\n",
      " 884: 'vault',\n",
      " 885: 'velvet',\n",
      " 886: 'vending machine',\n",
      " 887: 'vestment',\n",
      " 888: 'viaduct',\n",
      " 889: 'violin, fiddle',\n",
      " 890: 'volleyball',\n",
      " 891: 'waffle iron',\n",
      " 892: 'wall clock',\n",
      " 893: 'wallet, billfold, notecase, pocketbook',\n",
      " 894: 'wardrobe, closet, press',\n",
      " 895: 'warplane, military plane',\n",
      " 896: 'washbasin, handbasin, washbowl, lavabo, wash-hand basin',\n",
      " 897: 'washer, automatic washer, washing machine',\n",
      " 898: 'water bottle',\n",
      " 899: 'water jug',\n",
      " 900: 'water tower',\n",
      " 901: 'whiskey jug',\n",
      " 902: 'whistle',\n",
      " 903: 'wig',\n",
      " 904: 'window screen',\n",
      " 905: 'window shade',\n",
      " 906: 'Windsor tie',\n",
      " 907: 'wine bottle',\n",
      " 908: 'wing',\n",
      " 909: 'wok',\n",
      " 910: 'wooden spoon',\n",
      " 911: 'wool, woolen, woollen',\n",
      " 912: 'worm fence, snake fence, snake-rail fence, Virginia fence',\n",
      " 913: 'wreck',\n",
      " 914: 'yawl',\n",
      " 915: 'yurt',\n",
      " 916: 'web site, website, internet site, site',\n",
      " 917: 'comic book',\n",
      " 918: 'crossword puzzle, crossword',\n",
      " 919: 'street sign',\n",
      " 920: 'traffic light, traffic signal, stoplight',\n",
      " 921: 'book jacket, dust cover, dust jacket, dust wrapper',\n",
      " 922: 'menu',\n",
      " 923: 'plate',\n",
      " 924: 'guacamole',\n",
      " 925: 'consomme',\n",
      " 926: 'hot pot, hotpot',\n",
      " 927: 'trifle',\n",
      " 928: 'ice cream, icecream',\n",
      " 929: 'ice lolly, lolly, lollipop, popsicle',\n",
      " 930: 'French loaf',\n",
      " 931: 'bagel, beigel',\n",
      " 932: 'pretzel',\n",
      " 933: 'cheeseburger',\n",
      " 934: 'hotdog, hot dog, red hot',\n",
      " 935: 'mashed potato',\n",
      " 936: 'head cabbage',\n",
      " 937: 'broccoli',\n",
      " 938: 'cauliflower',\n",
      " 939: 'zucchini, courgette',\n",
      " 940: 'spaghetti squash',\n",
      " 941: 'acorn squash',\n",
      " 942: 'butternut squash',\n",
      " 943: 'cucumber, cuke',\n",
      " 944: 'artichoke, globe artichoke',\n",
      " 945: 'bell pepper',\n",
      " 946: 'cardoon',\n",
      " 947: 'mushroom',\n",
      " 948: 'Granny Smith',\n",
      " 949: 'strawberry',\n",
      " 950: 'orange',\n",
      " 951: 'lemon',\n",
      " 952: 'fig',\n",
      " 953: 'pineapple, ananas',\n",
      " 954: 'banana',\n",
      " 955: 'jackfruit, jak, jack',\n",
      " 956: 'custard apple',\n",
      " 957: 'pomegranate',\n",
      " 958: 'hay',\n",
      " 959: 'carbonara',\n",
      " 960: 'chocolate sauce, chocolate syrup',\n",
      " 961: 'dough',\n",
      " 962: 'meat loaf, meatloaf',\n",
      " 963: 'pizza, pizza pie',\n",
      " 964: 'potpie',\n",
      " 965: 'burrito',\n",
      " 966: 'red wine',\n",
      " 967: 'espresso',\n",
      " 968: 'cup',\n",
      " 969: 'eggnog',\n",
      " 970: 'alp',\n",
      " 971: 'bubble',\n",
      " 972: 'cliff, drop, drop-off',\n",
      " 973: 'coral reef',\n",
      " 974: 'geyser',\n",
      " 975: 'lakeside, lakeshore',\n",
      " 976: 'promontory, headland, head, foreland',\n",
      " 977: 'sandbar, sand bar',\n",
      " 978: 'seashore, coast, seacoast, sea-coast',\n",
      " 979: 'valley, vale',\n",
      " 980: 'volcano',\n",
      " 981: 'ballplayer, baseball player',\n",
      " 982: 'groom, bridegroom',\n",
      " 983: 'scuba diver',\n",
      " 984: 'rapeseed',\n",
      " 985: 'daisy',\n",
      " 986: \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\",\n",
      " 987: 'corn',\n",
      " 988: 'acorn',\n",
      " 989: 'hip, rose hip, rosehip',\n",
      " 990: 'buckeye, horse chestnut, conker',\n",
      " 991: 'coral fungus',\n",
      " 992: 'agaric',\n",
      " 993: 'gyromitra',\n",
      " 994: 'stinkhorn, carrion fungus',\n",
      " 995: 'earthstar',\n",
      " 996: 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa',\n",
      " 997: 'bolete',\n",
      " 998: 'ear, spike, capitulum',\n",
      " 999: 'toilet tissue, toilet paper, bathroom tissue'}\n",
      "68/2:\n",
      "from tensorflow.keras.applications import EfficientNetB4\n",
      "from tensorflow.image import resize\n",
      "model = EfficientNetB4(weights='imagenet')\n",
      "\n",
      "def center_crop(img, new_width=None, new_height=None):        \n",
      "\n",
      "    width = img.shape[1]\n",
      "    height = img.shape[0]\n",
      "\n",
      "    if new_width is None:\n",
      "        new_width = min(width, height)\n",
      "\n",
      "    if new_height is None:\n",
      "        new_height = min(width, height)\n",
      "\n",
      "    left = int(np.ceil((width - new_width) / 2))\n",
      "    right = width - int(np.floor((width - new_width) / 2))\n",
      "\n",
      "    top = int(np.ceil((height - new_height) / 2))\n",
      "    bottom = height - int(np.floor((height - new_height) / 2))\n",
      "\n",
      "    if len(img.shape) == 2:\n",
      "        center_cropped_img = img[top:bottom, left:right]\n",
      "    else:\n",
      "        center_cropped_img = img[top:bottom, left:right, ...]\n",
      "\n",
      "    return center_cropped_img\n",
      "\n",
      "class_dict = {0: 'tench, Tinca tinca',\n",
      " 1: 'goldfish, Carassius auratus',\n",
      " 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n",
      " 3: 'tiger shark, Galeocerdo cuvieri',\n",
      " 4: 'hammerhead, hammerhead shark',\n",
      " 5: 'electric ray, crampfish, numbfish, torpedo',\n",
      " 6: 'stingray',\n",
      " 7: 'cock',\n",
      " 8: 'hen',\n",
      " 9: 'ostrich, Struthio camelus',\n",
      " 10: 'brambling, Fringilla montifringilla',\n",
      " 11: 'goldfinch, Carduelis carduelis',\n",
      " 12: 'house finch, linnet, Carpodacus mexicanus',\n",
      " 13: 'junco, snowbird',\n",
      " 14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea',\n",
      " 15: 'robin, American robin, Turdus migratorius',\n",
      " 16: 'bulbul',\n",
      " 17: 'jay',\n",
      " 18: 'magpie',\n",
      " 19: 'chickadee',\n",
      " 20: 'water ouzel, dipper',\n",
      " 21: 'kite',\n",
      " 22: 'bald eagle, American eagle, Haliaeetus leucocephalus',\n",
      " 23: 'vulture',\n",
      " 24: 'great grey owl, great gray owl, Strix nebulosa',\n",
      " 25: 'European fire salamander, Salamandra salamandra',\n",
      " 26: 'common newt, Triturus vulgaris',\n",
      " 27: 'eft',\n",
      " 28: 'spotted salamander, Ambystoma maculatum',\n",
      " 29: 'axolotl, mud puppy, Ambystoma mexicanum',\n",
      " 30: 'bullfrog, Rana catesbeiana',\n",
      " 31: 'tree frog, tree-frog',\n",
      " 32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui',\n",
      " 33: 'loggerhead, loggerhead turtle, Caretta caretta',\n",
      " 34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea',\n",
      " 35: 'mud turtle',\n",
      " 36: 'terrapin',\n",
      " 37: 'box turtle, box tortoise',\n",
      " 38: 'banded gecko',\n",
      " 39: 'common iguana, iguana, Iguana iguana',\n",
      " 40: 'American chameleon, anole, Anolis carolinensis',\n",
      " 41: 'whiptail, whiptail lizard',\n",
      " 42: 'agama',\n",
      " 43: 'frilled lizard, Chlamydosaurus kingi',\n",
      " 44: 'alligator lizard',\n",
      " 45: 'Gila monster, Heloderma suspectum',\n",
      " 46: 'green lizard, Lacerta viridis',\n",
      " 47: 'African chameleon, Chamaeleo chamaeleon',\n",
      " 48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis',\n",
      " 49: 'African crocodile, Nile crocodile, Crocodylus niloticus',\n",
      " 50: 'American alligator, Alligator mississipiensis',\n",
      " 51: 'triceratops',\n",
      " 52: 'thunder snake, worm snake, Carphophis amoenus',\n",
      " 53: 'ringneck snake, ring-necked snake, ring snake',\n",
      " 54: 'hognose snake, puff adder, sand viper',\n",
      " 55: 'green snake, grass snake',\n",
      " 56: 'king snake, kingsnake',\n",
      " 57: 'garter snake, grass snake',\n",
      " 58: 'water snake',\n",
      " 59: 'vine snake',\n",
      " 60: 'night snake, Hypsiglena torquata',\n",
      " 61: 'boa constrictor, Constrictor constrictor',\n",
      " 62: 'rock python, rock snake, Python sebae',\n",
      " 63: 'Indian cobra, Naja naja',\n",
      " 64: 'green mamba',\n",
      " 65: 'sea snake',\n",
      " 66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus',\n",
      " 67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus',\n",
      " 68: 'sidewinder, horned rattlesnake, Crotalus cerastes',\n",
      " 69: 'trilobite',\n",
      " 70: 'harvestman, daddy longlegs, Phalangium opilio',\n",
      " 71: 'scorpion',\n",
      " 72: 'black and gold garden spider, Argiope aurantia',\n",
      " 73: 'barn spider, Araneus cavaticus',\n",
      " 74: 'garden spider, Aranea diademata',\n",
      " 75: 'black widow, Latrodectus mactans',\n",
      " 76: 'tarantula',\n",
      " 77: 'wolf spider, hunting spider',\n",
      " 78: 'tick',\n",
      " 79: 'centipede',\n",
      " 80: 'black grouse',\n",
      " 81: 'ptarmigan',\n",
      " 82: 'ruffed grouse, partridge, Bonasa umbellus',\n",
      " 83: 'prairie chicken, prairie grouse, prairie fowl',\n",
      " 84: 'peacock',\n",
      " 85: 'quail',\n",
      " 86: 'partridge',\n",
      " 87: 'African grey, African gray, Psittacus erithacus',\n",
      " 88: 'macaw',\n",
      " 89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n",
      " 90: 'lorikeet',\n",
      " 91: 'coucal',\n",
      " 92: 'bee eater',\n",
      " 93: 'hornbill',\n",
      " 94: 'hummingbird',\n",
      " 95: 'jacamar',\n",
      " 96: 'toucan',\n",
      " 97: 'drake',\n",
      " 98: 'red-breasted merganser, Mergus serrator',\n",
      " 99: 'goose',\n",
      " 100: 'black swan, Cygnus atratus',\n",
      " 101: 'tusker',\n",
      " 102: 'echidna, spiny anteater, anteater',\n",
      " 103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n",
      " 104: 'wallaby, brush kangaroo',\n",
      " 105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n",
      " 106: 'wombat',\n",
      " 107: 'jellyfish',\n",
      " 108: 'sea anemone, anemone',\n",
      " 109: 'brain coral',\n",
      " 110: 'flatworm, platyhelminth',\n",
      " 111: 'nematode, nematode worm, roundworm',\n",
      " 112: 'conch',\n",
      " 113: 'snail',\n",
      " 114: 'slug',\n",
      " 115: 'sea slug, nudibranch',\n",
      " 116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore',\n",
      " 117: 'chambered nautilus, pearly nautilus, nautilus',\n",
      " 118: 'Dungeness crab, Cancer magister',\n",
      " 119: 'rock crab, Cancer irroratus',\n",
      " 120: 'fiddler crab',\n",
      " 121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica',\n",
      " 122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus',\n",
      " 123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish',\n",
      " 124: 'crayfish, crawfish, crawdad, crawdaddy',\n",
      " 125: 'hermit crab',\n",
      " 126: 'isopod',\n",
      " 127: 'white stork, Ciconia ciconia',\n",
      " 128: 'black stork, Ciconia nigra',\n",
      " 129: 'spoonbill',\n",
      " 130: 'flamingo',\n",
      " 131: 'little blue heron, Egretta caerulea',\n",
      " 132: 'American egret, great white heron, Egretta albus',\n",
      " 133: 'bittern',\n",
      " 134: 'crane',\n",
      " 135: 'limpkin, Aramus pictus',\n",
      " 136: 'European gallinule, Porphyrio porphyrio',\n",
      " 137: 'American coot, marsh hen, mud hen, water hen, Fulica americana',\n",
      " 138: 'bustard',\n",
      " 139: 'ruddy turnstone, Arenaria interpres',\n",
      " 140: 'red-backed sandpiper, dunlin, Erolia alpina',\n",
      " 141: 'redshank, Tringa totanus',\n",
      " 142: 'dowitcher',\n",
      " 143: 'oystercatcher, oyster catcher',\n",
      " 144: 'pelican',\n",
      " 145: 'king penguin, Aptenodytes patagonica',\n",
      " 146: 'albatross, mollymawk',\n",
      " 147: 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus',\n",
      " 148: 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca',\n",
      " 149: 'dugong, Dugong dugon',\n",
      " 150: 'sea lion',\n",
      " 151: 'Chihuahua',\n",
      " 152: 'Japanese spaniel',\n",
      " 153: 'Maltese dog, Maltese terrier, Maltese',\n",
      " 154: 'Pekinese, Pekingese, Peke',\n",
      " 155: 'Shih-Tzu',\n",
      " 156: 'Blenheim spaniel',\n",
      " 157: 'papillon',\n",
      " 158: 'toy terrier',\n",
      " 159: 'Rhodesian ridgeback',\n",
      " 160: 'Afghan hound, Afghan',\n",
      " 161: 'basset, basset hound',\n",
      " 162: 'beagle',\n",
      " 163: 'bloodhound, sleuthhound',\n",
      " 164: 'bluetick',\n",
      " 165: 'black-and-tan coonhound',\n",
      " 166: 'Walker hound, Walker foxhound',\n",
      " 167: 'English foxhound',\n",
      " 168: 'redbone',\n",
      " 169: 'borzoi, Russian wolfhound',\n",
      " 170: 'Irish wolfhound',\n",
      " 171: 'Italian greyhound',\n",
      " 172: 'whippet',\n",
      " 173: 'Ibizan hound, Ibizan Podenco',\n",
      " 174: 'Norwegian elkhound, elkhound',\n",
      " 175: 'otterhound, otter hound',\n",
      " 176: 'Saluki, gazelle hound',\n",
      " 177: 'Scottish deerhound, deerhound',\n",
      " 178: 'Weimaraner',\n",
      " 179: 'Staffordshire bullterrier, Staffordshire bull terrier',\n",
      " 180: 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier',\n",
      " 181: 'Bedlington terrier',\n",
      " 182: 'Border terrier',\n",
      " 183: 'Kerry blue terrier',\n",
      " 184: 'Irish terrier',\n",
      " 185: 'Norfolk terrier',\n",
      " 186: 'Norwich terrier',\n",
      " 187: 'Yorkshire terrier',\n",
      " 188: 'wire-haired fox terrier',\n",
      " 189: 'Lakeland terrier',\n",
      " 190: 'Sealyham terrier, Sealyham',\n",
      " 191: 'Airedale, Airedale terrier',\n",
      " 192: 'cairn, cairn terrier',\n",
      " 193: 'Australian terrier',\n",
      " 194: 'Dandie Dinmont, Dandie Dinmont terrier',\n",
      " 195: 'Boston bull, Boston terrier',\n",
      " 196: 'miniature schnauzer',\n",
      " 197: 'giant schnauzer',\n",
      " 198: 'standard schnauzer',\n",
      " 199: 'Scotch terrier, Scottish terrier, Scottie',\n",
      " 200: 'Tibetan terrier, chrysanthemum dog',\n",
      " 201: 'silky terrier, Sydney silky',\n",
      " 202: 'soft-coated wheaten terrier',\n",
      " 203: 'West Highland white terrier',\n",
      " 204: 'Lhasa, Lhasa apso',\n",
      " 205: 'flat-coated retriever',\n",
      " 206: 'curly-coated retriever',\n",
      " 207: 'golden retriever',\n",
      " 208: 'Labrador retriever',\n",
      " 209: 'Chesapeake Bay retriever',\n",
      " 210: 'German short-haired pointer',\n",
      " 211: 'vizsla, Hungarian pointer',\n",
      " 212: 'English setter',\n",
      " 213: 'Irish setter, red setter',\n",
      " 214: 'Gordon setter',\n",
      " 215: 'Brittany spaniel',\n",
      " 216: 'clumber, clumber spaniel',\n",
      " 217: 'English springer, English springer spaniel',\n",
      " 218: 'Welsh springer spaniel',\n",
      " 219: 'cocker spaniel, English cocker spaniel, cocker',\n",
      " 220: 'Sussex spaniel',\n",
      " 221: 'Irish water spaniel',\n",
      " 222: 'kuvasz',\n",
      " 223: 'schipperke',\n",
      " 224: 'groenendael',\n",
      " 225: 'malinois',\n",
      " 226: 'briard',\n",
      " 227: 'kelpie',\n",
      " 228: 'komondor',\n",
      " 229: 'Old English sheepdog, bobtail',\n",
      " 230: 'Shetland sheepdog, Shetland sheep dog, Shetland',\n",
      " 231: 'collie',\n",
      " 232: 'Border collie',\n",
      " 233: 'Bouvier des Flandres, Bouviers des Flandres',\n",
      " 234: 'Rottweiler',\n",
      " 235: 'German shepherd, German shepherd dog, German police dog, alsatian',\n",
      " 236: 'Doberman, Doberman pinscher',\n",
      " 237: 'miniature pinscher',\n",
      " 238: 'Greater Swiss Mountain dog',\n",
      " 239: 'Bernese mountain dog',\n",
      " 240: 'Appenzeller',\n",
      " 241: 'EntleBucher',\n",
      " 242: 'boxer',\n",
      " 243: 'bull mastiff',\n",
      " 244: 'Tibetan mastiff',\n",
      " 245: 'French bulldog',\n",
      " 246: 'Great Dane',\n",
      " 247: 'Saint Bernard, St Bernard',\n",
      " 248: 'Eskimo dog, husky',\n",
      " 249: 'malamute, malemute, Alaskan malamute',\n",
      " 250: 'Siberian husky',\n",
      " 251: 'dalmatian, coach dog, carriage dog',\n",
      " 252: 'affenpinscher, monkey pinscher, monkey dog',\n",
      " 253: 'basenji',\n",
      " 254: 'pug, pug-dog',\n",
      " 255: 'Leonberg',\n",
      " 256: 'Newfoundland, Newfoundland dog',\n",
      " 257: 'Great Pyrenees',\n",
      " 258: 'Samoyed, Samoyede',\n",
      " 259: 'Pomeranian',\n",
      " 260: 'chow, chow chow',\n",
      " 261: 'keeshond',\n",
      " 262: 'Brabancon griffon',\n",
      " 263: 'Pembroke, Pembroke Welsh corgi',\n",
      " 264: 'Cardigan, Cardigan Welsh corgi',\n",
      " 265: 'toy poodle',\n",
      " 266: 'miniature poodle',\n",
      " 267: 'standard poodle',\n",
      " 268: 'Mexican hairless',\n",
      " 269: 'timber wolf, grey wolf, gray wolf, Canis lupus',\n",
      " 270: 'white wolf, Arctic wolf, Canis lupus tundrarum',\n",
      " 271: 'red wolf, maned wolf, Canis rufus, Canis niger',\n",
      " 272: 'coyote, prairie wolf, brush wolf, Canis latrans',\n",
      " 273: 'dingo, warrigal, warragal, Canis dingo',\n",
      " 274: 'dhole, Cuon alpinus',\n",
      " 275: 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus',\n",
      " 276: 'hyena, hyaena',\n",
      " 277: 'red fox, Vulpes vulpes',\n",
      " 278: 'kit fox, Vulpes macrotis',\n",
      " 279: 'Arctic fox, white fox, Alopex lagopus',\n",
      " 280: 'grey fox, gray fox, Urocyon cinereoargenteus',\n",
      " 281: 'tabby, tabby cat',\n",
      " 282: 'tiger cat',\n",
      " 283: 'Persian cat',\n",
      " 284: 'Siamese cat, Siamese',\n",
      " 285: 'Egyptian cat',\n",
      " 286: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n",
      " 287: 'lynx, catamount',\n",
      " 288: 'leopard, Panthera pardus',\n",
      " 289: 'snow leopard, ounce, Panthera uncia',\n",
      " 290: 'jaguar, panther, Panthera onca, Felis onca',\n",
      " 291: 'lion, king of beasts, Panthera leo',\n",
      " 292: 'tiger, Panthera tigris',\n",
      " 293: 'cheetah, chetah, Acinonyx jubatus',\n",
      " 294: 'brown bear, bruin, Ursus arctos',\n",
      " 295: 'American black bear, black bear, Ursus americanus, Euarctos americanus',\n",
      " 296: 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus',\n",
      " 297: 'sloth bear, Melursus ursinus, Ursus ursinus',\n",
      " 298: 'mongoose',\n",
      " 299: 'meerkat, mierkat',\n",
      " 300: 'tiger beetle',\n",
      " 301: 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle',\n",
      " 302: 'ground beetle, carabid beetle',\n",
      " 303: 'long-horned beetle, longicorn, longicorn beetle',\n",
      " 304: 'leaf beetle, chrysomelid',\n",
      " 305: 'dung beetle',\n",
      " 306: 'rhinoceros beetle',\n",
      " 307: 'weevil',\n",
      " 308: 'fly',\n",
      " 309: 'bee',\n",
      " 310: 'ant, emmet, pismire',\n",
      " 311: 'grasshopper, hopper',\n",
      " 312: 'cricket',\n",
      " 313: 'walking stick, walkingstick, stick insect',\n",
      " 314: 'cockroach, roach',\n",
      " 315: 'mantis, mantid',\n",
      " 316: 'cicada, cicala',\n",
      " 317: 'leafhopper',\n",
      " 318: 'lacewing, lacewing fly',\n",
      " 319: \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\",\n",
      " 320: 'damselfly',\n",
      " 321: 'admiral',\n",
      " 322: 'ringlet, ringlet butterfly',\n",
      " 323: 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus',\n",
      " 324: 'cabbage butterfly',\n",
      " 325: 'sulphur butterfly, sulfur butterfly',\n",
      " 326: 'lycaenid, lycaenid butterfly',\n",
      " 327: 'starfish, sea star',\n",
      " 328: 'sea urchin',\n",
      " 329: 'sea cucumber, holothurian',\n",
      " 330: 'wood rabbit, cottontail, cottontail rabbit',\n",
      " 331: 'hare',\n",
      " 332: 'Angora, Angora rabbit',\n",
      " 333: 'hamster',\n",
      " 334: 'porcupine, hedgehog',\n",
      " 335: 'fox squirrel, eastern fox squirrel, Sciurus niger',\n",
      " 336: 'marmot',\n",
      " 337: 'beaver',\n",
      " 338: 'guinea pig, Cavia cobaya',\n",
      " 339: 'sorrel',\n",
      " 340: 'zebra',\n",
      " 341: 'hog, pig, grunter, squealer, Sus scrofa',\n",
      " 342: 'wild boar, boar, Sus scrofa',\n",
      " 343: 'warthog',\n",
      " 344: 'hippopotamus, hippo, river horse, Hippopotamus amphibius',\n",
      " 345: 'ox',\n",
      " 346: 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis',\n",
      " 347: 'bison',\n",
      " 348: 'ram, tup',\n",
      " 349: 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis',\n",
      " 350: 'ibex, Capra ibex',\n",
      " 351: 'hartebeest',\n",
      " 352: 'impala, Aepyceros melampus',\n",
      " 353: 'gazelle',\n",
      " 354: 'Arabian camel, dromedary, Camelus dromedarius',\n",
      " 355: 'llama',\n",
      " 356: 'weasel',\n",
      " 357: 'mink',\n",
      " 358: 'polecat, fitch, foulmart, foumart, Mustela putorius',\n",
      " 359: 'black-footed ferret, ferret, Mustela nigripes',\n",
      " 360: 'otter',\n",
      " 361: 'skunk, polecat, wood pussy',\n",
      " 362: 'badger',\n",
      " 363: 'armadillo',\n",
      " 364: 'three-toed sloth, ai, Bradypus tridactylus',\n",
      " 365: 'orangutan, orang, orangutang, Pongo pygmaeus',\n",
      " 366: 'gorilla, Gorilla gorilla',\n",
      " 367: 'chimpanzee, chimp, Pan troglodytes',\n",
      " 368: 'gibbon, Hylobates lar',\n",
      " 369: 'siamang, Hylobates syndactylus, Symphalangus syndactylus',\n",
      " 370: 'guenon, guenon monkey',\n",
      " 371: 'patas, hussar monkey, Erythrocebus patas',\n",
      " 372: 'baboon',\n",
      " 373: 'macaque',\n",
      " 374: 'langur',\n",
      " 375: 'colobus, colobus monkey',\n",
      " 376: 'proboscis monkey, Nasalis larvatus',\n",
      " 377: 'marmoset',\n",
      " 378: 'capuchin, ringtail, Cebus capucinus',\n",
      " 379: 'howler monkey, howler',\n",
      " 380: 'titi, titi monkey',\n",
      " 381: 'spider monkey, Ateles geoffroyi',\n",
      " 382: 'squirrel monkey, Saimiri sciureus',\n",
      " 383: 'Madagascar cat, ring-tailed lemur, Lemur catta',\n",
      " 384: 'indri, indris, Indri indri, Indri brevicaudatus',\n",
      " 385: 'Indian elephant, Elephas maximus',\n",
      " 386: 'African elephant, Loxodonta africana',\n",
      " 387: 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens',\n",
      " 388: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca',\n",
      " 389: 'barracouta, snoek',\n",
      " 390: 'eel',\n",
      " 391: 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch',\n",
      " 392: 'rock beauty, Holocanthus tricolor',\n",
      " 393: 'anemone fish',\n",
      " 394: 'sturgeon',\n",
      " 395: 'gar, garfish, garpike, billfish, Lepisosteus osseus',\n",
      " 396: 'lionfish',\n",
      " 397: 'puffer, pufferfish, blowfish, globefish',\n",
      " 398: 'abacus',\n",
      " 399: 'abaya',\n",
      " 400: \"academic gown, academic robe, judge's robe\",\n",
      " 401: 'accordion, piano accordion, squeeze box',\n",
      " 402: 'acoustic guitar',\n",
      " 403: 'aircraft carrier, carrier, flattop, attack aircraft carrier',\n",
      " 404: 'airliner',\n",
      " 405: 'airship, dirigible',\n",
      " 406: 'altar',\n",
      " 407: 'ambulance',\n",
      " 408: 'amphibian, amphibious vehicle',\n",
      " 409: 'analog clock',\n",
      " 410: 'apiary, bee house',\n",
      " 411: 'apron',\n",
      " 412: 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin',\n",
      " 413: 'assault rifle, assault gun',\n",
      " 414: 'backpack, back pack, knapsack, packsack, rucksack, haversack',\n",
      " 415: 'bakery, bakeshop, bakehouse',\n",
      " 416: 'balance beam, beam',\n",
      " 417: 'balloon',\n",
      " 418: 'ballpoint, ballpoint pen, ballpen, Biro',\n",
      " 419: 'Band Aid',\n",
      " 420: 'banjo',\n",
      " 421: 'bannister, banister, balustrade, balusters, handrail',\n",
      " 422: 'barbell',\n",
      " 423: 'barber chair',\n",
      " 424: 'barbershop',\n",
      " 425: 'barn',\n",
      " 426: 'barometer',\n",
      " 427: 'barrel, cask',\n",
      " 428: 'barrow, garden cart, lawn cart, wheelbarrow',\n",
      " 429: 'baseball',\n",
      " 430: 'basketball',\n",
      " 431: 'bassinet',\n",
      " 432: 'bassoon',\n",
      " 433: 'bathing cap, swimming cap',\n",
      " 434: 'bath towel',\n",
      " 435: 'bathtub, bathing tub, bath, tub',\n",
      " 436: 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon',\n",
      " 437: 'beacon, lighthouse, beacon light, pharos',\n",
      " 438: 'beaker',\n",
      " 439: 'bearskin, busby, shako',\n",
      " 440: 'beer bottle',\n",
      " 441: 'beer glass',\n",
      " 442: 'bell cote, bell cot',\n",
      " 443: 'bib',\n",
      " 444: 'bicycle-built-for-two, tandem bicycle, tandem',\n",
      " 445: 'bikini, two-piece',\n",
      " 446: 'binder, ring-binder',\n",
      " 447: 'binoculars, field glasses, opera glasses',\n",
      " 448: 'birdhouse',\n",
      " 449: 'boathouse',\n",
      " 450: 'bobsled, bobsleigh, bob',\n",
      " 451: 'bolo tie, bolo, bola tie, bola',\n",
      " 452: 'bonnet, poke bonnet',\n",
      " 453: 'bookcase',\n",
      " 454: 'bookshop, bookstore, bookstall',\n",
      " 455: 'bottlecap',\n",
      " 456: 'bow',\n",
      " 457: 'bow tie, bow-tie, bowtie',\n",
      " 458: 'brass, memorial tablet, plaque',\n",
      " 459: 'brassiere, bra, bandeau',\n",
      " 460: 'breakwater, groin, groyne, mole, bulwark, seawall, jetty',\n",
      " 461: 'breastplate, aegis, egis',\n",
      " 462: 'broom',\n",
      " 463: 'bucket, pail',\n",
      " 464: 'buckle',\n",
      " 465: 'bulletproof vest',\n",
      " 466: 'bullet train, bullet',\n",
      " 467: 'butcher shop, meat market',\n",
      " 468: 'cab, hack, taxi, taxicab',\n",
      " 469: 'caldron, cauldron',\n",
      " 470: 'candle, taper, wax light',\n",
      " 471: 'cannon',\n",
      " 472: 'canoe',\n",
      " 473: 'can opener, tin opener',\n",
      " 474: 'cardigan',\n",
      " 475: 'car mirror',\n",
      " 476: 'carousel, carrousel, merry-go-round, roundabout, whirligig',\n",
      " 477: \"carpenter's kit, tool kit\",\n",
      " 478: 'carton',\n",
      " 479: 'car wheel',\n",
      " 480: 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM',\n",
      " 481: 'cassette',\n",
      " 482: 'cassette player',\n",
      " 483: 'castle',\n",
      " 484: 'catamaran',\n",
      " 485: 'CD player',\n",
      " 486: 'cello, violoncello',\n",
      " 487: 'cellular telephone, cellular phone, cellphone, cell, mobile phone',\n",
      " 488: 'chain',\n",
      " 489: 'chainlink fence',\n",
      " 490: 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour',\n",
      " 491: 'chain saw, chainsaw',\n",
      " 492: 'chest',\n",
      " 493: 'chiffonier, commode',\n",
      " 494: 'chime, bell, gong',\n",
      " 495: 'china cabinet, china closet',\n",
      " 496: 'Christmas stocking',\n",
      " 497: 'church, church building',\n",
      " 498: 'cinema, movie theater, movie theatre, movie house, picture palace',\n",
      " 499: 'cleaver, meat cleaver, chopper',\n",
      " 500: 'cliff dwelling',\n",
      " 501: 'cloak',\n",
      " 502: 'clog, geta, patten, sabot',\n",
      " 503: 'cocktail shaker',\n",
      " 504: 'coffee mug',\n",
      " 505: 'coffeepot',\n",
      " 506: 'coil, spiral, volute, whorl, helix',\n",
      " 507: 'combination lock',\n",
      " 508: 'computer keyboard, keypad',\n",
      " 509: 'confectionery, confectionary, candy store',\n",
      " 510: 'container ship, containership, container vessel',\n",
      " 511: 'convertible',\n",
      " 512: 'corkscrew, bottle screw',\n",
      " 513: 'cornet, horn, trumpet, trump',\n",
      " 514: 'cowboy boot',\n",
      " 515: 'cowboy hat, ten-gallon hat',\n",
      " 516: 'cradle',\n",
      " 517: 'crane',\n",
      " 518: 'crash helmet',\n",
      " 519: 'crate',\n",
      " 520: 'crib, cot',\n",
      " 521: 'Crock Pot',\n",
      " 522: 'croquet ball',\n",
      " 523: 'crutch',\n",
      " 524: 'cuirass',\n",
      " 525: 'dam, dike, dyke',\n",
      " 526: 'desk',\n",
      " 527: 'desktop computer',\n",
      " 528: 'dial telephone, dial phone',\n",
      " 529: 'diaper, nappy, napkin',\n",
      " 530: 'digital clock',\n",
      " 531: 'digital watch',\n",
      " 532: 'dining table, board',\n",
      " 533: 'dishrag, dishcloth',\n",
      " 534: 'dishwasher, dish washer, dishwashing machine',\n",
      " 535: 'disk brake, disc brake',\n",
      " 536: 'dock, dockage, docking facility',\n",
      " 537: 'dogsled, dog sled, dog sleigh',\n",
      " 538: 'dome',\n",
      " 539: 'doormat, welcome mat',\n",
      " 540: 'drilling platform, offshore rig',\n",
      " 541: 'drum, membranophone, tympan',\n",
      " 542: 'drumstick',\n",
      " 543: 'dumbbell',\n",
      " 544: 'Dutch oven',\n",
      " 545: 'electric fan, blower',\n",
      " 546: 'electric guitar',\n",
      " 547: 'electric locomotive',\n",
      " 548: 'entertainment center',\n",
      " 549: 'envelope',\n",
      " 550: 'espresso maker',\n",
      " 551: 'face powder',\n",
      " 552: 'feather boa, boa',\n",
      " 553: 'file, file cabinet, filing cabinet',\n",
      " 554: 'fireboat',\n",
      " 555: 'fire engine, fire truck',\n",
      " 556: 'fire screen, fireguard',\n",
      " 557: 'flagpole, flagstaff',\n",
      " 558: 'flute, transverse flute',\n",
      " 559: 'folding chair',\n",
      " 560: 'football helmet',\n",
      " 561: 'forklift',\n",
      " 562: 'fountain',\n",
      " 563: 'fountain pen',\n",
      " 564: 'four-poster',\n",
      " 565: 'freight car',\n",
      " 566: 'French horn, horn',\n",
      " 567: 'frying pan, frypan, skillet',\n",
      " 568: 'fur coat',\n",
      " 569: 'garbage truck, dustcart',\n",
      " 570: 'gasmask, respirator, gas helmet',\n",
      " 571: 'gas pump, gasoline pump, petrol pump, island dispenser',\n",
      " 572: 'goblet',\n",
      " 573: 'go-kart',\n",
      " 574: 'golf ball',\n",
      " 575: 'golfcart, golf cart',\n",
      " 576: 'gondola',\n",
      " 577: 'gong, tam-tam',\n",
      " 578: 'gown',\n",
      " 579: 'grand piano, grand',\n",
      " 580: 'greenhouse, nursery, glasshouse',\n",
      " 581: 'grille, radiator grille',\n",
      " 582: 'grocery store, grocery, food market, market',\n",
      " 583: 'guillotine',\n",
      " 584: 'hair slide',\n",
      " 585: 'hair spray',\n",
      " 586: 'half track',\n",
      " 587: 'hammer',\n",
      " 588: 'hamper',\n",
      " 589: 'hand blower, blow dryer, blow drier, hair dryer, hair drier',\n",
      " 590: 'hand-held computer, hand-held microcomputer',\n",
      " 591: 'handkerchief, hankie, hanky, hankey',\n",
      " 592: 'hard disc, hard disk, fixed disk',\n",
      " 593: 'harmonica, mouth organ, harp, mouth harp',\n",
      " 594: 'harp',\n",
      " 595: 'harvester, reaper',\n",
      " 596: 'hatchet',\n",
      " 597: 'holster',\n",
      " 598: 'home theater, home theatre',\n",
      " 599: 'honeycomb',\n",
      " 600: 'hook, claw',\n",
      " 601: 'hoopskirt, crinoline',\n",
      " 602: 'horizontal bar, high bar',\n",
      " 603: 'horse cart, horse-cart',\n",
      " 604: 'hourglass',\n",
      " 605: 'iPod',\n",
      " 606: 'iron, smoothing iron',\n",
      " 607: \"jack-o'-lantern\",\n",
      " 608: 'jean, blue jean, denim',\n",
      " 609: 'jeep, landrover',\n",
      " 610: 'jersey, T-shirt, tee shirt',\n",
      " 611: 'jigsaw puzzle',\n",
      " 612: 'jinrikisha, ricksha, rickshaw',\n",
      " 613: 'joystick',\n",
      " 614: 'kimono',\n",
      " 615: 'knee pad',\n",
      " 616: 'knot',\n",
      " 617: 'lab coat, laboratory coat',\n",
      " 618: 'ladle',\n",
      " 619: 'lampshade, lamp shade',\n",
      " 620: 'laptop, laptop computer',\n",
      " 621: 'lawn mower, mower',\n",
      " 622: 'lens cap, lens cover',\n",
      " 623: 'letter opener, paper knife, paperknife',\n",
      " 624: 'library',\n",
      " 625: 'lifeboat',\n",
      " 626: 'lighter, light, igniter, ignitor',\n",
      " 627: 'limousine, limo',\n",
      " 628: 'liner, ocean liner',\n",
      " 629: 'lipstick, lip rouge',\n",
      " 630: 'Loafer',\n",
      " 631: 'lotion',\n",
      " 632: 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system',\n",
      " 633: \"loupe, jeweler's loupe\",\n",
      " 634: 'lumbermill, sawmill',\n",
      " 635: 'magnetic compass',\n",
      " 636: 'mailbag, postbag',\n",
      " 637: 'mailbox, letter box',\n",
      " 638: 'maillot',\n",
      " 639: 'maillot, tank suit',\n",
      " 640: 'manhole cover',\n",
      " 641: 'maraca',\n",
      " 642: 'marimba, xylophone',\n",
      " 643: 'mask',\n",
      " 644: 'matchstick',\n",
      " 645: 'maypole',\n",
      " 646: 'maze, labyrinth',\n",
      " 647: 'measuring cup',\n",
      " 648: 'medicine chest, medicine cabinet',\n",
      " 649: 'megalith, megalithic structure',\n",
      " 650: 'microphone, mike',\n",
      " 651: 'microwave, microwave oven',\n",
      " 652: 'military uniform',\n",
      " 653: 'milk can',\n",
      " 654: 'minibus',\n",
      " 655: 'miniskirt, mini',\n",
      " 656: 'minivan',\n",
      " 657: 'missile',\n",
      " 658: 'mitten',\n",
      " 659: 'mixing bowl',\n",
      " 660: 'mobile home, manufactured home',\n",
      " 661: 'Model T',\n",
      " 662: 'modem',\n",
      " 663: 'monastery',\n",
      " 664: 'monitor',\n",
      " 665: 'moped',\n",
      " 666: 'mortar',\n",
      " 667: 'mortarboard',\n",
      " 668: 'mosque',\n",
      " 669: 'mosquito net',\n",
      " 670: 'motor scooter, scooter',\n",
      " 671: 'mountain bike, all-terrain bike, off-roader',\n",
      " 672: 'mountain tent',\n",
      " 673: 'mouse, computer mouse',\n",
      " 674: 'mousetrap',\n",
      " 675: 'moving van',\n",
      " 676: 'muzzle',\n",
      " 677: 'nail',\n",
      " 678: 'neck brace',\n",
      " 679: 'necklace',\n",
      " 680: 'nipple',\n",
      " 681: 'notebook, notebook computer',\n",
      " 682: 'obelisk',\n",
      " 683: 'oboe, hautboy, hautbois',\n",
      " 684: 'ocarina, sweet potato',\n",
      " 685: 'odometer, hodometer, mileometer, milometer',\n",
      " 686: 'oil filter',\n",
      " 687: 'organ, pipe organ',\n",
      " 688: 'oscilloscope, scope, cathode-ray oscilloscope, CRO',\n",
      " 689: 'overskirt',\n",
      " 690: 'oxcart',\n",
      " 691: 'oxygen mask',\n",
      " 692: 'packet',\n",
      " 693: 'paddle, boat paddle',\n",
      " 694: 'paddlewheel, paddle wheel',\n",
      " 695: 'padlock',\n",
      " 696: 'paintbrush',\n",
      " 697: \"pajama, pyjama, pj's, jammies\",\n",
      " 698: 'palace',\n",
      " 699: 'panpipe, pandean pipe, syrinx',\n",
      " 700: 'paper towel',\n",
      " 701: 'parachute, chute',\n",
      " 702: 'parallel bars, bars',\n",
      " 703: 'park bench',\n",
      " 704: 'parking meter',\n",
      " 705: 'passenger car, coach, carriage',\n",
      " 706: 'patio, terrace',\n",
      " 707: 'pay-phone, pay-station',\n",
      " 708: 'pedestal, plinth, footstall',\n",
      " 709: 'pencil box, pencil case',\n",
      " 710: 'pencil sharpener',\n",
      " 711: 'perfume, essence',\n",
      " 712: 'Petri dish',\n",
      " 713: 'photocopier',\n",
      " 714: 'pick, plectrum, plectron',\n",
      " 715: 'pickelhaube',\n",
      " 716: 'picket fence, paling',\n",
      " 717: 'pickup, pickup truck',\n",
      " 718: 'pier',\n",
      " 719: 'piggy bank, penny bank',\n",
      " 720: 'pill bottle',\n",
      " 721: 'pillow',\n",
      " 722: 'ping-pong ball',\n",
      " 723: 'pinwheel',\n",
      " 724: 'pirate, pirate ship',\n",
      " 725: 'pitcher, ewer',\n",
      " 726: \"plane, carpenter's plane, woodworking plane\",\n",
      " 727: 'planetarium',\n",
      " 728: 'plastic bag',\n",
      " 729: 'plate rack',\n",
      " 730: 'plow, plough',\n",
      " 731: \"plunger, plumber's helper\",\n",
      " 732: 'Polaroid camera, Polaroid Land camera',\n",
      " 733: 'pole',\n",
      " 734: 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria',\n",
      " 735: 'poncho',\n",
      " 736: 'pool table, billiard table, snooker table',\n",
      " 737: 'pop bottle, soda bottle',\n",
      " 738: 'pot, flowerpot',\n",
      " 739: \"potter's wheel\",\n",
      " 740: 'power drill',\n",
      " 741: 'prayer rug, prayer mat',\n",
      " 742: 'printer',\n",
      " 743: 'prison, prison house',\n",
      " 744: 'projectile, missile',\n",
      " 745: 'projector',\n",
      " 746: 'puck, hockey puck',\n",
      " 747: 'punching bag, punch bag, punching ball, punchball',\n",
      " 748: 'purse',\n",
      " 749: 'quill, quill pen',\n",
      " 750: 'quilt, comforter, comfort, puff',\n",
      " 751: 'racer, race car, racing car',\n",
      " 752: 'racket, racquet',\n",
      " 753: 'radiator',\n",
      " 754: 'radio, wireless',\n",
      " 755: 'radio telescope, radio reflector',\n",
      " 756: 'rain barrel',\n",
      " 757: 'recreational vehicle, RV, R.V.',\n",
      " 758: 'reel',\n",
      " 759: 'reflex camera',\n",
      " 760: 'refrigerator, icebox',\n",
      " 761: 'remote control, remote',\n",
      " 762: 'restaurant, eating house, eating place, eatery',\n",
      " 763: 'revolver, six-gun, six-shooter',\n",
      " 764: 'rifle',\n",
      " 765: 'rocking chair, rocker',\n",
      " 766: 'rotisserie',\n",
      " 767: 'rubber eraser, rubber, pencil eraser',\n",
      " 768: 'rugby ball',\n",
      " 769: 'rule, ruler',\n",
      " 770: 'running shoe',\n",
      " 771: 'safe',\n",
      " 772: 'safety pin',\n",
      " 773: 'saltshaker, salt shaker',\n",
      " 774: 'sandal',\n",
      " 775: 'sarong',\n",
      " 776: 'sax, saxophone',\n",
      " 777: 'scabbard',\n",
      " 778: 'scale, weighing machine',\n",
      " 779: 'school bus',\n",
      " 780: 'schooner',\n",
      " 781: 'scoreboard',\n",
      " 782: 'screen, CRT screen',\n",
      " 783: 'screw',\n",
      " 784: 'screwdriver',\n",
      " 785: 'seat belt, seatbelt',\n",
      " 786: 'sewing machine',\n",
      " 787: 'shield, buckler',\n",
      " 788: 'shoe shop, shoe-shop, shoe store',\n",
      " 789: 'shoji',\n",
      " 790: 'shopping basket',\n",
      " 791: 'shopping cart',\n",
      " 792: 'shovel',\n",
      " 793: 'shower cap',\n",
      " 794: 'shower curtain',\n",
      " 795: 'ski',\n",
      " 796: 'ski mask',\n",
      " 797: 'sleeping bag',\n",
      " 798: 'slide rule, slipstick',\n",
      " 799: 'sliding door',\n",
      " 800: 'slot, one-armed bandit',\n",
      " 801: 'snorkel',\n",
      " 802: 'snowmobile',\n",
      " 803: 'snowplow, snowplough',\n",
      " 804: 'soap dispenser',\n",
      " 805: 'soccer ball',\n",
      " 806: 'sock',\n",
      " 807: 'solar dish, solar collector, solar furnace',\n",
      " 808: 'sombrero',\n",
      " 809: 'soup bowl',\n",
      " 810: 'space bar',\n",
      " 811: 'space heater',\n",
      " 812: 'space shuttle',\n",
      " 813: 'spatula',\n",
      " 814: 'speedboat',\n",
      " 815: \"spider web, spider's web\",\n",
      " 816: 'spindle',\n",
      " 817: 'sports car, sport car',\n",
      " 818: 'spotlight, spot',\n",
      " 819: 'stage',\n",
      " 820: 'steam locomotive',\n",
      " 821: 'steel arch bridge',\n",
      " 822: 'steel drum',\n",
      " 823: 'stethoscope',\n",
      " 824: 'stole',\n",
      " 825: 'stone wall',\n",
      " 826: 'stopwatch, stop watch',\n",
      " 827: 'stove',\n",
      " 828: 'strainer',\n",
      " 829: 'streetcar, tram, tramcar, trolley, trolley car',\n",
      " 830: 'stretcher',\n",
      " 831: 'studio couch, day bed',\n",
      " 832: 'stupa, tope',\n",
      " 833: 'submarine, pigboat, sub, U-boat',\n",
      " 834: 'suit, suit of clothes',\n",
      " 835: 'sundial',\n",
      " 836: 'sunglass',\n",
      " 837: 'sunglasses, dark glasses, shades',\n",
      " 838: 'sunscreen, sunblock, sun blocker',\n",
      " 839: 'suspension bridge',\n",
      " 840: 'swab, swob, mop',\n",
      " 841: 'sweatshirt',\n",
      " 842: 'swimming trunks, bathing trunks',\n",
      " 843: 'swing',\n",
      " 844: 'switch, electric switch, electrical switch',\n",
      " 845: 'syringe',\n",
      " 846: 'table lamp',\n",
      " 847: 'tank, army tank, armored combat vehicle, armoured combat vehicle',\n",
      " 848: 'tape player',\n",
      " 849: 'teapot',\n",
      " 850: 'teddy, teddy bear',\n",
      " 851: 'television, television system',\n",
      " 852: 'tennis ball',\n",
      " 853: 'thatch, thatched roof',\n",
      " 854: 'theater curtain, theatre curtain',\n",
      " 855: 'thimble',\n",
      " 856: 'thresher, thrasher, threshing machine',\n",
      " 857: 'throne',\n",
      " 858: 'tile roof',\n",
      " 859: 'toaster',\n",
      " 860: 'tobacco shop, tobacconist shop, tobacconist',\n",
      " 861: 'toilet seat',\n",
      " 862: 'torch',\n",
      " 863: 'totem pole',\n",
      " 864: 'tow truck, tow car, wrecker',\n",
      " 865: 'toyshop',\n",
      " 866: 'tractor',\n",
      " 867: 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi',\n",
      " 868: 'tray',\n",
      " 869: 'trench coat',\n",
      " 870: 'tricycle, trike, velocipede',\n",
      " 871: 'trimaran',\n",
      " 872: 'tripod',\n",
      " 873: 'triumphal arch',\n",
      " 874: 'trolleybus, trolley coach, trackless trolley',\n",
      " 875: 'trombone',\n",
      " 876: 'tub, vat',\n",
      " 877: 'turnstile',\n",
      " 878: 'typewriter keyboard',\n",
      " 879: 'umbrella',\n",
      " 880: 'unicycle, monocycle',\n",
      " 881: 'upright, upright piano',\n",
      " 882: 'vacuum, vacuum cleaner',\n",
      " 883: 'vase',\n",
      " 884: 'vault',\n",
      " 885: 'velvet',\n",
      " 886: 'vending machine',\n",
      " 887: 'vestment',\n",
      " 888: 'viaduct',\n",
      " 889: 'violin, fiddle',\n",
      " 890: 'volleyball',\n",
      " 891: 'waffle iron',\n",
      " 892: 'wall clock',\n",
      " 893: 'wallet, billfold, notecase, pocketbook',\n",
      " 894: 'wardrobe, closet, press',\n",
      " 895: 'warplane, military plane',\n",
      " 896: 'washbasin, handbasin, washbowl, lavabo, wash-hand basin',\n",
      " 897: 'washer, automatic washer, washing machine',\n",
      " 898: 'water bottle',\n",
      " 899: 'water jug',\n",
      " 900: 'water tower',\n",
      " 901: 'whiskey jug',\n",
      " 902: 'whistle',\n",
      " 903: 'wig',\n",
      " 904: 'window screen',\n",
      " 905: 'window shade',\n",
      " 906: 'Windsor tie',\n",
      " 907: 'wine bottle',\n",
      " 908: 'wing',\n",
      " 909: 'wok',\n",
      " 910: 'wooden spoon',\n",
      " 911: 'wool, woolen, woollen',\n",
      " 912: 'worm fence, snake fence, snake-rail fence, Virginia fence',\n",
      " 913: 'wreck',\n",
      " 914: 'yawl',\n",
      " 915: 'yurt',\n",
      " 916: 'web site, website, internet site, site',\n",
      " 917: 'comic book',\n",
      " 918: 'crossword puzzle, crossword',\n",
      " 919: 'street sign',\n",
      " 920: 'traffic light, traffic signal, stoplight',\n",
      " 921: 'book jacket, dust cover, dust jacket, dust wrapper',\n",
      " 922: 'menu',\n",
      " 923: 'plate',\n",
      " 924: 'guacamole',\n",
      " 925: 'consomme',\n",
      " 926: 'hot pot, hotpot',\n",
      " 927: 'trifle',\n",
      " 928: 'ice cream, icecream',\n",
      " 929: 'ice lolly, lolly, lollipop, popsicle',\n",
      " 930: 'French loaf',\n",
      " 931: 'bagel, beigel',\n",
      " 932: 'pretzel',\n",
      " 933: 'cheeseburger',\n",
      " 934: 'hotdog, hot dog, red hot',\n",
      " 935: 'mashed potato',\n",
      " 936: 'head cabbage',\n",
      " 937: 'broccoli',\n",
      " 938: 'cauliflower',\n",
      " 939: 'zucchini, courgette',\n",
      " 940: 'spaghetti squash',\n",
      " 941: 'acorn squash',\n",
      " 942: 'butternut squash',\n",
      " 943: 'cucumber, cuke',\n",
      " 944: 'artichoke, globe artichoke',\n",
      " 945: 'bell pepper',\n",
      " 946: 'cardoon',\n",
      " 947: 'mushroom',\n",
      " 948: 'Granny Smith',\n",
      " 949: 'strawberry',\n",
      " 950: 'orange',\n",
      " 951: 'lemon',\n",
      " 952: 'fig',\n",
      " 953: 'pineapple, ananas',\n",
      " 954: 'banana',\n",
      " 955: 'jackfruit, jak, jack',\n",
      " 956: 'custard apple',\n",
      " 957: 'pomegranate',\n",
      " 958: 'hay',\n",
      " 959: 'carbonara',\n",
      " 960: 'chocolate sauce, chocolate syrup',\n",
      " 961: 'dough',\n",
      " 962: 'meat loaf, meatloaf',\n",
      " 963: 'pizza, pizza pie',\n",
      " 964: 'potpie',\n",
      " 965: 'burrito',\n",
      " 966: 'red wine',\n",
      " 967: 'espresso',\n",
      " 968: 'cup',\n",
      " 969: 'eggnog',\n",
      " 970: 'alp',\n",
      " 971: 'bubble',\n",
      " 972: 'cliff, drop, drop-off',\n",
      " 973: 'coral reef',\n",
      " 974: 'geyser',\n",
      " 975: 'lakeside, lakeshore',\n",
      " 976: 'promontory, headland, head, foreland',\n",
      " 977: 'sandbar, sand bar',\n",
      " 978: 'seashore, coast, seacoast, sea-coast',\n",
      " 979: 'valley, vale',\n",
      " 980: 'volcano',\n",
      " 981: 'ballplayer, baseball player',\n",
      " 982: 'groom, bridegroom',\n",
      " 983: 'scuba diver',\n",
      " 984: 'rapeseed',\n",
      " 985: 'daisy',\n",
      " 986: \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\",\n",
      " 987: 'corn',\n",
      " 988: 'acorn',\n",
      " 989: 'hip, rose hip, rosehip',\n",
      " 990: 'buckeye, horse chestnut, conker',\n",
      " 991: 'coral fungus',\n",
      " 992: 'agaric',\n",
      " 993: 'gyromitra',\n",
      " 994: 'stinkhorn, carrion fungus',\n",
      " 995: 'earthstar',\n",
      " 996: 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa',\n",
      " 997: 'bolete',\n",
      " 998: 'ear, spike, capitulum',\n",
      " 999: 'toilet tissue, toilet paper, bathroom tissue'}\n",
      "68/3:\n",
      "from tensorflow.python.keras.applications import EfficientNetB4\n",
      "from tensorflow.image import resize\n",
      "model = EfficientNetB4(weights='imagenet')\n",
      "\n",
      "def center_crop(img, new_width=None, new_height=None):        \n",
      "\n",
      "    width = img.shape[1]\n",
      "    height = img.shape[0]\n",
      "\n",
      "    if new_width is None:\n",
      "        new_width = min(width, height)\n",
      "\n",
      "    if new_height is None:\n",
      "        new_height = min(width, height)\n",
      "\n",
      "    left = int(np.ceil((width - new_width) / 2))\n",
      "    right = width - int(np.floor((width - new_width) / 2))\n",
      "\n",
      "    top = int(np.ceil((height - new_height) / 2))\n",
      "    bottom = height - int(np.floor((height - new_height) / 2))\n",
      "\n",
      "    if len(img.shape) == 2:\n",
      "        center_cropped_img = img[top:bottom, left:right]\n",
      "    else:\n",
      "        center_cropped_img = img[top:bottom, left:right, ...]\n",
      "\n",
      "    return center_cropped_img\n",
      "\n",
      "class_dict = {0: 'tench, Tinca tinca',\n",
      " 1: 'goldfish, Carassius auratus',\n",
      " 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n",
      " 3: 'tiger shark, Galeocerdo cuvieri',\n",
      " 4: 'hammerhead, hammerhead shark',\n",
      " 5: 'electric ray, crampfish, numbfish, torpedo',\n",
      " 6: 'stingray',\n",
      " 7: 'cock',\n",
      " 8: 'hen',\n",
      " 9: 'ostrich, Struthio camelus',\n",
      " 10: 'brambling, Fringilla montifringilla',\n",
      " 11: 'goldfinch, Carduelis carduelis',\n",
      " 12: 'house finch, linnet, Carpodacus mexicanus',\n",
      " 13: 'junco, snowbird',\n",
      " 14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea',\n",
      " 15: 'robin, American robin, Turdus migratorius',\n",
      " 16: 'bulbul',\n",
      " 17: 'jay',\n",
      " 18: 'magpie',\n",
      " 19: 'chickadee',\n",
      " 20: 'water ouzel, dipper',\n",
      " 21: 'kite',\n",
      " 22: 'bald eagle, American eagle, Haliaeetus leucocephalus',\n",
      " 23: 'vulture',\n",
      " 24: 'great grey owl, great gray owl, Strix nebulosa',\n",
      " 25: 'European fire salamander, Salamandra salamandra',\n",
      " 26: 'common newt, Triturus vulgaris',\n",
      " 27: 'eft',\n",
      " 28: 'spotted salamander, Ambystoma maculatum',\n",
      " 29: 'axolotl, mud puppy, Ambystoma mexicanum',\n",
      " 30: 'bullfrog, Rana catesbeiana',\n",
      " 31: 'tree frog, tree-frog',\n",
      " 32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui',\n",
      " 33: 'loggerhead, loggerhead turtle, Caretta caretta',\n",
      " 34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea',\n",
      " 35: 'mud turtle',\n",
      " 36: 'terrapin',\n",
      " 37: 'box turtle, box tortoise',\n",
      " 38: 'banded gecko',\n",
      " 39: 'common iguana, iguana, Iguana iguana',\n",
      " 40: 'American chameleon, anole, Anolis carolinensis',\n",
      " 41: 'whiptail, whiptail lizard',\n",
      " 42: 'agama',\n",
      " 43: 'frilled lizard, Chlamydosaurus kingi',\n",
      " 44: 'alligator lizard',\n",
      " 45: 'Gila monster, Heloderma suspectum',\n",
      " 46: 'green lizard, Lacerta viridis',\n",
      " 47: 'African chameleon, Chamaeleo chamaeleon',\n",
      " 48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis',\n",
      " 49: 'African crocodile, Nile crocodile, Crocodylus niloticus',\n",
      " 50: 'American alligator, Alligator mississipiensis',\n",
      " 51: 'triceratops',\n",
      " 52: 'thunder snake, worm snake, Carphophis amoenus',\n",
      " 53: 'ringneck snake, ring-necked snake, ring snake',\n",
      " 54: 'hognose snake, puff adder, sand viper',\n",
      " 55: 'green snake, grass snake',\n",
      " 56: 'king snake, kingsnake',\n",
      " 57: 'garter snake, grass snake',\n",
      " 58: 'water snake',\n",
      " 59: 'vine snake',\n",
      " 60: 'night snake, Hypsiglena torquata',\n",
      " 61: 'boa constrictor, Constrictor constrictor',\n",
      " 62: 'rock python, rock snake, Python sebae',\n",
      " 63: 'Indian cobra, Naja naja',\n",
      " 64: 'green mamba',\n",
      " 65: 'sea snake',\n",
      " 66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus',\n",
      " 67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus',\n",
      " 68: 'sidewinder, horned rattlesnake, Crotalus cerastes',\n",
      " 69: 'trilobite',\n",
      " 70: 'harvestman, daddy longlegs, Phalangium opilio',\n",
      " 71: 'scorpion',\n",
      " 72: 'black and gold garden spider, Argiope aurantia',\n",
      " 73: 'barn spider, Araneus cavaticus',\n",
      " 74: 'garden spider, Aranea diademata',\n",
      " 75: 'black widow, Latrodectus mactans',\n",
      " 76: 'tarantula',\n",
      " 77: 'wolf spider, hunting spider',\n",
      " 78: 'tick',\n",
      " 79: 'centipede',\n",
      " 80: 'black grouse',\n",
      " 81: 'ptarmigan',\n",
      " 82: 'ruffed grouse, partridge, Bonasa umbellus',\n",
      " 83: 'prairie chicken, prairie grouse, prairie fowl',\n",
      " 84: 'peacock',\n",
      " 85: 'quail',\n",
      " 86: 'partridge',\n",
      " 87: 'African grey, African gray, Psittacus erithacus',\n",
      " 88: 'macaw',\n",
      " 89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n",
      " 90: 'lorikeet',\n",
      " 91: 'coucal',\n",
      " 92: 'bee eater',\n",
      " 93: 'hornbill',\n",
      " 94: 'hummingbird',\n",
      " 95: 'jacamar',\n",
      " 96: 'toucan',\n",
      " 97: 'drake',\n",
      " 98: 'red-breasted merganser, Mergus serrator',\n",
      " 99: 'goose',\n",
      " 100: 'black swan, Cygnus atratus',\n",
      " 101: 'tusker',\n",
      " 102: 'echidna, spiny anteater, anteater',\n",
      " 103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n",
      " 104: 'wallaby, brush kangaroo',\n",
      " 105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n",
      " 106: 'wombat',\n",
      " 107: 'jellyfish',\n",
      " 108: 'sea anemone, anemone',\n",
      " 109: 'brain coral',\n",
      " 110: 'flatworm, platyhelminth',\n",
      " 111: 'nematode, nematode worm, roundworm',\n",
      " 112: 'conch',\n",
      " 113: 'snail',\n",
      " 114: 'slug',\n",
      " 115: 'sea slug, nudibranch',\n",
      " 116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore',\n",
      " 117: 'chambered nautilus, pearly nautilus, nautilus',\n",
      " 118: 'Dungeness crab, Cancer magister',\n",
      " 119: 'rock crab, Cancer irroratus',\n",
      " 120: 'fiddler crab',\n",
      " 121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica',\n",
      " 122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus',\n",
      " 123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish',\n",
      " 124: 'crayfish, crawfish, crawdad, crawdaddy',\n",
      " 125: 'hermit crab',\n",
      " 126: 'isopod',\n",
      " 127: 'white stork, Ciconia ciconia',\n",
      " 128: 'black stork, Ciconia nigra',\n",
      " 129: 'spoonbill',\n",
      " 130: 'flamingo',\n",
      " 131: 'little blue heron, Egretta caerulea',\n",
      " 132: 'American egret, great white heron, Egretta albus',\n",
      " 133: 'bittern',\n",
      " 134: 'crane',\n",
      " 135: 'limpkin, Aramus pictus',\n",
      " 136: 'European gallinule, Porphyrio porphyrio',\n",
      " 137: 'American coot, marsh hen, mud hen, water hen, Fulica americana',\n",
      " 138: 'bustard',\n",
      " 139: 'ruddy turnstone, Arenaria interpres',\n",
      " 140: 'red-backed sandpiper, dunlin, Erolia alpina',\n",
      " 141: 'redshank, Tringa totanus',\n",
      " 142: 'dowitcher',\n",
      " 143: 'oystercatcher, oyster catcher',\n",
      " 144: 'pelican',\n",
      " 145: 'king penguin, Aptenodytes patagonica',\n",
      " 146: 'albatross, mollymawk',\n",
      " 147: 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus',\n",
      " 148: 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca',\n",
      " 149: 'dugong, Dugong dugon',\n",
      " 150: 'sea lion',\n",
      " 151: 'Chihuahua',\n",
      " 152: 'Japanese spaniel',\n",
      " 153: 'Maltese dog, Maltese terrier, Maltese',\n",
      " 154: 'Pekinese, Pekingese, Peke',\n",
      " 155: 'Shih-Tzu',\n",
      " 156: 'Blenheim spaniel',\n",
      " 157: 'papillon',\n",
      " 158: 'toy terrier',\n",
      " 159: 'Rhodesian ridgeback',\n",
      " 160: 'Afghan hound, Afghan',\n",
      " 161: 'basset, basset hound',\n",
      " 162: 'beagle',\n",
      " 163: 'bloodhound, sleuthhound',\n",
      " 164: 'bluetick',\n",
      " 165: 'black-and-tan coonhound',\n",
      " 166: 'Walker hound, Walker foxhound',\n",
      " 167: 'English foxhound',\n",
      " 168: 'redbone',\n",
      " 169: 'borzoi, Russian wolfhound',\n",
      " 170: 'Irish wolfhound',\n",
      " 171: 'Italian greyhound',\n",
      " 172: 'whippet',\n",
      " 173: 'Ibizan hound, Ibizan Podenco',\n",
      " 174: 'Norwegian elkhound, elkhound',\n",
      " 175: 'otterhound, otter hound',\n",
      " 176: 'Saluki, gazelle hound',\n",
      " 177: 'Scottish deerhound, deerhound',\n",
      " 178: 'Weimaraner',\n",
      " 179: 'Staffordshire bullterrier, Staffordshire bull terrier',\n",
      " 180: 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier',\n",
      " 181: 'Bedlington terrier',\n",
      " 182: 'Border terrier',\n",
      " 183: 'Kerry blue terrier',\n",
      " 184: 'Irish terrier',\n",
      " 185: 'Norfolk terrier',\n",
      " 186: 'Norwich terrier',\n",
      " 187: 'Yorkshire terrier',\n",
      " 188: 'wire-haired fox terrier',\n",
      " 189: 'Lakeland terrier',\n",
      " 190: 'Sealyham terrier, Sealyham',\n",
      " 191: 'Airedale, Airedale terrier',\n",
      " 192: 'cairn, cairn terrier',\n",
      " 193: 'Australian terrier',\n",
      " 194: 'Dandie Dinmont, Dandie Dinmont terrier',\n",
      " 195: 'Boston bull, Boston terrier',\n",
      " 196: 'miniature schnauzer',\n",
      " 197: 'giant schnauzer',\n",
      " 198: 'standard schnauzer',\n",
      " 199: 'Scotch terrier, Scottish terrier, Scottie',\n",
      " 200: 'Tibetan terrier, chrysanthemum dog',\n",
      " 201: 'silky terrier, Sydney silky',\n",
      " 202: 'soft-coated wheaten terrier',\n",
      " 203: 'West Highland white terrier',\n",
      " 204: 'Lhasa, Lhasa apso',\n",
      " 205: 'flat-coated retriever',\n",
      " 206: 'curly-coated retriever',\n",
      " 207: 'golden retriever',\n",
      " 208: 'Labrador retriever',\n",
      " 209: 'Chesapeake Bay retriever',\n",
      " 210: 'German short-haired pointer',\n",
      " 211: 'vizsla, Hungarian pointer',\n",
      " 212: 'English setter',\n",
      " 213: 'Irish setter, red setter',\n",
      " 214: 'Gordon setter',\n",
      " 215: 'Brittany spaniel',\n",
      " 216: 'clumber, clumber spaniel',\n",
      " 217: 'English springer, English springer spaniel',\n",
      " 218: 'Welsh springer spaniel',\n",
      " 219: 'cocker spaniel, English cocker spaniel, cocker',\n",
      " 220: 'Sussex spaniel',\n",
      " 221: 'Irish water spaniel',\n",
      " 222: 'kuvasz',\n",
      " 223: 'schipperke',\n",
      " 224: 'groenendael',\n",
      " 225: 'malinois',\n",
      " 226: 'briard',\n",
      " 227: 'kelpie',\n",
      " 228: 'komondor',\n",
      " 229: 'Old English sheepdog, bobtail',\n",
      " 230: 'Shetland sheepdog, Shetland sheep dog, Shetland',\n",
      " 231: 'collie',\n",
      " 232: 'Border collie',\n",
      " 233: 'Bouvier des Flandres, Bouviers des Flandres',\n",
      " 234: 'Rottweiler',\n",
      " 235: 'German shepherd, German shepherd dog, German police dog, alsatian',\n",
      " 236: 'Doberman, Doberman pinscher',\n",
      " 237: 'miniature pinscher',\n",
      " 238: 'Greater Swiss Mountain dog',\n",
      " 239: 'Bernese mountain dog',\n",
      " 240: 'Appenzeller',\n",
      " 241: 'EntleBucher',\n",
      " 242: 'boxer',\n",
      " 243: 'bull mastiff',\n",
      " 244: 'Tibetan mastiff',\n",
      " 245: 'French bulldog',\n",
      " 246: 'Great Dane',\n",
      " 247: 'Saint Bernard, St Bernard',\n",
      " 248: 'Eskimo dog, husky',\n",
      " 249: 'malamute, malemute, Alaskan malamute',\n",
      " 250: 'Siberian husky',\n",
      " 251: 'dalmatian, coach dog, carriage dog',\n",
      " 252: 'affenpinscher, monkey pinscher, monkey dog',\n",
      " 253: 'basenji',\n",
      " 254: 'pug, pug-dog',\n",
      " 255: 'Leonberg',\n",
      " 256: 'Newfoundland, Newfoundland dog',\n",
      " 257: 'Great Pyrenees',\n",
      " 258: 'Samoyed, Samoyede',\n",
      " 259: 'Pomeranian',\n",
      " 260: 'chow, chow chow',\n",
      " 261: 'keeshond',\n",
      " 262: 'Brabancon griffon',\n",
      " 263: 'Pembroke, Pembroke Welsh corgi',\n",
      " 264: 'Cardigan, Cardigan Welsh corgi',\n",
      " 265: 'toy poodle',\n",
      " 266: 'miniature poodle',\n",
      " 267: 'standard poodle',\n",
      " 268: 'Mexican hairless',\n",
      " 269: 'timber wolf, grey wolf, gray wolf, Canis lupus',\n",
      " 270: 'white wolf, Arctic wolf, Canis lupus tundrarum',\n",
      " 271: 'red wolf, maned wolf, Canis rufus, Canis niger',\n",
      " 272: 'coyote, prairie wolf, brush wolf, Canis latrans',\n",
      " 273: 'dingo, warrigal, warragal, Canis dingo',\n",
      " 274: 'dhole, Cuon alpinus',\n",
      " 275: 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus',\n",
      " 276: 'hyena, hyaena',\n",
      " 277: 'red fox, Vulpes vulpes',\n",
      " 278: 'kit fox, Vulpes macrotis',\n",
      " 279: 'Arctic fox, white fox, Alopex lagopus',\n",
      " 280: 'grey fox, gray fox, Urocyon cinereoargenteus',\n",
      " 281: 'tabby, tabby cat',\n",
      " 282: 'tiger cat',\n",
      " 283: 'Persian cat',\n",
      " 284: 'Siamese cat, Siamese',\n",
      " 285: 'Egyptian cat',\n",
      " 286: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n",
      " 287: 'lynx, catamount',\n",
      " 288: 'leopard, Panthera pardus',\n",
      " 289: 'snow leopard, ounce, Panthera uncia',\n",
      " 290: 'jaguar, panther, Panthera onca, Felis onca',\n",
      " 291: 'lion, king of beasts, Panthera leo',\n",
      " 292: 'tiger, Panthera tigris',\n",
      " 293: 'cheetah, chetah, Acinonyx jubatus',\n",
      " 294: 'brown bear, bruin, Ursus arctos',\n",
      " 295: 'American black bear, black bear, Ursus americanus, Euarctos americanus',\n",
      " 296: 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus',\n",
      " 297: 'sloth bear, Melursus ursinus, Ursus ursinus',\n",
      " 298: 'mongoose',\n",
      " 299: 'meerkat, mierkat',\n",
      " 300: 'tiger beetle',\n",
      " 301: 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle',\n",
      " 302: 'ground beetle, carabid beetle',\n",
      " 303: 'long-horned beetle, longicorn, longicorn beetle',\n",
      " 304: 'leaf beetle, chrysomelid',\n",
      " 305: 'dung beetle',\n",
      " 306: 'rhinoceros beetle',\n",
      " 307: 'weevil',\n",
      " 308: 'fly',\n",
      " 309: 'bee',\n",
      " 310: 'ant, emmet, pismire',\n",
      " 311: 'grasshopper, hopper',\n",
      " 312: 'cricket',\n",
      " 313: 'walking stick, walkingstick, stick insect',\n",
      " 314: 'cockroach, roach',\n",
      " 315: 'mantis, mantid',\n",
      " 316: 'cicada, cicala',\n",
      " 317: 'leafhopper',\n",
      " 318: 'lacewing, lacewing fly',\n",
      " 319: \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\",\n",
      " 320: 'damselfly',\n",
      " 321: 'admiral',\n",
      " 322: 'ringlet, ringlet butterfly',\n",
      " 323: 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus',\n",
      " 324: 'cabbage butterfly',\n",
      " 325: 'sulphur butterfly, sulfur butterfly',\n",
      " 326: 'lycaenid, lycaenid butterfly',\n",
      " 327: 'starfish, sea star',\n",
      " 328: 'sea urchin',\n",
      " 329: 'sea cucumber, holothurian',\n",
      " 330: 'wood rabbit, cottontail, cottontail rabbit',\n",
      " 331: 'hare',\n",
      " 332: 'Angora, Angora rabbit',\n",
      " 333: 'hamster',\n",
      " 334: 'porcupine, hedgehog',\n",
      " 335: 'fox squirrel, eastern fox squirrel, Sciurus niger',\n",
      " 336: 'marmot',\n",
      " 337: 'beaver',\n",
      " 338: 'guinea pig, Cavia cobaya',\n",
      " 339: 'sorrel',\n",
      " 340: 'zebra',\n",
      " 341: 'hog, pig, grunter, squealer, Sus scrofa',\n",
      " 342: 'wild boar, boar, Sus scrofa',\n",
      " 343: 'warthog',\n",
      " 344: 'hippopotamus, hippo, river horse, Hippopotamus amphibius',\n",
      " 345: 'ox',\n",
      " 346: 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis',\n",
      " 347: 'bison',\n",
      " 348: 'ram, tup',\n",
      " 349: 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis',\n",
      " 350: 'ibex, Capra ibex',\n",
      " 351: 'hartebeest',\n",
      " 352: 'impala, Aepyceros melampus',\n",
      " 353: 'gazelle',\n",
      " 354: 'Arabian camel, dromedary, Camelus dromedarius',\n",
      " 355: 'llama',\n",
      " 356: 'weasel',\n",
      " 357: 'mink',\n",
      " 358: 'polecat, fitch, foulmart, foumart, Mustela putorius',\n",
      " 359: 'black-footed ferret, ferret, Mustela nigripes',\n",
      " 360: 'otter',\n",
      " 361: 'skunk, polecat, wood pussy',\n",
      " 362: 'badger',\n",
      " 363: 'armadillo',\n",
      " 364: 'three-toed sloth, ai, Bradypus tridactylus',\n",
      " 365: 'orangutan, orang, orangutang, Pongo pygmaeus',\n",
      " 366: 'gorilla, Gorilla gorilla',\n",
      " 367: 'chimpanzee, chimp, Pan troglodytes',\n",
      " 368: 'gibbon, Hylobates lar',\n",
      " 369: 'siamang, Hylobates syndactylus, Symphalangus syndactylus',\n",
      " 370: 'guenon, guenon monkey',\n",
      " 371: 'patas, hussar monkey, Erythrocebus patas',\n",
      " 372: 'baboon',\n",
      " 373: 'macaque',\n",
      " 374: 'langur',\n",
      " 375: 'colobus, colobus monkey',\n",
      " 376: 'proboscis monkey, Nasalis larvatus',\n",
      " 377: 'marmoset',\n",
      " 378: 'capuchin, ringtail, Cebus capucinus',\n",
      " 379: 'howler monkey, howler',\n",
      " 380: 'titi, titi monkey',\n",
      " 381: 'spider monkey, Ateles geoffroyi',\n",
      " 382: 'squirrel monkey, Saimiri sciureus',\n",
      " 383: 'Madagascar cat, ring-tailed lemur, Lemur catta',\n",
      " 384: 'indri, indris, Indri indri, Indri brevicaudatus',\n",
      " 385: 'Indian elephant, Elephas maximus',\n",
      " 386: 'African elephant, Loxodonta africana',\n",
      " 387: 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens',\n",
      " 388: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca',\n",
      " 389: 'barracouta, snoek',\n",
      " 390: 'eel',\n",
      " 391: 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch',\n",
      " 392: 'rock beauty, Holocanthus tricolor',\n",
      " 393: 'anemone fish',\n",
      " 394: 'sturgeon',\n",
      " 395: 'gar, garfish, garpike, billfish, Lepisosteus osseus',\n",
      " 396: 'lionfish',\n",
      " 397: 'puffer, pufferfish, blowfish, globefish',\n",
      " 398: 'abacus',\n",
      " 399: 'abaya',\n",
      " 400: \"academic gown, academic robe, judge's robe\",\n",
      " 401: 'accordion, piano accordion, squeeze box',\n",
      " 402: 'acoustic guitar',\n",
      " 403: 'aircraft carrier, carrier, flattop, attack aircraft carrier',\n",
      " 404: 'airliner',\n",
      " 405: 'airship, dirigible',\n",
      " 406: 'altar',\n",
      " 407: 'ambulance',\n",
      " 408: 'amphibian, amphibious vehicle',\n",
      " 409: 'analog clock',\n",
      " 410: 'apiary, bee house',\n",
      " 411: 'apron',\n",
      " 412: 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin',\n",
      " 413: 'assault rifle, assault gun',\n",
      " 414: 'backpack, back pack, knapsack, packsack, rucksack, haversack',\n",
      " 415: 'bakery, bakeshop, bakehouse',\n",
      " 416: 'balance beam, beam',\n",
      " 417: 'balloon',\n",
      " 418: 'ballpoint, ballpoint pen, ballpen, Biro',\n",
      " 419: 'Band Aid',\n",
      " 420: 'banjo',\n",
      " 421: 'bannister, banister, balustrade, balusters, handrail',\n",
      " 422: 'barbell',\n",
      " 423: 'barber chair',\n",
      " 424: 'barbershop',\n",
      " 425: 'barn',\n",
      " 426: 'barometer',\n",
      " 427: 'barrel, cask',\n",
      " 428: 'barrow, garden cart, lawn cart, wheelbarrow',\n",
      " 429: 'baseball',\n",
      " 430: 'basketball',\n",
      " 431: 'bassinet',\n",
      " 432: 'bassoon',\n",
      " 433: 'bathing cap, swimming cap',\n",
      " 434: 'bath towel',\n",
      " 435: 'bathtub, bathing tub, bath, tub',\n",
      " 436: 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon',\n",
      " 437: 'beacon, lighthouse, beacon light, pharos',\n",
      " 438: 'beaker',\n",
      " 439: 'bearskin, busby, shako',\n",
      " 440: 'beer bottle',\n",
      " 441: 'beer glass',\n",
      " 442: 'bell cote, bell cot',\n",
      " 443: 'bib',\n",
      " 444: 'bicycle-built-for-two, tandem bicycle, tandem',\n",
      " 445: 'bikini, two-piece',\n",
      " 446: 'binder, ring-binder',\n",
      " 447: 'binoculars, field glasses, opera glasses',\n",
      " 448: 'birdhouse',\n",
      " 449: 'boathouse',\n",
      " 450: 'bobsled, bobsleigh, bob',\n",
      " 451: 'bolo tie, bolo, bola tie, bola',\n",
      " 452: 'bonnet, poke bonnet',\n",
      " 453: 'bookcase',\n",
      " 454: 'bookshop, bookstore, bookstall',\n",
      " 455: 'bottlecap',\n",
      " 456: 'bow',\n",
      " 457: 'bow tie, bow-tie, bowtie',\n",
      " 458: 'brass, memorial tablet, plaque',\n",
      " 459: 'brassiere, bra, bandeau',\n",
      " 460: 'breakwater, groin, groyne, mole, bulwark, seawall, jetty',\n",
      " 461: 'breastplate, aegis, egis',\n",
      " 462: 'broom',\n",
      " 463: 'bucket, pail',\n",
      " 464: 'buckle',\n",
      " 465: 'bulletproof vest',\n",
      " 466: 'bullet train, bullet',\n",
      " 467: 'butcher shop, meat market',\n",
      " 468: 'cab, hack, taxi, taxicab',\n",
      " 469: 'caldron, cauldron',\n",
      " 470: 'candle, taper, wax light',\n",
      " 471: 'cannon',\n",
      " 472: 'canoe',\n",
      " 473: 'can opener, tin opener',\n",
      " 474: 'cardigan',\n",
      " 475: 'car mirror',\n",
      " 476: 'carousel, carrousel, merry-go-round, roundabout, whirligig',\n",
      " 477: \"carpenter's kit, tool kit\",\n",
      " 478: 'carton',\n",
      " 479: 'car wheel',\n",
      " 480: 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM',\n",
      " 481: 'cassette',\n",
      " 482: 'cassette player',\n",
      " 483: 'castle',\n",
      " 484: 'catamaran',\n",
      " 485: 'CD player',\n",
      " 486: 'cello, violoncello',\n",
      " 487: 'cellular telephone, cellular phone, cellphone, cell, mobile phone',\n",
      " 488: 'chain',\n",
      " 489: 'chainlink fence',\n",
      " 490: 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour',\n",
      " 491: 'chain saw, chainsaw',\n",
      " 492: 'chest',\n",
      " 493: 'chiffonier, commode',\n",
      " 494: 'chime, bell, gong',\n",
      " 495: 'china cabinet, china closet',\n",
      " 496: 'Christmas stocking',\n",
      " 497: 'church, church building',\n",
      " 498: 'cinema, movie theater, movie theatre, movie house, picture palace',\n",
      " 499: 'cleaver, meat cleaver, chopper',\n",
      " 500: 'cliff dwelling',\n",
      " 501: 'cloak',\n",
      " 502: 'clog, geta, patten, sabot',\n",
      " 503: 'cocktail shaker',\n",
      " 504: 'coffee mug',\n",
      " 505: 'coffeepot',\n",
      " 506: 'coil, spiral, volute, whorl, helix',\n",
      " 507: 'combination lock',\n",
      " 508: 'computer keyboard, keypad',\n",
      " 509: 'confectionery, confectionary, candy store',\n",
      " 510: 'container ship, containership, container vessel',\n",
      " 511: 'convertible',\n",
      " 512: 'corkscrew, bottle screw',\n",
      " 513: 'cornet, horn, trumpet, trump',\n",
      " 514: 'cowboy boot',\n",
      " 515: 'cowboy hat, ten-gallon hat',\n",
      " 516: 'cradle',\n",
      " 517: 'crane',\n",
      " 518: 'crash helmet',\n",
      " 519: 'crate',\n",
      " 520: 'crib, cot',\n",
      " 521: 'Crock Pot',\n",
      " 522: 'croquet ball',\n",
      " 523: 'crutch',\n",
      " 524: 'cuirass',\n",
      " 525: 'dam, dike, dyke',\n",
      " 526: 'desk',\n",
      " 527: 'desktop computer',\n",
      " 528: 'dial telephone, dial phone',\n",
      " 529: 'diaper, nappy, napkin',\n",
      " 530: 'digital clock',\n",
      " 531: 'digital watch',\n",
      " 532: 'dining table, board',\n",
      " 533: 'dishrag, dishcloth',\n",
      " 534: 'dishwasher, dish washer, dishwashing machine',\n",
      " 535: 'disk brake, disc brake',\n",
      " 536: 'dock, dockage, docking facility',\n",
      " 537: 'dogsled, dog sled, dog sleigh',\n",
      " 538: 'dome',\n",
      " 539: 'doormat, welcome mat',\n",
      " 540: 'drilling platform, offshore rig',\n",
      " 541: 'drum, membranophone, tympan',\n",
      " 542: 'drumstick',\n",
      " 543: 'dumbbell',\n",
      " 544: 'Dutch oven',\n",
      " 545: 'electric fan, blower',\n",
      " 546: 'electric guitar',\n",
      " 547: 'electric locomotive',\n",
      " 548: 'entertainment center',\n",
      " 549: 'envelope',\n",
      " 550: 'espresso maker',\n",
      " 551: 'face powder',\n",
      " 552: 'feather boa, boa',\n",
      " 553: 'file, file cabinet, filing cabinet',\n",
      " 554: 'fireboat',\n",
      " 555: 'fire engine, fire truck',\n",
      " 556: 'fire screen, fireguard',\n",
      " 557: 'flagpole, flagstaff',\n",
      " 558: 'flute, transverse flute',\n",
      " 559: 'folding chair',\n",
      " 560: 'football helmet',\n",
      " 561: 'forklift',\n",
      " 562: 'fountain',\n",
      " 563: 'fountain pen',\n",
      " 564: 'four-poster',\n",
      " 565: 'freight car',\n",
      " 566: 'French horn, horn',\n",
      " 567: 'frying pan, frypan, skillet',\n",
      " 568: 'fur coat',\n",
      " 569: 'garbage truck, dustcart',\n",
      " 570: 'gasmask, respirator, gas helmet',\n",
      " 571: 'gas pump, gasoline pump, petrol pump, island dispenser',\n",
      " 572: 'goblet',\n",
      " 573: 'go-kart',\n",
      " 574: 'golf ball',\n",
      " 575: 'golfcart, golf cart',\n",
      " 576: 'gondola',\n",
      " 577: 'gong, tam-tam',\n",
      " 578: 'gown',\n",
      " 579: 'grand piano, grand',\n",
      " 580: 'greenhouse, nursery, glasshouse',\n",
      " 581: 'grille, radiator grille',\n",
      " 582: 'grocery store, grocery, food market, market',\n",
      " 583: 'guillotine',\n",
      " 584: 'hair slide',\n",
      " 585: 'hair spray',\n",
      " 586: 'half track',\n",
      " 587: 'hammer',\n",
      " 588: 'hamper',\n",
      " 589: 'hand blower, blow dryer, blow drier, hair dryer, hair drier',\n",
      " 590: 'hand-held computer, hand-held microcomputer',\n",
      " 591: 'handkerchief, hankie, hanky, hankey',\n",
      " 592: 'hard disc, hard disk, fixed disk',\n",
      " 593: 'harmonica, mouth organ, harp, mouth harp',\n",
      " 594: 'harp',\n",
      " 595: 'harvester, reaper',\n",
      " 596: 'hatchet',\n",
      " 597: 'holster',\n",
      " 598: 'home theater, home theatre',\n",
      " 599: 'honeycomb',\n",
      " 600: 'hook, claw',\n",
      " 601: 'hoopskirt, crinoline',\n",
      " 602: 'horizontal bar, high bar',\n",
      " 603: 'horse cart, horse-cart',\n",
      " 604: 'hourglass',\n",
      " 605: 'iPod',\n",
      " 606: 'iron, smoothing iron',\n",
      " 607: \"jack-o'-lantern\",\n",
      " 608: 'jean, blue jean, denim',\n",
      " 609: 'jeep, landrover',\n",
      " 610: 'jersey, T-shirt, tee shirt',\n",
      " 611: 'jigsaw puzzle',\n",
      " 612: 'jinrikisha, ricksha, rickshaw',\n",
      " 613: 'joystick',\n",
      " 614: 'kimono',\n",
      " 615: 'knee pad',\n",
      " 616: 'knot',\n",
      " 617: 'lab coat, laboratory coat',\n",
      " 618: 'ladle',\n",
      " 619: 'lampshade, lamp shade',\n",
      " 620: 'laptop, laptop computer',\n",
      " 621: 'lawn mower, mower',\n",
      " 622: 'lens cap, lens cover',\n",
      " 623: 'letter opener, paper knife, paperknife',\n",
      " 624: 'library',\n",
      " 625: 'lifeboat',\n",
      " 626: 'lighter, light, igniter, ignitor',\n",
      " 627: 'limousine, limo',\n",
      " 628: 'liner, ocean liner',\n",
      " 629: 'lipstick, lip rouge',\n",
      " 630: 'Loafer',\n",
      " 631: 'lotion',\n",
      " 632: 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system',\n",
      " 633: \"loupe, jeweler's loupe\",\n",
      " 634: 'lumbermill, sawmill',\n",
      " 635: 'magnetic compass',\n",
      " 636: 'mailbag, postbag',\n",
      " 637: 'mailbox, letter box',\n",
      " 638: 'maillot',\n",
      " 639: 'maillot, tank suit',\n",
      " 640: 'manhole cover',\n",
      " 641: 'maraca',\n",
      " 642: 'marimba, xylophone',\n",
      " 643: 'mask',\n",
      " 644: 'matchstick',\n",
      " 645: 'maypole',\n",
      " 646: 'maze, labyrinth',\n",
      " 647: 'measuring cup',\n",
      " 648: 'medicine chest, medicine cabinet',\n",
      " 649: 'megalith, megalithic structure',\n",
      " 650: 'microphone, mike',\n",
      " 651: 'microwave, microwave oven',\n",
      " 652: 'military uniform',\n",
      " 653: 'milk can',\n",
      " 654: 'minibus',\n",
      " 655: 'miniskirt, mini',\n",
      " 656: 'minivan',\n",
      " 657: 'missile',\n",
      " 658: 'mitten',\n",
      " 659: 'mixing bowl',\n",
      " 660: 'mobile home, manufactured home',\n",
      " 661: 'Model T',\n",
      " 662: 'modem',\n",
      " 663: 'monastery',\n",
      " 664: 'monitor',\n",
      " 665: 'moped',\n",
      " 666: 'mortar',\n",
      " 667: 'mortarboard',\n",
      " 668: 'mosque',\n",
      " 669: 'mosquito net',\n",
      " 670: 'motor scooter, scooter',\n",
      " 671: 'mountain bike, all-terrain bike, off-roader',\n",
      " 672: 'mountain tent',\n",
      " 673: 'mouse, computer mouse',\n",
      " 674: 'mousetrap',\n",
      " 675: 'moving van',\n",
      " 676: 'muzzle',\n",
      " 677: 'nail',\n",
      " 678: 'neck brace',\n",
      " 679: 'necklace',\n",
      " 680: 'nipple',\n",
      " 681: 'notebook, notebook computer',\n",
      " 682: 'obelisk',\n",
      " 683: 'oboe, hautboy, hautbois',\n",
      " 684: 'ocarina, sweet potato',\n",
      " 685: 'odometer, hodometer, mileometer, milometer',\n",
      " 686: 'oil filter',\n",
      " 687: 'organ, pipe organ',\n",
      " 688: 'oscilloscope, scope, cathode-ray oscilloscope, CRO',\n",
      " 689: 'overskirt',\n",
      " 690: 'oxcart',\n",
      " 691: 'oxygen mask',\n",
      " 692: 'packet',\n",
      " 693: 'paddle, boat paddle',\n",
      " 694: 'paddlewheel, paddle wheel',\n",
      " 695: 'padlock',\n",
      " 696: 'paintbrush',\n",
      " 697: \"pajama, pyjama, pj's, jammies\",\n",
      " 698: 'palace',\n",
      " 699: 'panpipe, pandean pipe, syrinx',\n",
      " 700: 'paper towel',\n",
      " 701: 'parachute, chute',\n",
      " 702: 'parallel bars, bars',\n",
      " 703: 'park bench',\n",
      " 704: 'parking meter',\n",
      " 705: 'passenger car, coach, carriage',\n",
      " 706: 'patio, terrace',\n",
      " 707: 'pay-phone, pay-station',\n",
      " 708: 'pedestal, plinth, footstall',\n",
      " 709: 'pencil box, pencil case',\n",
      " 710: 'pencil sharpener',\n",
      " 711: 'perfume, essence',\n",
      " 712: 'Petri dish',\n",
      " 713: 'photocopier',\n",
      " 714: 'pick, plectrum, plectron',\n",
      " 715: 'pickelhaube',\n",
      " 716: 'picket fence, paling',\n",
      " 717: 'pickup, pickup truck',\n",
      " 718: 'pier',\n",
      " 719: 'piggy bank, penny bank',\n",
      " 720: 'pill bottle',\n",
      " 721: 'pillow',\n",
      " 722: 'ping-pong ball',\n",
      " 723: 'pinwheel',\n",
      " 724: 'pirate, pirate ship',\n",
      " 725: 'pitcher, ewer',\n",
      " 726: \"plane, carpenter's plane, woodworking plane\",\n",
      " 727: 'planetarium',\n",
      " 728: 'plastic bag',\n",
      " 729: 'plate rack',\n",
      " 730: 'plow, plough',\n",
      " 731: \"plunger, plumber's helper\",\n",
      " 732: 'Polaroid camera, Polaroid Land camera',\n",
      " 733: 'pole',\n",
      " 734: 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria',\n",
      " 735: 'poncho',\n",
      " 736: 'pool table, billiard table, snooker table',\n",
      " 737: 'pop bottle, soda bottle',\n",
      " 738: 'pot, flowerpot',\n",
      " 739: \"potter's wheel\",\n",
      " 740: 'power drill',\n",
      " 741: 'prayer rug, prayer mat',\n",
      " 742: 'printer',\n",
      " 743: 'prison, prison house',\n",
      " 744: 'projectile, missile',\n",
      " 745: 'projector',\n",
      " 746: 'puck, hockey puck',\n",
      " 747: 'punching bag, punch bag, punching ball, punchball',\n",
      " 748: 'purse',\n",
      " 749: 'quill, quill pen',\n",
      " 750: 'quilt, comforter, comfort, puff',\n",
      " 751: 'racer, race car, racing car',\n",
      " 752: 'racket, racquet',\n",
      " 753: 'radiator',\n",
      " 754: 'radio, wireless',\n",
      " 755: 'radio telescope, radio reflector',\n",
      " 756: 'rain barrel',\n",
      " 757: 'recreational vehicle, RV, R.V.',\n",
      " 758: 'reel',\n",
      " 759: 'reflex camera',\n",
      " 760: 'refrigerator, icebox',\n",
      " 761: 'remote control, remote',\n",
      " 762: 'restaurant, eating house, eating place, eatery',\n",
      " 763: 'revolver, six-gun, six-shooter',\n",
      " 764: 'rifle',\n",
      " 765: 'rocking chair, rocker',\n",
      " 766: 'rotisserie',\n",
      " 767: 'rubber eraser, rubber, pencil eraser',\n",
      " 768: 'rugby ball',\n",
      " 769: 'rule, ruler',\n",
      " 770: 'running shoe',\n",
      " 771: 'safe',\n",
      " 772: 'safety pin',\n",
      " 773: 'saltshaker, salt shaker',\n",
      " 774: 'sandal',\n",
      " 775: 'sarong',\n",
      " 776: 'sax, saxophone',\n",
      " 777: 'scabbard',\n",
      " 778: 'scale, weighing machine',\n",
      " 779: 'school bus',\n",
      " 780: 'schooner',\n",
      " 781: 'scoreboard',\n",
      " 782: 'screen, CRT screen',\n",
      " 783: 'screw',\n",
      " 784: 'screwdriver',\n",
      " 785: 'seat belt, seatbelt',\n",
      " 786: 'sewing machine',\n",
      " 787: 'shield, buckler',\n",
      " 788: 'shoe shop, shoe-shop, shoe store',\n",
      " 789: 'shoji',\n",
      " 790: 'shopping basket',\n",
      " 791: 'shopping cart',\n",
      " 792: 'shovel',\n",
      " 793: 'shower cap',\n",
      " 794: 'shower curtain',\n",
      " 795: 'ski',\n",
      " 796: 'ski mask',\n",
      " 797: 'sleeping bag',\n",
      " 798: 'slide rule, slipstick',\n",
      " 799: 'sliding door',\n",
      " 800: 'slot, one-armed bandit',\n",
      " 801: 'snorkel',\n",
      " 802: 'snowmobile',\n",
      " 803: 'snowplow, snowplough',\n",
      " 804: 'soap dispenser',\n",
      " 805: 'soccer ball',\n",
      " 806: 'sock',\n",
      " 807: 'solar dish, solar collector, solar furnace',\n",
      " 808: 'sombrero',\n",
      " 809: 'soup bowl',\n",
      " 810: 'space bar',\n",
      " 811: 'space heater',\n",
      " 812: 'space shuttle',\n",
      " 813: 'spatula',\n",
      " 814: 'speedboat',\n",
      " 815: \"spider web, spider's web\",\n",
      " 816: 'spindle',\n",
      " 817: 'sports car, sport car',\n",
      " 818: 'spotlight, spot',\n",
      " 819: 'stage',\n",
      " 820: 'steam locomotive',\n",
      " 821: 'steel arch bridge',\n",
      " 822: 'steel drum',\n",
      " 823: 'stethoscope',\n",
      " 824: 'stole',\n",
      " 825: 'stone wall',\n",
      " 826: 'stopwatch, stop watch',\n",
      " 827: 'stove',\n",
      " 828: 'strainer',\n",
      " 829: 'streetcar, tram, tramcar, trolley, trolley car',\n",
      " 830: 'stretcher',\n",
      " 831: 'studio couch, day bed',\n",
      " 832: 'stupa, tope',\n",
      " 833: 'submarine, pigboat, sub, U-boat',\n",
      " 834: 'suit, suit of clothes',\n",
      " 835: 'sundial',\n",
      " 836: 'sunglass',\n",
      " 837: 'sunglasses, dark glasses, shades',\n",
      " 838: 'sunscreen, sunblock, sun blocker',\n",
      " 839: 'suspension bridge',\n",
      " 840: 'swab, swob, mop',\n",
      " 841: 'sweatshirt',\n",
      " 842: 'swimming trunks, bathing trunks',\n",
      " 843: 'swing',\n",
      " 844: 'switch, electric switch, electrical switch',\n",
      " 845: 'syringe',\n",
      " 846: 'table lamp',\n",
      " 847: 'tank, army tank, armored combat vehicle, armoured combat vehicle',\n",
      " 848: 'tape player',\n",
      " 849: 'teapot',\n",
      " 850: 'teddy, teddy bear',\n",
      " 851: 'television, television system',\n",
      " 852: 'tennis ball',\n",
      " 853: 'thatch, thatched roof',\n",
      " 854: 'theater curtain, theatre curtain',\n",
      " 855: 'thimble',\n",
      " 856: 'thresher, thrasher, threshing machine',\n",
      " 857: 'throne',\n",
      " 858: 'tile roof',\n",
      " 859: 'toaster',\n",
      " 860: 'tobacco shop, tobacconist shop, tobacconist',\n",
      " 861: 'toilet seat',\n",
      " 862: 'torch',\n",
      " 863: 'totem pole',\n",
      " 864: 'tow truck, tow car, wrecker',\n",
      " 865: 'toyshop',\n",
      " 866: 'tractor',\n",
      " 867: 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi',\n",
      " 868: 'tray',\n",
      " 869: 'trench coat',\n",
      " 870: 'tricycle, trike, velocipede',\n",
      " 871: 'trimaran',\n",
      " 872: 'tripod',\n",
      " 873: 'triumphal arch',\n",
      " 874: 'trolleybus, trolley coach, trackless trolley',\n",
      " 875: 'trombone',\n",
      " 876: 'tub, vat',\n",
      " 877: 'turnstile',\n",
      " 878: 'typewriter keyboard',\n",
      " 879: 'umbrella',\n",
      " 880: 'unicycle, monocycle',\n",
      " 881: 'upright, upright piano',\n",
      " 882: 'vacuum, vacuum cleaner',\n",
      " 883: 'vase',\n",
      " 884: 'vault',\n",
      " 885: 'velvet',\n",
      " 886: 'vending machine',\n",
      " 887: 'vestment',\n",
      " 888: 'viaduct',\n",
      " 889: 'violin, fiddle',\n",
      " 890: 'volleyball',\n",
      " 891: 'waffle iron',\n",
      " 892: 'wall clock',\n",
      " 893: 'wallet, billfold, notecase, pocketbook',\n",
      " 894: 'wardrobe, closet, press',\n",
      " 895: 'warplane, military plane',\n",
      " 896: 'washbasin, handbasin, washbowl, lavabo, wash-hand basin',\n",
      " 897: 'washer, automatic washer, washing machine',\n",
      " 898: 'water bottle',\n",
      " 899: 'water jug',\n",
      " 900: 'water tower',\n",
      " 901: 'whiskey jug',\n",
      " 902: 'whistle',\n",
      " 903: 'wig',\n",
      " 904: 'window screen',\n",
      " 905: 'window shade',\n",
      " 906: 'Windsor tie',\n",
      " 907: 'wine bottle',\n",
      " 908: 'wing',\n",
      " 909: 'wok',\n",
      " 910: 'wooden spoon',\n",
      " 911: 'wool, woolen, woollen',\n",
      " 912: 'worm fence, snake fence, snake-rail fence, Virginia fence',\n",
      " 913: 'wreck',\n",
      " 914: 'yawl',\n",
      " 915: 'yurt',\n",
      " 916: 'web site, website, internet site, site',\n",
      " 917: 'comic book',\n",
      " 918: 'crossword puzzle, crossword',\n",
      " 919: 'street sign',\n",
      " 920: 'traffic light, traffic signal, stoplight',\n",
      " 921: 'book jacket, dust cover, dust jacket, dust wrapper',\n",
      " 922: 'menu',\n",
      " 923: 'plate',\n",
      " 924: 'guacamole',\n",
      " 925: 'consomme',\n",
      " 926: 'hot pot, hotpot',\n",
      " 927: 'trifle',\n",
      " 928: 'ice cream, icecream',\n",
      " 929: 'ice lolly, lolly, lollipop, popsicle',\n",
      " 930: 'French loaf',\n",
      " 931: 'bagel, beigel',\n",
      " 932: 'pretzel',\n",
      " 933: 'cheeseburger',\n",
      " 934: 'hotdog, hot dog, red hot',\n",
      " 935: 'mashed potato',\n",
      " 936: 'head cabbage',\n",
      " 937: 'broccoli',\n",
      " 938: 'cauliflower',\n",
      " 939: 'zucchini, courgette',\n",
      " 940: 'spaghetti squash',\n",
      " 941: 'acorn squash',\n",
      " 942: 'butternut squash',\n",
      " 943: 'cucumber, cuke',\n",
      " 944: 'artichoke, globe artichoke',\n",
      " 945: 'bell pepper',\n",
      " 946: 'cardoon',\n",
      " 947: 'mushroom',\n",
      " 948: 'Granny Smith',\n",
      " 949: 'strawberry',\n",
      " 950: 'orange',\n",
      " 951: 'lemon',\n",
      " 952: 'fig',\n",
      " 953: 'pineapple, ananas',\n",
      " 954: 'banana',\n",
      " 955: 'jackfruit, jak, jack',\n",
      " 956: 'custard apple',\n",
      " 957: 'pomegranate',\n",
      " 958: 'hay',\n",
      " 959: 'carbonara',\n",
      " 960: 'chocolate sauce, chocolate syrup',\n",
      " 961: 'dough',\n",
      " 962: 'meat loaf, meatloaf',\n",
      " 963: 'pizza, pizza pie',\n",
      " 964: 'potpie',\n",
      " 965: 'burrito',\n",
      " 966: 'red wine',\n",
      " 967: 'espresso',\n",
      " 968: 'cup',\n",
      " 969: 'eggnog',\n",
      " 970: 'alp',\n",
      " 971: 'bubble',\n",
      " 972: 'cliff, drop, drop-off',\n",
      " 973: 'coral reef',\n",
      " 974: 'geyser',\n",
      " 975: 'lakeside, lakeshore',\n",
      " 976: 'promontory, headland, head, foreland',\n",
      " 977: 'sandbar, sand bar',\n",
      " 978: 'seashore, coast, seacoast, sea-coast',\n",
      " 979: 'valley, vale',\n",
      " 980: 'volcano',\n",
      " 981: 'ballplayer, baseball player',\n",
      " 982: 'groom, bridegroom',\n",
      " 983: 'scuba diver',\n",
      " 984: 'rapeseed',\n",
      " 985: 'daisy',\n",
      " 986: \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\",\n",
      " 987: 'corn',\n",
      " 988: 'acorn',\n",
      " 989: 'hip, rose hip, rosehip',\n",
      " 990: 'buckeye, horse chestnut, conker',\n",
      " 991: 'coral fungus',\n",
      " 992: 'agaric',\n",
      " 993: 'gyromitra',\n",
      " 994: 'stinkhorn, carrion fungus',\n",
      " 995: 'earthstar',\n",
      " 996: 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa',\n",
      " 997: 'bolete',\n",
      " 998: 'ear, spike, capitulum',\n",
      " 999: 'toilet tissue, toilet paper, bathroom tissue'}\n",
      "68/4:\n",
      "from tensorflow.keras.applications import EfficientNetB4\n",
      "from tensorflow.image import resize\n",
      "model = EfficientNetB4(weights='imagenet')\n",
      "\n",
      "def center_crop(img, new_width=None, new_height=None):        \n",
      "\n",
      "    width = img.shape[1]\n",
      "    height = img.shape[0]\n",
      "\n",
      "    if new_width is None:\n",
      "        new_width = min(width, height)\n",
      "\n",
      "    if new_height is None:\n",
      "        new_height = min(width, height)\n",
      "\n",
      "    left = int(np.ceil((width - new_width) / 2))\n",
      "    right = width - int(np.floor((width - new_width) / 2))\n",
      "\n",
      "    top = int(np.ceil((height - new_height) / 2))\n",
      "    bottom = height - int(np.floor((height - new_height) / 2))\n",
      "\n",
      "    if len(img.shape) == 2:\n",
      "        center_cropped_img = img[top:bottom, left:right]\n",
      "    else:\n",
      "        center_cropped_img = img[top:bottom, left:right, ...]\n",
      "\n",
      "    return center_cropped_img\n",
      "\n",
      "class_dict = {0: 'tench, Tinca tinca',\n",
      " 1: 'goldfish, Carassius auratus',\n",
      " 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n",
      " 3: 'tiger shark, Galeocerdo cuvieri',\n",
      " 4: 'hammerhead, hammerhead shark',\n",
      " 5: 'electric ray, crampfish, numbfish, torpedo',\n",
      " 6: 'stingray',\n",
      " 7: 'cock',\n",
      " 8: 'hen',\n",
      " 9: 'ostrich, Struthio camelus',\n",
      " 10: 'brambling, Fringilla montifringilla',\n",
      " 11: 'goldfinch, Carduelis carduelis',\n",
      " 12: 'house finch, linnet, Carpodacus mexicanus',\n",
      " 13: 'junco, snowbird',\n",
      " 14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea',\n",
      " 15: 'robin, American robin, Turdus migratorius',\n",
      " 16: 'bulbul',\n",
      " 17: 'jay',\n",
      " 18: 'magpie',\n",
      " 19: 'chickadee',\n",
      " 20: 'water ouzel, dipper',\n",
      " 21: 'kite',\n",
      " 22: 'bald eagle, American eagle, Haliaeetus leucocephalus',\n",
      " 23: 'vulture',\n",
      " 24: 'great grey owl, great gray owl, Strix nebulosa',\n",
      " 25: 'European fire salamander, Salamandra salamandra',\n",
      " 26: 'common newt, Triturus vulgaris',\n",
      " 27: 'eft',\n",
      " 28: 'spotted salamander, Ambystoma maculatum',\n",
      " 29: 'axolotl, mud puppy, Ambystoma mexicanum',\n",
      " 30: 'bullfrog, Rana catesbeiana',\n",
      " 31: 'tree frog, tree-frog',\n",
      " 32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui',\n",
      " 33: 'loggerhead, loggerhead turtle, Caretta caretta',\n",
      " 34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea',\n",
      " 35: 'mud turtle',\n",
      " 36: 'terrapin',\n",
      " 37: 'box turtle, box tortoise',\n",
      " 38: 'banded gecko',\n",
      " 39: 'common iguana, iguana, Iguana iguana',\n",
      " 40: 'American chameleon, anole, Anolis carolinensis',\n",
      " 41: 'whiptail, whiptail lizard',\n",
      " 42: 'agama',\n",
      " 43: 'frilled lizard, Chlamydosaurus kingi',\n",
      " 44: 'alligator lizard',\n",
      " 45: 'Gila monster, Heloderma suspectum',\n",
      " 46: 'green lizard, Lacerta viridis',\n",
      " 47: 'African chameleon, Chamaeleo chamaeleon',\n",
      " 48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis',\n",
      " 49: 'African crocodile, Nile crocodile, Crocodylus niloticus',\n",
      " 50: 'American alligator, Alligator mississipiensis',\n",
      " 51: 'triceratops',\n",
      " 52: 'thunder snake, worm snake, Carphophis amoenus',\n",
      " 53: 'ringneck snake, ring-necked snake, ring snake',\n",
      " 54: 'hognose snake, puff adder, sand viper',\n",
      " 55: 'green snake, grass snake',\n",
      " 56: 'king snake, kingsnake',\n",
      " 57: 'garter snake, grass snake',\n",
      " 58: 'water snake',\n",
      " 59: 'vine snake',\n",
      " 60: 'night snake, Hypsiglena torquata',\n",
      " 61: 'boa constrictor, Constrictor constrictor',\n",
      " 62: 'rock python, rock snake, Python sebae',\n",
      " 63: 'Indian cobra, Naja naja',\n",
      " 64: 'green mamba',\n",
      " 65: 'sea snake',\n",
      " 66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus',\n",
      " 67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus',\n",
      " 68: 'sidewinder, horned rattlesnake, Crotalus cerastes',\n",
      " 69: 'trilobite',\n",
      " 70: 'harvestman, daddy longlegs, Phalangium opilio',\n",
      " 71: 'scorpion',\n",
      " 72: 'black and gold garden spider, Argiope aurantia',\n",
      " 73: 'barn spider, Araneus cavaticus',\n",
      " 74: 'garden spider, Aranea diademata',\n",
      " 75: 'black widow, Latrodectus mactans',\n",
      " 76: 'tarantula',\n",
      " 77: 'wolf spider, hunting spider',\n",
      " 78: 'tick',\n",
      " 79: 'centipede',\n",
      " 80: 'black grouse',\n",
      " 81: 'ptarmigan',\n",
      " 82: 'ruffed grouse, partridge, Bonasa umbellus',\n",
      " 83: 'prairie chicken, prairie grouse, prairie fowl',\n",
      " 84: 'peacock',\n",
      " 85: 'quail',\n",
      " 86: 'partridge',\n",
      " 87: 'African grey, African gray, Psittacus erithacus',\n",
      " 88: 'macaw',\n",
      " 89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n",
      " 90: 'lorikeet',\n",
      " 91: 'coucal',\n",
      " 92: 'bee eater',\n",
      " 93: 'hornbill',\n",
      " 94: 'hummingbird',\n",
      " 95: 'jacamar',\n",
      " 96: 'toucan',\n",
      " 97: 'drake',\n",
      " 98: 'red-breasted merganser, Mergus serrator',\n",
      " 99: 'goose',\n",
      " 100: 'black swan, Cygnus atratus',\n",
      " 101: 'tusker',\n",
      " 102: 'echidna, spiny anteater, anteater',\n",
      " 103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n",
      " 104: 'wallaby, brush kangaroo',\n",
      " 105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n",
      " 106: 'wombat',\n",
      " 107: 'jellyfish',\n",
      " 108: 'sea anemone, anemone',\n",
      " 109: 'brain coral',\n",
      " 110: 'flatworm, platyhelminth',\n",
      " 111: 'nematode, nematode worm, roundworm',\n",
      " 112: 'conch',\n",
      " 113: 'snail',\n",
      " 114: 'slug',\n",
      " 115: 'sea slug, nudibranch',\n",
      " 116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore',\n",
      " 117: 'chambered nautilus, pearly nautilus, nautilus',\n",
      " 118: 'Dungeness crab, Cancer magister',\n",
      " 119: 'rock crab, Cancer irroratus',\n",
      " 120: 'fiddler crab',\n",
      " 121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica',\n",
      " 122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus',\n",
      " 123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish',\n",
      " 124: 'crayfish, crawfish, crawdad, crawdaddy',\n",
      " 125: 'hermit crab',\n",
      " 126: 'isopod',\n",
      " 127: 'white stork, Ciconia ciconia',\n",
      " 128: 'black stork, Ciconia nigra',\n",
      " 129: 'spoonbill',\n",
      " 130: 'flamingo',\n",
      " 131: 'little blue heron, Egretta caerulea',\n",
      " 132: 'American egret, great white heron, Egretta albus',\n",
      " 133: 'bittern',\n",
      " 134: 'crane',\n",
      " 135: 'limpkin, Aramus pictus',\n",
      " 136: 'European gallinule, Porphyrio porphyrio',\n",
      " 137: 'American coot, marsh hen, mud hen, water hen, Fulica americana',\n",
      " 138: 'bustard',\n",
      " 139: 'ruddy turnstone, Arenaria interpres',\n",
      " 140: 'red-backed sandpiper, dunlin, Erolia alpina',\n",
      " 141: 'redshank, Tringa totanus',\n",
      " 142: 'dowitcher',\n",
      " 143: 'oystercatcher, oyster catcher',\n",
      " 144: 'pelican',\n",
      " 145: 'king penguin, Aptenodytes patagonica',\n",
      " 146: 'albatross, mollymawk',\n",
      " 147: 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus',\n",
      " 148: 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca',\n",
      " 149: 'dugong, Dugong dugon',\n",
      " 150: 'sea lion',\n",
      " 151: 'Chihuahua',\n",
      " 152: 'Japanese spaniel',\n",
      " 153: 'Maltese dog, Maltese terrier, Maltese',\n",
      " 154: 'Pekinese, Pekingese, Peke',\n",
      " 155: 'Shih-Tzu',\n",
      " 156: 'Blenheim spaniel',\n",
      " 157: 'papillon',\n",
      " 158: 'toy terrier',\n",
      " 159: 'Rhodesian ridgeback',\n",
      " 160: 'Afghan hound, Afghan',\n",
      " 161: 'basset, basset hound',\n",
      " 162: 'beagle',\n",
      " 163: 'bloodhound, sleuthhound',\n",
      " 164: 'bluetick',\n",
      " 165: 'black-and-tan coonhound',\n",
      " 166: 'Walker hound, Walker foxhound',\n",
      " 167: 'English foxhound',\n",
      " 168: 'redbone',\n",
      " 169: 'borzoi, Russian wolfhound',\n",
      " 170: 'Irish wolfhound',\n",
      " 171: 'Italian greyhound',\n",
      " 172: 'whippet',\n",
      " 173: 'Ibizan hound, Ibizan Podenco',\n",
      " 174: 'Norwegian elkhound, elkhound',\n",
      " 175: 'otterhound, otter hound',\n",
      " 176: 'Saluki, gazelle hound',\n",
      " 177: 'Scottish deerhound, deerhound',\n",
      " 178: 'Weimaraner',\n",
      " 179: 'Staffordshire bullterrier, Staffordshire bull terrier',\n",
      " 180: 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier',\n",
      " 181: 'Bedlington terrier',\n",
      " 182: 'Border terrier',\n",
      " 183: 'Kerry blue terrier',\n",
      " 184: 'Irish terrier',\n",
      " 185: 'Norfolk terrier',\n",
      " 186: 'Norwich terrier',\n",
      " 187: 'Yorkshire terrier',\n",
      " 188: 'wire-haired fox terrier',\n",
      " 189: 'Lakeland terrier',\n",
      " 190: 'Sealyham terrier, Sealyham',\n",
      " 191: 'Airedale, Airedale terrier',\n",
      " 192: 'cairn, cairn terrier',\n",
      " 193: 'Australian terrier',\n",
      " 194: 'Dandie Dinmont, Dandie Dinmont terrier',\n",
      " 195: 'Boston bull, Boston terrier',\n",
      " 196: 'miniature schnauzer',\n",
      " 197: 'giant schnauzer',\n",
      " 198: 'standard schnauzer',\n",
      " 199: 'Scotch terrier, Scottish terrier, Scottie',\n",
      " 200: 'Tibetan terrier, chrysanthemum dog',\n",
      " 201: 'silky terrier, Sydney silky',\n",
      " 202: 'soft-coated wheaten terrier',\n",
      " 203: 'West Highland white terrier',\n",
      " 204: 'Lhasa, Lhasa apso',\n",
      " 205: 'flat-coated retriever',\n",
      " 206: 'curly-coated retriever',\n",
      " 207: 'golden retriever',\n",
      " 208: 'Labrador retriever',\n",
      " 209: 'Chesapeake Bay retriever',\n",
      " 210: 'German short-haired pointer',\n",
      " 211: 'vizsla, Hungarian pointer',\n",
      " 212: 'English setter',\n",
      " 213: 'Irish setter, red setter',\n",
      " 214: 'Gordon setter',\n",
      " 215: 'Brittany spaniel',\n",
      " 216: 'clumber, clumber spaniel',\n",
      " 217: 'English springer, English springer spaniel',\n",
      " 218: 'Welsh springer spaniel',\n",
      " 219: 'cocker spaniel, English cocker spaniel, cocker',\n",
      " 220: 'Sussex spaniel',\n",
      " 221: 'Irish water spaniel',\n",
      " 222: 'kuvasz',\n",
      " 223: 'schipperke',\n",
      " 224: 'groenendael',\n",
      " 225: 'malinois',\n",
      " 226: 'briard',\n",
      " 227: 'kelpie',\n",
      " 228: 'komondor',\n",
      " 229: 'Old English sheepdog, bobtail',\n",
      " 230: 'Shetland sheepdog, Shetland sheep dog, Shetland',\n",
      " 231: 'collie',\n",
      " 232: 'Border collie',\n",
      " 233: 'Bouvier des Flandres, Bouviers des Flandres',\n",
      " 234: 'Rottweiler',\n",
      " 235: 'German shepherd, German shepherd dog, German police dog, alsatian',\n",
      " 236: 'Doberman, Doberman pinscher',\n",
      " 237: 'miniature pinscher',\n",
      " 238: 'Greater Swiss Mountain dog',\n",
      " 239: 'Bernese mountain dog',\n",
      " 240: 'Appenzeller',\n",
      " 241: 'EntleBucher',\n",
      " 242: 'boxer',\n",
      " 243: 'bull mastiff',\n",
      " 244: 'Tibetan mastiff',\n",
      " 245: 'French bulldog',\n",
      " 246: 'Great Dane',\n",
      " 247: 'Saint Bernard, St Bernard',\n",
      " 248: 'Eskimo dog, husky',\n",
      " 249: 'malamute, malemute, Alaskan malamute',\n",
      " 250: 'Siberian husky',\n",
      " 251: 'dalmatian, coach dog, carriage dog',\n",
      " 252: 'affenpinscher, monkey pinscher, monkey dog',\n",
      " 253: 'basenji',\n",
      " 254: 'pug, pug-dog',\n",
      " 255: 'Leonberg',\n",
      " 256: 'Newfoundland, Newfoundland dog',\n",
      " 257: 'Great Pyrenees',\n",
      " 258: 'Samoyed, Samoyede',\n",
      " 259: 'Pomeranian',\n",
      " 260: 'chow, chow chow',\n",
      " 261: 'keeshond',\n",
      " 262: 'Brabancon griffon',\n",
      " 263: 'Pembroke, Pembroke Welsh corgi',\n",
      " 264: 'Cardigan, Cardigan Welsh corgi',\n",
      " 265: 'toy poodle',\n",
      " 266: 'miniature poodle',\n",
      " 267: 'standard poodle',\n",
      " 268: 'Mexican hairless',\n",
      " 269: 'timber wolf, grey wolf, gray wolf, Canis lupus',\n",
      " 270: 'white wolf, Arctic wolf, Canis lupus tundrarum',\n",
      " 271: 'red wolf, maned wolf, Canis rufus, Canis niger',\n",
      " 272: 'coyote, prairie wolf, brush wolf, Canis latrans',\n",
      " 273: 'dingo, warrigal, warragal, Canis dingo',\n",
      " 274: 'dhole, Cuon alpinus',\n",
      " 275: 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus',\n",
      " 276: 'hyena, hyaena',\n",
      " 277: 'red fox, Vulpes vulpes',\n",
      " 278: 'kit fox, Vulpes macrotis',\n",
      " 279: 'Arctic fox, white fox, Alopex lagopus',\n",
      " 280: 'grey fox, gray fox, Urocyon cinereoargenteus',\n",
      " 281: 'tabby, tabby cat',\n",
      " 282: 'tiger cat',\n",
      " 283: 'Persian cat',\n",
      " 284: 'Siamese cat, Siamese',\n",
      " 285: 'Egyptian cat',\n",
      " 286: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n",
      " 287: 'lynx, catamount',\n",
      " 288: 'leopard, Panthera pardus',\n",
      " 289: 'snow leopard, ounce, Panthera uncia',\n",
      " 290: 'jaguar, panther, Panthera onca, Felis onca',\n",
      " 291: 'lion, king of beasts, Panthera leo',\n",
      " 292: 'tiger, Panthera tigris',\n",
      " 293: 'cheetah, chetah, Acinonyx jubatus',\n",
      " 294: 'brown bear, bruin, Ursus arctos',\n",
      " 295: 'American black bear, black bear, Ursus americanus, Euarctos americanus',\n",
      " 296: 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus',\n",
      " 297: 'sloth bear, Melursus ursinus, Ursus ursinus',\n",
      " 298: 'mongoose',\n",
      " 299: 'meerkat, mierkat',\n",
      " 300: 'tiger beetle',\n",
      " 301: 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle',\n",
      " 302: 'ground beetle, carabid beetle',\n",
      " 303: 'long-horned beetle, longicorn, longicorn beetle',\n",
      " 304: 'leaf beetle, chrysomelid',\n",
      " 305: 'dung beetle',\n",
      " 306: 'rhinoceros beetle',\n",
      " 307: 'weevil',\n",
      " 308: 'fly',\n",
      " 309: 'bee',\n",
      " 310: 'ant, emmet, pismire',\n",
      " 311: 'grasshopper, hopper',\n",
      " 312: 'cricket',\n",
      " 313: 'walking stick, walkingstick, stick insect',\n",
      " 314: 'cockroach, roach',\n",
      " 315: 'mantis, mantid',\n",
      " 316: 'cicada, cicala',\n",
      " 317: 'leafhopper',\n",
      " 318: 'lacewing, lacewing fly',\n",
      " 319: \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\",\n",
      " 320: 'damselfly',\n",
      " 321: 'admiral',\n",
      " 322: 'ringlet, ringlet butterfly',\n",
      " 323: 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus',\n",
      " 324: 'cabbage butterfly',\n",
      " 325: 'sulphur butterfly, sulfur butterfly',\n",
      " 326: 'lycaenid, lycaenid butterfly',\n",
      " 327: 'starfish, sea star',\n",
      " 328: 'sea urchin',\n",
      " 329: 'sea cucumber, holothurian',\n",
      " 330: 'wood rabbit, cottontail, cottontail rabbit',\n",
      " 331: 'hare',\n",
      " 332: 'Angora, Angora rabbit',\n",
      " 333: 'hamster',\n",
      " 334: 'porcupine, hedgehog',\n",
      " 335: 'fox squirrel, eastern fox squirrel, Sciurus niger',\n",
      " 336: 'marmot',\n",
      " 337: 'beaver',\n",
      " 338: 'guinea pig, Cavia cobaya',\n",
      " 339: 'sorrel',\n",
      " 340: 'zebra',\n",
      " 341: 'hog, pig, grunter, squealer, Sus scrofa',\n",
      " 342: 'wild boar, boar, Sus scrofa',\n",
      " 343: 'warthog',\n",
      " 344: 'hippopotamus, hippo, river horse, Hippopotamus amphibius',\n",
      " 345: 'ox',\n",
      " 346: 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis',\n",
      " 347: 'bison',\n",
      " 348: 'ram, tup',\n",
      " 349: 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis',\n",
      " 350: 'ibex, Capra ibex',\n",
      " 351: 'hartebeest',\n",
      " 352: 'impala, Aepyceros melampus',\n",
      " 353: 'gazelle',\n",
      " 354: 'Arabian camel, dromedary, Camelus dromedarius',\n",
      " 355: 'llama',\n",
      " 356: 'weasel',\n",
      " 357: 'mink',\n",
      " 358: 'polecat, fitch, foulmart, foumart, Mustela putorius',\n",
      " 359: 'black-footed ferret, ferret, Mustela nigripes',\n",
      " 360: 'otter',\n",
      " 361: 'skunk, polecat, wood pussy',\n",
      " 362: 'badger',\n",
      " 363: 'armadillo',\n",
      " 364: 'three-toed sloth, ai, Bradypus tridactylus',\n",
      " 365: 'orangutan, orang, orangutang, Pongo pygmaeus',\n",
      " 366: 'gorilla, Gorilla gorilla',\n",
      " 367: 'chimpanzee, chimp, Pan troglodytes',\n",
      " 368: 'gibbon, Hylobates lar',\n",
      " 369: 'siamang, Hylobates syndactylus, Symphalangus syndactylus',\n",
      " 370: 'guenon, guenon monkey',\n",
      " 371: 'patas, hussar monkey, Erythrocebus patas',\n",
      " 372: 'baboon',\n",
      " 373: 'macaque',\n",
      " 374: 'langur',\n",
      " 375: 'colobus, colobus monkey',\n",
      " 376: 'proboscis monkey, Nasalis larvatus',\n",
      " 377: 'marmoset',\n",
      " 378: 'capuchin, ringtail, Cebus capucinus',\n",
      " 379: 'howler monkey, howler',\n",
      " 380: 'titi, titi monkey',\n",
      " 381: 'spider monkey, Ateles geoffroyi',\n",
      " 382: 'squirrel monkey, Saimiri sciureus',\n",
      " 383: 'Madagascar cat, ring-tailed lemur, Lemur catta',\n",
      " 384: 'indri, indris, Indri indri, Indri brevicaudatus',\n",
      " 385: 'Indian elephant, Elephas maximus',\n",
      " 386: 'African elephant, Loxodonta africana',\n",
      " 387: 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens',\n",
      " 388: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca',\n",
      " 389: 'barracouta, snoek',\n",
      " 390: 'eel',\n",
      " 391: 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch',\n",
      " 392: 'rock beauty, Holocanthus tricolor',\n",
      " 393: 'anemone fish',\n",
      " 394: 'sturgeon',\n",
      " 395: 'gar, garfish, garpike, billfish, Lepisosteus osseus',\n",
      " 396: 'lionfish',\n",
      " 397: 'puffer, pufferfish, blowfish, globefish',\n",
      " 398: 'abacus',\n",
      " 399: 'abaya',\n",
      " 400: \"academic gown, academic robe, judge's robe\",\n",
      " 401: 'accordion, piano accordion, squeeze box',\n",
      " 402: 'acoustic guitar',\n",
      " 403: 'aircraft carrier, carrier, flattop, attack aircraft carrier',\n",
      " 404: 'airliner',\n",
      " 405: 'airship, dirigible',\n",
      " 406: 'altar',\n",
      " 407: 'ambulance',\n",
      " 408: 'amphibian, amphibious vehicle',\n",
      " 409: 'analog clock',\n",
      " 410: 'apiary, bee house',\n",
      " 411: 'apron',\n",
      " 412: 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin',\n",
      " 413: 'assault rifle, assault gun',\n",
      " 414: 'backpack, back pack, knapsack, packsack, rucksack, haversack',\n",
      " 415: 'bakery, bakeshop, bakehouse',\n",
      " 416: 'balance beam, beam',\n",
      " 417: 'balloon',\n",
      " 418: 'ballpoint, ballpoint pen, ballpen, Biro',\n",
      " 419: 'Band Aid',\n",
      " 420: 'banjo',\n",
      " 421: 'bannister, banister, balustrade, balusters, handrail',\n",
      " 422: 'barbell',\n",
      " 423: 'barber chair',\n",
      " 424: 'barbershop',\n",
      " 425: 'barn',\n",
      " 426: 'barometer',\n",
      " 427: 'barrel, cask',\n",
      " 428: 'barrow, garden cart, lawn cart, wheelbarrow',\n",
      " 429: 'baseball',\n",
      " 430: 'basketball',\n",
      " 431: 'bassinet',\n",
      " 432: 'bassoon',\n",
      " 433: 'bathing cap, swimming cap',\n",
      " 434: 'bath towel',\n",
      " 435: 'bathtub, bathing tub, bath, tub',\n",
      " 436: 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon',\n",
      " 437: 'beacon, lighthouse, beacon light, pharos',\n",
      " 438: 'beaker',\n",
      " 439: 'bearskin, busby, shako',\n",
      " 440: 'beer bottle',\n",
      " 441: 'beer glass',\n",
      " 442: 'bell cote, bell cot',\n",
      " 443: 'bib',\n",
      " 444: 'bicycle-built-for-two, tandem bicycle, tandem',\n",
      " 445: 'bikini, two-piece',\n",
      " 446: 'binder, ring-binder',\n",
      " 447: 'binoculars, field glasses, opera glasses',\n",
      " 448: 'birdhouse',\n",
      " 449: 'boathouse',\n",
      " 450: 'bobsled, bobsleigh, bob',\n",
      " 451: 'bolo tie, bolo, bola tie, bola',\n",
      " 452: 'bonnet, poke bonnet',\n",
      " 453: 'bookcase',\n",
      " 454: 'bookshop, bookstore, bookstall',\n",
      " 455: 'bottlecap',\n",
      " 456: 'bow',\n",
      " 457: 'bow tie, bow-tie, bowtie',\n",
      " 458: 'brass, memorial tablet, plaque',\n",
      " 459: 'brassiere, bra, bandeau',\n",
      " 460: 'breakwater, groin, groyne, mole, bulwark, seawall, jetty',\n",
      " 461: 'breastplate, aegis, egis',\n",
      " 462: 'broom',\n",
      " 463: 'bucket, pail',\n",
      " 464: 'buckle',\n",
      " 465: 'bulletproof vest',\n",
      " 466: 'bullet train, bullet',\n",
      " 467: 'butcher shop, meat market',\n",
      " 468: 'cab, hack, taxi, taxicab',\n",
      " 469: 'caldron, cauldron',\n",
      " 470: 'candle, taper, wax light',\n",
      " 471: 'cannon',\n",
      " 472: 'canoe',\n",
      " 473: 'can opener, tin opener',\n",
      " 474: 'cardigan',\n",
      " 475: 'car mirror',\n",
      " 476: 'carousel, carrousel, merry-go-round, roundabout, whirligig',\n",
      " 477: \"carpenter's kit, tool kit\",\n",
      " 478: 'carton',\n",
      " 479: 'car wheel',\n",
      " 480: 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM',\n",
      " 481: 'cassette',\n",
      " 482: 'cassette player',\n",
      " 483: 'castle',\n",
      " 484: 'catamaran',\n",
      " 485: 'CD player',\n",
      " 486: 'cello, violoncello',\n",
      " 487: 'cellular telephone, cellular phone, cellphone, cell, mobile phone',\n",
      " 488: 'chain',\n",
      " 489: 'chainlink fence',\n",
      " 490: 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour',\n",
      " 491: 'chain saw, chainsaw',\n",
      " 492: 'chest',\n",
      " 493: 'chiffonier, commode',\n",
      " 494: 'chime, bell, gong',\n",
      " 495: 'china cabinet, china closet',\n",
      " 496: 'Christmas stocking',\n",
      " 497: 'church, church building',\n",
      " 498: 'cinema, movie theater, movie theatre, movie house, picture palace',\n",
      " 499: 'cleaver, meat cleaver, chopper',\n",
      " 500: 'cliff dwelling',\n",
      " 501: 'cloak',\n",
      " 502: 'clog, geta, patten, sabot',\n",
      " 503: 'cocktail shaker',\n",
      " 504: 'coffee mug',\n",
      " 505: 'coffeepot',\n",
      " 506: 'coil, spiral, volute, whorl, helix',\n",
      " 507: 'combination lock',\n",
      " 508: 'computer keyboard, keypad',\n",
      " 509: 'confectionery, confectionary, candy store',\n",
      " 510: 'container ship, containership, container vessel',\n",
      " 511: 'convertible',\n",
      " 512: 'corkscrew, bottle screw',\n",
      " 513: 'cornet, horn, trumpet, trump',\n",
      " 514: 'cowboy boot',\n",
      " 515: 'cowboy hat, ten-gallon hat',\n",
      " 516: 'cradle',\n",
      " 517: 'crane',\n",
      " 518: 'crash helmet',\n",
      " 519: 'crate',\n",
      " 520: 'crib, cot',\n",
      " 521: 'Crock Pot',\n",
      " 522: 'croquet ball',\n",
      " 523: 'crutch',\n",
      " 524: 'cuirass',\n",
      " 525: 'dam, dike, dyke',\n",
      " 526: 'desk',\n",
      " 527: 'desktop computer',\n",
      " 528: 'dial telephone, dial phone',\n",
      " 529: 'diaper, nappy, napkin',\n",
      " 530: 'digital clock',\n",
      " 531: 'digital watch',\n",
      " 532: 'dining table, board',\n",
      " 533: 'dishrag, dishcloth',\n",
      " 534: 'dishwasher, dish washer, dishwashing machine',\n",
      " 535: 'disk brake, disc brake',\n",
      " 536: 'dock, dockage, docking facility',\n",
      " 537: 'dogsled, dog sled, dog sleigh',\n",
      " 538: 'dome',\n",
      " 539: 'doormat, welcome mat',\n",
      " 540: 'drilling platform, offshore rig',\n",
      " 541: 'drum, membranophone, tympan',\n",
      " 542: 'drumstick',\n",
      " 543: 'dumbbell',\n",
      " 544: 'Dutch oven',\n",
      " 545: 'electric fan, blower',\n",
      " 546: 'electric guitar',\n",
      " 547: 'electric locomotive',\n",
      " 548: 'entertainment center',\n",
      " 549: 'envelope',\n",
      " 550: 'espresso maker',\n",
      " 551: 'face powder',\n",
      " 552: 'feather boa, boa',\n",
      " 553: 'file, file cabinet, filing cabinet',\n",
      " 554: 'fireboat',\n",
      " 555: 'fire engine, fire truck',\n",
      " 556: 'fire screen, fireguard',\n",
      " 557: 'flagpole, flagstaff',\n",
      " 558: 'flute, transverse flute',\n",
      " 559: 'folding chair',\n",
      " 560: 'football helmet',\n",
      " 561: 'forklift',\n",
      " 562: 'fountain',\n",
      " 563: 'fountain pen',\n",
      " 564: 'four-poster',\n",
      " 565: 'freight car',\n",
      " 566: 'French horn, horn',\n",
      " 567: 'frying pan, frypan, skillet',\n",
      " 568: 'fur coat',\n",
      " 569: 'garbage truck, dustcart',\n",
      " 570: 'gasmask, respirator, gas helmet',\n",
      " 571: 'gas pump, gasoline pump, petrol pump, island dispenser',\n",
      " 572: 'goblet',\n",
      " 573: 'go-kart',\n",
      " 574: 'golf ball',\n",
      " 575: 'golfcart, golf cart',\n",
      " 576: 'gondola',\n",
      " 577: 'gong, tam-tam',\n",
      " 578: 'gown',\n",
      " 579: 'grand piano, grand',\n",
      " 580: 'greenhouse, nursery, glasshouse',\n",
      " 581: 'grille, radiator grille',\n",
      " 582: 'grocery store, grocery, food market, market',\n",
      " 583: 'guillotine',\n",
      " 584: 'hair slide',\n",
      " 585: 'hair spray',\n",
      " 586: 'half track',\n",
      " 587: 'hammer',\n",
      " 588: 'hamper',\n",
      " 589: 'hand blower, blow dryer, blow drier, hair dryer, hair drier',\n",
      " 590: 'hand-held computer, hand-held microcomputer',\n",
      " 591: 'handkerchief, hankie, hanky, hankey',\n",
      " 592: 'hard disc, hard disk, fixed disk',\n",
      " 593: 'harmonica, mouth organ, harp, mouth harp',\n",
      " 594: 'harp',\n",
      " 595: 'harvester, reaper',\n",
      " 596: 'hatchet',\n",
      " 597: 'holster',\n",
      " 598: 'home theater, home theatre',\n",
      " 599: 'honeycomb',\n",
      " 600: 'hook, claw',\n",
      " 601: 'hoopskirt, crinoline',\n",
      " 602: 'horizontal bar, high bar',\n",
      " 603: 'horse cart, horse-cart',\n",
      " 604: 'hourglass',\n",
      " 605: 'iPod',\n",
      " 606: 'iron, smoothing iron',\n",
      " 607: \"jack-o'-lantern\",\n",
      " 608: 'jean, blue jean, denim',\n",
      " 609: 'jeep, landrover',\n",
      " 610: 'jersey, T-shirt, tee shirt',\n",
      " 611: 'jigsaw puzzle',\n",
      " 612: 'jinrikisha, ricksha, rickshaw',\n",
      " 613: 'joystick',\n",
      " 614: 'kimono',\n",
      " 615: 'knee pad',\n",
      " 616: 'knot',\n",
      " 617: 'lab coat, laboratory coat',\n",
      " 618: 'ladle',\n",
      " 619: 'lampshade, lamp shade',\n",
      " 620: 'laptop, laptop computer',\n",
      " 621: 'lawn mower, mower',\n",
      " 622: 'lens cap, lens cover',\n",
      " 623: 'letter opener, paper knife, paperknife',\n",
      " 624: 'library',\n",
      " 625: 'lifeboat',\n",
      " 626: 'lighter, light, igniter, ignitor',\n",
      " 627: 'limousine, limo',\n",
      " 628: 'liner, ocean liner',\n",
      " 629: 'lipstick, lip rouge',\n",
      " 630: 'Loafer',\n",
      " 631: 'lotion',\n",
      " 632: 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system',\n",
      " 633: \"loupe, jeweler's loupe\",\n",
      " 634: 'lumbermill, sawmill',\n",
      " 635: 'magnetic compass',\n",
      " 636: 'mailbag, postbag',\n",
      " 637: 'mailbox, letter box',\n",
      " 638: 'maillot',\n",
      " 639: 'maillot, tank suit',\n",
      " 640: 'manhole cover',\n",
      " 641: 'maraca',\n",
      " 642: 'marimba, xylophone',\n",
      " 643: 'mask',\n",
      " 644: 'matchstick',\n",
      " 645: 'maypole',\n",
      " 646: 'maze, labyrinth',\n",
      " 647: 'measuring cup',\n",
      " 648: 'medicine chest, medicine cabinet',\n",
      " 649: 'megalith, megalithic structure',\n",
      " 650: 'microphone, mike',\n",
      " 651: 'microwave, microwave oven',\n",
      " 652: 'military uniform',\n",
      " 653: 'milk can',\n",
      " 654: 'minibus',\n",
      " 655: 'miniskirt, mini',\n",
      " 656: 'minivan',\n",
      " 657: 'missile',\n",
      " 658: 'mitten',\n",
      " 659: 'mixing bowl',\n",
      " 660: 'mobile home, manufactured home',\n",
      " 661: 'Model T',\n",
      " 662: 'modem',\n",
      " 663: 'monastery',\n",
      " 664: 'monitor',\n",
      " 665: 'moped',\n",
      " 666: 'mortar',\n",
      " 667: 'mortarboard',\n",
      " 668: 'mosque',\n",
      " 669: 'mosquito net',\n",
      " 670: 'motor scooter, scooter',\n",
      " 671: 'mountain bike, all-terrain bike, off-roader',\n",
      " 672: 'mountain tent',\n",
      " 673: 'mouse, computer mouse',\n",
      " 674: 'mousetrap',\n",
      " 675: 'moving van',\n",
      " 676: 'muzzle',\n",
      " 677: 'nail',\n",
      " 678: 'neck brace',\n",
      " 679: 'necklace',\n",
      " 680: 'nipple',\n",
      " 681: 'notebook, notebook computer',\n",
      " 682: 'obelisk',\n",
      " 683: 'oboe, hautboy, hautbois',\n",
      " 684: 'ocarina, sweet potato',\n",
      " 685: 'odometer, hodometer, mileometer, milometer',\n",
      " 686: 'oil filter',\n",
      " 687: 'organ, pipe organ',\n",
      " 688: 'oscilloscope, scope, cathode-ray oscilloscope, CRO',\n",
      " 689: 'overskirt',\n",
      " 690: 'oxcart',\n",
      " 691: 'oxygen mask',\n",
      " 692: 'packet',\n",
      " 693: 'paddle, boat paddle',\n",
      " 694: 'paddlewheel, paddle wheel',\n",
      " 695: 'padlock',\n",
      " 696: 'paintbrush',\n",
      " 697: \"pajama, pyjama, pj's, jammies\",\n",
      " 698: 'palace',\n",
      " 699: 'panpipe, pandean pipe, syrinx',\n",
      " 700: 'paper towel',\n",
      " 701: 'parachute, chute',\n",
      " 702: 'parallel bars, bars',\n",
      " 703: 'park bench',\n",
      " 704: 'parking meter',\n",
      " 705: 'passenger car, coach, carriage',\n",
      " 706: 'patio, terrace',\n",
      " 707: 'pay-phone, pay-station',\n",
      " 708: 'pedestal, plinth, footstall',\n",
      " 709: 'pencil box, pencil case',\n",
      " 710: 'pencil sharpener',\n",
      " 711: 'perfume, essence',\n",
      " 712: 'Petri dish',\n",
      " 713: 'photocopier',\n",
      " 714: 'pick, plectrum, plectron',\n",
      " 715: 'pickelhaube',\n",
      " 716: 'picket fence, paling',\n",
      " 717: 'pickup, pickup truck',\n",
      " 718: 'pier',\n",
      " 719: 'piggy bank, penny bank',\n",
      " 720: 'pill bottle',\n",
      " 721: 'pillow',\n",
      " 722: 'ping-pong ball',\n",
      " 723: 'pinwheel',\n",
      " 724: 'pirate, pirate ship',\n",
      " 725: 'pitcher, ewer',\n",
      " 726: \"plane, carpenter's plane, woodworking plane\",\n",
      " 727: 'planetarium',\n",
      " 728: 'plastic bag',\n",
      " 729: 'plate rack',\n",
      " 730: 'plow, plough',\n",
      " 731: \"plunger, plumber's helper\",\n",
      " 732: 'Polaroid camera, Polaroid Land camera',\n",
      " 733: 'pole',\n",
      " 734: 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria',\n",
      " 735: 'poncho',\n",
      " 736: 'pool table, billiard table, snooker table',\n",
      " 737: 'pop bottle, soda bottle',\n",
      " 738: 'pot, flowerpot',\n",
      " 739: \"potter's wheel\",\n",
      " 740: 'power drill',\n",
      " 741: 'prayer rug, prayer mat',\n",
      " 742: 'printer',\n",
      " 743: 'prison, prison house',\n",
      " 744: 'projectile, missile',\n",
      " 745: 'projector',\n",
      " 746: 'puck, hockey puck',\n",
      " 747: 'punching bag, punch bag, punching ball, punchball',\n",
      " 748: 'purse',\n",
      " 749: 'quill, quill pen',\n",
      " 750: 'quilt, comforter, comfort, puff',\n",
      " 751: 'racer, race car, racing car',\n",
      " 752: 'racket, racquet',\n",
      " 753: 'radiator',\n",
      " 754: 'radio, wireless',\n",
      " 755: 'radio telescope, radio reflector',\n",
      " 756: 'rain barrel',\n",
      " 757: 'recreational vehicle, RV, R.V.',\n",
      " 758: 'reel',\n",
      " 759: 'reflex camera',\n",
      " 760: 'refrigerator, icebox',\n",
      " 761: 'remote control, remote',\n",
      " 762: 'restaurant, eating house, eating place, eatery',\n",
      " 763: 'revolver, six-gun, six-shooter',\n",
      " 764: 'rifle',\n",
      " 765: 'rocking chair, rocker',\n",
      " 766: 'rotisserie',\n",
      " 767: 'rubber eraser, rubber, pencil eraser',\n",
      " 768: 'rugby ball',\n",
      " 769: 'rule, ruler',\n",
      " 770: 'running shoe',\n",
      " 771: 'safe',\n",
      " 772: 'safety pin',\n",
      " 773: 'saltshaker, salt shaker',\n",
      " 774: 'sandal',\n",
      " 775: 'sarong',\n",
      " 776: 'sax, saxophone',\n",
      " 777: 'scabbard',\n",
      " 778: 'scale, weighing machine',\n",
      " 779: 'school bus',\n",
      " 780: 'schooner',\n",
      " 781: 'scoreboard',\n",
      " 782: 'screen, CRT screen',\n",
      " 783: 'screw',\n",
      " 784: 'screwdriver',\n",
      " 785: 'seat belt, seatbelt',\n",
      " 786: 'sewing machine',\n",
      " 787: 'shield, buckler',\n",
      " 788: 'shoe shop, shoe-shop, shoe store',\n",
      " 789: 'shoji',\n",
      " 790: 'shopping basket',\n",
      " 791: 'shopping cart',\n",
      " 792: 'shovel',\n",
      " 793: 'shower cap',\n",
      " 794: 'shower curtain',\n",
      " 795: 'ski',\n",
      " 796: 'ski mask',\n",
      " 797: 'sleeping bag',\n",
      " 798: 'slide rule, slipstick',\n",
      " 799: 'sliding door',\n",
      " 800: 'slot, one-armed bandit',\n",
      " 801: 'snorkel',\n",
      " 802: 'snowmobile',\n",
      " 803: 'snowplow, snowplough',\n",
      " 804: 'soap dispenser',\n",
      " 805: 'soccer ball',\n",
      " 806: 'sock',\n",
      " 807: 'solar dish, solar collector, solar furnace',\n",
      " 808: 'sombrero',\n",
      " 809: 'soup bowl',\n",
      " 810: 'space bar',\n",
      " 811: 'space heater',\n",
      " 812: 'space shuttle',\n",
      " 813: 'spatula',\n",
      " 814: 'speedboat',\n",
      " 815: \"spider web, spider's web\",\n",
      " 816: 'spindle',\n",
      " 817: 'sports car, sport car',\n",
      " 818: 'spotlight, spot',\n",
      " 819: 'stage',\n",
      " 820: 'steam locomotive',\n",
      " 821: 'steel arch bridge',\n",
      " 822: 'steel drum',\n",
      " 823: 'stethoscope',\n",
      " 824: 'stole',\n",
      " 825: 'stone wall',\n",
      " 826: 'stopwatch, stop watch',\n",
      " 827: 'stove',\n",
      " 828: 'strainer',\n",
      " 829: 'streetcar, tram, tramcar, trolley, trolley car',\n",
      " 830: 'stretcher',\n",
      " 831: 'studio couch, day bed',\n",
      " 832: 'stupa, tope',\n",
      " 833: 'submarine, pigboat, sub, U-boat',\n",
      " 834: 'suit, suit of clothes',\n",
      " 835: 'sundial',\n",
      " 836: 'sunglass',\n",
      " 837: 'sunglasses, dark glasses, shades',\n",
      " 838: 'sunscreen, sunblock, sun blocker',\n",
      " 839: 'suspension bridge',\n",
      " 840: 'swab, swob, mop',\n",
      " 841: 'sweatshirt',\n",
      " 842: 'swimming trunks, bathing trunks',\n",
      " 843: 'swing',\n",
      " 844: 'switch, electric switch, electrical switch',\n",
      " 845: 'syringe',\n",
      " 846: 'table lamp',\n",
      " 847: 'tank, army tank, armored combat vehicle, armoured combat vehicle',\n",
      " 848: 'tape player',\n",
      " 849: 'teapot',\n",
      " 850: 'teddy, teddy bear',\n",
      " 851: 'television, television system',\n",
      " 852: 'tennis ball',\n",
      " 853: 'thatch, thatched roof',\n",
      " 854: 'theater curtain, theatre curtain',\n",
      " 855: 'thimble',\n",
      " 856: 'thresher, thrasher, threshing machine',\n",
      " 857: 'throne',\n",
      " 858: 'tile roof',\n",
      " 859: 'toaster',\n",
      " 860: 'tobacco shop, tobacconist shop, tobacconist',\n",
      " 861: 'toilet seat',\n",
      " 862: 'torch',\n",
      " 863: 'totem pole',\n",
      " 864: 'tow truck, tow car, wrecker',\n",
      " 865: 'toyshop',\n",
      " 866: 'tractor',\n",
      " 867: 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi',\n",
      " 868: 'tray',\n",
      " 869: 'trench coat',\n",
      " 870: 'tricycle, trike, velocipede',\n",
      " 871: 'trimaran',\n",
      " 872: 'tripod',\n",
      " 873: 'triumphal arch',\n",
      " 874: 'trolleybus, trolley coach, trackless trolley',\n",
      " 875: 'trombone',\n",
      " 876: 'tub, vat',\n",
      " 877: 'turnstile',\n",
      " 878: 'typewriter keyboard',\n",
      " 879: 'umbrella',\n",
      " 880: 'unicycle, monocycle',\n",
      " 881: 'upright, upright piano',\n",
      " 882: 'vacuum, vacuum cleaner',\n",
      " 883: 'vase',\n",
      " 884: 'vault',\n",
      " 885: 'velvet',\n",
      " 886: 'vending machine',\n",
      " 887: 'vestment',\n",
      " 888: 'viaduct',\n",
      " 889: 'violin, fiddle',\n",
      " 890: 'volleyball',\n",
      " 891: 'waffle iron',\n",
      " 892: 'wall clock',\n",
      " 893: 'wallet, billfold, notecase, pocketbook',\n",
      " 894: 'wardrobe, closet, press',\n",
      " 895: 'warplane, military plane',\n",
      " 896: 'washbasin, handbasin, washbowl, lavabo, wash-hand basin',\n",
      " 897: 'washer, automatic washer, washing machine',\n",
      " 898: 'water bottle',\n",
      " 899: 'water jug',\n",
      " 900: 'water tower',\n",
      " 901: 'whiskey jug',\n",
      " 902: 'whistle',\n",
      " 903: 'wig',\n",
      " 904: 'window screen',\n",
      " 905: 'window shade',\n",
      " 906: 'Windsor tie',\n",
      " 907: 'wine bottle',\n",
      " 908: 'wing',\n",
      " 909: 'wok',\n",
      " 910: 'wooden spoon',\n",
      " 911: 'wool, woolen, woollen',\n",
      " 912: 'worm fence, snake fence, snake-rail fence, Virginia fence',\n",
      " 913: 'wreck',\n",
      " 914: 'yawl',\n",
      " 915: 'yurt',\n",
      " 916: 'web site, website, internet site, site',\n",
      " 917: 'comic book',\n",
      " 918: 'crossword puzzle, crossword',\n",
      " 919: 'street sign',\n",
      " 920: 'traffic light, traffic signal, stoplight',\n",
      " 921: 'book jacket, dust cover, dust jacket, dust wrapper',\n",
      " 922: 'menu',\n",
      " 923: 'plate',\n",
      " 924: 'guacamole',\n",
      " 925: 'consomme',\n",
      " 926: 'hot pot, hotpot',\n",
      " 927: 'trifle',\n",
      " 928: 'ice cream, icecream',\n",
      " 929: 'ice lolly, lolly, lollipop, popsicle',\n",
      " 930: 'French loaf',\n",
      " 931: 'bagel, beigel',\n",
      " 932: 'pretzel',\n",
      " 933: 'cheeseburger',\n",
      " 934: 'hotdog, hot dog, red hot',\n",
      " 935: 'mashed potato',\n",
      " 936: 'head cabbage',\n",
      " 937: 'broccoli',\n",
      " 938: 'cauliflower',\n",
      " 939: 'zucchini, courgette',\n",
      " 940: 'spaghetti squash',\n",
      " 941: 'acorn squash',\n",
      " 942: 'butternut squash',\n",
      " 943: 'cucumber, cuke',\n",
      " 944: 'artichoke, globe artichoke',\n",
      " 945: 'bell pepper',\n",
      " 946: 'cardoon',\n",
      " 947: 'mushroom',\n",
      " 948: 'Granny Smith',\n",
      " 949: 'strawberry',\n",
      " 950: 'orange',\n",
      " 951: 'lemon',\n",
      " 952: 'fig',\n",
      " 953: 'pineapple, ananas',\n",
      " 954: 'banana',\n",
      " 955: 'jackfruit, jak, jack',\n",
      " 956: 'custard apple',\n",
      " 957: 'pomegranate',\n",
      " 958: 'hay',\n",
      " 959: 'carbonara',\n",
      " 960: 'chocolate sauce, chocolate syrup',\n",
      " 961: 'dough',\n",
      " 962: 'meat loaf, meatloaf',\n",
      " 963: 'pizza, pizza pie',\n",
      " 964: 'potpie',\n",
      " 965: 'burrito',\n",
      " 966: 'red wine',\n",
      " 967: 'espresso',\n",
      " 968: 'cup',\n",
      " 969: 'eggnog',\n",
      " 970: 'alp',\n",
      " 971: 'bubble',\n",
      " 972: 'cliff, drop, drop-off',\n",
      " 973: 'coral reef',\n",
      " 974: 'geyser',\n",
      " 975: 'lakeside, lakeshore',\n",
      " 976: 'promontory, headland, head, foreland',\n",
      " 977: 'sandbar, sand bar',\n",
      " 978: 'seashore, coast, seacoast, sea-coast',\n",
      " 979: 'valley, vale',\n",
      " 980: 'volcano',\n",
      " 981: 'ballplayer, baseball player',\n",
      " 982: 'groom, bridegroom',\n",
      " 983: 'scuba diver',\n",
      " 984: 'rapeseed',\n",
      " 985: 'daisy',\n",
      " 986: \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\",\n",
      " 987: 'corn',\n",
      " 988: 'acorn',\n",
      " 989: 'hip, rose hip, rosehip',\n",
      " 990: 'buckeye, horse chestnut, conker',\n",
      " 991: 'coral fungus',\n",
      " 992: 'agaric',\n",
      " 993: 'gyromitra',\n",
      " 994: 'stinkhorn, carrion fungus',\n",
      " 995: 'earthstar',\n",
      " 996: 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa',\n",
      " 997: 'bolete',\n",
      " 998: 'ear, spike, capitulum',\n",
      " 999: 'toilet tissue, toilet paper, bathroom tissue'}\n",
      "69/1:\n",
      "from tensorflow.keras.applications import EfficientNetB4\n",
      "from tensorflow.image import resize\n",
      "model = EfficientNetB4(weights='imagenet')\n",
      "\n",
      "def center_crop(img, new_width=None, new_height=None):        \n",
      "\n",
      "    width = img.shape[1]\n",
      "    height = img.shape[0]\n",
      "\n",
      "    if new_width is None:\n",
      "        new_width = min(width, height)\n",
      "\n",
      "    if new_height is None:\n",
      "        new_height = min(width, height)\n",
      "\n",
      "    left = int(np.ceil((width - new_width) / 2))\n",
      "    right = width - int(np.floor((width - new_width) / 2))\n",
      "\n",
      "    top = int(np.ceil((height - new_height) / 2))\n",
      "    bottom = height - int(np.floor((height - new_height) / 2))\n",
      "\n",
      "    if len(img.shape) == 2:\n",
      "        center_cropped_img = img[top:bottom, left:right]\n",
      "    else:\n",
      "        center_cropped_img = img[top:bottom, left:right, ...]\n",
      "\n",
      "    return center_cropped_img\n",
      "\n",
      "class_dict = {0: 'tench, Tinca tinca',\n",
      " 1: 'goldfish, Carassius auratus',\n",
      " 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n",
      " 3: 'tiger shark, Galeocerdo cuvieri',\n",
      " 4: 'hammerhead, hammerhead shark',\n",
      " 5: 'electric ray, crampfish, numbfish, torpedo',\n",
      " 6: 'stingray',\n",
      " 7: 'cock',\n",
      " 8: 'hen',\n",
      " 9: 'ostrich, Struthio camelus',\n",
      " 10: 'brambling, Fringilla montifringilla',\n",
      " 11: 'goldfinch, Carduelis carduelis',\n",
      " 12: 'house finch, linnet, Carpodacus mexicanus',\n",
      " 13: 'junco, snowbird',\n",
      " 14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea',\n",
      " 15: 'robin, American robin, Turdus migratorius',\n",
      " 16: 'bulbul',\n",
      " 17: 'jay',\n",
      " 18: 'magpie',\n",
      " 19: 'chickadee',\n",
      " 20: 'water ouzel, dipper',\n",
      " 21: 'kite',\n",
      " 22: 'bald eagle, American eagle, Haliaeetus leucocephalus',\n",
      " 23: 'vulture',\n",
      " 24: 'great grey owl, great gray owl, Strix nebulosa',\n",
      " 25: 'European fire salamander, Salamandra salamandra',\n",
      " 26: 'common newt, Triturus vulgaris',\n",
      " 27: 'eft',\n",
      " 28: 'spotted salamander, Ambystoma maculatum',\n",
      " 29: 'axolotl, mud puppy, Ambystoma mexicanum',\n",
      " 30: 'bullfrog, Rana catesbeiana',\n",
      " 31: 'tree frog, tree-frog',\n",
      " 32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui',\n",
      " 33: 'loggerhead, loggerhead turtle, Caretta caretta',\n",
      " 34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea',\n",
      " 35: 'mud turtle',\n",
      " 36: 'terrapin',\n",
      " 37: 'box turtle, box tortoise',\n",
      " 38: 'banded gecko',\n",
      " 39: 'common iguana, iguana, Iguana iguana',\n",
      " 40: 'American chameleon, anole, Anolis carolinensis',\n",
      " 41: 'whiptail, whiptail lizard',\n",
      " 42: 'agama',\n",
      " 43: 'frilled lizard, Chlamydosaurus kingi',\n",
      " 44: 'alligator lizard',\n",
      " 45: 'Gila monster, Heloderma suspectum',\n",
      " 46: 'green lizard, Lacerta viridis',\n",
      " 47: 'African chameleon, Chamaeleo chamaeleon',\n",
      " 48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis',\n",
      " 49: 'African crocodile, Nile crocodile, Crocodylus niloticus',\n",
      " 50: 'American alligator, Alligator mississipiensis',\n",
      " 51: 'triceratops',\n",
      " 52: 'thunder snake, worm snake, Carphophis amoenus',\n",
      " 53: 'ringneck snake, ring-necked snake, ring snake',\n",
      " 54: 'hognose snake, puff adder, sand viper',\n",
      " 55: 'green snake, grass snake',\n",
      " 56: 'king snake, kingsnake',\n",
      " 57: 'garter snake, grass snake',\n",
      " 58: 'water snake',\n",
      " 59: 'vine snake',\n",
      " 60: 'night snake, Hypsiglena torquata',\n",
      " 61: 'boa constrictor, Constrictor constrictor',\n",
      " 62: 'rock python, rock snake, Python sebae',\n",
      " 63: 'Indian cobra, Naja naja',\n",
      " 64: 'green mamba',\n",
      " 65: 'sea snake',\n",
      " 66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus',\n",
      " 67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus',\n",
      " 68: 'sidewinder, horned rattlesnake, Crotalus cerastes',\n",
      " 69: 'trilobite',\n",
      " 70: 'harvestman, daddy longlegs, Phalangium opilio',\n",
      " 71: 'scorpion',\n",
      " 72: 'black and gold garden spider, Argiope aurantia',\n",
      " 73: 'barn spider, Araneus cavaticus',\n",
      " 74: 'garden spider, Aranea diademata',\n",
      " 75: 'black widow, Latrodectus mactans',\n",
      " 76: 'tarantula',\n",
      " 77: 'wolf spider, hunting spider',\n",
      " 78: 'tick',\n",
      " 79: 'centipede',\n",
      " 80: 'black grouse',\n",
      " 81: 'ptarmigan',\n",
      " 82: 'ruffed grouse, partridge, Bonasa umbellus',\n",
      " 83: 'prairie chicken, prairie grouse, prairie fowl',\n",
      " 84: 'peacock',\n",
      " 85: 'quail',\n",
      " 86: 'partridge',\n",
      " 87: 'African grey, African gray, Psittacus erithacus',\n",
      " 88: 'macaw',\n",
      " 89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n",
      " 90: 'lorikeet',\n",
      " 91: 'coucal',\n",
      " 92: 'bee eater',\n",
      " 93: 'hornbill',\n",
      " 94: 'hummingbird',\n",
      " 95: 'jacamar',\n",
      " 96: 'toucan',\n",
      " 97: 'drake',\n",
      " 98: 'red-breasted merganser, Mergus serrator',\n",
      " 99: 'goose',\n",
      " 100: 'black swan, Cygnus atratus',\n",
      " 101: 'tusker',\n",
      " 102: 'echidna, spiny anteater, anteater',\n",
      " 103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n",
      " 104: 'wallaby, brush kangaroo',\n",
      " 105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n",
      " 106: 'wombat',\n",
      " 107: 'jellyfish',\n",
      " 108: 'sea anemone, anemone',\n",
      " 109: 'brain coral',\n",
      " 110: 'flatworm, platyhelminth',\n",
      " 111: 'nematode, nematode worm, roundworm',\n",
      " 112: 'conch',\n",
      " 113: 'snail',\n",
      " 114: 'slug',\n",
      " 115: 'sea slug, nudibranch',\n",
      " 116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore',\n",
      " 117: 'chambered nautilus, pearly nautilus, nautilus',\n",
      " 118: 'Dungeness crab, Cancer magister',\n",
      " 119: 'rock crab, Cancer irroratus',\n",
      " 120: 'fiddler crab',\n",
      " 121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica',\n",
      " 122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus',\n",
      " 123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish',\n",
      " 124: 'crayfish, crawfish, crawdad, crawdaddy',\n",
      " 125: 'hermit crab',\n",
      " 126: 'isopod',\n",
      " 127: 'white stork, Ciconia ciconia',\n",
      " 128: 'black stork, Ciconia nigra',\n",
      " 129: 'spoonbill',\n",
      " 130: 'flamingo',\n",
      " 131: 'little blue heron, Egretta caerulea',\n",
      " 132: 'American egret, great white heron, Egretta albus',\n",
      " 133: 'bittern',\n",
      " 134: 'crane',\n",
      " 135: 'limpkin, Aramus pictus',\n",
      " 136: 'European gallinule, Porphyrio porphyrio',\n",
      " 137: 'American coot, marsh hen, mud hen, water hen, Fulica americana',\n",
      " 138: 'bustard',\n",
      " 139: 'ruddy turnstone, Arenaria interpres',\n",
      " 140: 'red-backed sandpiper, dunlin, Erolia alpina',\n",
      " 141: 'redshank, Tringa totanus',\n",
      " 142: 'dowitcher',\n",
      " 143: 'oystercatcher, oyster catcher',\n",
      " 144: 'pelican',\n",
      " 145: 'king penguin, Aptenodytes patagonica',\n",
      " 146: 'albatross, mollymawk',\n",
      " 147: 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus',\n",
      " 148: 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca',\n",
      " 149: 'dugong, Dugong dugon',\n",
      " 150: 'sea lion',\n",
      " 151: 'Chihuahua',\n",
      " 152: 'Japanese spaniel',\n",
      " 153: 'Maltese dog, Maltese terrier, Maltese',\n",
      " 154: 'Pekinese, Pekingese, Peke',\n",
      " 155: 'Shih-Tzu',\n",
      " 156: 'Blenheim spaniel',\n",
      " 157: 'papillon',\n",
      " 158: 'toy terrier',\n",
      " 159: 'Rhodesian ridgeback',\n",
      " 160: 'Afghan hound, Afghan',\n",
      " 161: 'basset, basset hound',\n",
      " 162: 'beagle',\n",
      " 163: 'bloodhound, sleuthhound',\n",
      " 164: 'bluetick',\n",
      " 165: 'black-and-tan coonhound',\n",
      " 166: 'Walker hound, Walker foxhound',\n",
      " 167: 'English foxhound',\n",
      " 168: 'redbone',\n",
      " 169: 'borzoi, Russian wolfhound',\n",
      " 170: 'Irish wolfhound',\n",
      " 171: 'Italian greyhound',\n",
      " 172: 'whippet',\n",
      " 173: 'Ibizan hound, Ibizan Podenco',\n",
      " 174: 'Norwegian elkhound, elkhound',\n",
      " 175: 'otterhound, otter hound',\n",
      " 176: 'Saluki, gazelle hound',\n",
      " 177: 'Scottish deerhound, deerhound',\n",
      " 178: 'Weimaraner',\n",
      " 179: 'Staffordshire bullterrier, Staffordshire bull terrier',\n",
      " 180: 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier',\n",
      " 181: 'Bedlington terrier',\n",
      " 182: 'Border terrier',\n",
      " 183: 'Kerry blue terrier',\n",
      " 184: 'Irish terrier',\n",
      " 185: 'Norfolk terrier',\n",
      " 186: 'Norwich terrier',\n",
      " 187: 'Yorkshire terrier',\n",
      " 188: 'wire-haired fox terrier',\n",
      " 189: 'Lakeland terrier',\n",
      " 190: 'Sealyham terrier, Sealyham',\n",
      " 191: 'Airedale, Airedale terrier',\n",
      " 192: 'cairn, cairn terrier',\n",
      " 193: 'Australian terrier',\n",
      " 194: 'Dandie Dinmont, Dandie Dinmont terrier',\n",
      " 195: 'Boston bull, Boston terrier',\n",
      " 196: 'miniature schnauzer',\n",
      " 197: 'giant schnauzer',\n",
      " 198: 'standard schnauzer',\n",
      " 199: 'Scotch terrier, Scottish terrier, Scottie',\n",
      " 200: 'Tibetan terrier, chrysanthemum dog',\n",
      " 201: 'silky terrier, Sydney silky',\n",
      " 202: 'soft-coated wheaten terrier',\n",
      " 203: 'West Highland white terrier',\n",
      " 204: 'Lhasa, Lhasa apso',\n",
      " 205: 'flat-coated retriever',\n",
      " 206: 'curly-coated retriever',\n",
      " 207: 'golden retriever',\n",
      " 208: 'Labrador retriever',\n",
      " 209: 'Chesapeake Bay retriever',\n",
      " 210: 'German short-haired pointer',\n",
      " 211: 'vizsla, Hungarian pointer',\n",
      " 212: 'English setter',\n",
      " 213: 'Irish setter, red setter',\n",
      " 214: 'Gordon setter',\n",
      " 215: 'Brittany spaniel',\n",
      " 216: 'clumber, clumber spaniel',\n",
      " 217: 'English springer, English springer spaniel',\n",
      " 218: 'Welsh springer spaniel',\n",
      " 219: 'cocker spaniel, English cocker spaniel, cocker',\n",
      " 220: 'Sussex spaniel',\n",
      " 221: 'Irish water spaniel',\n",
      " 222: 'kuvasz',\n",
      " 223: 'schipperke',\n",
      " 224: 'groenendael',\n",
      " 225: 'malinois',\n",
      " 226: 'briard',\n",
      " 227: 'kelpie',\n",
      " 228: 'komondor',\n",
      " 229: 'Old English sheepdog, bobtail',\n",
      " 230: 'Shetland sheepdog, Shetland sheep dog, Shetland',\n",
      " 231: 'collie',\n",
      " 232: 'Border collie',\n",
      " 233: 'Bouvier des Flandres, Bouviers des Flandres',\n",
      " 234: 'Rottweiler',\n",
      " 235: 'German shepherd, German shepherd dog, German police dog, alsatian',\n",
      " 236: 'Doberman, Doberman pinscher',\n",
      " 237: 'miniature pinscher',\n",
      " 238: 'Greater Swiss Mountain dog',\n",
      " 239: 'Bernese mountain dog',\n",
      " 240: 'Appenzeller',\n",
      " 241: 'EntleBucher',\n",
      " 242: 'boxer',\n",
      " 243: 'bull mastiff',\n",
      " 244: 'Tibetan mastiff',\n",
      " 245: 'French bulldog',\n",
      " 246: 'Great Dane',\n",
      " 247: 'Saint Bernard, St Bernard',\n",
      " 248: 'Eskimo dog, husky',\n",
      " 249: 'malamute, malemute, Alaskan malamute',\n",
      " 250: 'Siberian husky',\n",
      " 251: 'dalmatian, coach dog, carriage dog',\n",
      " 252: 'affenpinscher, monkey pinscher, monkey dog',\n",
      " 253: 'basenji',\n",
      " 254: 'pug, pug-dog',\n",
      " 255: 'Leonberg',\n",
      " 256: 'Newfoundland, Newfoundland dog',\n",
      " 257: 'Great Pyrenees',\n",
      " 258: 'Samoyed, Samoyede',\n",
      " 259: 'Pomeranian',\n",
      " 260: 'chow, chow chow',\n",
      " 261: 'keeshond',\n",
      " 262: 'Brabancon griffon',\n",
      " 263: 'Pembroke, Pembroke Welsh corgi',\n",
      " 264: 'Cardigan, Cardigan Welsh corgi',\n",
      " 265: 'toy poodle',\n",
      " 266: 'miniature poodle',\n",
      " 267: 'standard poodle',\n",
      " 268: 'Mexican hairless',\n",
      " 269: 'timber wolf, grey wolf, gray wolf, Canis lupus',\n",
      " 270: 'white wolf, Arctic wolf, Canis lupus tundrarum',\n",
      " 271: 'red wolf, maned wolf, Canis rufus, Canis niger',\n",
      " 272: 'coyote, prairie wolf, brush wolf, Canis latrans',\n",
      " 273: 'dingo, warrigal, warragal, Canis dingo',\n",
      " 274: 'dhole, Cuon alpinus',\n",
      " 275: 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus',\n",
      " 276: 'hyena, hyaena',\n",
      " 277: 'red fox, Vulpes vulpes',\n",
      " 278: 'kit fox, Vulpes macrotis',\n",
      " 279: 'Arctic fox, white fox, Alopex lagopus',\n",
      " 280: 'grey fox, gray fox, Urocyon cinereoargenteus',\n",
      " 281: 'tabby, tabby cat',\n",
      " 282: 'tiger cat',\n",
      " 283: 'Persian cat',\n",
      " 284: 'Siamese cat, Siamese',\n",
      " 285: 'Egyptian cat',\n",
      " 286: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n",
      " 287: 'lynx, catamount',\n",
      " 288: 'leopard, Panthera pardus',\n",
      " 289: 'snow leopard, ounce, Panthera uncia',\n",
      " 290: 'jaguar, panther, Panthera onca, Felis onca',\n",
      " 291: 'lion, king of beasts, Panthera leo',\n",
      " 292: 'tiger, Panthera tigris',\n",
      " 293: 'cheetah, chetah, Acinonyx jubatus',\n",
      " 294: 'brown bear, bruin, Ursus arctos',\n",
      " 295: 'American black bear, black bear, Ursus americanus, Euarctos americanus',\n",
      " 296: 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus',\n",
      " 297: 'sloth bear, Melursus ursinus, Ursus ursinus',\n",
      " 298: 'mongoose',\n",
      " 299: 'meerkat, mierkat',\n",
      " 300: 'tiger beetle',\n",
      " 301: 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle',\n",
      " 302: 'ground beetle, carabid beetle',\n",
      " 303: 'long-horned beetle, longicorn, longicorn beetle',\n",
      " 304: 'leaf beetle, chrysomelid',\n",
      " 305: 'dung beetle',\n",
      " 306: 'rhinoceros beetle',\n",
      " 307: 'weevil',\n",
      " 308: 'fly',\n",
      " 309: 'bee',\n",
      " 310: 'ant, emmet, pismire',\n",
      " 311: 'grasshopper, hopper',\n",
      " 312: 'cricket',\n",
      " 313: 'walking stick, walkingstick, stick insect',\n",
      " 314: 'cockroach, roach',\n",
      " 315: 'mantis, mantid',\n",
      " 316: 'cicada, cicala',\n",
      " 317: 'leafhopper',\n",
      " 318: 'lacewing, lacewing fly',\n",
      " 319: \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\",\n",
      " 320: 'damselfly',\n",
      " 321: 'admiral',\n",
      " 322: 'ringlet, ringlet butterfly',\n",
      " 323: 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus',\n",
      " 324: 'cabbage butterfly',\n",
      " 325: 'sulphur butterfly, sulfur butterfly',\n",
      " 326: 'lycaenid, lycaenid butterfly',\n",
      " 327: 'starfish, sea star',\n",
      " 328: 'sea urchin',\n",
      " 329: 'sea cucumber, holothurian',\n",
      " 330: 'wood rabbit, cottontail, cottontail rabbit',\n",
      " 331: 'hare',\n",
      " 332: 'Angora, Angora rabbit',\n",
      " 333: 'hamster',\n",
      " 334: 'porcupine, hedgehog',\n",
      " 335: 'fox squirrel, eastern fox squirrel, Sciurus niger',\n",
      " 336: 'marmot',\n",
      " 337: 'beaver',\n",
      " 338: 'guinea pig, Cavia cobaya',\n",
      " 339: 'sorrel',\n",
      " 340: 'zebra',\n",
      " 341: 'hog, pig, grunter, squealer, Sus scrofa',\n",
      " 342: 'wild boar, boar, Sus scrofa',\n",
      " 343: 'warthog',\n",
      " 344: 'hippopotamus, hippo, river horse, Hippopotamus amphibius',\n",
      " 345: 'ox',\n",
      " 346: 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis',\n",
      " 347: 'bison',\n",
      " 348: 'ram, tup',\n",
      " 349: 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis',\n",
      " 350: 'ibex, Capra ibex',\n",
      " 351: 'hartebeest',\n",
      " 352: 'impala, Aepyceros melampus',\n",
      " 353: 'gazelle',\n",
      " 354: 'Arabian camel, dromedary, Camelus dromedarius',\n",
      " 355: 'llama',\n",
      " 356: 'weasel',\n",
      " 357: 'mink',\n",
      " 358: 'polecat, fitch, foulmart, foumart, Mustela putorius',\n",
      " 359: 'black-footed ferret, ferret, Mustela nigripes',\n",
      " 360: 'otter',\n",
      " 361: 'skunk, polecat, wood pussy',\n",
      " 362: 'badger',\n",
      " 363: 'armadillo',\n",
      " 364: 'three-toed sloth, ai, Bradypus tridactylus',\n",
      " 365: 'orangutan, orang, orangutang, Pongo pygmaeus',\n",
      " 366: 'gorilla, Gorilla gorilla',\n",
      " 367: 'chimpanzee, chimp, Pan troglodytes',\n",
      " 368: 'gibbon, Hylobates lar',\n",
      " 369: 'siamang, Hylobates syndactylus, Symphalangus syndactylus',\n",
      " 370: 'guenon, guenon monkey',\n",
      " 371: 'patas, hussar monkey, Erythrocebus patas',\n",
      " 372: 'baboon',\n",
      " 373: 'macaque',\n",
      " 374: 'langur',\n",
      " 375: 'colobus, colobus monkey',\n",
      " 376: 'proboscis monkey, Nasalis larvatus',\n",
      " 377: 'marmoset',\n",
      " 378: 'capuchin, ringtail, Cebus capucinus',\n",
      " 379: 'howler monkey, howler',\n",
      " 380: 'titi, titi monkey',\n",
      " 381: 'spider monkey, Ateles geoffroyi',\n",
      " 382: 'squirrel monkey, Saimiri sciureus',\n",
      " 383: 'Madagascar cat, ring-tailed lemur, Lemur catta',\n",
      " 384: 'indri, indris, Indri indri, Indri brevicaudatus',\n",
      " 385: 'Indian elephant, Elephas maximus',\n",
      " 386: 'African elephant, Loxodonta africana',\n",
      " 387: 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens',\n",
      " 388: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca',\n",
      " 389: 'barracouta, snoek',\n",
      " 390: 'eel',\n",
      " 391: 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch',\n",
      " 392: 'rock beauty, Holocanthus tricolor',\n",
      " 393: 'anemone fish',\n",
      " 394: 'sturgeon',\n",
      " 395: 'gar, garfish, garpike, billfish, Lepisosteus osseus',\n",
      " 396: 'lionfish',\n",
      " 397: 'puffer, pufferfish, blowfish, globefish',\n",
      " 398: 'abacus',\n",
      " 399: 'abaya',\n",
      " 400: \"academic gown, academic robe, judge's robe\",\n",
      " 401: 'accordion, piano accordion, squeeze box',\n",
      " 402: 'acoustic guitar',\n",
      " 403: 'aircraft carrier, carrier, flattop, attack aircraft carrier',\n",
      " 404: 'airliner',\n",
      " 405: 'airship, dirigible',\n",
      " 406: 'altar',\n",
      " 407: 'ambulance',\n",
      " 408: 'amphibian, amphibious vehicle',\n",
      " 409: 'analog clock',\n",
      " 410: 'apiary, bee house',\n",
      " 411: 'apron',\n",
      " 412: 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin',\n",
      " 413: 'assault rifle, assault gun',\n",
      " 414: 'backpack, back pack, knapsack, packsack, rucksack, haversack',\n",
      " 415: 'bakery, bakeshop, bakehouse',\n",
      " 416: 'balance beam, beam',\n",
      " 417: 'balloon',\n",
      " 418: 'ballpoint, ballpoint pen, ballpen, Biro',\n",
      " 419: 'Band Aid',\n",
      " 420: 'banjo',\n",
      " 421: 'bannister, banister, balustrade, balusters, handrail',\n",
      " 422: 'barbell',\n",
      " 423: 'barber chair',\n",
      " 424: 'barbershop',\n",
      " 425: 'barn',\n",
      " 426: 'barometer',\n",
      " 427: 'barrel, cask',\n",
      " 428: 'barrow, garden cart, lawn cart, wheelbarrow',\n",
      " 429: 'baseball',\n",
      " 430: 'basketball',\n",
      " 431: 'bassinet',\n",
      " 432: 'bassoon',\n",
      " 433: 'bathing cap, swimming cap',\n",
      " 434: 'bath towel',\n",
      " 435: 'bathtub, bathing tub, bath, tub',\n",
      " 436: 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon',\n",
      " 437: 'beacon, lighthouse, beacon light, pharos',\n",
      " 438: 'beaker',\n",
      " 439: 'bearskin, busby, shako',\n",
      " 440: 'beer bottle',\n",
      " 441: 'beer glass',\n",
      " 442: 'bell cote, bell cot',\n",
      " 443: 'bib',\n",
      " 444: 'bicycle-built-for-two, tandem bicycle, tandem',\n",
      " 445: 'bikini, two-piece',\n",
      " 446: 'binder, ring-binder',\n",
      " 447: 'binoculars, field glasses, opera glasses',\n",
      " 448: 'birdhouse',\n",
      " 449: 'boathouse',\n",
      " 450: 'bobsled, bobsleigh, bob',\n",
      " 451: 'bolo tie, bolo, bola tie, bola',\n",
      " 452: 'bonnet, poke bonnet',\n",
      " 453: 'bookcase',\n",
      " 454: 'bookshop, bookstore, bookstall',\n",
      " 455: 'bottlecap',\n",
      " 456: 'bow',\n",
      " 457: 'bow tie, bow-tie, bowtie',\n",
      " 458: 'brass, memorial tablet, plaque',\n",
      " 459: 'brassiere, bra, bandeau',\n",
      " 460: 'breakwater, groin, groyne, mole, bulwark, seawall, jetty',\n",
      " 461: 'breastplate, aegis, egis',\n",
      " 462: 'broom',\n",
      " 463: 'bucket, pail',\n",
      " 464: 'buckle',\n",
      " 465: 'bulletproof vest',\n",
      " 466: 'bullet train, bullet',\n",
      " 467: 'butcher shop, meat market',\n",
      " 468: 'cab, hack, taxi, taxicab',\n",
      " 469: 'caldron, cauldron',\n",
      " 470: 'candle, taper, wax light',\n",
      " 471: 'cannon',\n",
      " 472: 'canoe',\n",
      " 473: 'can opener, tin opener',\n",
      " 474: 'cardigan',\n",
      " 475: 'car mirror',\n",
      " 476: 'carousel, carrousel, merry-go-round, roundabout, whirligig',\n",
      " 477: \"carpenter's kit, tool kit\",\n",
      " 478: 'carton',\n",
      " 479: 'car wheel',\n",
      " 480: 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM',\n",
      " 481: 'cassette',\n",
      " 482: 'cassette player',\n",
      " 483: 'castle',\n",
      " 484: 'catamaran',\n",
      " 485: 'CD player',\n",
      " 486: 'cello, violoncello',\n",
      " 487: 'cellular telephone, cellular phone, cellphone, cell, mobile phone',\n",
      " 488: 'chain',\n",
      " 489: 'chainlink fence',\n",
      " 490: 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour',\n",
      " 491: 'chain saw, chainsaw',\n",
      " 492: 'chest',\n",
      " 493: 'chiffonier, commode',\n",
      " 494: 'chime, bell, gong',\n",
      " 495: 'china cabinet, china closet',\n",
      " 496: 'Christmas stocking',\n",
      " 497: 'church, church building',\n",
      " 498: 'cinema, movie theater, movie theatre, movie house, picture palace',\n",
      " 499: 'cleaver, meat cleaver, chopper',\n",
      " 500: 'cliff dwelling',\n",
      " 501: 'cloak',\n",
      " 502: 'clog, geta, patten, sabot',\n",
      " 503: 'cocktail shaker',\n",
      " 504: 'coffee mug',\n",
      " 505: 'coffeepot',\n",
      " 506: 'coil, spiral, volute, whorl, helix',\n",
      " 507: 'combination lock',\n",
      " 508: 'computer keyboard, keypad',\n",
      " 509: 'confectionery, confectionary, candy store',\n",
      " 510: 'container ship, containership, container vessel',\n",
      " 511: 'convertible',\n",
      " 512: 'corkscrew, bottle screw',\n",
      " 513: 'cornet, horn, trumpet, trump',\n",
      " 514: 'cowboy boot',\n",
      " 515: 'cowboy hat, ten-gallon hat',\n",
      " 516: 'cradle',\n",
      " 517: 'crane',\n",
      " 518: 'crash helmet',\n",
      " 519: 'crate',\n",
      " 520: 'crib, cot',\n",
      " 521: 'Crock Pot',\n",
      " 522: 'croquet ball',\n",
      " 523: 'crutch',\n",
      " 524: 'cuirass',\n",
      " 525: 'dam, dike, dyke',\n",
      " 526: 'desk',\n",
      " 527: 'desktop computer',\n",
      " 528: 'dial telephone, dial phone',\n",
      " 529: 'diaper, nappy, napkin',\n",
      " 530: 'digital clock',\n",
      " 531: 'digital watch',\n",
      " 532: 'dining table, board',\n",
      " 533: 'dishrag, dishcloth',\n",
      " 534: 'dishwasher, dish washer, dishwashing machine',\n",
      " 535: 'disk brake, disc brake',\n",
      " 536: 'dock, dockage, docking facility',\n",
      " 537: 'dogsled, dog sled, dog sleigh',\n",
      " 538: 'dome',\n",
      " 539: 'doormat, welcome mat',\n",
      " 540: 'drilling platform, offshore rig',\n",
      " 541: 'drum, membranophone, tympan',\n",
      " 542: 'drumstick',\n",
      " 543: 'dumbbell',\n",
      " 544: 'Dutch oven',\n",
      " 545: 'electric fan, blower',\n",
      " 546: 'electric guitar',\n",
      " 547: 'electric locomotive',\n",
      " 548: 'entertainment center',\n",
      " 549: 'envelope',\n",
      " 550: 'espresso maker',\n",
      " 551: 'face powder',\n",
      " 552: 'feather boa, boa',\n",
      " 553: 'file, file cabinet, filing cabinet',\n",
      " 554: 'fireboat',\n",
      " 555: 'fire engine, fire truck',\n",
      " 556: 'fire screen, fireguard',\n",
      " 557: 'flagpole, flagstaff',\n",
      " 558: 'flute, transverse flute',\n",
      " 559: 'folding chair',\n",
      " 560: 'football helmet',\n",
      " 561: 'forklift',\n",
      " 562: 'fountain',\n",
      " 563: 'fountain pen',\n",
      " 564: 'four-poster',\n",
      " 565: 'freight car',\n",
      " 566: 'French horn, horn',\n",
      " 567: 'frying pan, frypan, skillet',\n",
      " 568: 'fur coat',\n",
      " 569: 'garbage truck, dustcart',\n",
      " 570: 'gasmask, respirator, gas helmet',\n",
      " 571: 'gas pump, gasoline pump, petrol pump, island dispenser',\n",
      " 572: 'goblet',\n",
      " 573: 'go-kart',\n",
      " 574: 'golf ball',\n",
      " 575: 'golfcart, golf cart',\n",
      " 576: 'gondola',\n",
      " 577: 'gong, tam-tam',\n",
      " 578: 'gown',\n",
      " 579: 'grand piano, grand',\n",
      " 580: 'greenhouse, nursery, glasshouse',\n",
      " 581: 'grille, radiator grille',\n",
      " 582: 'grocery store, grocery, food market, market',\n",
      " 583: 'guillotine',\n",
      " 584: 'hair slide',\n",
      " 585: 'hair spray',\n",
      " 586: 'half track',\n",
      " 587: 'hammer',\n",
      " 588: 'hamper',\n",
      " 589: 'hand blower, blow dryer, blow drier, hair dryer, hair drier',\n",
      " 590: 'hand-held computer, hand-held microcomputer',\n",
      " 591: 'handkerchief, hankie, hanky, hankey',\n",
      " 592: 'hard disc, hard disk, fixed disk',\n",
      " 593: 'harmonica, mouth organ, harp, mouth harp',\n",
      " 594: 'harp',\n",
      " 595: 'harvester, reaper',\n",
      " 596: 'hatchet',\n",
      " 597: 'holster',\n",
      " 598: 'home theater, home theatre',\n",
      " 599: 'honeycomb',\n",
      " 600: 'hook, claw',\n",
      " 601: 'hoopskirt, crinoline',\n",
      " 602: 'horizontal bar, high bar',\n",
      " 603: 'horse cart, horse-cart',\n",
      " 604: 'hourglass',\n",
      " 605: 'iPod',\n",
      " 606: 'iron, smoothing iron',\n",
      " 607: \"jack-o'-lantern\",\n",
      " 608: 'jean, blue jean, denim',\n",
      " 609: 'jeep, landrover',\n",
      " 610: 'jersey, T-shirt, tee shirt',\n",
      " 611: 'jigsaw puzzle',\n",
      " 612: 'jinrikisha, ricksha, rickshaw',\n",
      " 613: 'joystick',\n",
      " 614: 'kimono',\n",
      " 615: 'knee pad',\n",
      " 616: 'knot',\n",
      " 617: 'lab coat, laboratory coat',\n",
      " 618: 'ladle',\n",
      " 619: 'lampshade, lamp shade',\n",
      " 620: 'laptop, laptop computer',\n",
      " 621: 'lawn mower, mower',\n",
      " 622: 'lens cap, lens cover',\n",
      " 623: 'letter opener, paper knife, paperknife',\n",
      " 624: 'library',\n",
      " 625: 'lifeboat',\n",
      " 626: 'lighter, light, igniter, ignitor',\n",
      " 627: 'limousine, limo',\n",
      " 628: 'liner, ocean liner',\n",
      " 629: 'lipstick, lip rouge',\n",
      " 630: 'Loafer',\n",
      " 631: 'lotion',\n",
      " 632: 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system',\n",
      " 633: \"loupe, jeweler's loupe\",\n",
      " 634: 'lumbermill, sawmill',\n",
      " 635: 'magnetic compass',\n",
      " 636: 'mailbag, postbag',\n",
      " 637: 'mailbox, letter box',\n",
      " 638: 'maillot',\n",
      " 639: 'maillot, tank suit',\n",
      " 640: 'manhole cover',\n",
      " 641: 'maraca',\n",
      " 642: 'marimba, xylophone',\n",
      " 643: 'mask',\n",
      " 644: 'matchstick',\n",
      " 645: 'maypole',\n",
      " 646: 'maze, labyrinth',\n",
      " 647: 'measuring cup',\n",
      " 648: 'medicine chest, medicine cabinet',\n",
      " 649: 'megalith, megalithic structure',\n",
      " 650: 'microphone, mike',\n",
      " 651: 'microwave, microwave oven',\n",
      " 652: 'military uniform',\n",
      " 653: 'milk can',\n",
      " 654: 'minibus',\n",
      " 655: 'miniskirt, mini',\n",
      " 656: 'minivan',\n",
      " 657: 'missile',\n",
      " 658: 'mitten',\n",
      " 659: 'mixing bowl',\n",
      " 660: 'mobile home, manufactured home',\n",
      " 661: 'Model T',\n",
      " 662: 'modem',\n",
      " 663: 'monastery',\n",
      " 664: 'monitor',\n",
      " 665: 'moped',\n",
      " 666: 'mortar',\n",
      " 667: 'mortarboard',\n",
      " 668: 'mosque',\n",
      " 669: 'mosquito net',\n",
      " 670: 'motor scooter, scooter',\n",
      " 671: 'mountain bike, all-terrain bike, off-roader',\n",
      " 672: 'mountain tent',\n",
      " 673: 'mouse, computer mouse',\n",
      " 674: 'mousetrap',\n",
      " 675: 'moving van',\n",
      " 676: 'muzzle',\n",
      " 677: 'nail',\n",
      " 678: 'neck brace',\n",
      " 679: 'necklace',\n",
      " 680: 'nipple',\n",
      " 681: 'notebook, notebook computer',\n",
      " 682: 'obelisk',\n",
      " 683: 'oboe, hautboy, hautbois',\n",
      " 684: 'ocarina, sweet potato',\n",
      " 685: 'odometer, hodometer, mileometer, milometer',\n",
      " 686: 'oil filter',\n",
      " 687: 'organ, pipe organ',\n",
      " 688: 'oscilloscope, scope, cathode-ray oscilloscope, CRO',\n",
      " 689: 'overskirt',\n",
      " 690: 'oxcart',\n",
      " 691: 'oxygen mask',\n",
      " 692: 'packet',\n",
      " 693: 'paddle, boat paddle',\n",
      " 694: 'paddlewheel, paddle wheel',\n",
      " 695: 'padlock',\n",
      " 696: 'paintbrush',\n",
      " 697: \"pajama, pyjama, pj's, jammies\",\n",
      " 698: 'palace',\n",
      " 699: 'panpipe, pandean pipe, syrinx',\n",
      " 700: 'paper towel',\n",
      " 701: 'parachute, chute',\n",
      " 702: 'parallel bars, bars',\n",
      " 703: 'park bench',\n",
      " 704: 'parking meter',\n",
      " 705: 'passenger car, coach, carriage',\n",
      " 706: 'patio, terrace',\n",
      " 707: 'pay-phone, pay-station',\n",
      " 708: 'pedestal, plinth, footstall',\n",
      " 709: 'pencil box, pencil case',\n",
      " 710: 'pencil sharpener',\n",
      " 711: 'perfume, essence',\n",
      " 712: 'Petri dish',\n",
      " 713: 'photocopier',\n",
      " 714: 'pick, plectrum, plectron',\n",
      " 715: 'pickelhaube',\n",
      " 716: 'picket fence, paling',\n",
      " 717: 'pickup, pickup truck',\n",
      " 718: 'pier',\n",
      " 719: 'piggy bank, penny bank',\n",
      " 720: 'pill bottle',\n",
      " 721: 'pillow',\n",
      " 722: 'ping-pong ball',\n",
      " 723: 'pinwheel',\n",
      " 724: 'pirate, pirate ship',\n",
      " 725: 'pitcher, ewer',\n",
      " 726: \"plane, carpenter's plane, woodworking plane\",\n",
      " 727: 'planetarium',\n",
      " 728: 'plastic bag',\n",
      " 729: 'plate rack',\n",
      " 730: 'plow, plough',\n",
      " 731: \"plunger, plumber's helper\",\n",
      " 732: 'Polaroid camera, Polaroid Land camera',\n",
      " 733: 'pole',\n",
      " 734: 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria',\n",
      " 735: 'poncho',\n",
      " 736: 'pool table, billiard table, snooker table',\n",
      " 737: 'pop bottle, soda bottle',\n",
      " 738: 'pot, flowerpot',\n",
      " 739: \"potter's wheel\",\n",
      " 740: 'power drill',\n",
      " 741: 'prayer rug, prayer mat',\n",
      " 742: 'printer',\n",
      " 743: 'prison, prison house',\n",
      " 744: 'projectile, missile',\n",
      " 745: 'projector',\n",
      " 746: 'puck, hockey puck',\n",
      " 747: 'punching bag, punch bag, punching ball, punchball',\n",
      " 748: 'purse',\n",
      " 749: 'quill, quill pen',\n",
      " 750: 'quilt, comforter, comfort, puff',\n",
      " 751: 'racer, race car, racing car',\n",
      " 752: 'racket, racquet',\n",
      " 753: 'radiator',\n",
      " 754: 'radio, wireless',\n",
      " 755: 'radio telescope, radio reflector',\n",
      " 756: 'rain barrel',\n",
      " 757: 'recreational vehicle, RV, R.V.',\n",
      " 758: 'reel',\n",
      " 759: 'reflex camera',\n",
      " 760: 'refrigerator, icebox',\n",
      " 761: 'remote control, remote',\n",
      " 762: 'restaurant, eating house, eating place, eatery',\n",
      " 763: 'revolver, six-gun, six-shooter',\n",
      " 764: 'rifle',\n",
      " 765: 'rocking chair, rocker',\n",
      " 766: 'rotisserie',\n",
      " 767: 'rubber eraser, rubber, pencil eraser',\n",
      " 768: 'rugby ball',\n",
      " 769: 'rule, ruler',\n",
      " 770: 'running shoe',\n",
      " 771: 'safe',\n",
      " 772: 'safety pin',\n",
      " 773: 'saltshaker, salt shaker',\n",
      " 774: 'sandal',\n",
      " 775: 'sarong',\n",
      " 776: 'sax, saxophone',\n",
      " 777: 'scabbard',\n",
      " 778: 'scale, weighing machine',\n",
      " 779: 'school bus',\n",
      " 780: 'schooner',\n",
      " 781: 'scoreboard',\n",
      " 782: 'screen, CRT screen',\n",
      " 783: 'screw',\n",
      " 784: 'screwdriver',\n",
      " 785: 'seat belt, seatbelt',\n",
      " 786: 'sewing machine',\n",
      " 787: 'shield, buckler',\n",
      " 788: 'shoe shop, shoe-shop, shoe store',\n",
      " 789: 'shoji',\n",
      " 790: 'shopping basket',\n",
      " 791: 'shopping cart',\n",
      " 792: 'shovel',\n",
      " 793: 'shower cap',\n",
      " 794: 'shower curtain',\n",
      " 795: 'ski',\n",
      " 796: 'ski mask',\n",
      " 797: 'sleeping bag',\n",
      " 798: 'slide rule, slipstick',\n",
      " 799: 'sliding door',\n",
      " 800: 'slot, one-armed bandit',\n",
      " 801: 'snorkel',\n",
      " 802: 'snowmobile',\n",
      " 803: 'snowplow, snowplough',\n",
      " 804: 'soap dispenser',\n",
      " 805: 'soccer ball',\n",
      " 806: 'sock',\n",
      " 807: 'solar dish, solar collector, solar furnace',\n",
      " 808: 'sombrero',\n",
      " 809: 'soup bowl',\n",
      " 810: 'space bar',\n",
      " 811: 'space heater',\n",
      " 812: 'space shuttle',\n",
      " 813: 'spatula',\n",
      " 814: 'speedboat',\n",
      " 815: \"spider web, spider's web\",\n",
      " 816: 'spindle',\n",
      " 817: 'sports car, sport car',\n",
      " 818: 'spotlight, spot',\n",
      " 819: 'stage',\n",
      " 820: 'steam locomotive',\n",
      " 821: 'steel arch bridge',\n",
      " 822: 'steel drum',\n",
      " 823: 'stethoscope',\n",
      " 824: 'stole',\n",
      " 825: 'stone wall',\n",
      " 826: 'stopwatch, stop watch',\n",
      " 827: 'stove',\n",
      " 828: 'strainer',\n",
      " 829: 'streetcar, tram, tramcar, trolley, trolley car',\n",
      " 830: 'stretcher',\n",
      " 831: 'studio couch, day bed',\n",
      " 832: 'stupa, tope',\n",
      " 833: 'submarine, pigboat, sub, U-boat',\n",
      " 834: 'suit, suit of clothes',\n",
      " 835: 'sundial',\n",
      " 836: 'sunglass',\n",
      " 837: 'sunglasses, dark glasses, shades',\n",
      " 838: 'sunscreen, sunblock, sun blocker',\n",
      " 839: 'suspension bridge',\n",
      " 840: 'swab, swob, mop',\n",
      " 841: 'sweatshirt',\n",
      " 842: 'swimming trunks, bathing trunks',\n",
      " 843: 'swing',\n",
      " 844: 'switch, electric switch, electrical switch',\n",
      " 845: 'syringe',\n",
      " 846: 'table lamp',\n",
      " 847: 'tank, army tank, armored combat vehicle, armoured combat vehicle',\n",
      " 848: 'tape player',\n",
      " 849: 'teapot',\n",
      " 850: 'teddy, teddy bear',\n",
      " 851: 'television, television system',\n",
      " 852: 'tennis ball',\n",
      " 853: 'thatch, thatched roof',\n",
      " 854: 'theater curtain, theatre curtain',\n",
      " 855: 'thimble',\n",
      " 856: 'thresher, thrasher, threshing machine',\n",
      " 857: 'throne',\n",
      " 858: 'tile roof',\n",
      " 859: 'toaster',\n",
      " 860: 'tobacco shop, tobacconist shop, tobacconist',\n",
      " 861: 'toilet seat',\n",
      " 862: 'torch',\n",
      " 863: 'totem pole',\n",
      " 864: 'tow truck, tow car, wrecker',\n",
      " 865: 'toyshop',\n",
      " 866: 'tractor',\n",
      " 867: 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi',\n",
      " 868: 'tray',\n",
      " 869: 'trench coat',\n",
      " 870: 'tricycle, trike, velocipede',\n",
      " 871: 'trimaran',\n",
      " 872: 'tripod',\n",
      " 873: 'triumphal arch',\n",
      " 874: 'trolleybus, trolley coach, trackless trolley',\n",
      " 875: 'trombone',\n",
      " 876: 'tub, vat',\n",
      " 877: 'turnstile',\n",
      " 878: 'typewriter keyboard',\n",
      " 879: 'umbrella',\n",
      " 880: 'unicycle, monocycle',\n",
      " 881: 'upright, upright piano',\n",
      " 882: 'vacuum, vacuum cleaner',\n",
      " 883: 'vase',\n",
      " 884: 'vault',\n",
      " 885: 'velvet',\n",
      " 886: 'vending machine',\n",
      " 887: 'vestment',\n",
      " 888: 'viaduct',\n",
      " 889: 'violin, fiddle',\n",
      " 890: 'volleyball',\n",
      " 891: 'waffle iron',\n",
      " 892: 'wall clock',\n",
      " 893: 'wallet, billfold, notecase, pocketbook',\n",
      " 894: 'wardrobe, closet, press',\n",
      " 895: 'warplane, military plane',\n",
      " 896: 'washbasin, handbasin, washbowl, lavabo, wash-hand basin',\n",
      " 897: 'washer, automatic washer, washing machine',\n",
      " 898: 'water bottle',\n",
      " 899: 'water jug',\n",
      " 900: 'water tower',\n",
      " 901: 'whiskey jug',\n",
      " 902: 'whistle',\n",
      " 903: 'wig',\n",
      " 904: 'window screen',\n",
      " 905: 'window shade',\n",
      " 906: 'Windsor tie',\n",
      " 907: 'wine bottle',\n",
      " 908: 'wing',\n",
      " 909: 'wok',\n",
      " 910: 'wooden spoon',\n",
      " 911: 'wool, woolen, woollen',\n",
      " 912: 'worm fence, snake fence, snake-rail fence, Virginia fence',\n",
      " 913: 'wreck',\n",
      " 914: 'yawl',\n",
      " 915: 'yurt',\n",
      " 916: 'web site, website, internet site, site',\n",
      " 917: 'comic book',\n",
      " 918: 'crossword puzzle, crossword',\n",
      " 919: 'street sign',\n",
      " 920: 'traffic light, traffic signal, stoplight',\n",
      " 921: 'book jacket, dust cover, dust jacket, dust wrapper',\n",
      " 922: 'menu',\n",
      " 923: 'plate',\n",
      " 924: 'guacamole',\n",
      " 925: 'consomme',\n",
      " 926: 'hot pot, hotpot',\n",
      " 927: 'trifle',\n",
      " 928: 'ice cream, icecream',\n",
      " 929: 'ice lolly, lolly, lollipop, popsicle',\n",
      " 930: 'French loaf',\n",
      " 931: 'bagel, beigel',\n",
      " 932: 'pretzel',\n",
      " 933: 'cheeseburger',\n",
      " 934: 'hotdog, hot dog, red hot',\n",
      " 935: 'mashed potato',\n",
      " 936: 'head cabbage',\n",
      " 937: 'broccoli',\n",
      " 938: 'cauliflower',\n",
      " 939: 'zucchini, courgette',\n",
      " 940: 'spaghetti squash',\n",
      " 941: 'acorn squash',\n",
      " 942: 'butternut squash',\n",
      " 943: 'cucumber, cuke',\n",
      " 944: 'artichoke, globe artichoke',\n",
      " 945: 'bell pepper',\n",
      " 946: 'cardoon',\n",
      " 947: 'mushroom',\n",
      " 948: 'Granny Smith',\n",
      " 949: 'strawberry',\n",
      " 950: 'orange',\n",
      " 951: 'lemon',\n",
      " 952: 'fig',\n",
      " 953: 'pineapple, ananas',\n",
      " 954: 'banana',\n",
      " 955: 'jackfruit, jak, jack',\n",
      " 956: 'custard apple',\n",
      " 957: 'pomegranate',\n",
      " 958: 'hay',\n",
      " 959: 'carbonara',\n",
      " 960: 'chocolate sauce, chocolate syrup',\n",
      " 961: 'dough',\n",
      " 962: 'meat loaf, meatloaf',\n",
      " 963: 'pizza, pizza pie',\n",
      " 964: 'potpie',\n",
      " 965: 'burrito',\n",
      " 966: 'red wine',\n",
      " 967: 'espresso',\n",
      " 968: 'cup',\n",
      " 969: 'eggnog',\n",
      " 970: 'alp',\n",
      " 971: 'bubble',\n",
      " 972: 'cliff, drop, drop-off',\n",
      " 973: 'coral reef',\n",
      " 974: 'geyser',\n",
      " 975: 'lakeside, lakeshore',\n",
      " 976: 'promontory, headland, head, foreland',\n",
      " 977: 'sandbar, sand bar',\n",
      " 978: 'seashore, coast, seacoast, sea-coast',\n",
      " 979: 'valley, vale',\n",
      " 980: 'volcano',\n",
      " 981: 'ballplayer, baseball player',\n",
      " 982: 'groom, bridegroom',\n",
      " 983: 'scuba diver',\n",
      " 984: 'rapeseed',\n",
      " 985: 'daisy',\n",
      " 986: \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\",\n",
      " 987: 'corn',\n",
      " 988: 'acorn',\n",
      " 989: 'hip, rose hip, rosehip',\n",
      " 990: 'buckeye, horse chestnut, conker',\n",
      " 991: 'coral fungus',\n",
      " 992: 'agaric',\n",
      " 993: 'gyromitra',\n",
      " 994: 'stinkhorn, carrion fungus',\n",
      " 995: 'earthstar',\n",
      " 996: 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa',\n",
      " 997: 'bolete',\n",
      " 998: 'ear, spike, capitulum',\n",
      " 999: 'toilet tissue, toilet paper, bathroom tissue'}\n",
      "70/1:\n",
      "from tensorflow.keras.applications import EfficientNetB4\n",
      "from tensorflow.image import resize\n",
      "model = EfficientNetB4(weights='imagenet')\n",
      "\n",
      "def center_crop(img, new_width=None, new_height=None):        \n",
      "\n",
      "    width = img.shape[1]\n",
      "    height = img.shape[0]\n",
      "\n",
      "    if new_width is None:\n",
      "        new_width = min(width, height)\n",
      "\n",
      "    if new_height is None:\n",
      "        new_height = min(width, height)\n",
      "\n",
      "    left = int(np.ceil((width - new_width) / 2))\n",
      "    right = width - int(np.floor((width - new_width) / 2))\n",
      "\n",
      "    top = int(np.ceil((height - new_height) / 2))\n",
      "    bottom = height - int(np.floor((height - new_height) / 2))\n",
      "\n",
      "    if len(img.shape) == 2:\n",
      "        center_cropped_img = img[top:bottom, left:right]\n",
      "    else:\n",
      "        center_cropped_img = img[top:bottom, left:right, ...]\n",
      "\n",
      "    return center_cropped_img\n",
      "\n",
      "class_dict = {0: 'tench, Tinca tinca',\n",
      " 1: 'goldfish, Carassius auratus',\n",
      " 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n",
      " 3: 'tiger shark, Galeocerdo cuvieri',\n",
      " 4: 'hammerhead, hammerhead shark',\n",
      " 5: 'electric ray, crampfish, numbfish, torpedo',\n",
      " 6: 'stingray',\n",
      " 7: 'cock',\n",
      " 8: 'hen',\n",
      " 9: 'ostrich, Struthio camelus',\n",
      " 10: 'brambling, Fringilla montifringilla',\n",
      " 11: 'goldfinch, Carduelis carduelis',\n",
      " 12: 'house finch, linnet, Carpodacus mexicanus',\n",
      " 13: 'junco, snowbird',\n",
      " 14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea',\n",
      " 15: 'robin, American robin, Turdus migratorius',\n",
      " 16: 'bulbul',\n",
      " 17: 'jay',\n",
      " 18: 'magpie',\n",
      " 19: 'chickadee',\n",
      " 20: 'water ouzel, dipper',\n",
      " 21: 'kite',\n",
      " 22: 'bald eagle, American eagle, Haliaeetus leucocephalus',\n",
      " 23: 'vulture',\n",
      " 24: 'great grey owl, great gray owl, Strix nebulosa',\n",
      " 25: 'European fire salamander, Salamandra salamandra',\n",
      " 26: 'common newt, Triturus vulgaris',\n",
      " 27: 'eft',\n",
      " 28: 'spotted salamander, Ambystoma maculatum',\n",
      " 29: 'axolotl, mud puppy, Ambystoma mexicanum',\n",
      " 30: 'bullfrog, Rana catesbeiana',\n",
      " 31: 'tree frog, tree-frog',\n",
      " 32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui',\n",
      " 33: 'loggerhead, loggerhead turtle, Caretta caretta',\n",
      " 34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea',\n",
      " 35: 'mud turtle',\n",
      " 36: 'terrapin',\n",
      " 37: 'box turtle, box tortoise',\n",
      " 38: 'banded gecko',\n",
      " 39: 'common iguana, iguana, Iguana iguana',\n",
      " 40: 'American chameleon, anole, Anolis carolinensis',\n",
      " 41: 'whiptail, whiptail lizard',\n",
      " 42: 'agama',\n",
      " 43: 'frilled lizard, Chlamydosaurus kingi',\n",
      " 44: 'alligator lizard',\n",
      " 45: 'Gila monster, Heloderma suspectum',\n",
      " 46: 'green lizard, Lacerta viridis',\n",
      " 47: 'African chameleon, Chamaeleo chamaeleon',\n",
      " 48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis',\n",
      " 49: 'African crocodile, Nile crocodile, Crocodylus niloticus',\n",
      " 50: 'American alligator, Alligator mississipiensis',\n",
      " 51: 'triceratops',\n",
      " 52: 'thunder snake, worm snake, Carphophis amoenus',\n",
      " 53: 'ringneck snake, ring-necked snake, ring snake',\n",
      " 54: 'hognose snake, puff adder, sand viper',\n",
      " 55: 'green snake, grass snake',\n",
      " 56: 'king snake, kingsnake',\n",
      " 57: 'garter snake, grass snake',\n",
      " 58: 'water snake',\n",
      " 59: 'vine snake',\n",
      " 60: 'night snake, Hypsiglena torquata',\n",
      " 61: 'boa constrictor, Constrictor constrictor',\n",
      " 62: 'rock python, rock snake, Python sebae',\n",
      " 63: 'Indian cobra, Naja naja',\n",
      " 64: 'green mamba',\n",
      " 65: 'sea snake',\n",
      " 66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus',\n",
      " 67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus',\n",
      " 68: 'sidewinder, horned rattlesnake, Crotalus cerastes',\n",
      " 69: 'trilobite',\n",
      " 70: 'harvestman, daddy longlegs, Phalangium opilio',\n",
      " 71: 'scorpion',\n",
      " 72: 'black and gold garden spider, Argiope aurantia',\n",
      " 73: 'barn spider, Araneus cavaticus',\n",
      " 74: 'garden spider, Aranea diademata',\n",
      " 75: 'black widow, Latrodectus mactans',\n",
      " 76: 'tarantula',\n",
      " 77: 'wolf spider, hunting spider',\n",
      " 78: 'tick',\n",
      " 79: 'centipede',\n",
      " 80: 'black grouse',\n",
      " 81: 'ptarmigan',\n",
      " 82: 'ruffed grouse, partridge, Bonasa umbellus',\n",
      " 83: 'prairie chicken, prairie grouse, prairie fowl',\n",
      " 84: 'peacock',\n",
      " 85: 'quail',\n",
      " 86: 'partridge',\n",
      " 87: 'African grey, African gray, Psittacus erithacus',\n",
      " 88: 'macaw',\n",
      " 89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n",
      " 90: 'lorikeet',\n",
      " 91: 'coucal',\n",
      " 92: 'bee eater',\n",
      " 93: 'hornbill',\n",
      " 94: 'hummingbird',\n",
      " 95: 'jacamar',\n",
      " 96: 'toucan',\n",
      " 97: 'drake',\n",
      " 98: 'red-breasted merganser, Mergus serrator',\n",
      " 99: 'goose',\n",
      " 100: 'black swan, Cygnus atratus',\n",
      " 101: 'tusker',\n",
      " 102: 'echidna, spiny anteater, anteater',\n",
      " 103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n",
      " 104: 'wallaby, brush kangaroo',\n",
      " 105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n",
      " 106: 'wombat',\n",
      " 107: 'jellyfish',\n",
      " 108: 'sea anemone, anemone',\n",
      " 109: 'brain coral',\n",
      " 110: 'flatworm, platyhelminth',\n",
      " 111: 'nematode, nematode worm, roundworm',\n",
      " 112: 'conch',\n",
      " 113: 'snail',\n",
      " 114: 'slug',\n",
      " 115: 'sea slug, nudibranch',\n",
      " 116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore',\n",
      " 117: 'chambered nautilus, pearly nautilus, nautilus',\n",
      " 118: 'Dungeness crab, Cancer magister',\n",
      " 119: 'rock crab, Cancer irroratus',\n",
      " 120: 'fiddler crab',\n",
      " 121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica',\n",
      " 122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus',\n",
      " 123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish',\n",
      " 124: 'crayfish, crawfish, crawdad, crawdaddy',\n",
      " 125: 'hermit crab',\n",
      " 126: 'isopod',\n",
      " 127: 'white stork, Ciconia ciconia',\n",
      " 128: 'black stork, Ciconia nigra',\n",
      " 129: 'spoonbill',\n",
      " 130: 'flamingo',\n",
      " 131: 'little blue heron, Egretta caerulea',\n",
      " 132: 'American egret, great white heron, Egretta albus',\n",
      " 133: 'bittern',\n",
      " 134: 'crane',\n",
      " 135: 'limpkin, Aramus pictus',\n",
      " 136: 'European gallinule, Porphyrio porphyrio',\n",
      " 137: 'American coot, marsh hen, mud hen, water hen, Fulica americana',\n",
      " 138: 'bustard',\n",
      " 139: 'ruddy turnstone, Arenaria interpres',\n",
      " 140: 'red-backed sandpiper, dunlin, Erolia alpina',\n",
      " 141: 'redshank, Tringa totanus',\n",
      " 142: 'dowitcher',\n",
      " 143: 'oystercatcher, oyster catcher',\n",
      " 144: 'pelican',\n",
      " 145: 'king penguin, Aptenodytes patagonica',\n",
      " 146: 'albatross, mollymawk',\n",
      " 147: 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus',\n",
      " 148: 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca',\n",
      " 149: 'dugong, Dugong dugon',\n",
      " 150: 'sea lion',\n",
      " 151: 'Chihuahua',\n",
      " 152: 'Japanese spaniel',\n",
      " 153: 'Maltese dog, Maltese terrier, Maltese',\n",
      " 154: 'Pekinese, Pekingese, Peke',\n",
      " 155: 'Shih-Tzu',\n",
      " 156: 'Blenheim spaniel',\n",
      " 157: 'papillon',\n",
      " 158: 'toy terrier',\n",
      " 159: 'Rhodesian ridgeback',\n",
      " 160: 'Afghan hound, Afghan',\n",
      " 161: 'basset, basset hound',\n",
      " 162: 'beagle',\n",
      " 163: 'bloodhound, sleuthhound',\n",
      " 164: 'bluetick',\n",
      " 165: 'black-and-tan coonhound',\n",
      " 166: 'Walker hound, Walker foxhound',\n",
      " 167: 'English foxhound',\n",
      " 168: 'redbone',\n",
      " 169: 'borzoi, Russian wolfhound',\n",
      " 170: 'Irish wolfhound',\n",
      " 171: 'Italian greyhound',\n",
      " 172: 'whippet',\n",
      " 173: 'Ibizan hound, Ibizan Podenco',\n",
      " 174: 'Norwegian elkhound, elkhound',\n",
      " 175: 'otterhound, otter hound',\n",
      " 176: 'Saluki, gazelle hound',\n",
      " 177: 'Scottish deerhound, deerhound',\n",
      " 178: 'Weimaraner',\n",
      " 179: 'Staffordshire bullterrier, Staffordshire bull terrier',\n",
      " 180: 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier',\n",
      " 181: 'Bedlington terrier',\n",
      " 182: 'Border terrier',\n",
      " 183: 'Kerry blue terrier',\n",
      " 184: 'Irish terrier',\n",
      " 185: 'Norfolk terrier',\n",
      " 186: 'Norwich terrier',\n",
      " 187: 'Yorkshire terrier',\n",
      " 188: 'wire-haired fox terrier',\n",
      " 189: 'Lakeland terrier',\n",
      " 190: 'Sealyham terrier, Sealyham',\n",
      " 191: 'Airedale, Airedale terrier',\n",
      " 192: 'cairn, cairn terrier',\n",
      " 193: 'Australian terrier',\n",
      " 194: 'Dandie Dinmont, Dandie Dinmont terrier',\n",
      " 195: 'Boston bull, Boston terrier',\n",
      " 196: 'miniature schnauzer',\n",
      " 197: 'giant schnauzer',\n",
      " 198: 'standard schnauzer',\n",
      " 199: 'Scotch terrier, Scottish terrier, Scottie',\n",
      " 200: 'Tibetan terrier, chrysanthemum dog',\n",
      " 201: 'silky terrier, Sydney silky',\n",
      " 202: 'soft-coated wheaten terrier',\n",
      " 203: 'West Highland white terrier',\n",
      " 204: 'Lhasa, Lhasa apso',\n",
      " 205: 'flat-coated retriever',\n",
      " 206: 'curly-coated retriever',\n",
      " 207: 'golden retriever',\n",
      " 208: 'Labrador retriever',\n",
      " 209: 'Chesapeake Bay retriever',\n",
      " 210: 'German short-haired pointer',\n",
      " 211: 'vizsla, Hungarian pointer',\n",
      " 212: 'English setter',\n",
      " 213: 'Irish setter, red setter',\n",
      " 214: 'Gordon setter',\n",
      " 215: 'Brittany spaniel',\n",
      " 216: 'clumber, clumber spaniel',\n",
      " 217: 'English springer, English springer spaniel',\n",
      " 218: 'Welsh springer spaniel',\n",
      " 219: 'cocker spaniel, English cocker spaniel, cocker',\n",
      " 220: 'Sussex spaniel',\n",
      " 221: 'Irish water spaniel',\n",
      " 222: 'kuvasz',\n",
      " 223: 'schipperke',\n",
      " 224: 'groenendael',\n",
      " 225: 'malinois',\n",
      " 226: 'briard',\n",
      " 227: 'kelpie',\n",
      " 228: 'komondor',\n",
      " 229: 'Old English sheepdog, bobtail',\n",
      " 230: 'Shetland sheepdog, Shetland sheep dog, Shetland',\n",
      " 231: 'collie',\n",
      " 232: 'Border collie',\n",
      " 233: 'Bouvier des Flandres, Bouviers des Flandres',\n",
      " 234: 'Rottweiler',\n",
      " 235: 'German shepherd, German shepherd dog, German police dog, alsatian',\n",
      " 236: 'Doberman, Doberman pinscher',\n",
      " 237: 'miniature pinscher',\n",
      " 238: 'Greater Swiss Mountain dog',\n",
      " 239: 'Bernese mountain dog',\n",
      " 240: 'Appenzeller',\n",
      " 241: 'EntleBucher',\n",
      " 242: 'boxer',\n",
      " 243: 'bull mastiff',\n",
      " 244: 'Tibetan mastiff',\n",
      " 245: 'French bulldog',\n",
      " 246: 'Great Dane',\n",
      " 247: 'Saint Bernard, St Bernard',\n",
      " 248: 'Eskimo dog, husky',\n",
      " 249: 'malamute, malemute, Alaskan malamute',\n",
      " 250: 'Siberian husky',\n",
      " 251: 'dalmatian, coach dog, carriage dog',\n",
      " 252: 'affenpinscher, monkey pinscher, monkey dog',\n",
      " 253: 'basenji',\n",
      " 254: 'pug, pug-dog',\n",
      " 255: 'Leonberg',\n",
      " 256: 'Newfoundland, Newfoundland dog',\n",
      " 257: 'Great Pyrenees',\n",
      " 258: 'Samoyed, Samoyede',\n",
      " 259: 'Pomeranian',\n",
      " 260: 'chow, chow chow',\n",
      " 261: 'keeshond',\n",
      " 262: 'Brabancon griffon',\n",
      " 263: 'Pembroke, Pembroke Welsh corgi',\n",
      " 264: 'Cardigan, Cardigan Welsh corgi',\n",
      " 265: 'toy poodle',\n",
      " 266: 'miniature poodle',\n",
      " 267: 'standard poodle',\n",
      " 268: 'Mexican hairless',\n",
      " 269: 'timber wolf, grey wolf, gray wolf, Canis lupus',\n",
      " 270: 'white wolf, Arctic wolf, Canis lupus tundrarum',\n",
      " 271: 'red wolf, maned wolf, Canis rufus, Canis niger',\n",
      " 272: 'coyote, prairie wolf, brush wolf, Canis latrans',\n",
      " 273: 'dingo, warrigal, warragal, Canis dingo',\n",
      " 274: 'dhole, Cuon alpinus',\n",
      " 275: 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus',\n",
      " 276: 'hyena, hyaena',\n",
      " 277: 'red fox, Vulpes vulpes',\n",
      " 278: 'kit fox, Vulpes macrotis',\n",
      " 279: 'Arctic fox, white fox, Alopex lagopus',\n",
      " 280: 'grey fox, gray fox, Urocyon cinereoargenteus',\n",
      " 281: 'tabby, tabby cat',\n",
      " 282: 'tiger cat',\n",
      " 283: 'Persian cat',\n",
      " 284: 'Siamese cat, Siamese',\n",
      " 285: 'Egyptian cat',\n",
      " 286: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n",
      " 287: 'lynx, catamount',\n",
      " 288: 'leopard, Panthera pardus',\n",
      " 289: 'snow leopard, ounce, Panthera uncia',\n",
      " 290: 'jaguar, panther, Panthera onca, Felis onca',\n",
      " 291: 'lion, king of beasts, Panthera leo',\n",
      " 292: 'tiger, Panthera tigris',\n",
      " 293: 'cheetah, chetah, Acinonyx jubatus',\n",
      " 294: 'brown bear, bruin, Ursus arctos',\n",
      " 295: 'American black bear, black bear, Ursus americanus, Euarctos americanus',\n",
      " 296: 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus',\n",
      " 297: 'sloth bear, Melursus ursinus, Ursus ursinus',\n",
      " 298: 'mongoose',\n",
      " 299: 'meerkat, mierkat',\n",
      " 300: 'tiger beetle',\n",
      " 301: 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle',\n",
      " 302: 'ground beetle, carabid beetle',\n",
      " 303: 'long-horned beetle, longicorn, longicorn beetle',\n",
      " 304: 'leaf beetle, chrysomelid',\n",
      " 305: 'dung beetle',\n",
      " 306: 'rhinoceros beetle',\n",
      " 307: 'weevil',\n",
      " 308: 'fly',\n",
      " 309: 'bee',\n",
      " 310: 'ant, emmet, pismire',\n",
      " 311: 'grasshopper, hopper',\n",
      " 312: 'cricket',\n",
      " 313: 'walking stick, walkingstick, stick insect',\n",
      " 314: 'cockroach, roach',\n",
      " 315: 'mantis, mantid',\n",
      " 316: 'cicada, cicala',\n",
      " 317: 'leafhopper',\n",
      " 318: 'lacewing, lacewing fly',\n",
      " 319: \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\",\n",
      " 320: 'damselfly',\n",
      " 321: 'admiral',\n",
      " 322: 'ringlet, ringlet butterfly',\n",
      " 323: 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus',\n",
      " 324: 'cabbage butterfly',\n",
      " 325: 'sulphur butterfly, sulfur butterfly',\n",
      " 326: 'lycaenid, lycaenid butterfly',\n",
      " 327: 'starfish, sea star',\n",
      " 328: 'sea urchin',\n",
      " 329: 'sea cucumber, holothurian',\n",
      " 330: 'wood rabbit, cottontail, cottontail rabbit',\n",
      " 331: 'hare',\n",
      " 332: 'Angora, Angora rabbit',\n",
      " 333: 'hamster',\n",
      " 334: 'porcupine, hedgehog',\n",
      " 335: 'fox squirrel, eastern fox squirrel, Sciurus niger',\n",
      " 336: 'marmot',\n",
      " 337: 'beaver',\n",
      " 338: 'guinea pig, Cavia cobaya',\n",
      " 339: 'sorrel',\n",
      " 340: 'zebra',\n",
      " 341: 'hog, pig, grunter, squealer, Sus scrofa',\n",
      " 342: 'wild boar, boar, Sus scrofa',\n",
      " 343: 'warthog',\n",
      " 344: 'hippopotamus, hippo, river horse, Hippopotamus amphibius',\n",
      " 345: 'ox',\n",
      " 346: 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis',\n",
      " 347: 'bison',\n",
      " 348: 'ram, tup',\n",
      " 349: 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis',\n",
      " 350: 'ibex, Capra ibex',\n",
      " 351: 'hartebeest',\n",
      " 352: 'impala, Aepyceros melampus',\n",
      " 353: 'gazelle',\n",
      " 354: 'Arabian camel, dromedary, Camelus dromedarius',\n",
      " 355: 'llama',\n",
      " 356: 'weasel',\n",
      " 357: 'mink',\n",
      " 358: 'polecat, fitch, foulmart, foumart, Mustela putorius',\n",
      " 359: 'black-footed ferret, ferret, Mustela nigripes',\n",
      " 360: 'otter',\n",
      " 361: 'skunk, polecat, wood pussy',\n",
      " 362: 'badger',\n",
      " 363: 'armadillo',\n",
      " 364: 'three-toed sloth, ai, Bradypus tridactylus',\n",
      " 365: 'orangutan, orang, orangutang, Pongo pygmaeus',\n",
      " 366: 'gorilla, Gorilla gorilla',\n",
      " 367: 'chimpanzee, chimp, Pan troglodytes',\n",
      " 368: 'gibbon, Hylobates lar',\n",
      " 369: 'siamang, Hylobates syndactylus, Symphalangus syndactylus',\n",
      " 370: 'guenon, guenon monkey',\n",
      " 371: 'patas, hussar monkey, Erythrocebus patas',\n",
      " 372: 'baboon',\n",
      " 373: 'macaque',\n",
      " 374: 'langur',\n",
      " 375: 'colobus, colobus monkey',\n",
      " 376: 'proboscis monkey, Nasalis larvatus',\n",
      " 377: 'marmoset',\n",
      " 378: 'capuchin, ringtail, Cebus capucinus',\n",
      " 379: 'howler monkey, howler',\n",
      " 380: 'titi, titi monkey',\n",
      " 381: 'spider monkey, Ateles geoffroyi',\n",
      " 382: 'squirrel monkey, Saimiri sciureus',\n",
      " 383: 'Madagascar cat, ring-tailed lemur, Lemur catta',\n",
      " 384: 'indri, indris, Indri indri, Indri brevicaudatus',\n",
      " 385: 'Indian elephant, Elephas maximus',\n",
      " 386: 'African elephant, Loxodonta africana',\n",
      " 387: 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens',\n",
      " 388: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca',\n",
      " 389: 'barracouta, snoek',\n",
      " 390: 'eel',\n",
      " 391: 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch',\n",
      " 392: 'rock beauty, Holocanthus tricolor',\n",
      " 393: 'anemone fish',\n",
      " 394: 'sturgeon',\n",
      " 395: 'gar, garfish, garpike, billfish, Lepisosteus osseus',\n",
      " 396: 'lionfish',\n",
      " 397: 'puffer, pufferfish, blowfish, globefish',\n",
      " 398: 'abacus',\n",
      " 399: 'abaya',\n",
      " 400: \"academic gown, academic robe, judge's robe\",\n",
      " 401: 'accordion, piano accordion, squeeze box',\n",
      " 402: 'acoustic guitar',\n",
      " 403: 'aircraft carrier, carrier, flattop, attack aircraft carrier',\n",
      " 404: 'airliner',\n",
      " 405: 'airship, dirigible',\n",
      " 406: 'altar',\n",
      " 407: 'ambulance',\n",
      " 408: 'amphibian, amphibious vehicle',\n",
      " 409: 'analog clock',\n",
      " 410: 'apiary, bee house',\n",
      " 411: 'apron',\n",
      " 412: 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin',\n",
      " 413: 'assault rifle, assault gun',\n",
      " 414: 'backpack, back pack, knapsack, packsack, rucksack, haversack',\n",
      " 415: 'bakery, bakeshop, bakehouse',\n",
      " 416: 'balance beam, beam',\n",
      " 417: 'balloon',\n",
      " 418: 'ballpoint, ballpoint pen, ballpen, Biro',\n",
      " 419: 'Band Aid',\n",
      " 420: 'banjo',\n",
      " 421: 'bannister, banister, balustrade, balusters, handrail',\n",
      " 422: 'barbell',\n",
      " 423: 'barber chair',\n",
      " 424: 'barbershop',\n",
      " 425: 'barn',\n",
      " 426: 'barometer',\n",
      " 427: 'barrel, cask',\n",
      " 428: 'barrow, garden cart, lawn cart, wheelbarrow',\n",
      " 429: 'baseball',\n",
      " 430: 'basketball',\n",
      " 431: 'bassinet',\n",
      " 432: 'bassoon',\n",
      " 433: 'bathing cap, swimming cap',\n",
      " 434: 'bath towel',\n",
      " 435: 'bathtub, bathing tub, bath, tub',\n",
      " 436: 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon',\n",
      " 437: 'beacon, lighthouse, beacon light, pharos',\n",
      " 438: 'beaker',\n",
      " 439: 'bearskin, busby, shako',\n",
      " 440: 'beer bottle',\n",
      " 441: 'beer glass',\n",
      " 442: 'bell cote, bell cot',\n",
      " 443: 'bib',\n",
      " 444: 'bicycle-built-for-two, tandem bicycle, tandem',\n",
      " 445: 'bikini, two-piece',\n",
      " 446: 'binder, ring-binder',\n",
      " 447: 'binoculars, field glasses, opera glasses',\n",
      " 448: 'birdhouse',\n",
      " 449: 'boathouse',\n",
      " 450: 'bobsled, bobsleigh, bob',\n",
      " 451: 'bolo tie, bolo, bola tie, bola',\n",
      " 452: 'bonnet, poke bonnet',\n",
      " 453: 'bookcase',\n",
      " 454: 'bookshop, bookstore, bookstall',\n",
      " 455: 'bottlecap',\n",
      " 456: 'bow',\n",
      " 457: 'bow tie, bow-tie, bowtie',\n",
      " 458: 'brass, memorial tablet, plaque',\n",
      " 459: 'brassiere, bra, bandeau',\n",
      " 460: 'breakwater, groin, groyne, mole, bulwark, seawall, jetty',\n",
      " 461: 'breastplate, aegis, egis',\n",
      " 462: 'broom',\n",
      " 463: 'bucket, pail',\n",
      " 464: 'buckle',\n",
      " 465: 'bulletproof vest',\n",
      " 466: 'bullet train, bullet',\n",
      " 467: 'butcher shop, meat market',\n",
      " 468: 'cab, hack, taxi, taxicab',\n",
      " 469: 'caldron, cauldron',\n",
      " 470: 'candle, taper, wax light',\n",
      " 471: 'cannon',\n",
      " 472: 'canoe',\n",
      " 473: 'can opener, tin opener',\n",
      " 474: 'cardigan',\n",
      " 475: 'car mirror',\n",
      " 476: 'carousel, carrousel, merry-go-round, roundabout, whirligig',\n",
      " 477: \"carpenter's kit, tool kit\",\n",
      " 478: 'carton',\n",
      " 479: 'car wheel',\n",
      " 480: 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM',\n",
      " 481: 'cassette',\n",
      " 482: 'cassette player',\n",
      " 483: 'castle',\n",
      " 484: 'catamaran',\n",
      " 485: 'CD player',\n",
      " 486: 'cello, violoncello',\n",
      " 487: 'cellular telephone, cellular phone, cellphone, cell, mobile phone',\n",
      " 488: 'chain',\n",
      " 489: 'chainlink fence',\n",
      " 490: 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour',\n",
      " 491: 'chain saw, chainsaw',\n",
      " 492: 'chest',\n",
      " 493: 'chiffonier, commode',\n",
      " 494: 'chime, bell, gong',\n",
      " 495: 'china cabinet, china closet',\n",
      " 496: 'Christmas stocking',\n",
      " 497: 'church, church building',\n",
      " 498: 'cinema, movie theater, movie theatre, movie house, picture palace',\n",
      " 499: 'cleaver, meat cleaver, chopper',\n",
      " 500: 'cliff dwelling',\n",
      " 501: 'cloak',\n",
      " 502: 'clog, geta, patten, sabot',\n",
      " 503: 'cocktail shaker',\n",
      " 504: 'coffee mug',\n",
      " 505: 'coffeepot',\n",
      " 506: 'coil, spiral, volute, whorl, helix',\n",
      " 507: 'combination lock',\n",
      " 508: 'computer keyboard, keypad',\n",
      " 509: 'confectionery, confectionary, candy store',\n",
      " 510: 'container ship, containership, container vessel',\n",
      " 511: 'convertible',\n",
      " 512: 'corkscrew, bottle screw',\n",
      " 513: 'cornet, horn, trumpet, trump',\n",
      " 514: 'cowboy boot',\n",
      " 515: 'cowboy hat, ten-gallon hat',\n",
      " 516: 'cradle',\n",
      " 517: 'crane',\n",
      " 518: 'crash helmet',\n",
      " 519: 'crate',\n",
      " 520: 'crib, cot',\n",
      " 521: 'Crock Pot',\n",
      " 522: 'croquet ball',\n",
      " 523: 'crutch',\n",
      " 524: 'cuirass',\n",
      " 525: 'dam, dike, dyke',\n",
      " 526: 'desk',\n",
      " 527: 'desktop computer',\n",
      " 528: 'dial telephone, dial phone',\n",
      " 529: 'diaper, nappy, napkin',\n",
      " 530: 'digital clock',\n",
      " 531: 'digital watch',\n",
      " 532: 'dining table, board',\n",
      " 533: 'dishrag, dishcloth',\n",
      " 534: 'dishwasher, dish washer, dishwashing machine',\n",
      " 535: 'disk brake, disc brake',\n",
      " 536: 'dock, dockage, docking facility',\n",
      " 537: 'dogsled, dog sled, dog sleigh',\n",
      " 538: 'dome',\n",
      " 539: 'doormat, welcome mat',\n",
      " 540: 'drilling platform, offshore rig',\n",
      " 541: 'drum, membranophone, tympan',\n",
      " 542: 'drumstick',\n",
      " 543: 'dumbbell',\n",
      " 544: 'Dutch oven',\n",
      " 545: 'electric fan, blower',\n",
      " 546: 'electric guitar',\n",
      " 547: 'electric locomotive',\n",
      " 548: 'entertainment center',\n",
      " 549: 'envelope',\n",
      " 550: 'espresso maker',\n",
      " 551: 'face powder',\n",
      " 552: 'feather boa, boa',\n",
      " 553: 'file, file cabinet, filing cabinet',\n",
      " 554: 'fireboat',\n",
      " 555: 'fire engine, fire truck',\n",
      " 556: 'fire screen, fireguard',\n",
      " 557: 'flagpole, flagstaff',\n",
      " 558: 'flute, transverse flute',\n",
      " 559: 'folding chair',\n",
      " 560: 'football helmet',\n",
      " 561: 'forklift',\n",
      " 562: 'fountain',\n",
      " 563: 'fountain pen',\n",
      " 564: 'four-poster',\n",
      " 565: 'freight car',\n",
      " 566: 'French horn, horn',\n",
      " 567: 'frying pan, frypan, skillet',\n",
      " 568: 'fur coat',\n",
      " 569: 'garbage truck, dustcart',\n",
      " 570: 'gasmask, respirator, gas helmet',\n",
      " 571: 'gas pump, gasoline pump, petrol pump, island dispenser',\n",
      " 572: 'goblet',\n",
      " 573: 'go-kart',\n",
      " 574: 'golf ball',\n",
      " 575: 'golfcart, golf cart',\n",
      " 576: 'gondola',\n",
      " 577: 'gong, tam-tam',\n",
      " 578: 'gown',\n",
      " 579: 'grand piano, grand',\n",
      " 580: 'greenhouse, nursery, glasshouse',\n",
      " 581: 'grille, radiator grille',\n",
      " 582: 'grocery store, grocery, food market, market',\n",
      " 583: 'guillotine',\n",
      " 584: 'hair slide',\n",
      " 585: 'hair spray',\n",
      " 586: 'half track',\n",
      " 587: 'hammer',\n",
      " 588: 'hamper',\n",
      " 589: 'hand blower, blow dryer, blow drier, hair dryer, hair drier',\n",
      " 590: 'hand-held computer, hand-held microcomputer',\n",
      " 591: 'handkerchief, hankie, hanky, hankey',\n",
      " 592: 'hard disc, hard disk, fixed disk',\n",
      " 593: 'harmonica, mouth organ, harp, mouth harp',\n",
      " 594: 'harp',\n",
      " 595: 'harvester, reaper',\n",
      " 596: 'hatchet',\n",
      " 597: 'holster',\n",
      " 598: 'home theater, home theatre',\n",
      " 599: 'honeycomb',\n",
      " 600: 'hook, claw',\n",
      " 601: 'hoopskirt, crinoline',\n",
      " 602: 'horizontal bar, high bar',\n",
      " 603: 'horse cart, horse-cart',\n",
      " 604: 'hourglass',\n",
      " 605: 'iPod',\n",
      " 606: 'iron, smoothing iron',\n",
      " 607: \"jack-o'-lantern\",\n",
      " 608: 'jean, blue jean, denim',\n",
      " 609: 'jeep, landrover',\n",
      " 610: 'jersey, T-shirt, tee shirt',\n",
      " 611: 'jigsaw puzzle',\n",
      " 612: 'jinrikisha, ricksha, rickshaw',\n",
      " 613: 'joystick',\n",
      " 614: 'kimono',\n",
      " 615: 'knee pad',\n",
      " 616: 'knot',\n",
      " 617: 'lab coat, laboratory coat',\n",
      " 618: 'ladle',\n",
      " 619: 'lampshade, lamp shade',\n",
      " 620: 'laptop, laptop computer',\n",
      " 621: 'lawn mower, mower',\n",
      " 622: 'lens cap, lens cover',\n",
      " 623: 'letter opener, paper knife, paperknife',\n",
      " 624: 'library',\n",
      " 625: 'lifeboat',\n",
      " 626: 'lighter, light, igniter, ignitor',\n",
      " 627: 'limousine, limo',\n",
      " 628: 'liner, ocean liner',\n",
      " 629: 'lipstick, lip rouge',\n",
      " 630: 'Loafer',\n",
      " 631: 'lotion',\n",
      " 632: 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system',\n",
      " 633: \"loupe, jeweler's loupe\",\n",
      " 634: 'lumbermill, sawmill',\n",
      " 635: 'magnetic compass',\n",
      " 636: 'mailbag, postbag',\n",
      " 637: 'mailbox, letter box',\n",
      " 638: 'maillot',\n",
      " 639: 'maillot, tank suit',\n",
      " 640: 'manhole cover',\n",
      " 641: 'maraca',\n",
      " 642: 'marimba, xylophone',\n",
      " 643: 'mask',\n",
      " 644: 'matchstick',\n",
      " 645: 'maypole',\n",
      " 646: 'maze, labyrinth',\n",
      " 647: 'measuring cup',\n",
      " 648: 'medicine chest, medicine cabinet',\n",
      " 649: 'megalith, megalithic structure',\n",
      " 650: 'microphone, mike',\n",
      " 651: 'microwave, microwave oven',\n",
      " 652: 'military uniform',\n",
      " 653: 'milk can',\n",
      " 654: 'minibus',\n",
      " 655: 'miniskirt, mini',\n",
      " 656: 'minivan',\n",
      " 657: 'missile',\n",
      " 658: 'mitten',\n",
      " 659: 'mixing bowl',\n",
      " 660: 'mobile home, manufactured home',\n",
      " 661: 'Model T',\n",
      " 662: 'modem',\n",
      " 663: 'monastery',\n",
      " 664: 'monitor',\n",
      " 665: 'moped',\n",
      " 666: 'mortar',\n",
      " 667: 'mortarboard',\n",
      " 668: 'mosque',\n",
      " 669: 'mosquito net',\n",
      " 670: 'motor scooter, scooter',\n",
      " 671: 'mountain bike, all-terrain bike, off-roader',\n",
      " 672: 'mountain tent',\n",
      " 673: 'mouse, computer mouse',\n",
      " 674: 'mousetrap',\n",
      " 675: 'moving van',\n",
      " 676: 'muzzle',\n",
      " 677: 'nail',\n",
      " 678: 'neck brace',\n",
      " 679: 'necklace',\n",
      " 680: 'nipple',\n",
      " 681: 'notebook, notebook computer',\n",
      " 682: 'obelisk',\n",
      " 683: 'oboe, hautboy, hautbois',\n",
      " 684: 'ocarina, sweet potato',\n",
      " 685: 'odometer, hodometer, mileometer, milometer',\n",
      " 686: 'oil filter',\n",
      " 687: 'organ, pipe organ',\n",
      " 688: 'oscilloscope, scope, cathode-ray oscilloscope, CRO',\n",
      " 689: 'overskirt',\n",
      " 690: 'oxcart',\n",
      " 691: 'oxygen mask',\n",
      " 692: 'packet',\n",
      " 693: 'paddle, boat paddle',\n",
      " 694: 'paddlewheel, paddle wheel',\n",
      " 695: 'padlock',\n",
      " 696: 'paintbrush',\n",
      " 697: \"pajama, pyjama, pj's, jammies\",\n",
      " 698: 'palace',\n",
      " 699: 'panpipe, pandean pipe, syrinx',\n",
      " 700: 'paper towel',\n",
      " 701: 'parachute, chute',\n",
      " 702: 'parallel bars, bars',\n",
      " 703: 'park bench',\n",
      " 704: 'parking meter',\n",
      " 705: 'passenger car, coach, carriage',\n",
      " 706: 'patio, terrace',\n",
      " 707: 'pay-phone, pay-station',\n",
      " 708: 'pedestal, plinth, footstall',\n",
      " 709: 'pencil box, pencil case',\n",
      " 710: 'pencil sharpener',\n",
      " 711: 'perfume, essence',\n",
      " 712: 'Petri dish',\n",
      " 713: 'photocopier',\n",
      " 714: 'pick, plectrum, plectron',\n",
      " 715: 'pickelhaube',\n",
      " 716: 'picket fence, paling',\n",
      " 717: 'pickup, pickup truck',\n",
      " 718: 'pier',\n",
      " 719: 'piggy bank, penny bank',\n",
      " 720: 'pill bottle',\n",
      " 721: 'pillow',\n",
      " 722: 'ping-pong ball',\n",
      " 723: 'pinwheel',\n",
      " 724: 'pirate, pirate ship',\n",
      " 725: 'pitcher, ewer',\n",
      " 726: \"plane, carpenter's plane, woodworking plane\",\n",
      " 727: 'planetarium',\n",
      " 728: 'plastic bag',\n",
      " 729: 'plate rack',\n",
      " 730: 'plow, plough',\n",
      " 731: \"plunger, plumber's helper\",\n",
      " 732: 'Polaroid camera, Polaroid Land camera',\n",
      " 733: 'pole',\n",
      " 734: 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria',\n",
      " 735: 'poncho',\n",
      " 736: 'pool table, billiard table, snooker table',\n",
      " 737: 'pop bottle, soda bottle',\n",
      " 738: 'pot, flowerpot',\n",
      " 739: \"potter's wheel\",\n",
      " 740: 'power drill',\n",
      " 741: 'prayer rug, prayer mat',\n",
      " 742: 'printer',\n",
      " 743: 'prison, prison house',\n",
      " 744: 'projectile, missile',\n",
      " 745: 'projector',\n",
      " 746: 'puck, hockey puck',\n",
      " 747: 'punching bag, punch bag, punching ball, punchball',\n",
      " 748: 'purse',\n",
      " 749: 'quill, quill pen',\n",
      " 750: 'quilt, comforter, comfort, puff',\n",
      " 751: 'racer, race car, racing car',\n",
      " 752: 'racket, racquet',\n",
      " 753: 'radiator',\n",
      " 754: 'radio, wireless',\n",
      " 755: 'radio telescope, radio reflector',\n",
      " 756: 'rain barrel',\n",
      " 757: 'recreational vehicle, RV, R.V.',\n",
      " 758: 'reel',\n",
      " 759: 'reflex camera',\n",
      " 760: 'refrigerator, icebox',\n",
      " 761: 'remote control, remote',\n",
      " 762: 'restaurant, eating house, eating place, eatery',\n",
      " 763: 'revolver, six-gun, six-shooter',\n",
      " 764: 'rifle',\n",
      " 765: 'rocking chair, rocker',\n",
      " 766: 'rotisserie',\n",
      " 767: 'rubber eraser, rubber, pencil eraser',\n",
      " 768: 'rugby ball',\n",
      " 769: 'rule, ruler',\n",
      " 770: 'running shoe',\n",
      " 771: 'safe',\n",
      " 772: 'safety pin',\n",
      " 773: 'saltshaker, salt shaker',\n",
      " 774: 'sandal',\n",
      " 775: 'sarong',\n",
      " 776: 'sax, saxophone',\n",
      " 777: 'scabbard',\n",
      " 778: 'scale, weighing machine',\n",
      " 779: 'school bus',\n",
      " 780: 'schooner',\n",
      " 781: 'scoreboard',\n",
      " 782: 'screen, CRT screen',\n",
      " 783: 'screw',\n",
      " 784: 'screwdriver',\n",
      " 785: 'seat belt, seatbelt',\n",
      " 786: 'sewing machine',\n",
      " 787: 'shield, buckler',\n",
      " 788: 'shoe shop, shoe-shop, shoe store',\n",
      " 789: 'shoji',\n",
      " 790: 'shopping basket',\n",
      " 791: 'shopping cart',\n",
      " 792: 'shovel',\n",
      " 793: 'shower cap',\n",
      " 794: 'shower curtain',\n",
      " 795: 'ski',\n",
      " 796: 'ski mask',\n",
      " 797: 'sleeping bag',\n",
      " 798: 'slide rule, slipstick',\n",
      " 799: 'sliding door',\n",
      " 800: 'slot, one-armed bandit',\n",
      " 801: 'snorkel',\n",
      " 802: 'snowmobile',\n",
      " 803: 'snowplow, snowplough',\n",
      " 804: 'soap dispenser',\n",
      " 805: 'soccer ball',\n",
      " 806: 'sock',\n",
      " 807: 'solar dish, solar collector, solar furnace',\n",
      " 808: 'sombrero',\n",
      " 809: 'soup bowl',\n",
      " 810: 'space bar',\n",
      " 811: 'space heater',\n",
      " 812: 'space shuttle',\n",
      " 813: 'spatula',\n",
      " 814: 'speedboat',\n",
      " 815: \"spider web, spider's web\",\n",
      " 816: 'spindle',\n",
      " 817: 'sports car, sport car',\n",
      " 818: 'spotlight, spot',\n",
      " 819: 'stage',\n",
      " 820: 'steam locomotive',\n",
      " 821: 'steel arch bridge',\n",
      " 822: 'steel drum',\n",
      " 823: 'stethoscope',\n",
      " 824: 'stole',\n",
      " 825: 'stone wall',\n",
      " 826: 'stopwatch, stop watch',\n",
      " 827: 'stove',\n",
      " 828: 'strainer',\n",
      " 829: 'streetcar, tram, tramcar, trolley, trolley car',\n",
      " 830: 'stretcher',\n",
      " 831: 'studio couch, day bed',\n",
      " 832: 'stupa, tope',\n",
      " 833: 'submarine, pigboat, sub, U-boat',\n",
      " 834: 'suit, suit of clothes',\n",
      " 835: 'sundial',\n",
      " 836: 'sunglass',\n",
      " 837: 'sunglasses, dark glasses, shades',\n",
      " 838: 'sunscreen, sunblock, sun blocker',\n",
      " 839: 'suspension bridge',\n",
      " 840: 'swab, swob, mop',\n",
      " 841: 'sweatshirt',\n",
      " 842: 'swimming trunks, bathing trunks',\n",
      " 843: 'swing',\n",
      " 844: 'switch, electric switch, electrical switch',\n",
      " 845: 'syringe',\n",
      " 846: 'table lamp',\n",
      " 847: 'tank, army tank, armored combat vehicle, armoured combat vehicle',\n",
      " 848: 'tape player',\n",
      " 849: 'teapot',\n",
      " 850: 'teddy, teddy bear',\n",
      " 851: 'television, television system',\n",
      " 852: 'tennis ball',\n",
      " 853: 'thatch, thatched roof',\n",
      " 854: 'theater curtain, theatre curtain',\n",
      " 855: 'thimble',\n",
      " 856: 'thresher, thrasher, threshing machine',\n",
      " 857: 'throne',\n",
      " 858: 'tile roof',\n",
      " 859: 'toaster',\n",
      " 860: 'tobacco shop, tobacconist shop, tobacconist',\n",
      " 861: 'toilet seat',\n",
      " 862: 'torch',\n",
      " 863: 'totem pole',\n",
      " 864: 'tow truck, tow car, wrecker',\n",
      " 865: 'toyshop',\n",
      " 866: 'tractor',\n",
      " 867: 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi',\n",
      " 868: 'tray',\n",
      " 869: 'trench coat',\n",
      " 870: 'tricycle, trike, velocipede',\n",
      " 871: 'trimaran',\n",
      " 872: 'tripod',\n",
      " 873: 'triumphal arch',\n",
      " 874: 'trolleybus, trolley coach, trackless trolley',\n",
      " 875: 'trombone',\n",
      " 876: 'tub, vat',\n",
      " 877: 'turnstile',\n",
      " 878: 'typewriter keyboard',\n",
      " 879: 'umbrella',\n",
      " 880: 'unicycle, monocycle',\n",
      " 881: 'upright, upright piano',\n",
      " 882: 'vacuum, vacuum cleaner',\n",
      " 883: 'vase',\n",
      " 884: 'vault',\n",
      " 885: 'velvet',\n",
      " 886: 'vending machine',\n",
      " 887: 'vestment',\n",
      " 888: 'viaduct',\n",
      " 889: 'violin, fiddle',\n",
      " 890: 'volleyball',\n",
      " 891: 'waffle iron',\n",
      " 892: 'wall clock',\n",
      " 893: 'wallet, billfold, notecase, pocketbook',\n",
      " 894: 'wardrobe, closet, press',\n",
      " 895: 'warplane, military plane',\n",
      " 896: 'washbasin, handbasin, washbowl, lavabo, wash-hand basin',\n",
      " 897: 'washer, automatic washer, washing machine',\n",
      " 898: 'water bottle',\n",
      " 899: 'water jug',\n",
      " 900: 'water tower',\n",
      " 901: 'whiskey jug',\n",
      " 902: 'whistle',\n",
      " 903: 'wig',\n",
      " 904: 'window screen',\n",
      " 905: 'window shade',\n",
      " 906: 'Windsor tie',\n",
      " 907: 'wine bottle',\n",
      " 908: 'wing',\n",
      " 909: 'wok',\n",
      " 910: 'wooden spoon',\n",
      " 911: 'wool, woolen, woollen',\n",
      " 912: 'worm fence, snake fence, snake-rail fence, Virginia fence',\n",
      " 913: 'wreck',\n",
      " 914: 'yawl',\n",
      " 915: 'yurt',\n",
      " 916: 'web site, website, internet site, site',\n",
      " 917: 'comic book',\n",
      " 918: 'crossword puzzle, crossword',\n",
      " 919: 'street sign',\n",
      " 920: 'traffic light, traffic signal, stoplight',\n",
      " 921: 'book jacket, dust cover, dust jacket, dust wrapper',\n",
      " 922: 'menu',\n",
      " 923: 'plate',\n",
      " 924: 'guacamole',\n",
      " 925: 'consomme',\n",
      " 926: 'hot pot, hotpot',\n",
      " 927: 'trifle',\n",
      " 928: 'ice cream, icecream',\n",
      " 929: 'ice lolly, lolly, lollipop, popsicle',\n",
      " 930: 'French loaf',\n",
      " 931: 'bagel, beigel',\n",
      " 932: 'pretzel',\n",
      " 933: 'cheeseburger',\n",
      " 934: 'hotdog, hot dog, red hot',\n",
      " 935: 'mashed potato',\n",
      " 936: 'head cabbage',\n",
      " 937: 'broccoli',\n",
      " 938: 'cauliflower',\n",
      " 939: 'zucchini, courgette',\n",
      " 940: 'spaghetti squash',\n",
      " 941: 'acorn squash',\n",
      " 942: 'butternut squash',\n",
      " 943: 'cucumber, cuke',\n",
      " 944: 'artichoke, globe artichoke',\n",
      " 945: 'bell pepper',\n",
      " 946: 'cardoon',\n",
      " 947: 'mushroom',\n",
      " 948: 'Granny Smith',\n",
      " 949: 'strawberry',\n",
      " 950: 'orange',\n",
      " 951: 'lemon',\n",
      " 952: 'fig',\n",
      " 953: 'pineapple, ananas',\n",
      " 954: 'banana',\n",
      " 955: 'jackfruit, jak, jack',\n",
      " 956: 'custard apple',\n",
      " 957: 'pomegranate',\n",
      " 958: 'hay',\n",
      " 959: 'carbonara',\n",
      " 960: 'chocolate sauce, chocolate syrup',\n",
      " 961: 'dough',\n",
      " 962: 'meat loaf, meatloaf',\n",
      " 963: 'pizza, pizza pie',\n",
      " 964: 'potpie',\n",
      " 965: 'burrito',\n",
      " 966: 'red wine',\n",
      " 967: 'espresso',\n",
      " 968: 'cup',\n",
      " 969: 'eggnog',\n",
      " 970: 'alp',\n",
      " 971: 'bubble',\n",
      " 972: 'cliff, drop, drop-off',\n",
      " 973: 'coral reef',\n",
      " 974: 'geyser',\n",
      " 975: 'lakeside, lakeshore',\n",
      " 976: 'promontory, headland, head, foreland',\n",
      " 977: 'sandbar, sand bar',\n",
      " 978: 'seashore, coast, seacoast, sea-coast',\n",
      " 979: 'valley, vale',\n",
      " 980: 'volcano',\n",
      " 981: 'ballplayer, baseball player',\n",
      " 982: 'groom, bridegroom',\n",
      " 983: 'scuba diver',\n",
      " 984: 'rapeseed',\n",
      " 985: 'daisy',\n",
      " 986: \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\",\n",
      " 987: 'corn',\n",
      " 988: 'acorn',\n",
      " 989: 'hip, rose hip, rosehip',\n",
      " 990: 'buckeye, horse chestnut, conker',\n",
      " 991: 'coral fungus',\n",
      " 992: 'agaric',\n",
      " 993: 'gyromitra',\n",
      " 994: 'stinkhorn, carrion fungus',\n",
      " 995: 'earthstar',\n",
      " 996: 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa',\n",
      " 997: 'bolete',\n",
      " 998: 'ear, spike, capitulum',\n",
      " 999: 'toilet tissue, toilet paper, bathroom tissue'}\n",
      "71/1:\n",
      "from tensorflow.keras.applications import EfficientNetB4\n",
      "from tensorflow.image import resize\n",
      "model = EfficientNetB4(weights='imagenet')\n",
      "\n",
      "def center_crop(img, new_width=None, new_height=None):        \n",
      "\n",
      "    width = img.shape[1]\n",
      "    height = img.shape[0]\n",
      "\n",
      "    if new_width is None:\n",
      "        new_width = min(width, height)\n",
      "\n",
      "    if new_height is None:\n",
      "        new_height = min(width, height)\n",
      "\n",
      "    left = int(np.ceil((width - new_width) / 2))\n",
      "    right = width - int(np.floor((width - new_width) / 2))\n",
      "\n",
      "    top = int(np.ceil((height - new_height) / 2))\n",
      "    bottom = height - int(np.floor((height - new_height) / 2))\n",
      "\n",
      "    if len(img.shape) == 2:\n",
      "        center_cropped_img = img[top:bottom, left:right]\n",
      "    else:\n",
      "        center_cropped_img = img[top:bottom, left:right, ...]\n",
      "\n",
      "    return center_cropped_img\n",
      "\n",
      "class_dict = {0: 'tench, Tinca tinca',\n",
      " 1: 'goldfish, Carassius auratus',\n",
      " 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n",
      " 3: 'tiger shark, Galeocerdo cuvieri',\n",
      " 4: 'hammerhead, hammerhead shark',\n",
      " 5: 'electric ray, crampfish, numbfish, torpedo',\n",
      " 6: 'stingray',\n",
      " 7: 'cock',\n",
      " 8: 'hen',\n",
      " 9: 'ostrich, Struthio camelus',\n",
      " 10: 'brambling, Fringilla montifringilla',\n",
      " 11: 'goldfinch, Carduelis carduelis',\n",
      " 12: 'house finch, linnet, Carpodacus mexicanus',\n",
      " 13: 'junco, snowbird',\n",
      " 14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea',\n",
      " 15: 'robin, American robin, Turdus migratorius',\n",
      " 16: 'bulbul',\n",
      " 17: 'jay',\n",
      " 18: 'magpie',\n",
      " 19: 'chickadee',\n",
      " 20: 'water ouzel, dipper',\n",
      " 21: 'kite',\n",
      " 22: 'bald eagle, American eagle, Haliaeetus leucocephalus',\n",
      " 23: 'vulture',\n",
      " 24: 'great grey owl, great gray owl, Strix nebulosa',\n",
      " 25: 'European fire salamander, Salamandra salamandra',\n",
      " 26: 'common newt, Triturus vulgaris',\n",
      " 27: 'eft',\n",
      " 28: 'spotted salamander, Ambystoma maculatum',\n",
      " 29: 'axolotl, mud puppy, Ambystoma mexicanum',\n",
      " 30: 'bullfrog, Rana catesbeiana',\n",
      " 31: 'tree frog, tree-frog',\n",
      " 32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui',\n",
      " 33: 'loggerhead, loggerhead turtle, Caretta caretta',\n",
      " 34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea',\n",
      " 35: 'mud turtle',\n",
      " 36: 'terrapin',\n",
      " 37: 'box turtle, box tortoise',\n",
      " 38: 'banded gecko',\n",
      " 39: 'common iguana, iguana, Iguana iguana',\n",
      " 40: 'American chameleon, anole, Anolis carolinensis',\n",
      " 41: 'whiptail, whiptail lizard',\n",
      " 42: 'agama',\n",
      " 43: 'frilled lizard, Chlamydosaurus kingi',\n",
      " 44: 'alligator lizard',\n",
      " 45: 'Gila monster, Heloderma suspectum',\n",
      " 46: 'green lizard, Lacerta viridis',\n",
      " 47: 'African chameleon, Chamaeleo chamaeleon',\n",
      " 48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis',\n",
      " 49: 'African crocodile, Nile crocodile, Crocodylus niloticus',\n",
      " 50: 'American alligator, Alligator mississipiensis',\n",
      " 51: 'triceratops',\n",
      " 52: 'thunder snake, worm snake, Carphophis amoenus',\n",
      " 53: 'ringneck snake, ring-necked snake, ring snake',\n",
      " 54: 'hognose snake, puff adder, sand viper',\n",
      " 55: 'green snake, grass snake',\n",
      " 56: 'king snake, kingsnake',\n",
      " 57: 'garter snake, grass snake',\n",
      " 58: 'water snake',\n",
      " 59: 'vine snake',\n",
      " 60: 'night snake, Hypsiglena torquata',\n",
      " 61: 'boa constrictor, Constrictor constrictor',\n",
      " 62: 'rock python, rock snake, Python sebae',\n",
      " 63: 'Indian cobra, Naja naja',\n",
      " 64: 'green mamba',\n",
      " 65: 'sea snake',\n",
      " 66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus',\n",
      " 67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus',\n",
      " 68: 'sidewinder, horned rattlesnake, Crotalus cerastes',\n",
      " 69: 'trilobite',\n",
      " 70: 'harvestman, daddy longlegs, Phalangium opilio',\n",
      " 71: 'scorpion',\n",
      " 72: 'black and gold garden spider, Argiope aurantia',\n",
      " 73: 'barn spider, Araneus cavaticus',\n",
      " 74: 'garden spider, Aranea diademata',\n",
      " 75: 'black widow, Latrodectus mactans',\n",
      " 76: 'tarantula',\n",
      " 77: 'wolf spider, hunting spider',\n",
      " 78: 'tick',\n",
      " 79: 'centipede',\n",
      " 80: 'black grouse',\n",
      " 81: 'ptarmigan',\n",
      " 82: 'ruffed grouse, partridge, Bonasa umbellus',\n",
      " 83: 'prairie chicken, prairie grouse, prairie fowl',\n",
      " 84: 'peacock',\n",
      " 85: 'quail',\n",
      " 86: 'partridge',\n",
      " 87: 'African grey, African gray, Psittacus erithacus',\n",
      " 88: 'macaw',\n",
      " 89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n",
      " 90: 'lorikeet',\n",
      " 91: 'coucal',\n",
      " 92: 'bee eater',\n",
      " 93: 'hornbill',\n",
      " 94: 'hummingbird',\n",
      " 95: 'jacamar',\n",
      " 96: 'toucan',\n",
      " 97: 'drake',\n",
      " 98: 'red-breasted merganser, Mergus serrator',\n",
      " 99: 'goose',\n",
      " 100: 'black swan, Cygnus atratus',\n",
      " 101: 'tusker',\n",
      " 102: 'echidna, spiny anteater, anteater',\n",
      " 103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n",
      " 104: 'wallaby, brush kangaroo',\n",
      " 105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n",
      " 106: 'wombat',\n",
      " 107: 'jellyfish',\n",
      " 108: 'sea anemone, anemone',\n",
      " 109: 'brain coral',\n",
      " 110: 'flatworm, platyhelminth',\n",
      " 111: 'nematode, nematode worm, roundworm',\n",
      " 112: 'conch',\n",
      " 113: 'snail',\n",
      " 114: 'slug',\n",
      " 115: 'sea slug, nudibranch',\n",
      " 116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore',\n",
      " 117: 'chambered nautilus, pearly nautilus, nautilus',\n",
      " 118: 'Dungeness crab, Cancer magister',\n",
      " 119: 'rock crab, Cancer irroratus',\n",
      " 120: 'fiddler crab',\n",
      " 121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica',\n",
      " 122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus',\n",
      " 123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish',\n",
      " 124: 'crayfish, crawfish, crawdad, crawdaddy',\n",
      " 125: 'hermit crab',\n",
      " 126: 'isopod',\n",
      " 127: 'white stork, Ciconia ciconia',\n",
      " 128: 'black stork, Ciconia nigra',\n",
      " 129: 'spoonbill',\n",
      " 130: 'flamingo',\n",
      " 131: 'little blue heron, Egretta caerulea',\n",
      " 132: 'American egret, great white heron, Egretta albus',\n",
      " 133: 'bittern',\n",
      " 134: 'crane',\n",
      " 135: 'limpkin, Aramus pictus',\n",
      " 136: 'European gallinule, Porphyrio porphyrio',\n",
      " 137: 'American coot, marsh hen, mud hen, water hen, Fulica americana',\n",
      " 138: 'bustard',\n",
      " 139: 'ruddy turnstone, Arenaria interpres',\n",
      " 140: 'red-backed sandpiper, dunlin, Erolia alpina',\n",
      " 141: 'redshank, Tringa totanus',\n",
      " 142: 'dowitcher',\n",
      " 143: 'oystercatcher, oyster catcher',\n",
      " 144: 'pelican',\n",
      " 145: 'king penguin, Aptenodytes patagonica',\n",
      " 146: 'albatross, mollymawk',\n",
      " 147: 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus',\n",
      " 148: 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca',\n",
      " 149: 'dugong, Dugong dugon',\n",
      " 150: 'sea lion',\n",
      " 151: 'Chihuahua',\n",
      " 152: 'Japanese spaniel',\n",
      " 153: 'Maltese dog, Maltese terrier, Maltese',\n",
      " 154: 'Pekinese, Pekingese, Peke',\n",
      " 155: 'Shih-Tzu',\n",
      " 156: 'Blenheim spaniel',\n",
      " 157: 'papillon',\n",
      " 158: 'toy terrier',\n",
      " 159: 'Rhodesian ridgeback',\n",
      " 160: 'Afghan hound, Afghan',\n",
      " 161: 'basset, basset hound',\n",
      " 162: 'beagle',\n",
      " 163: 'bloodhound, sleuthhound',\n",
      " 164: 'bluetick',\n",
      " 165: 'black-and-tan coonhound',\n",
      " 166: 'Walker hound, Walker foxhound',\n",
      " 167: 'English foxhound',\n",
      " 168: 'redbone',\n",
      " 169: 'borzoi, Russian wolfhound',\n",
      " 170: 'Irish wolfhound',\n",
      " 171: 'Italian greyhound',\n",
      " 172: 'whippet',\n",
      " 173: 'Ibizan hound, Ibizan Podenco',\n",
      " 174: 'Norwegian elkhound, elkhound',\n",
      " 175: 'otterhound, otter hound',\n",
      " 176: 'Saluki, gazelle hound',\n",
      " 177: 'Scottish deerhound, deerhound',\n",
      " 178: 'Weimaraner',\n",
      " 179: 'Staffordshire bullterrier, Staffordshire bull terrier',\n",
      " 180: 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier',\n",
      " 181: 'Bedlington terrier',\n",
      " 182: 'Border terrier',\n",
      " 183: 'Kerry blue terrier',\n",
      " 184: 'Irish terrier',\n",
      " 185: 'Norfolk terrier',\n",
      " 186: 'Norwich terrier',\n",
      " 187: 'Yorkshire terrier',\n",
      " 188: 'wire-haired fox terrier',\n",
      " 189: 'Lakeland terrier',\n",
      " 190: 'Sealyham terrier, Sealyham',\n",
      " 191: 'Airedale, Airedale terrier',\n",
      " 192: 'cairn, cairn terrier',\n",
      " 193: 'Australian terrier',\n",
      " 194: 'Dandie Dinmont, Dandie Dinmont terrier',\n",
      " 195: 'Boston bull, Boston terrier',\n",
      " 196: 'miniature schnauzer',\n",
      " 197: 'giant schnauzer',\n",
      " 198: 'standard schnauzer',\n",
      " 199: 'Scotch terrier, Scottish terrier, Scottie',\n",
      " 200: 'Tibetan terrier, chrysanthemum dog',\n",
      " 201: 'silky terrier, Sydney silky',\n",
      " 202: 'soft-coated wheaten terrier',\n",
      " 203: 'West Highland white terrier',\n",
      " 204: 'Lhasa, Lhasa apso',\n",
      " 205: 'flat-coated retriever',\n",
      " 206: 'curly-coated retriever',\n",
      " 207: 'golden retriever',\n",
      " 208: 'Labrador retriever',\n",
      " 209: 'Chesapeake Bay retriever',\n",
      " 210: 'German short-haired pointer',\n",
      " 211: 'vizsla, Hungarian pointer',\n",
      " 212: 'English setter',\n",
      " 213: 'Irish setter, red setter',\n",
      " 214: 'Gordon setter',\n",
      " 215: 'Brittany spaniel',\n",
      " 216: 'clumber, clumber spaniel',\n",
      " 217: 'English springer, English springer spaniel',\n",
      " 218: 'Welsh springer spaniel',\n",
      " 219: 'cocker spaniel, English cocker spaniel, cocker',\n",
      " 220: 'Sussex spaniel',\n",
      " 221: 'Irish water spaniel',\n",
      " 222: 'kuvasz',\n",
      " 223: 'schipperke',\n",
      " 224: 'groenendael',\n",
      " 225: 'malinois',\n",
      " 226: 'briard',\n",
      " 227: 'kelpie',\n",
      " 228: 'komondor',\n",
      " 229: 'Old English sheepdog, bobtail',\n",
      " 230: 'Shetland sheepdog, Shetland sheep dog, Shetland',\n",
      " 231: 'collie',\n",
      " 232: 'Border collie',\n",
      " 233: 'Bouvier des Flandres, Bouviers des Flandres',\n",
      " 234: 'Rottweiler',\n",
      " 235: 'German shepherd, German shepherd dog, German police dog, alsatian',\n",
      " 236: 'Doberman, Doberman pinscher',\n",
      " 237: 'miniature pinscher',\n",
      " 238: 'Greater Swiss Mountain dog',\n",
      " 239: 'Bernese mountain dog',\n",
      " 240: 'Appenzeller',\n",
      " 241: 'EntleBucher',\n",
      " 242: 'boxer',\n",
      " 243: 'bull mastiff',\n",
      " 244: 'Tibetan mastiff',\n",
      " 245: 'French bulldog',\n",
      " 246: 'Great Dane',\n",
      " 247: 'Saint Bernard, St Bernard',\n",
      " 248: 'Eskimo dog, husky',\n",
      " 249: 'malamute, malemute, Alaskan malamute',\n",
      " 250: 'Siberian husky',\n",
      " 251: 'dalmatian, coach dog, carriage dog',\n",
      " 252: 'affenpinscher, monkey pinscher, monkey dog',\n",
      " 253: 'basenji',\n",
      " 254: 'pug, pug-dog',\n",
      " 255: 'Leonberg',\n",
      " 256: 'Newfoundland, Newfoundland dog',\n",
      " 257: 'Great Pyrenees',\n",
      " 258: 'Samoyed, Samoyede',\n",
      " 259: 'Pomeranian',\n",
      " 260: 'chow, chow chow',\n",
      " 261: 'keeshond',\n",
      " 262: 'Brabancon griffon',\n",
      " 263: 'Pembroke, Pembroke Welsh corgi',\n",
      " 264: 'Cardigan, Cardigan Welsh corgi',\n",
      " 265: 'toy poodle',\n",
      " 266: 'miniature poodle',\n",
      " 267: 'standard poodle',\n",
      " 268: 'Mexican hairless',\n",
      " 269: 'timber wolf, grey wolf, gray wolf, Canis lupus',\n",
      " 270: 'white wolf, Arctic wolf, Canis lupus tundrarum',\n",
      " 271: 'red wolf, maned wolf, Canis rufus, Canis niger',\n",
      " 272: 'coyote, prairie wolf, brush wolf, Canis latrans',\n",
      " 273: 'dingo, warrigal, warragal, Canis dingo',\n",
      " 274: 'dhole, Cuon alpinus',\n",
      " 275: 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus',\n",
      " 276: 'hyena, hyaena',\n",
      " 277: 'red fox, Vulpes vulpes',\n",
      " 278: 'kit fox, Vulpes macrotis',\n",
      " 279: 'Arctic fox, white fox, Alopex lagopus',\n",
      " 280: 'grey fox, gray fox, Urocyon cinereoargenteus',\n",
      " 281: 'tabby, tabby cat',\n",
      " 282: 'tiger cat',\n",
      " 283: 'Persian cat',\n",
      " 284: 'Siamese cat, Siamese',\n",
      " 285: 'Egyptian cat',\n",
      " 286: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n",
      " 287: 'lynx, catamount',\n",
      " 288: 'leopard, Panthera pardus',\n",
      " 289: 'snow leopard, ounce, Panthera uncia',\n",
      " 290: 'jaguar, panther, Panthera onca, Felis onca',\n",
      " 291: 'lion, king of beasts, Panthera leo',\n",
      " 292: 'tiger, Panthera tigris',\n",
      " 293: 'cheetah, chetah, Acinonyx jubatus',\n",
      " 294: 'brown bear, bruin, Ursus arctos',\n",
      " 295: 'American black bear, black bear, Ursus americanus, Euarctos americanus',\n",
      " 296: 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus',\n",
      " 297: 'sloth bear, Melursus ursinus, Ursus ursinus',\n",
      " 298: 'mongoose',\n",
      " 299: 'meerkat, mierkat',\n",
      " 300: 'tiger beetle',\n",
      " 301: 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle',\n",
      " 302: 'ground beetle, carabid beetle',\n",
      " 303: 'long-horned beetle, longicorn, longicorn beetle',\n",
      " 304: 'leaf beetle, chrysomelid',\n",
      " 305: 'dung beetle',\n",
      " 306: 'rhinoceros beetle',\n",
      " 307: 'weevil',\n",
      " 308: 'fly',\n",
      " 309: 'bee',\n",
      " 310: 'ant, emmet, pismire',\n",
      " 311: 'grasshopper, hopper',\n",
      " 312: 'cricket',\n",
      " 313: 'walking stick, walkingstick, stick insect',\n",
      " 314: 'cockroach, roach',\n",
      " 315: 'mantis, mantid',\n",
      " 316: 'cicada, cicala',\n",
      " 317: 'leafhopper',\n",
      " 318: 'lacewing, lacewing fly',\n",
      " 319: \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\",\n",
      " 320: 'damselfly',\n",
      " 321: 'admiral',\n",
      " 322: 'ringlet, ringlet butterfly',\n",
      " 323: 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus',\n",
      " 324: 'cabbage butterfly',\n",
      " 325: 'sulphur butterfly, sulfur butterfly',\n",
      " 326: 'lycaenid, lycaenid butterfly',\n",
      " 327: 'starfish, sea star',\n",
      " 328: 'sea urchin',\n",
      " 329: 'sea cucumber, holothurian',\n",
      " 330: 'wood rabbit, cottontail, cottontail rabbit',\n",
      " 331: 'hare',\n",
      " 332: 'Angora, Angora rabbit',\n",
      " 333: 'hamster',\n",
      " 334: 'porcupine, hedgehog',\n",
      " 335: 'fox squirrel, eastern fox squirrel, Sciurus niger',\n",
      " 336: 'marmot',\n",
      " 337: 'beaver',\n",
      " 338: 'guinea pig, Cavia cobaya',\n",
      " 339: 'sorrel',\n",
      " 340: 'zebra',\n",
      " 341: 'hog, pig, grunter, squealer, Sus scrofa',\n",
      " 342: 'wild boar, boar, Sus scrofa',\n",
      " 343: 'warthog',\n",
      " 344: 'hippopotamus, hippo, river horse, Hippopotamus amphibius',\n",
      " 345: 'ox',\n",
      " 346: 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis',\n",
      " 347: 'bison',\n",
      " 348: 'ram, tup',\n",
      " 349: 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis',\n",
      " 350: 'ibex, Capra ibex',\n",
      " 351: 'hartebeest',\n",
      " 352: 'impala, Aepyceros melampus',\n",
      " 353: 'gazelle',\n",
      " 354: 'Arabian camel, dromedary, Camelus dromedarius',\n",
      " 355: 'llama',\n",
      " 356: 'weasel',\n",
      " 357: 'mink',\n",
      " 358: 'polecat, fitch, foulmart, foumart, Mustela putorius',\n",
      " 359: 'black-footed ferret, ferret, Mustela nigripes',\n",
      " 360: 'otter',\n",
      " 361: 'skunk, polecat, wood pussy',\n",
      " 362: 'badger',\n",
      " 363: 'armadillo',\n",
      " 364: 'three-toed sloth, ai, Bradypus tridactylus',\n",
      " 365: 'orangutan, orang, orangutang, Pongo pygmaeus',\n",
      " 366: 'gorilla, Gorilla gorilla',\n",
      " 367: 'chimpanzee, chimp, Pan troglodytes',\n",
      " 368: 'gibbon, Hylobates lar',\n",
      " 369: 'siamang, Hylobates syndactylus, Symphalangus syndactylus',\n",
      " 370: 'guenon, guenon monkey',\n",
      " 371: 'patas, hussar monkey, Erythrocebus patas',\n",
      " 372: 'baboon',\n",
      " 373: 'macaque',\n",
      " 374: 'langur',\n",
      " 375: 'colobus, colobus monkey',\n",
      " 376: 'proboscis monkey, Nasalis larvatus',\n",
      " 377: 'marmoset',\n",
      " 378: 'capuchin, ringtail, Cebus capucinus',\n",
      " 379: 'howler monkey, howler',\n",
      " 380: 'titi, titi monkey',\n",
      " 381: 'spider monkey, Ateles geoffroyi',\n",
      " 382: 'squirrel monkey, Saimiri sciureus',\n",
      " 383: 'Madagascar cat, ring-tailed lemur, Lemur catta',\n",
      " 384: 'indri, indris, Indri indri, Indri brevicaudatus',\n",
      " 385: 'Indian elephant, Elephas maximus',\n",
      " 386: 'African elephant, Loxodonta africana',\n",
      " 387: 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens',\n",
      " 388: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca',\n",
      " 389: 'barracouta, snoek',\n",
      " 390: 'eel',\n",
      " 391: 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch',\n",
      " 392: 'rock beauty, Holocanthus tricolor',\n",
      " 393: 'anemone fish',\n",
      " 394: 'sturgeon',\n",
      " 395: 'gar, garfish, garpike, billfish, Lepisosteus osseus',\n",
      " 396: 'lionfish',\n",
      " 397: 'puffer, pufferfish, blowfish, globefish',\n",
      " 398: 'abacus',\n",
      " 399: 'abaya',\n",
      " 400: \"academic gown, academic robe, judge's robe\",\n",
      " 401: 'accordion, piano accordion, squeeze box',\n",
      " 402: 'acoustic guitar',\n",
      " 403: 'aircraft carrier, carrier, flattop, attack aircraft carrier',\n",
      " 404: 'airliner',\n",
      " 405: 'airship, dirigible',\n",
      " 406: 'altar',\n",
      " 407: 'ambulance',\n",
      " 408: 'amphibian, amphibious vehicle',\n",
      " 409: 'analog clock',\n",
      " 410: 'apiary, bee house',\n",
      " 411: 'apron',\n",
      " 412: 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin',\n",
      " 413: 'assault rifle, assault gun',\n",
      " 414: 'backpack, back pack, knapsack, packsack, rucksack, haversack',\n",
      " 415: 'bakery, bakeshop, bakehouse',\n",
      " 416: 'balance beam, beam',\n",
      " 417: 'balloon',\n",
      " 418: 'ballpoint, ballpoint pen, ballpen, Biro',\n",
      " 419: 'Band Aid',\n",
      " 420: 'banjo',\n",
      " 421: 'bannister, banister, balustrade, balusters, handrail',\n",
      " 422: 'barbell',\n",
      " 423: 'barber chair',\n",
      " 424: 'barbershop',\n",
      " 425: 'barn',\n",
      " 426: 'barometer',\n",
      " 427: 'barrel, cask',\n",
      " 428: 'barrow, garden cart, lawn cart, wheelbarrow',\n",
      " 429: 'baseball',\n",
      " 430: 'basketball',\n",
      " 431: 'bassinet',\n",
      " 432: 'bassoon',\n",
      " 433: 'bathing cap, swimming cap',\n",
      " 434: 'bath towel',\n",
      " 435: 'bathtub, bathing tub, bath, tub',\n",
      " 436: 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon',\n",
      " 437: 'beacon, lighthouse, beacon light, pharos',\n",
      " 438: 'beaker',\n",
      " 439: 'bearskin, busby, shako',\n",
      " 440: 'beer bottle',\n",
      " 441: 'beer glass',\n",
      " 442: 'bell cote, bell cot',\n",
      " 443: 'bib',\n",
      " 444: 'bicycle-built-for-two, tandem bicycle, tandem',\n",
      " 445: 'bikini, two-piece',\n",
      " 446: 'binder, ring-binder',\n",
      " 447: 'binoculars, field glasses, opera glasses',\n",
      " 448: 'birdhouse',\n",
      " 449: 'boathouse',\n",
      " 450: 'bobsled, bobsleigh, bob',\n",
      " 451: 'bolo tie, bolo, bola tie, bola',\n",
      " 452: 'bonnet, poke bonnet',\n",
      " 453: 'bookcase',\n",
      " 454: 'bookshop, bookstore, bookstall',\n",
      " 455: 'bottlecap',\n",
      " 456: 'bow',\n",
      " 457: 'bow tie, bow-tie, bowtie',\n",
      " 458: 'brass, memorial tablet, plaque',\n",
      " 459: 'brassiere, bra, bandeau',\n",
      " 460: 'breakwater, groin, groyne, mole, bulwark, seawall, jetty',\n",
      " 461: 'breastplate, aegis, egis',\n",
      " 462: 'broom',\n",
      " 463: 'bucket, pail',\n",
      " 464: 'buckle',\n",
      " 465: 'bulletproof vest',\n",
      " 466: 'bullet train, bullet',\n",
      " 467: 'butcher shop, meat market',\n",
      " 468: 'cab, hack, taxi, taxicab',\n",
      " 469: 'caldron, cauldron',\n",
      " 470: 'candle, taper, wax light',\n",
      " 471: 'cannon',\n",
      " 472: 'canoe',\n",
      " 473: 'can opener, tin opener',\n",
      " 474: 'cardigan',\n",
      " 475: 'car mirror',\n",
      " 476: 'carousel, carrousel, merry-go-round, roundabout, whirligig',\n",
      " 477: \"carpenter's kit, tool kit\",\n",
      " 478: 'carton',\n",
      " 479: 'car wheel',\n",
      " 480: 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM',\n",
      " 481: 'cassette',\n",
      " 482: 'cassette player',\n",
      " 483: 'castle',\n",
      " 484: 'catamaran',\n",
      " 485: 'CD player',\n",
      " 486: 'cello, violoncello',\n",
      " 487: 'cellular telephone, cellular phone, cellphone, cell, mobile phone',\n",
      " 488: 'chain',\n",
      " 489: 'chainlink fence',\n",
      " 490: 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour',\n",
      " 491: 'chain saw, chainsaw',\n",
      " 492: 'chest',\n",
      " 493: 'chiffonier, commode',\n",
      " 494: 'chime, bell, gong',\n",
      " 495: 'china cabinet, china closet',\n",
      " 496: 'Christmas stocking',\n",
      " 497: 'church, church building',\n",
      " 498: 'cinema, movie theater, movie theatre, movie house, picture palace',\n",
      " 499: 'cleaver, meat cleaver, chopper',\n",
      " 500: 'cliff dwelling',\n",
      " 501: 'cloak',\n",
      " 502: 'clog, geta, patten, sabot',\n",
      " 503: 'cocktail shaker',\n",
      " 504: 'coffee mug',\n",
      " 505: 'coffeepot',\n",
      " 506: 'coil, spiral, volute, whorl, helix',\n",
      " 507: 'combination lock',\n",
      " 508: 'computer keyboard, keypad',\n",
      " 509: 'confectionery, confectionary, candy store',\n",
      " 510: 'container ship, containership, container vessel',\n",
      " 511: 'convertible',\n",
      " 512: 'corkscrew, bottle screw',\n",
      " 513: 'cornet, horn, trumpet, trump',\n",
      " 514: 'cowboy boot',\n",
      " 515: 'cowboy hat, ten-gallon hat',\n",
      " 516: 'cradle',\n",
      " 517: 'crane',\n",
      " 518: 'crash helmet',\n",
      " 519: 'crate',\n",
      " 520: 'crib, cot',\n",
      " 521: 'Crock Pot',\n",
      " 522: 'croquet ball',\n",
      " 523: 'crutch',\n",
      " 524: 'cuirass',\n",
      " 525: 'dam, dike, dyke',\n",
      " 526: 'desk',\n",
      " 527: 'desktop computer',\n",
      " 528: 'dial telephone, dial phone',\n",
      " 529: 'diaper, nappy, napkin',\n",
      " 530: 'digital clock',\n",
      " 531: 'digital watch',\n",
      " 532: 'dining table, board',\n",
      " 533: 'dishrag, dishcloth',\n",
      " 534: 'dishwasher, dish washer, dishwashing machine',\n",
      " 535: 'disk brake, disc brake',\n",
      " 536: 'dock, dockage, docking facility',\n",
      " 537: 'dogsled, dog sled, dog sleigh',\n",
      " 538: 'dome',\n",
      " 539: 'doormat, welcome mat',\n",
      " 540: 'drilling platform, offshore rig',\n",
      " 541: 'drum, membranophone, tympan',\n",
      " 542: 'drumstick',\n",
      " 543: 'dumbbell',\n",
      " 544: 'Dutch oven',\n",
      " 545: 'electric fan, blower',\n",
      " 546: 'electric guitar',\n",
      " 547: 'electric locomotive',\n",
      " 548: 'entertainment center',\n",
      " 549: 'envelope',\n",
      " 550: 'espresso maker',\n",
      " 551: 'face powder',\n",
      " 552: 'feather boa, boa',\n",
      " 553: 'file, file cabinet, filing cabinet',\n",
      " 554: 'fireboat',\n",
      " 555: 'fire engine, fire truck',\n",
      " 556: 'fire screen, fireguard',\n",
      " 557: 'flagpole, flagstaff',\n",
      " 558: 'flute, transverse flute',\n",
      " 559: 'folding chair',\n",
      " 560: 'football helmet',\n",
      " 561: 'forklift',\n",
      " 562: 'fountain',\n",
      " 563: 'fountain pen',\n",
      " 564: 'four-poster',\n",
      " 565: 'freight car',\n",
      " 566: 'French horn, horn',\n",
      " 567: 'frying pan, frypan, skillet',\n",
      " 568: 'fur coat',\n",
      " 569: 'garbage truck, dustcart',\n",
      " 570: 'gasmask, respirator, gas helmet',\n",
      " 571: 'gas pump, gasoline pump, petrol pump, island dispenser',\n",
      " 572: 'goblet',\n",
      " 573: 'go-kart',\n",
      " 574: 'golf ball',\n",
      " 575: 'golfcart, golf cart',\n",
      " 576: 'gondola',\n",
      " 577: 'gong, tam-tam',\n",
      " 578: 'gown',\n",
      " 579: 'grand piano, grand',\n",
      " 580: 'greenhouse, nursery, glasshouse',\n",
      " 581: 'grille, radiator grille',\n",
      " 582: 'grocery store, grocery, food market, market',\n",
      " 583: 'guillotine',\n",
      " 584: 'hair slide',\n",
      " 585: 'hair spray',\n",
      " 586: 'half track',\n",
      " 587: 'hammer',\n",
      " 588: 'hamper',\n",
      " 589: 'hand blower, blow dryer, blow drier, hair dryer, hair drier',\n",
      " 590: 'hand-held computer, hand-held microcomputer',\n",
      " 591: 'handkerchief, hankie, hanky, hankey',\n",
      " 592: 'hard disc, hard disk, fixed disk',\n",
      " 593: 'harmonica, mouth organ, harp, mouth harp',\n",
      " 594: 'harp',\n",
      " 595: 'harvester, reaper',\n",
      " 596: 'hatchet',\n",
      " 597: 'holster',\n",
      " 598: 'home theater, home theatre',\n",
      " 599: 'honeycomb',\n",
      " 600: 'hook, claw',\n",
      " 601: 'hoopskirt, crinoline',\n",
      " 602: 'horizontal bar, high bar',\n",
      " 603: 'horse cart, horse-cart',\n",
      " 604: 'hourglass',\n",
      " 605: 'iPod',\n",
      " 606: 'iron, smoothing iron',\n",
      " 607: \"jack-o'-lantern\",\n",
      " 608: 'jean, blue jean, denim',\n",
      " 609: 'jeep, landrover',\n",
      " 610: 'jersey, T-shirt, tee shirt',\n",
      " 611: 'jigsaw puzzle',\n",
      " 612: 'jinrikisha, ricksha, rickshaw',\n",
      " 613: 'joystick',\n",
      " 614: 'kimono',\n",
      " 615: 'knee pad',\n",
      " 616: 'knot',\n",
      " 617: 'lab coat, laboratory coat',\n",
      " 618: 'ladle',\n",
      " 619: 'lampshade, lamp shade',\n",
      " 620: 'laptop, laptop computer',\n",
      " 621: 'lawn mower, mower',\n",
      " 622: 'lens cap, lens cover',\n",
      " 623: 'letter opener, paper knife, paperknife',\n",
      " 624: 'library',\n",
      " 625: 'lifeboat',\n",
      " 626: 'lighter, light, igniter, ignitor',\n",
      " 627: 'limousine, limo',\n",
      " 628: 'liner, ocean liner',\n",
      " 629: 'lipstick, lip rouge',\n",
      " 630: 'Loafer',\n",
      " 631: 'lotion',\n",
      " 632: 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system',\n",
      " 633: \"loupe, jeweler's loupe\",\n",
      " 634: 'lumbermill, sawmill',\n",
      " 635: 'magnetic compass',\n",
      " 636: 'mailbag, postbag',\n",
      " 637: 'mailbox, letter box',\n",
      " 638: 'maillot',\n",
      " 639: 'maillot, tank suit',\n",
      " 640: 'manhole cover',\n",
      " 641: 'maraca',\n",
      " 642: 'marimba, xylophone',\n",
      " 643: 'mask',\n",
      " 644: 'matchstick',\n",
      " 645: 'maypole',\n",
      " 646: 'maze, labyrinth',\n",
      " 647: 'measuring cup',\n",
      " 648: 'medicine chest, medicine cabinet',\n",
      " 649: 'megalith, megalithic structure',\n",
      " 650: 'microphone, mike',\n",
      " 651: 'microwave, microwave oven',\n",
      " 652: 'military uniform',\n",
      " 653: 'milk can',\n",
      " 654: 'minibus',\n",
      " 655: 'miniskirt, mini',\n",
      " 656: 'minivan',\n",
      " 657: 'missile',\n",
      " 658: 'mitten',\n",
      " 659: 'mixing bowl',\n",
      " 660: 'mobile home, manufactured home',\n",
      " 661: 'Model T',\n",
      " 662: 'modem',\n",
      " 663: 'monastery',\n",
      " 664: 'monitor',\n",
      " 665: 'moped',\n",
      " 666: 'mortar',\n",
      " 667: 'mortarboard',\n",
      " 668: 'mosque',\n",
      " 669: 'mosquito net',\n",
      " 670: 'motor scooter, scooter',\n",
      " 671: 'mountain bike, all-terrain bike, off-roader',\n",
      " 672: 'mountain tent',\n",
      " 673: 'mouse, computer mouse',\n",
      " 674: 'mousetrap',\n",
      " 675: 'moving van',\n",
      " 676: 'muzzle',\n",
      " 677: 'nail',\n",
      " 678: 'neck brace',\n",
      " 679: 'necklace',\n",
      " 680: 'nipple',\n",
      " 681: 'notebook, notebook computer',\n",
      " 682: 'obelisk',\n",
      " 683: 'oboe, hautboy, hautbois',\n",
      " 684: 'ocarina, sweet potato',\n",
      " 685: 'odometer, hodometer, mileometer, milometer',\n",
      " 686: 'oil filter',\n",
      " 687: 'organ, pipe organ',\n",
      " 688: 'oscilloscope, scope, cathode-ray oscilloscope, CRO',\n",
      " 689: 'overskirt',\n",
      " 690: 'oxcart',\n",
      " 691: 'oxygen mask',\n",
      " 692: 'packet',\n",
      " 693: 'paddle, boat paddle',\n",
      " 694: 'paddlewheel, paddle wheel',\n",
      " 695: 'padlock',\n",
      " 696: 'paintbrush',\n",
      " 697: \"pajama, pyjama, pj's, jammies\",\n",
      " 698: 'palace',\n",
      " 699: 'panpipe, pandean pipe, syrinx',\n",
      " 700: 'paper towel',\n",
      " 701: 'parachute, chute',\n",
      " 702: 'parallel bars, bars',\n",
      " 703: 'park bench',\n",
      " 704: 'parking meter',\n",
      " 705: 'passenger car, coach, carriage',\n",
      " 706: 'patio, terrace',\n",
      " 707: 'pay-phone, pay-station',\n",
      " 708: 'pedestal, plinth, footstall',\n",
      " 709: 'pencil box, pencil case',\n",
      " 710: 'pencil sharpener',\n",
      " 711: 'perfume, essence',\n",
      " 712: 'Petri dish',\n",
      " 713: 'photocopier',\n",
      " 714: 'pick, plectrum, plectron',\n",
      " 715: 'pickelhaube',\n",
      " 716: 'picket fence, paling',\n",
      " 717: 'pickup, pickup truck',\n",
      " 718: 'pier',\n",
      " 719: 'piggy bank, penny bank',\n",
      " 720: 'pill bottle',\n",
      " 721: 'pillow',\n",
      " 722: 'ping-pong ball',\n",
      " 723: 'pinwheel',\n",
      " 724: 'pirate, pirate ship',\n",
      " 725: 'pitcher, ewer',\n",
      " 726: \"plane, carpenter's plane, woodworking plane\",\n",
      " 727: 'planetarium',\n",
      " 728: 'plastic bag',\n",
      " 729: 'plate rack',\n",
      " 730: 'plow, plough',\n",
      " 731: \"plunger, plumber's helper\",\n",
      " 732: 'Polaroid camera, Polaroid Land camera',\n",
      " 733: 'pole',\n",
      " 734: 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria',\n",
      " 735: 'poncho',\n",
      " 736: 'pool table, billiard table, snooker table',\n",
      " 737: 'pop bottle, soda bottle',\n",
      " 738: 'pot, flowerpot',\n",
      " 739: \"potter's wheel\",\n",
      " 740: 'power drill',\n",
      " 741: 'prayer rug, prayer mat',\n",
      " 742: 'printer',\n",
      " 743: 'prison, prison house',\n",
      " 744: 'projectile, missile',\n",
      " 745: 'projector',\n",
      " 746: 'puck, hockey puck',\n",
      " 747: 'punching bag, punch bag, punching ball, punchball',\n",
      " 748: 'purse',\n",
      " 749: 'quill, quill pen',\n",
      " 750: 'quilt, comforter, comfort, puff',\n",
      " 751: 'racer, race car, racing car',\n",
      " 752: 'racket, racquet',\n",
      " 753: 'radiator',\n",
      " 754: 'radio, wireless',\n",
      " 755: 'radio telescope, radio reflector',\n",
      " 756: 'rain barrel',\n",
      " 757: 'recreational vehicle, RV, R.V.',\n",
      " 758: 'reel',\n",
      " 759: 'reflex camera',\n",
      " 760: 'refrigerator, icebox',\n",
      " 761: 'remote control, remote',\n",
      " 762: 'restaurant, eating house, eating place, eatery',\n",
      " 763: 'revolver, six-gun, six-shooter',\n",
      " 764: 'rifle',\n",
      " 765: 'rocking chair, rocker',\n",
      " 766: 'rotisserie',\n",
      " 767: 'rubber eraser, rubber, pencil eraser',\n",
      " 768: 'rugby ball',\n",
      " 769: 'rule, ruler',\n",
      " 770: 'running shoe',\n",
      " 771: 'safe',\n",
      " 772: 'safety pin',\n",
      " 773: 'saltshaker, salt shaker',\n",
      " 774: 'sandal',\n",
      " 775: 'sarong',\n",
      " 776: 'sax, saxophone',\n",
      " 777: 'scabbard',\n",
      " 778: 'scale, weighing machine',\n",
      " 779: 'school bus',\n",
      " 780: 'schooner',\n",
      " 781: 'scoreboard',\n",
      " 782: 'screen, CRT screen',\n",
      " 783: 'screw',\n",
      " 784: 'screwdriver',\n",
      " 785: 'seat belt, seatbelt',\n",
      " 786: 'sewing machine',\n",
      " 787: 'shield, buckler',\n",
      " 788: 'shoe shop, shoe-shop, shoe store',\n",
      " 789: 'shoji',\n",
      " 790: 'shopping basket',\n",
      " 791: 'shopping cart',\n",
      " 792: 'shovel',\n",
      " 793: 'shower cap',\n",
      " 794: 'shower curtain',\n",
      " 795: 'ski',\n",
      " 796: 'ski mask',\n",
      " 797: 'sleeping bag',\n",
      " 798: 'slide rule, slipstick',\n",
      " 799: 'sliding door',\n",
      " 800: 'slot, one-armed bandit',\n",
      " 801: 'snorkel',\n",
      " 802: 'snowmobile',\n",
      " 803: 'snowplow, snowplough',\n",
      " 804: 'soap dispenser',\n",
      " 805: 'soccer ball',\n",
      " 806: 'sock',\n",
      " 807: 'solar dish, solar collector, solar furnace',\n",
      " 808: 'sombrero',\n",
      " 809: 'soup bowl',\n",
      " 810: 'space bar',\n",
      " 811: 'space heater',\n",
      " 812: 'space shuttle',\n",
      " 813: 'spatula',\n",
      " 814: 'speedboat',\n",
      " 815: \"spider web, spider's web\",\n",
      " 816: 'spindle',\n",
      " 817: 'sports car, sport car',\n",
      " 818: 'spotlight, spot',\n",
      " 819: 'stage',\n",
      " 820: 'steam locomotive',\n",
      " 821: 'steel arch bridge',\n",
      " 822: 'steel drum',\n",
      " 823: 'stethoscope',\n",
      " 824: 'stole',\n",
      " 825: 'stone wall',\n",
      " 826: 'stopwatch, stop watch',\n",
      " 827: 'stove',\n",
      " 828: 'strainer',\n",
      " 829: 'streetcar, tram, tramcar, trolley, trolley car',\n",
      " 830: 'stretcher',\n",
      " 831: 'studio couch, day bed',\n",
      " 832: 'stupa, tope',\n",
      " 833: 'submarine, pigboat, sub, U-boat',\n",
      " 834: 'suit, suit of clothes',\n",
      " 835: 'sundial',\n",
      " 836: 'sunglass',\n",
      " 837: 'sunglasses, dark glasses, shades',\n",
      " 838: 'sunscreen, sunblock, sun blocker',\n",
      " 839: 'suspension bridge',\n",
      " 840: 'swab, swob, mop',\n",
      " 841: 'sweatshirt',\n",
      " 842: 'swimming trunks, bathing trunks',\n",
      " 843: 'swing',\n",
      " 844: 'switch, electric switch, electrical switch',\n",
      " 845: 'syringe',\n",
      " 846: 'table lamp',\n",
      " 847: 'tank, army tank, armored combat vehicle, armoured combat vehicle',\n",
      " 848: 'tape player',\n",
      " 849: 'teapot',\n",
      " 850: 'teddy, teddy bear',\n",
      " 851: 'television, television system',\n",
      " 852: 'tennis ball',\n",
      " 853: 'thatch, thatched roof',\n",
      " 854: 'theater curtain, theatre curtain',\n",
      " 855: 'thimble',\n",
      " 856: 'thresher, thrasher, threshing machine',\n",
      " 857: 'throne',\n",
      " 858: 'tile roof',\n",
      " 859: 'toaster',\n",
      " 860: 'tobacco shop, tobacconist shop, tobacconist',\n",
      " 861: 'toilet seat',\n",
      " 862: 'torch',\n",
      " 863: 'totem pole',\n",
      " 864: 'tow truck, tow car, wrecker',\n",
      " 865: 'toyshop',\n",
      " 866: 'tractor',\n",
      " 867: 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi',\n",
      " 868: 'tray',\n",
      " 869: 'trench coat',\n",
      " 870: 'tricycle, trike, velocipede',\n",
      " 871: 'trimaran',\n",
      " 872: 'tripod',\n",
      " 873: 'triumphal arch',\n",
      " 874: 'trolleybus, trolley coach, trackless trolley',\n",
      " 875: 'trombone',\n",
      " 876: 'tub, vat',\n",
      " 877: 'turnstile',\n",
      " 878: 'typewriter keyboard',\n",
      " 879: 'umbrella',\n",
      " 880: 'unicycle, monocycle',\n",
      " 881: 'upright, upright piano',\n",
      " 882: 'vacuum, vacuum cleaner',\n",
      " 883: 'vase',\n",
      " 884: 'vault',\n",
      " 885: 'velvet',\n",
      " 886: 'vending machine',\n",
      " 887: 'vestment',\n",
      " 888: 'viaduct',\n",
      " 889: 'violin, fiddle',\n",
      " 890: 'volleyball',\n",
      " 891: 'waffle iron',\n",
      " 892: 'wall clock',\n",
      " 893: 'wallet, billfold, notecase, pocketbook',\n",
      " 894: 'wardrobe, closet, press',\n",
      " 895: 'warplane, military plane',\n",
      " 896: 'washbasin, handbasin, washbowl, lavabo, wash-hand basin',\n",
      " 897: 'washer, automatic washer, washing machine',\n",
      " 898: 'water bottle',\n",
      " 899: 'water jug',\n",
      " 900: 'water tower',\n",
      " 901: 'whiskey jug',\n",
      " 902: 'whistle',\n",
      " 903: 'wig',\n",
      " 904: 'window screen',\n",
      " 905: 'window shade',\n",
      " 906: 'Windsor tie',\n",
      " 907: 'wine bottle',\n",
      " 908: 'wing',\n",
      " 909: 'wok',\n",
      " 910: 'wooden spoon',\n",
      " 911: 'wool, woolen, woollen',\n",
      " 912: 'worm fence, snake fence, snake-rail fence, Virginia fence',\n",
      " 913: 'wreck',\n",
      " 914: 'yawl',\n",
      " 915: 'yurt',\n",
      " 916: 'web site, website, internet site, site',\n",
      " 917: 'comic book',\n",
      " 918: 'crossword puzzle, crossword',\n",
      " 919: 'street sign',\n",
      " 920: 'traffic light, traffic signal, stoplight',\n",
      " 921: 'book jacket, dust cover, dust jacket, dust wrapper',\n",
      " 922: 'menu',\n",
      " 923: 'plate',\n",
      " 924: 'guacamole',\n",
      " 925: 'consomme',\n",
      " 926: 'hot pot, hotpot',\n",
      " 927: 'trifle',\n",
      " 928: 'ice cream, icecream',\n",
      " 929: 'ice lolly, lolly, lollipop, popsicle',\n",
      " 930: 'French loaf',\n",
      " 931: 'bagel, beigel',\n",
      " 932: 'pretzel',\n",
      " 933: 'cheeseburger',\n",
      " 934: 'hotdog, hot dog, red hot',\n",
      " 935: 'mashed potato',\n",
      " 936: 'head cabbage',\n",
      " 937: 'broccoli',\n",
      " 938: 'cauliflower',\n",
      " 939: 'zucchini, courgette',\n",
      " 940: 'spaghetti squash',\n",
      " 941: 'acorn squash',\n",
      " 942: 'butternut squash',\n",
      " 943: 'cucumber, cuke',\n",
      " 944: 'artichoke, globe artichoke',\n",
      " 945: 'bell pepper',\n",
      " 946: 'cardoon',\n",
      " 947: 'mushroom',\n",
      " 948: 'Granny Smith',\n",
      " 949: 'strawberry',\n",
      " 950: 'orange',\n",
      " 951: 'lemon',\n",
      " 952: 'fig',\n",
      " 953: 'pineapple, ananas',\n",
      " 954: 'banana',\n",
      " 955: 'jackfruit, jak, jack',\n",
      " 956: 'custard apple',\n",
      " 957: 'pomegranate',\n",
      " 958: 'hay',\n",
      " 959: 'carbonara',\n",
      " 960: 'chocolate sauce, chocolate syrup',\n",
      " 961: 'dough',\n",
      " 962: 'meat loaf, meatloaf',\n",
      " 963: 'pizza, pizza pie',\n",
      " 964: 'potpie',\n",
      " 965: 'burrito',\n",
      " 966: 'red wine',\n",
      " 967: 'espresso',\n",
      " 968: 'cup',\n",
      " 969: 'eggnog',\n",
      " 970: 'alp',\n",
      " 971: 'bubble',\n",
      " 972: 'cliff, drop, drop-off',\n",
      " 973: 'coral reef',\n",
      " 974: 'geyser',\n",
      " 975: 'lakeside, lakeshore',\n",
      " 976: 'promontory, headland, head, foreland',\n",
      " 977: 'sandbar, sand bar',\n",
      " 978: 'seashore, coast, seacoast, sea-coast',\n",
      " 979: 'valley, vale',\n",
      " 980: 'volcano',\n",
      " 981: 'ballplayer, baseball player',\n",
      " 982: 'groom, bridegroom',\n",
      " 983: 'scuba diver',\n",
      " 984: 'rapeseed',\n",
      " 985: 'daisy',\n",
      " 986: \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\",\n",
      " 987: 'corn',\n",
      " 988: 'acorn',\n",
      " 989: 'hip, rose hip, rosehip',\n",
      " 990: 'buckeye, horse chestnut, conker',\n",
      " 991: 'coral fungus',\n",
      " 992: 'agaric',\n",
      " 993: 'gyromitra',\n",
      " 994: 'stinkhorn, carrion fungus',\n",
      " 995: 'earthstar',\n",
      " 996: 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa',\n",
      " 997: 'bolete',\n",
      " 998: 'ear, spike, capitulum',\n",
      " 999: 'toilet tissue, toilet paper, bathroom tissue'}\n",
      "71/2:\n",
      "from tensorflow.keras.applications import EfficientNetB4\n",
      "from tensorflow.image import resize\n",
      "model = EfficientNetB4(weights='imagenet')\n",
      "\n",
      "def center_crop(img, new_width=None, new_height=None):        \n",
      "\n",
      "    width = img.shape[1]\n",
      "    height = img.shape[0]\n",
      "\n",
      "    if new_width is None:\n",
      "        new_width = min(width, height)\n",
      "\n",
      "    if new_height is None:\n",
      "        new_height = min(width, height)\n",
      "\n",
      "    left = int(np.ceil((width - new_width) / 2))\n",
      "    right = width - int(np.floor((width - new_width) / 2))\n",
      "\n",
      "    top = int(np.ceil((height - new_height) / 2))\n",
      "    bottom = height - int(np.floor((height - new_height) / 2))\n",
      "\n",
      "    if len(img.shape) == 2:\n",
      "        center_cropped_img = img[top:bottom, left:right]\n",
      "    else:\n",
      "        center_cropped_img = img[top:bottom, left:right, ...]\n",
      "\n",
      "    return center_cropped_img\n",
      "\n",
      "class_dict = {0: 'tench, Tinca tinca',\n",
      " 1: 'goldfish, Carassius auratus',\n",
      " 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n",
      " 3: 'tiger shark, Galeocerdo cuvieri',\n",
      " 4: 'hammerhead, hammerhead shark',\n",
      " 5: 'electric ray, crampfish, numbfish, torpedo',\n",
      " 6: 'stingray',\n",
      " 7: 'cock',\n",
      " 8: 'hen',\n",
      " 9: 'ostrich, Struthio camelus',\n",
      " 10: 'brambling, Fringilla montifringilla',\n",
      " 11: 'goldfinch, Carduelis carduelis',\n",
      " 12: 'house finch, linnet, Carpodacus mexicanus',\n",
      " 13: 'junco, snowbird',\n",
      " 14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea',\n",
      " 15: 'robin, American robin, Turdus migratorius',\n",
      " 16: 'bulbul',\n",
      " 17: 'jay',\n",
      " 18: 'magpie',\n",
      " 19: 'chickadee',\n",
      " 20: 'water ouzel, dipper',\n",
      " 21: 'kite',\n",
      " 22: 'bald eagle, American eagle, Haliaeetus leucocephalus',\n",
      " 23: 'vulture',\n",
      " 24: 'great grey owl, great gray owl, Strix nebulosa',\n",
      " 25: 'European fire salamander, Salamandra salamandra',\n",
      " 26: 'common newt, Triturus vulgaris',\n",
      " 27: 'eft',\n",
      " 28: 'spotted salamander, Ambystoma maculatum',\n",
      " 29: 'axolotl, mud puppy, Ambystoma mexicanum',\n",
      " 30: 'bullfrog, Rana catesbeiana',\n",
      " 31: 'tree frog, tree-frog',\n",
      " 32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui',\n",
      " 33: 'loggerhead, loggerhead turtle, Caretta caretta',\n",
      " 34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea',\n",
      " 35: 'mud turtle',\n",
      " 36: 'terrapin',\n",
      " 37: 'box turtle, box tortoise',\n",
      " 38: 'banded gecko',\n",
      " 39: 'common iguana, iguana, Iguana iguana',\n",
      " 40: 'American chameleon, anole, Anolis carolinensis',\n",
      " 41: 'whiptail, whiptail lizard',\n",
      " 42: 'agama',\n",
      " 43: 'frilled lizard, Chlamydosaurus kingi',\n",
      " 44: 'alligator lizard',\n",
      " 45: 'Gila monster, Heloderma suspectum',\n",
      " 46: 'green lizard, Lacerta viridis',\n",
      " 47: 'African chameleon, Chamaeleo chamaeleon',\n",
      " 48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis',\n",
      " 49: 'African crocodile, Nile crocodile, Crocodylus niloticus',\n",
      " 50: 'American alligator, Alligator mississipiensis',\n",
      " 51: 'triceratops',\n",
      " 52: 'thunder snake, worm snake, Carphophis amoenus',\n",
      " 53: 'ringneck snake, ring-necked snake, ring snake',\n",
      " 54: 'hognose snake, puff adder, sand viper',\n",
      " 55: 'green snake, grass snake',\n",
      " 56: 'king snake, kingsnake',\n",
      " 57: 'garter snake, grass snake',\n",
      " 58: 'water snake',\n",
      " 59: 'vine snake',\n",
      " 60: 'night snake, Hypsiglena torquata',\n",
      " 61: 'boa constrictor, Constrictor constrictor',\n",
      " 62: 'rock python, rock snake, Python sebae',\n",
      " 63: 'Indian cobra, Naja naja',\n",
      " 64: 'green mamba',\n",
      " 65: 'sea snake',\n",
      " 66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus',\n",
      " 67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus',\n",
      " 68: 'sidewinder, horned rattlesnake, Crotalus cerastes',\n",
      " 69: 'trilobite',\n",
      " 70: 'harvestman, daddy longlegs, Phalangium opilio',\n",
      " 71: 'scorpion',\n",
      " 72: 'black and gold garden spider, Argiope aurantia',\n",
      " 73: 'barn spider, Araneus cavaticus',\n",
      " 74: 'garden spider, Aranea diademata',\n",
      " 75: 'black widow, Latrodectus mactans',\n",
      " 76: 'tarantula',\n",
      " 77: 'wolf spider, hunting spider',\n",
      " 78: 'tick',\n",
      " 79: 'centipede',\n",
      " 80: 'black grouse',\n",
      " 81: 'ptarmigan',\n",
      " 82: 'ruffed grouse, partridge, Bonasa umbellus',\n",
      " 83: 'prairie chicken, prairie grouse, prairie fowl',\n",
      " 84: 'peacock',\n",
      " 85: 'quail',\n",
      " 86: 'partridge',\n",
      " 87: 'African grey, African gray, Psittacus erithacus',\n",
      " 88: 'macaw',\n",
      " 89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n",
      " 90: 'lorikeet',\n",
      " 91: 'coucal',\n",
      " 92: 'bee eater',\n",
      " 93: 'hornbill',\n",
      " 94: 'hummingbird',\n",
      " 95: 'jacamar',\n",
      " 96: 'toucan',\n",
      " 97: 'drake',\n",
      " 98: 'red-breasted merganser, Mergus serrator',\n",
      " 99: 'goose',\n",
      " 100: 'black swan, Cygnus atratus',\n",
      " 101: 'tusker',\n",
      " 102: 'echidna, spiny anteater, anteater',\n",
      " 103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n",
      " 104: 'wallaby, brush kangaroo',\n",
      " 105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n",
      " 106: 'wombat',\n",
      " 107: 'jellyfish',\n",
      " 108: 'sea anemone, anemone',\n",
      " 109: 'brain coral',\n",
      " 110: 'flatworm, platyhelminth',\n",
      " 111: 'nematode, nematode worm, roundworm',\n",
      " 112: 'conch',\n",
      " 113: 'snail',\n",
      " 114: 'slug',\n",
      " 115: 'sea slug, nudibranch',\n",
      " 116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore',\n",
      " 117: 'chambered nautilus, pearly nautilus, nautilus',\n",
      " 118: 'Dungeness crab, Cancer magister',\n",
      " 119: 'rock crab, Cancer irroratus',\n",
      " 120: 'fiddler crab',\n",
      " 121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica',\n",
      " 122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus',\n",
      " 123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish',\n",
      " 124: 'crayfish, crawfish, crawdad, crawdaddy',\n",
      " 125: 'hermit crab',\n",
      " 126: 'isopod',\n",
      " 127: 'white stork, Ciconia ciconia',\n",
      " 128: 'black stork, Ciconia nigra',\n",
      " 129: 'spoonbill',\n",
      " 130: 'flamingo',\n",
      " 131: 'little blue heron, Egretta caerulea',\n",
      " 132: 'American egret, great white heron, Egretta albus',\n",
      " 133: 'bittern',\n",
      " 134: 'crane',\n",
      " 135: 'limpkin, Aramus pictus',\n",
      " 136: 'European gallinule, Porphyrio porphyrio',\n",
      " 137: 'American coot, marsh hen, mud hen, water hen, Fulica americana',\n",
      " 138: 'bustard',\n",
      " 139: 'ruddy turnstone, Arenaria interpres',\n",
      " 140: 'red-backed sandpiper, dunlin, Erolia alpina',\n",
      " 141: 'redshank, Tringa totanus',\n",
      " 142: 'dowitcher',\n",
      " 143: 'oystercatcher, oyster catcher',\n",
      " 144: 'pelican',\n",
      " 145: 'king penguin, Aptenodytes patagonica',\n",
      " 146: 'albatross, mollymawk',\n",
      " 147: 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus',\n",
      " 148: 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca',\n",
      " 149: 'dugong, Dugong dugon',\n",
      " 150: 'sea lion',\n",
      " 151: 'Chihuahua',\n",
      " 152: 'Japanese spaniel',\n",
      " 153: 'Maltese dog, Maltese terrier, Maltese',\n",
      " 154: 'Pekinese, Pekingese, Peke',\n",
      " 155: 'Shih-Tzu',\n",
      " 156: 'Blenheim spaniel',\n",
      " 157: 'papillon',\n",
      " 158: 'toy terrier',\n",
      " 159: 'Rhodesian ridgeback',\n",
      " 160: 'Afghan hound, Afghan',\n",
      " 161: 'basset, basset hound',\n",
      " 162: 'beagle',\n",
      " 163: 'bloodhound, sleuthhound',\n",
      " 164: 'bluetick',\n",
      " 165: 'black-and-tan coonhound',\n",
      " 166: 'Walker hound, Walker foxhound',\n",
      " 167: 'English foxhound',\n",
      " 168: 'redbone',\n",
      " 169: 'borzoi, Russian wolfhound',\n",
      " 170: 'Irish wolfhound',\n",
      " 171: 'Italian greyhound',\n",
      " 172: 'whippet',\n",
      " 173: 'Ibizan hound, Ibizan Podenco',\n",
      " 174: 'Norwegian elkhound, elkhound',\n",
      " 175: 'otterhound, otter hound',\n",
      " 176: 'Saluki, gazelle hound',\n",
      " 177: 'Scottish deerhound, deerhound',\n",
      " 178: 'Weimaraner',\n",
      " 179: 'Staffordshire bullterrier, Staffordshire bull terrier',\n",
      " 180: 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier',\n",
      " 181: 'Bedlington terrier',\n",
      " 182: 'Border terrier',\n",
      " 183: 'Kerry blue terrier',\n",
      " 184: 'Irish terrier',\n",
      " 185: 'Norfolk terrier',\n",
      " 186: 'Norwich terrier',\n",
      " 187: 'Yorkshire terrier',\n",
      " 188: 'wire-haired fox terrier',\n",
      " 189: 'Lakeland terrier',\n",
      " 190: 'Sealyham terrier, Sealyham',\n",
      " 191: 'Airedale, Airedale terrier',\n",
      " 192: 'cairn, cairn terrier',\n",
      " 193: 'Australian terrier',\n",
      " 194: 'Dandie Dinmont, Dandie Dinmont terrier',\n",
      " 195: 'Boston bull, Boston terrier',\n",
      " 196: 'miniature schnauzer',\n",
      " 197: 'giant schnauzer',\n",
      " 198: 'standard schnauzer',\n",
      " 199: 'Scotch terrier, Scottish terrier, Scottie',\n",
      " 200: 'Tibetan terrier, chrysanthemum dog',\n",
      " 201: 'silky terrier, Sydney silky',\n",
      " 202: 'soft-coated wheaten terrier',\n",
      " 203: 'West Highland white terrier',\n",
      " 204: 'Lhasa, Lhasa apso',\n",
      " 205: 'flat-coated retriever',\n",
      " 206: 'curly-coated retriever',\n",
      " 207: 'golden retriever',\n",
      " 208: 'Labrador retriever',\n",
      " 209: 'Chesapeake Bay retriever',\n",
      " 210: 'German short-haired pointer',\n",
      " 211: 'vizsla, Hungarian pointer',\n",
      " 212: 'English setter',\n",
      " 213: 'Irish setter, red setter',\n",
      " 214: 'Gordon setter',\n",
      " 215: 'Brittany spaniel',\n",
      " 216: 'clumber, clumber spaniel',\n",
      " 217: 'English springer, English springer spaniel',\n",
      " 218: 'Welsh springer spaniel',\n",
      " 219: 'cocker spaniel, English cocker spaniel, cocker',\n",
      " 220: 'Sussex spaniel',\n",
      " 221: 'Irish water spaniel',\n",
      " 222: 'kuvasz',\n",
      " 223: 'schipperke',\n",
      " 224: 'groenendael',\n",
      " 225: 'malinois',\n",
      " 226: 'briard',\n",
      " 227: 'kelpie',\n",
      " 228: 'komondor',\n",
      " 229: 'Old English sheepdog, bobtail',\n",
      " 230: 'Shetland sheepdog, Shetland sheep dog, Shetland',\n",
      " 231: 'collie',\n",
      " 232: 'Border collie',\n",
      " 233: 'Bouvier des Flandres, Bouviers des Flandres',\n",
      " 234: 'Rottweiler',\n",
      " 235: 'German shepherd, German shepherd dog, German police dog, alsatian',\n",
      " 236: 'Doberman, Doberman pinscher',\n",
      " 237: 'miniature pinscher',\n",
      " 238: 'Greater Swiss Mountain dog',\n",
      " 239: 'Bernese mountain dog',\n",
      " 240: 'Appenzeller',\n",
      " 241: 'EntleBucher',\n",
      " 242: 'boxer',\n",
      " 243: 'bull mastiff',\n",
      " 244: 'Tibetan mastiff',\n",
      " 245: 'French bulldog',\n",
      " 246: 'Great Dane',\n",
      " 247: 'Saint Bernard, St Bernard',\n",
      " 248: 'Eskimo dog, husky',\n",
      " 249: 'malamute, malemute, Alaskan malamute',\n",
      " 250: 'Siberian husky',\n",
      " 251: 'dalmatian, coach dog, carriage dog',\n",
      " 252: 'affenpinscher, monkey pinscher, monkey dog',\n",
      " 253: 'basenji',\n",
      " 254: 'pug, pug-dog',\n",
      " 255: 'Leonberg',\n",
      " 256: 'Newfoundland, Newfoundland dog',\n",
      " 257: 'Great Pyrenees',\n",
      " 258: 'Samoyed, Samoyede',\n",
      " 259: 'Pomeranian',\n",
      " 260: 'chow, chow chow',\n",
      " 261: 'keeshond',\n",
      " 262: 'Brabancon griffon',\n",
      " 263: 'Pembroke, Pembroke Welsh corgi',\n",
      " 264: 'Cardigan, Cardigan Welsh corgi',\n",
      " 265: 'toy poodle',\n",
      " 266: 'miniature poodle',\n",
      " 267: 'standard poodle',\n",
      " 268: 'Mexican hairless',\n",
      " 269: 'timber wolf, grey wolf, gray wolf, Canis lupus',\n",
      " 270: 'white wolf, Arctic wolf, Canis lupus tundrarum',\n",
      " 271: 'red wolf, maned wolf, Canis rufus, Canis niger',\n",
      " 272: 'coyote, prairie wolf, brush wolf, Canis latrans',\n",
      " 273: 'dingo, warrigal, warragal, Canis dingo',\n",
      " 274: 'dhole, Cuon alpinus',\n",
      " 275: 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus',\n",
      " 276: 'hyena, hyaena',\n",
      " 277: 'red fox, Vulpes vulpes',\n",
      " 278: 'kit fox, Vulpes macrotis',\n",
      " 279: 'Arctic fox, white fox, Alopex lagopus',\n",
      " 280: 'grey fox, gray fox, Urocyon cinereoargenteus',\n",
      " 281: 'tabby, tabby cat',\n",
      " 282: 'tiger cat',\n",
      " 283: 'Persian cat',\n",
      " 284: 'Siamese cat, Siamese',\n",
      " 285: 'Egyptian cat',\n",
      " 286: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n",
      " 287: 'lynx, catamount',\n",
      " 288: 'leopard, Panthera pardus',\n",
      " 289: 'snow leopard, ounce, Panthera uncia',\n",
      " 290: 'jaguar, panther, Panthera onca, Felis onca',\n",
      " 291: 'lion, king of beasts, Panthera leo',\n",
      " 292: 'tiger, Panthera tigris',\n",
      " 293: 'cheetah, chetah, Acinonyx jubatus',\n",
      " 294: 'brown bear, bruin, Ursus arctos',\n",
      " 295: 'American black bear, black bear, Ursus americanus, Euarctos americanus',\n",
      " 296: 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus',\n",
      " 297: 'sloth bear, Melursus ursinus, Ursus ursinus',\n",
      " 298: 'mongoose',\n",
      " 299: 'meerkat, mierkat',\n",
      " 300: 'tiger beetle',\n",
      " 301: 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle',\n",
      " 302: 'ground beetle, carabid beetle',\n",
      " 303: 'long-horned beetle, longicorn, longicorn beetle',\n",
      " 304: 'leaf beetle, chrysomelid',\n",
      " 305: 'dung beetle',\n",
      " 306: 'rhinoceros beetle',\n",
      " 307: 'weevil',\n",
      " 308: 'fly',\n",
      " 309: 'bee',\n",
      " 310: 'ant, emmet, pismire',\n",
      " 311: 'grasshopper, hopper',\n",
      " 312: 'cricket',\n",
      " 313: 'walking stick, walkingstick, stick insect',\n",
      " 314: 'cockroach, roach',\n",
      " 315: 'mantis, mantid',\n",
      " 316: 'cicada, cicala',\n",
      " 317: 'leafhopper',\n",
      " 318: 'lacewing, lacewing fly',\n",
      " 319: \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\",\n",
      " 320: 'damselfly',\n",
      " 321: 'admiral',\n",
      " 322: 'ringlet, ringlet butterfly',\n",
      " 323: 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus',\n",
      " 324: 'cabbage butterfly',\n",
      " 325: 'sulphur butterfly, sulfur butterfly',\n",
      " 326: 'lycaenid, lycaenid butterfly',\n",
      " 327: 'starfish, sea star',\n",
      " 328: 'sea urchin',\n",
      " 329: 'sea cucumber, holothurian',\n",
      " 330: 'wood rabbit, cottontail, cottontail rabbit',\n",
      " 331: 'hare',\n",
      " 332: 'Angora, Angora rabbit',\n",
      " 333: 'hamster',\n",
      " 334: 'porcupine, hedgehog',\n",
      " 335: 'fox squirrel, eastern fox squirrel, Sciurus niger',\n",
      " 336: 'marmot',\n",
      " 337: 'beaver',\n",
      " 338: 'guinea pig, Cavia cobaya',\n",
      " 339: 'sorrel',\n",
      " 340: 'zebra',\n",
      " 341: 'hog, pig, grunter, squealer, Sus scrofa',\n",
      " 342: 'wild boar, boar, Sus scrofa',\n",
      " 343: 'warthog',\n",
      " 344: 'hippopotamus, hippo, river horse, Hippopotamus amphibius',\n",
      " 345: 'ox',\n",
      " 346: 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis',\n",
      " 347: 'bison',\n",
      " 348: 'ram, tup',\n",
      " 349: 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis',\n",
      " 350: 'ibex, Capra ibex',\n",
      " 351: 'hartebeest',\n",
      " 352: 'impala, Aepyceros melampus',\n",
      " 353: 'gazelle',\n",
      " 354: 'Arabian camel, dromedary, Camelus dromedarius',\n",
      " 355: 'llama',\n",
      " 356: 'weasel',\n",
      " 357: 'mink',\n",
      " 358: 'polecat, fitch, foulmart, foumart, Mustela putorius',\n",
      " 359: 'black-footed ferret, ferret, Mustela nigripes',\n",
      " 360: 'otter',\n",
      " 361: 'skunk, polecat, wood pussy',\n",
      " 362: 'badger',\n",
      " 363: 'armadillo',\n",
      " 364: 'three-toed sloth, ai, Bradypus tridactylus',\n",
      " 365: 'orangutan, orang, orangutang, Pongo pygmaeus',\n",
      " 366: 'gorilla, Gorilla gorilla',\n",
      " 367: 'chimpanzee, chimp, Pan troglodytes',\n",
      " 368: 'gibbon, Hylobates lar',\n",
      " 369: 'siamang, Hylobates syndactylus, Symphalangus syndactylus',\n",
      " 370: 'guenon, guenon monkey',\n",
      " 371: 'patas, hussar monkey, Erythrocebus patas',\n",
      " 372: 'baboon',\n",
      " 373: 'macaque',\n",
      " 374: 'langur',\n",
      " 375: 'colobus, colobus monkey',\n",
      " 376: 'proboscis monkey, Nasalis larvatus',\n",
      " 377: 'marmoset',\n",
      " 378: 'capuchin, ringtail, Cebus capucinus',\n",
      " 379: 'howler monkey, howler',\n",
      " 380: 'titi, titi monkey',\n",
      " 381: 'spider monkey, Ateles geoffroyi',\n",
      " 382: 'squirrel monkey, Saimiri sciureus',\n",
      " 383: 'Madagascar cat, ring-tailed lemur, Lemur catta',\n",
      " 384: 'indri, indris, Indri indri, Indri brevicaudatus',\n",
      " 385: 'Indian elephant, Elephas maximus',\n",
      " 386: 'African elephant, Loxodonta africana',\n",
      " 387: 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens',\n",
      " 388: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca',\n",
      " 389: 'barracouta, snoek',\n",
      " 390: 'eel',\n",
      " 391: 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch',\n",
      " 392: 'rock beauty, Holocanthus tricolor',\n",
      " 393: 'anemone fish',\n",
      " 394: 'sturgeon',\n",
      " 395: 'gar, garfish, garpike, billfish, Lepisosteus osseus',\n",
      " 396: 'lionfish',\n",
      " 397: 'puffer, pufferfish, blowfish, globefish',\n",
      " 398: 'abacus',\n",
      " 399: 'abaya',\n",
      " 400: \"academic gown, academic robe, judge's robe\",\n",
      " 401: 'accordion, piano accordion, squeeze box',\n",
      " 402: 'acoustic guitar',\n",
      " 403: 'aircraft carrier, carrier, flattop, attack aircraft carrier',\n",
      " 404: 'airliner',\n",
      " 405: 'airship, dirigible',\n",
      " 406: 'altar',\n",
      " 407: 'ambulance',\n",
      " 408: 'amphibian, amphibious vehicle',\n",
      " 409: 'analog clock',\n",
      " 410: 'apiary, bee house',\n",
      " 411: 'apron',\n",
      " 412: 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin',\n",
      " 413: 'assault rifle, assault gun',\n",
      " 414: 'backpack, back pack, knapsack, packsack, rucksack, haversack',\n",
      " 415: 'bakery, bakeshop, bakehouse',\n",
      " 416: 'balance beam, beam',\n",
      " 417: 'balloon',\n",
      " 418: 'ballpoint, ballpoint pen, ballpen, Biro',\n",
      " 419: 'Band Aid',\n",
      " 420: 'banjo',\n",
      " 421: 'bannister, banister, balustrade, balusters, handrail',\n",
      " 422: 'barbell',\n",
      " 423: 'barber chair',\n",
      " 424: 'barbershop',\n",
      " 425: 'barn',\n",
      " 426: 'barometer',\n",
      " 427: 'barrel, cask',\n",
      " 428: 'barrow, garden cart, lawn cart, wheelbarrow',\n",
      " 429: 'baseball',\n",
      " 430: 'basketball',\n",
      " 431: 'bassinet',\n",
      " 432: 'bassoon',\n",
      " 433: 'bathing cap, swimming cap',\n",
      " 434: 'bath towel',\n",
      " 435: 'bathtub, bathing tub, bath, tub',\n",
      " 436: 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon',\n",
      " 437: 'beacon, lighthouse, beacon light, pharos',\n",
      " 438: 'beaker',\n",
      " 439: 'bearskin, busby, shako',\n",
      " 440: 'beer bottle',\n",
      " 441: 'beer glass',\n",
      " 442: 'bell cote, bell cot',\n",
      " 443: 'bib',\n",
      " 444: 'bicycle-built-for-two, tandem bicycle, tandem',\n",
      " 445: 'bikini, two-piece',\n",
      " 446: 'binder, ring-binder',\n",
      " 447: 'binoculars, field glasses, opera glasses',\n",
      " 448: 'birdhouse',\n",
      " 449: 'boathouse',\n",
      " 450: 'bobsled, bobsleigh, bob',\n",
      " 451: 'bolo tie, bolo, bola tie, bola',\n",
      " 452: 'bonnet, poke bonnet',\n",
      " 453: 'bookcase',\n",
      " 454: 'bookshop, bookstore, bookstall',\n",
      " 455: 'bottlecap',\n",
      " 456: 'bow',\n",
      " 457: 'bow tie, bow-tie, bowtie',\n",
      " 458: 'brass, memorial tablet, plaque',\n",
      " 459: 'brassiere, bra, bandeau',\n",
      " 460: 'breakwater, groin, groyne, mole, bulwark, seawall, jetty',\n",
      " 461: 'breastplate, aegis, egis',\n",
      " 462: 'broom',\n",
      " 463: 'bucket, pail',\n",
      " 464: 'buckle',\n",
      " 465: 'bulletproof vest',\n",
      " 466: 'bullet train, bullet',\n",
      " 467: 'butcher shop, meat market',\n",
      " 468: 'cab, hack, taxi, taxicab',\n",
      " 469: 'caldron, cauldron',\n",
      " 470: 'candle, taper, wax light',\n",
      " 471: 'cannon',\n",
      " 472: 'canoe',\n",
      " 473: 'can opener, tin opener',\n",
      " 474: 'cardigan',\n",
      " 475: 'car mirror',\n",
      " 476: 'carousel, carrousel, merry-go-round, roundabout, whirligig',\n",
      " 477: \"carpenter's kit, tool kit\",\n",
      " 478: 'carton',\n",
      " 479: 'car wheel',\n",
      " 480: 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM',\n",
      " 481: 'cassette',\n",
      " 482: 'cassette player',\n",
      " 483: 'castle',\n",
      " 484: 'catamaran',\n",
      " 485: 'CD player',\n",
      " 486: 'cello, violoncello',\n",
      " 487: 'cellular telephone, cellular phone, cellphone, cell, mobile phone',\n",
      " 488: 'chain',\n",
      " 489: 'chainlink fence',\n",
      " 490: 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour',\n",
      " 491: 'chain saw, chainsaw',\n",
      " 492: 'chest',\n",
      " 493: 'chiffonier, commode',\n",
      " 494: 'chime, bell, gong',\n",
      " 495: 'china cabinet, china closet',\n",
      " 496: 'Christmas stocking',\n",
      " 497: 'church, church building',\n",
      " 498: 'cinema, movie theater, movie theatre, movie house, picture palace',\n",
      " 499: 'cleaver, meat cleaver, chopper',\n",
      " 500: 'cliff dwelling',\n",
      " 501: 'cloak',\n",
      " 502: 'clog, geta, patten, sabot',\n",
      " 503: 'cocktail shaker',\n",
      " 504: 'coffee mug',\n",
      " 505: 'coffeepot',\n",
      " 506: 'coil, spiral, volute, whorl, helix',\n",
      " 507: 'combination lock',\n",
      " 508: 'computer keyboard, keypad',\n",
      " 509: 'confectionery, confectionary, candy store',\n",
      " 510: 'container ship, containership, container vessel',\n",
      " 511: 'convertible',\n",
      " 512: 'corkscrew, bottle screw',\n",
      " 513: 'cornet, horn, trumpet, trump',\n",
      " 514: 'cowboy boot',\n",
      " 515: 'cowboy hat, ten-gallon hat',\n",
      " 516: 'cradle',\n",
      " 517: 'crane',\n",
      " 518: 'crash helmet',\n",
      " 519: 'crate',\n",
      " 520: 'crib, cot',\n",
      " 521: 'Crock Pot',\n",
      " 522: 'croquet ball',\n",
      " 523: 'crutch',\n",
      " 524: 'cuirass',\n",
      " 525: 'dam, dike, dyke',\n",
      " 526: 'desk',\n",
      " 527: 'desktop computer',\n",
      " 528: 'dial telephone, dial phone',\n",
      " 529: 'diaper, nappy, napkin',\n",
      " 530: 'digital clock',\n",
      " 531: 'digital watch',\n",
      " 532: 'dining table, board',\n",
      " 533: 'dishrag, dishcloth',\n",
      " 534: 'dishwasher, dish washer, dishwashing machine',\n",
      " 535: 'disk brake, disc brake',\n",
      " 536: 'dock, dockage, docking facility',\n",
      " 537: 'dogsled, dog sled, dog sleigh',\n",
      " 538: 'dome',\n",
      " 539: 'doormat, welcome mat',\n",
      " 540: 'drilling platform, offshore rig',\n",
      " 541: 'drum, membranophone, tympan',\n",
      " 542: 'drumstick',\n",
      " 543: 'dumbbell',\n",
      " 544: 'Dutch oven',\n",
      " 545: 'electric fan, blower',\n",
      " 546: 'electric guitar',\n",
      " 547: 'electric locomotive',\n",
      " 548: 'entertainment center',\n",
      " 549: 'envelope',\n",
      " 550: 'espresso maker',\n",
      " 551: 'face powder',\n",
      " 552: 'feather boa, boa',\n",
      " 553: 'file, file cabinet, filing cabinet',\n",
      " 554: 'fireboat',\n",
      " 555: 'fire engine, fire truck',\n",
      " 556: 'fire screen, fireguard',\n",
      " 557: 'flagpole, flagstaff',\n",
      " 558: 'flute, transverse flute',\n",
      " 559: 'folding chair',\n",
      " 560: 'football helmet',\n",
      " 561: 'forklift',\n",
      " 562: 'fountain',\n",
      " 563: 'fountain pen',\n",
      " 564: 'four-poster',\n",
      " 565: 'freight car',\n",
      " 566: 'French horn, horn',\n",
      " 567: 'frying pan, frypan, skillet',\n",
      " 568: 'fur coat',\n",
      " 569: 'garbage truck, dustcart',\n",
      " 570: 'gasmask, respirator, gas helmet',\n",
      " 571: 'gas pump, gasoline pump, petrol pump, island dispenser',\n",
      " 572: 'goblet',\n",
      " 573: 'go-kart',\n",
      " 574: 'golf ball',\n",
      " 575: 'golfcart, golf cart',\n",
      " 576: 'gondola',\n",
      " 577: 'gong, tam-tam',\n",
      " 578: 'gown',\n",
      " 579: 'grand piano, grand',\n",
      " 580: 'greenhouse, nursery, glasshouse',\n",
      " 581: 'grille, radiator grille',\n",
      " 582: 'grocery store, grocery, food market, market',\n",
      " 583: 'guillotine',\n",
      " 584: 'hair slide',\n",
      " 585: 'hair spray',\n",
      " 586: 'half track',\n",
      " 587: 'hammer',\n",
      " 588: 'hamper',\n",
      " 589: 'hand blower, blow dryer, blow drier, hair dryer, hair drier',\n",
      " 590: 'hand-held computer, hand-held microcomputer',\n",
      " 591: 'handkerchief, hankie, hanky, hankey',\n",
      " 592: 'hard disc, hard disk, fixed disk',\n",
      " 593: 'harmonica, mouth organ, harp, mouth harp',\n",
      " 594: 'harp',\n",
      " 595: 'harvester, reaper',\n",
      " 596: 'hatchet',\n",
      " 597: 'holster',\n",
      " 598: 'home theater, home theatre',\n",
      " 599: 'honeycomb',\n",
      " 600: 'hook, claw',\n",
      " 601: 'hoopskirt, crinoline',\n",
      " 602: 'horizontal bar, high bar',\n",
      " 603: 'horse cart, horse-cart',\n",
      " 604: 'hourglass',\n",
      " 605: 'iPod',\n",
      " 606: 'iron, smoothing iron',\n",
      " 607: \"jack-o'-lantern\",\n",
      " 608: 'jean, blue jean, denim',\n",
      " 609: 'jeep, landrover',\n",
      " 610: 'jersey, T-shirt, tee shirt',\n",
      " 611: 'jigsaw puzzle',\n",
      " 612: 'jinrikisha, ricksha, rickshaw',\n",
      " 613: 'joystick',\n",
      " 614: 'kimono',\n",
      " 615: 'knee pad',\n",
      " 616: 'knot',\n",
      " 617: 'lab coat, laboratory coat',\n",
      " 618: 'ladle',\n",
      " 619: 'lampshade, lamp shade',\n",
      " 620: 'laptop, laptop computer',\n",
      " 621: 'lawn mower, mower',\n",
      " 622: 'lens cap, lens cover',\n",
      " 623: 'letter opener, paper knife, paperknife',\n",
      " 624: 'library',\n",
      " 625: 'lifeboat',\n",
      " 626: 'lighter, light, igniter, ignitor',\n",
      " 627: 'limousine, limo',\n",
      " 628: 'liner, ocean liner',\n",
      " 629: 'lipstick, lip rouge',\n",
      " 630: 'Loafer',\n",
      " 631: 'lotion',\n",
      " 632: 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system',\n",
      " 633: \"loupe, jeweler's loupe\",\n",
      " 634: 'lumbermill, sawmill',\n",
      " 635: 'magnetic compass',\n",
      " 636: 'mailbag, postbag',\n",
      " 637: 'mailbox, letter box',\n",
      " 638: 'maillot',\n",
      " 639: 'maillot, tank suit',\n",
      " 640: 'manhole cover',\n",
      " 641: 'maraca',\n",
      " 642: 'marimba, xylophone',\n",
      " 643: 'mask',\n",
      " 644: 'matchstick',\n",
      " 645: 'maypole',\n",
      " 646: 'maze, labyrinth',\n",
      " 647: 'measuring cup',\n",
      " 648: 'medicine chest, medicine cabinet',\n",
      " 649: 'megalith, megalithic structure',\n",
      " 650: 'microphone, mike',\n",
      " 651: 'microwave, microwave oven',\n",
      " 652: 'military uniform',\n",
      " 653: 'milk can',\n",
      " 654: 'minibus',\n",
      " 655: 'miniskirt, mini',\n",
      " 656: 'minivan',\n",
      " 657: 'missile',\n",
      " 658: 'mitten',\n",
      " 659: 'mixing bowl',\n",
      " 660: 'mobile home, manufactured home',\n",
      " 661: 'Model T',\n",
      " 662: 'modem',\n",
      " 663: 'monastery',\n",
      " 664: 'monitor',\n",
      " 665: 'moped',\n",
      " 666: 'mortar',\n",
      " 667: 'mortarboard',\n",
      " 668: 'mosque',\n",
      " 669: 'mosquito net',\n",
      " 670: 'motor scooter, scooter',\n",
      " 671: 'mountain bike, all-terrain bike, off-roader',\n",
      " 672: 'mountain tent',\n",
      " 673: 'mouse, computer mouse',\n",
      " 674: 'mousetrap',\n",
      " 675: 'moving van',\n",
      " 676: 'muzzle',\n",
      " 677: 'nail',\n",
      " 678: 'neck brace',\n",
      " 679: 'necklace',\n",
      " 680: 'nipple',\n",
      " 681: 'notebook, notebook computer',\n",
      " 682: 'obelisk',\n",
      " 683: 'oboe, hautboy, hautbois',\n",
      " 684: 'ocarina, sweet potato',\n",
      " 685: 'odometer, hodometer, mileometer, milometer',\n",
      " 686: 'oil filter',\n",
      " 687: 'organ, pipe organ',\n",
      " 688: 'oscilloscope, scope, cathode-ray oscilloscope, CRO',\n",
      " 689: 'overskirt',\n",
      " 690: 'oxcart',\n",
      " 691: 'oxygen mask',\n",
      " 692: 'packet',\n",
      " 693: 'paddle, boat paddle',\n",
      " 694: 'paddlewheel, paddle wheel',\n",
      " 695: 'padlock',\n",
      " 696: 'paintbrush',\n",
      " 697: \"pajama, pyjama, pj's, jammies\",\n",
      " 698: 'palace',\n",
      " 699: 'panpipe, pandean pipe, syrinx',\n",
      " 700: 'paper towel',\n",
      " 701: 'parachute, chute',\n",
      " 702: 'parallel bars, bars',\n",
      " 703: 'park bench',\n",
      " 704: 'parking meter',\n",
      " 705: 'passenger car, coach, carriage',\n",
      " 706: 'patio, terrace',\n",
      " 707: 'pay-phone, pay-station',\n",
      " 708: 'pedestal, plinth, footstall',\n",
      " 709: 'pencil box, pencil case',\n",
      " 710: 'pencil sharpener',\n",
      " 711: 'perfume, essence',\n",
      " 712: 'Petri dish',\n",
      " 713: 'photocopier',\n",
      " 714: 'pick, plectrum, plectron',\n",
      " 715: 'pickelhaube',\n",
      " 716: 'picket fence, paling',\n",
      " 717: 'pickup, pickup truck',\n",
      " 718: 'pier',\n",
      " 719: 'piggy bank, penny bank',\n",
      " 720: 'pill bottle',\n",
      " 721: 'pillow',\n",
      " 722: 'ping-pong ball',\n",
      " 723: 'pinwheel',\n",
      " 724: 'pirate, pirate ship',\n",
      " 725: 'pitcher, ewer',\n",
      " 726: \"plane, carpenter's plane, woodworking plane\",\n",
      " 727: 'planetarium',\n",
      " 728: 'plastic bag',\n",
      " 729: 'plate rack',\n",
      " 730: 'plow, plough',\n",
      " 731: \"plunger, plumber's helper\",\n",
      " 732: 'Polaroid camera, Polaroid Land camera',\n",
      " 733: 'pole',\n",
      " 734: 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria',\n",
      " 735: 'poncho',\n",
      " 736: 'pool table, billiard table, snooker table',\n",
      " 737: 'pop bottle, soda bottle',\n",
      " 738: 'pot, flowerpot',\n",
      " 739: \"potter's wheel\",\n",
      " 740: 'power drill',\n",
      " 741: 'prayer rug, prayer mat',\n",
      " 742: 'printer',\n",
      " 743: 'prison, prison house',\n",
      " 744: 'projectile, missile',\n",
      " 745: 'projector',\n",
      " 746: 'puck, hockey puck',\n",
      " 747: 'punching bag, punch bag, punching ball, punchball',\n",
      " 748: 'purse',\n",
      " 749: 'quill, quill pen',\n",
      " 750: 'quilt, comforter, comfort, puff',\n",
      " 751: 'racer, race car, racing car',\n",
      " 752: 'racket, racquet',\n",
      " 753: 'radiator',\n",
      " 754: 'radio, wireless',\n",
      " 755: 'radio telescope, radio reflector',\n",
      " 756: 'rain barrel',\n",
      " 757: 'recreational vehicle, RV, R.V.',\n",
      " 758: 'reel',\n",
      " 759: 'reflex camera',\n",
      " 760: 'refrigerator, icebox',\n",
      " 761: 'remote control, remote',\n",
      " 762: 'restaurant, eating house, eating place, eatery',\n",
      " 763: 'revolver, six-gun, six-shooter',\n",
      " 764: 'rifle',\n",
      " 765: 'rocking chair, rocker',\n",
      " 766: 'rotisserie',\n",
      " 767: 'rubber eraser, rubber, pencil eraser',\n",
      " 768: 'rugby ball',\n",
      " 769: 'rule, ruler',\n",
      " 770: 'running shoe',\n",
      " 771: 'safe',\n",
      " 772: 'safety pin',\n",
      " 773: 'saltshaker, salt shaker',\n",
      " 774: 'sandal',\n",
      " 775: 'sarong',\n",
      " 776: 'sax, saxophone',\n",
      " 777: 'scabbard',\n",
      " 778: 'scale, weighing machine',\n",
      " 779: 'school bus',\n",
      " 780: 'schooner',\n",
      " 781: 'scoreboard',\n",
      " 782: 'screen, CRT screen',\n",
      " 783: 'screw',\n",
      " 784: 'screwdriver',\n",
      " 785: 'seat belt, seatbelt',\n",
      " 786: 'sewing machine',\n",
      " 787: 'shield, buckler',\n",
      " 788: 'shoe shop, shoe-shop, shoe store',\n",
      " 789: 'shoji',\n",
      " 790: 'shopping basket',\n",
      " 791: 'shopping cart',\n",
      " 792: 'shovel',\n",
      " 793: 'shower cap',\n",
      " 794: 'shower curtain',\n",
      " 795: 'ski',\n",
      " 796: 'ski mask',\n",
      " 797: 'sleeping bag',\n",
      " 798: 'slide rule, slipstick',\n",
      " 799: 'sliding door',\n",
      " 800: 'slot, one-armed bandit',\n",
      " 801: 'snorkel',\n",
      " 802: 'snowmobile',\n",
      " 803: 'snowplow, snowplough',\n",
      " 804: 'soap dispenser',\n",
      " 805: 'soccer ball',\n",
      " 806: 'sock',\n",
      " 807: 'solar dish, solar collector, solar furnace',\n",
      " 808: 'sombrero',\n",
      " 809: 'soup bowl',\n",
      " 810: 'space bar',\n",
      " 811: 'space heater',\n",
      " 812: 'space shuttle',\n",
      " 813: 'spatula',\n",
      " 814: 'speedboat',\n",
      " 815: \"spider web, spider's web\",\n",
      " 816: 'spindle',\n",
      " 817: 'sports car, sport car',\n",
      " 818: 'spotlight, spot',\n",
      " 819: 'stage',\n",
      " 820: 'steam locomotive',\n",
      " 821: 'steel arch bridge',\n",
      " 822: 'steel drum',\n",
      " 823: 'stethoscope',\n",
      " 824: 'stole',\n",
      " 825: 'stone wall',\n",
      " 826: 'stopwatch, stop watch',\n",
      " 827: 'stove',\n",
      " 828: 'strainer',\n",
      " 829: 'streetcar, tram, tramcar, trolley, trolley car',\n",
      " 830: 'stretcher',\n",
      " 831: 'studio couch, day bed',\n",
      " 832: 'stupa, tope',\n",
      " 833: 'submarine, pigboat, sub, U-boat',\n",
      " 834: 'suit, suit of clothes',\n",
      " 835: 'sundial',\n",
      " 836: 'sunglass',\n",
      " 837: 'sunglasses, dark glasses, shades',\n",
      " 838: 'sunscreen, sunblock, sun blocker',\n",
      " 839: 'suspension bridge',\n",
      " 840: 'swab, swob, mop',\n",
      " 841: 'sweatshirt',\n",
      " 842: 'swimming trunks, bathing trunks',\n",
      " 843: 'swing',\n",
      " 844: 'switch, electric switch, electrical switch',\n",
      " 845: 'syringe',\n",
      " 846: 'table lamp',\n",
      " 847: 'tank, army tank, armored combat vehicle, armoured combat vehicle',\n",
      " 848: 'tape player',\n",
      " 849: 'teapot',\n",
      " 850: 'teddy, teddy bear',\n",
      " 851: 'television, television system',\n",
      " 852: 'tennis ball',\n",
      " 853: 'thatch, thatched roof',\n",
      " 854: 'theater curtain, theatre curtain',\n",
      " 855: 'thimble',\n",
      " 856: 'thresher, thrasher, threshing machine',\n",
      " 857: 'throne',\n",
      " 858: 'tile roof',\n",
      " 859: 'toaster',\n",
      " 860: 'tobacco shop, tobacconist shop, tobacconist',\n",
      " 861: 'toilet seat',\n",
      " 862: 'torch',\n",
      " 863: 'totem pole',\n",
      " 864: 'tow truck, tow car, wrecker',\n",
      " 865: 'toyshop',\n",
      " 866: 'tractor',\n",
      " 867: 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi',\n",
      " 868: 'tray',\n",
      " 869: 'trench coat',\n",
      " 870: 'tricycle, trike, velocipede',\n",
      " 871: 'trimaran',\n",
      " 872: 'tripod',\n",
      " 873: 'triumphal arch',\n",
      " 874: 'trolleybus, trolley coach, trackless trolley',\n",
      " 875: 'trombone',\n",
      " 876: 'tub, vat',\n",
      " 877: 'turnstile',\n",
      " 878: 'typewriter keyboard',\n",
      " 879: 'umbrella',\n",
      " 880: 'unicycle, monocycle',\n",
      " 881: 'upright, upright piano',\n",
      " 882: 'vacuum, vacuum cleaner',\n",
      " 883: 'vase',\n",
      " 884: 'vault',\n",
      " 885: 'velvet',\n",
      " 886: 'vending machine',\n",
      " 887: 'vestment',\n",
      " 888: 'viaduct',\n",
      " 889: 'violin, fiddle',\n",
      " 890: 'volleyball',\n",
      " 891: 'waffle iron',\n",
      " 892: 'wall clock',\n",
      " 893: 'wallet, billfold, notecase, pocketbook',\n",
      " 894: 'wardrobe, closet, press',\n",
      " 895: 'warplane, military plane',\n",
      " 896: 'washbasin, handbasin, washbowl, lavabo, wash-hand basin',\n",
      " 897: 'washer, automatic washer, washing machine',\n",
      " 898: 'water bottle',\n",
      " 899: 'water jug',\n",
      " 900: 'water tower',\n",
      " 901: 'whiskey jug',\n",
      " 902: 'whistle',\n",
      " 903: 'wig',\n",
      " 904: 'window screen',\n",
      " 905: 'window shade',\n",
      " 906: 'Windsor tie',\n",
      " 907: 'wine bottle',\n",
      " 908: 'wing',\n",
      " 909: 'wok',\n",
      " 910: 'wooden spoon',\n",
      " 911: 'wool, woolen, woollen',\n",
      " 912: 'worm fence, snake fence, snake-rail fence, Virginia fence',\n",
      " 913: 'wreck',\n",
      " 914: 'yawl',\n",
      " 915: 'yurt',\n",
      " 916: 'web site, website, internet site, site',\n",
      " 917: 'comic book',\n",
      " 918: 'crossword puzzle, crossword',\n",
      " 919: 'street sign',\n",
      " 920: 'traffic light, traffic signal, stoplight',\n",
      " 921: 'book jacket, dust cover, dust jacket, dust wrapper',\n",
      " 922: 'menu',\n",
      " 923: 'plate',\n",
      " 924: 'guacamole',\n",
      " 925: 'consomme',\n",
      " 926: 'hot pot, hotpot',\n",
      " 927: 'trifle',\n",
      " 928: 'ice cream, icecream',\n",
      " 929: 'ice lolly, lolly, lollipop, popsicle',\n",
      " 930: 'French loaf',\n",
      " 931: 'bagel, beigel',\n",
      " 932: 'pretzel',\n",
      " 933: 'cheeseburger',\n",
      " 934: 'hotdog, hot dog, red hot',\n",
      " 935: 'mashed potato',\n",
      " 936: 'head cabbage',\n",
      " 937: 'broccoli',\n",
      " 938: 'cauliflower',\n",
      " 939: 'zucchini, courgette',\n",
      " 940: 'spaghetti squash',\n",
      " 941: 'acorn squash',\n",
      " 942: 'butternut squash',\n",
      " 943: 'cucumber, cuke',\n",
      " 944: 'artichoke, globe artichoke',\n",
      " 945: 'bell pepper',\n",
      " 946: 'cardoon',\n",
      " 947: 'mushroom',\n",
      " 948: 'Granny Smith',\n",
      " 949: 'strawberry',\n",
      " 950: 'orange',\n",
      " 951: 'lemon',\n",
      " 952: 'fig',\n",
      " 953: 'pineapple, ananas',\n",
      " 954: 'banana',\n",
      " 955: 'jackfruit, jak, jack',\n",
      " 956: 'custard apple',\n",
      " 957: 'pomegranate',\n",
      " 958: 'hay',\n",
      " 959: 'carbonara',\n",
      " 960: 'chocolate sauce, chocolate syrup',\n",
      " 961: 'dough',\n",
      " 962: 'meat loaf, meatloaf',\n",
      " 963: 'pizza, pizza pie',\n",
      " 964: 'potpie',\n",
      " 965: 'burrito',\n",
      " 966: 'red wine',\n",
      " 967: 'espresso',\n",
      " 968: 'cup',\n",
      " 969: 'eggnog',\n",
      " 970: 'alp',\n",
      " 971: 'bubble',\n",
      " 972: 'cliff, drop, drop-off',\n",
      " 973: 'coral reef',\n",
      " 974: 'geyser',\n",
      " 975: 'lakeside, lakeshore',\n",
      " 976: 'promontory, headland, head, foreland',\n",
      " 977: 'sandbar, sand bar',\n",
      " 978: 'seashore, coast, seacoast, sea-coast',\n",
      " 979: 'valley, vale',\n",
      " 980: 'volcano',\n",
      " 981: 'ballplayer, baseball player',\n",
      " 982: 'groom, bridegroom',\n",
      " 983: 'scuba diver',\n",
      " 984: 'rapeseed',\n",
      " 985: 'daisy',\n",
      " 986: \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\",\n",
      " 987: 'corn',\n",
      " 988: 'acorn',\n",
      " 989: 'hip, rose hip, rosehip',\n",
      " 990: 'buckeye, horse chestnut, conker',\n",
      " 991: 'coral fungus',\n",
      " 992: 'agaric',\n",
      " 993: 'gyromitra',\n",
      " 994: 'stinkhorn, carrion fungus',\n",
      " 995: 'earthstar',\n",
      " 996: 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa',\n",
      " 997: 'bolete',\n",
      " 998: 'ear, spike, capitulum',\n",
      " 999: 'toilet tissue, toilet paper, bathroom tissue'}\n",
      "71/3:\n",
      "from tensorflow.keras.applications import EfficientNetB4\n",
      "import tensorflow as tf\n",
      "model = EfficientNetB4(weights='imagenet')\n",
      "\n",
      "def center_crop(img, new_width=None, new_height=None):        \n",
      "\n",
      "    width = img.shape[1]\n",
      "    height = img.shape[0]\n",
      "\n",
      "    if new_width is None:\n",
      "        new_width = min(width, height)\n",
      "\n",
      "    if new_height is None:\n",
      "        new_height = min(width, height)\n",
      "\n",
      "    left = int(np.ceil((width - new_width) / 2))\n",
      "    right = width - int(np.floor((width - new_width) / 2))\n",
      "\n",
      "    top = int(np.ceil((height - new_height) / 2))\n",
      "    bottom = height - int(np.floor((height - new_height) / 2))\n",
      "\n",
      "    if len(img.shape) == 2:\n",
      "        center_cropped_img = img[top:bottom, left:right]\n",
      "    else:\n",
      "        center_cropped_img = img[top:bottom, left:right, ...]\n",
      "\n",
      "    return center_cropped_img\n",
      "\n",
      "class_dict = {0: 'tench, Tinca tinca',\n",
      " 1: 'goldfish, Carassius auratus',\n",
      " 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n",
      " 3: 'tiger shark, Galeocerdo cuvieri',\n",
      " 4: 'hammerhead, hammerhead shark',\n",
      " 5: 'electric ray, crampfish, numbfish, torpedo',\n",
      " 6: 'stingray',\n",
      " 7: 'cock',\n",
      " 8: 'hen',\n",
      " 9: 'ostrich, Struthio camelus',\n",
      " 10: 'brambling, Fringilla montifringilla',\n",
      " 11: 'goldfinch, Carduelis carduelis',\n",
      " 12: 'house finch, linnet, Carpodacus mexicanus',\n",
      " 13: 'junco, snowbird',\n",
      " 14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea',\n",
      " 15: 'robin, American robin, Turdus migratorius',\n",
      " 16: 'bulbul',\n",
      " 17: 'jay',\n",
      " 18: 'magpie',\n",
      " 19: 'chickadee',\n",
      " 20: 'water ouzel, dipper',\n",
      " 21: 'kite',\n",
      " 22: 'bald eagle, American eagle, Haliaeetus leucocephalus',\n",
      " 23: 'vulture',\n",
      " 24: 'great grey owl, great gray owl, Strix nebulosa',\n",
      " 25: 'European fire salamander, Salamandra salamandra',\n",
      " 26: 'common newt, Triturus vulgaris',\n",
      " 27: 'eft',\n",
      " 28: 'spotted salamander, Ambystoma maculatum',\n",
      " 29: 'axolotl, mud puppy, Ambystoma mexicanum',\n",
      " 30: 'bullfrog, Rana catesbeiana',\n",
      " 31: 'tree frog, tree-frog',\n",
      " 32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui',\n",
      " 33: 'loggerhead, loggerhead turtle, Caretta caretta',\n",
      " 34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea',\n",
      " 35: 'mud turtle',\n",
      " 36: 'terrapin',\n",
      " 37: 'box turtle, box tortoise',\n",
      " 38: 'banded gecko',\n",
      " 39: 'common iguana, iguana, Iguana iguana',\n",
      " 40: 'American chameleon, anole, Anolis carolinensis',\n",
      " 41: 'whiptail, whiptail lizard',\n",
      " 42: 'agama',\n",
      " 43: 'frilled lizard, Chlamydosaurus kingi',\n",
      " 44: 'alligator lizard',\n",
      " 45: 'Gila monster, Heloderma suspectum',\n",
      " 46: 'green lizard, Lacerta viridis',\n",
      " 47: 'African chameleon, Chamaeleo chamaeleon',\n",
      " 48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis',\n",
      " 49: 'African crocodile, Nile crocodile, Crocodylus niloticus',\n",
      " 50: 'American alligator, Alligator mississipiensis',\n",
      " 51: 'triceratops',\n",
      " 52: 'thunder snake, worm snake, Carphophis amoenus',\n",
      " 53: 'ringneck snake, ring-necked snake, ring snake',\n",
      " 54: 'hognose snake, puff adder, sand viper',\n",
      " 55: 'green snake, grass snake',\n",
      " 56: 'king snake, kingsnake',\n",
      " 57: 'garter snake, grass snake',\n",
      " 58: 'water snake',\n",
      " 59: 'vine snake',\n",
      " 60: 'night snake, Hypsiglena torquata',\n",
      " 61: 'boa constrictor, Constrictor constrictor',\n",
      " 62: 'rock python, rock snake, Python sebae',\n",
      " 63: 'Indian cobra, Naja naja',\n",
      " 64: 'green mamba',\n",
      " 65: 'sea snake',\n",
      " 66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus',\n",
      " 67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus',\n",
      " 68: 'sidewinder, horned rattlesnake, Crotalus cerastes',\n",
      " 69: 'trilobite',\n",
      " 70: 'harvestman, daddy longlegs, Phalangium opilio',\n",
      " 71: 'scorpion',\n",
      " 72: 'black and gold garden spider, Argiope aurantia',\n",
      " 73: 'barn spider, Araneus cavaticus',\n",
      " 74: 'garden spider, Aranea diademata',\n",
      " 75: 'black widow, Latrodectus mactans',\n",
      " 76: 'tarantula',\n",
      " 77: 'wolf spider, hunting spider',\n",
      " 78: 'tick',\n",
      " 79: 'centipede',\n",
      " 80: 'black grouse',\n",
      " 81: 'ptarmigan',\n",
      " 82: 'ruffed grouse, partridge, Bonasa umbellus',\n",
      " 83: 'prairie chicken, prairie grouse, prairie fowl',\n",
      " 84: 'peacock',\n",
      " 85: 'quail',\n",
      " 86: 'partridge',\n",
      " 87: 'African grey, African gray, Psittacus erithacus',\n",
      " 88: 'macaw',\n",
      " 89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n",
      " 90: 'lorikeet',\n",
      " 91: 'coucal',\n",
      " 92: 'bee eater',\n",
      " 93: 'hornbill',\n",
      " 94: 'hummingbird',\n",
      " 95: 'jacamar',\n",
      " 96: 'toucan',\n",
      " 97: 'drake',\n",
      " 98: 'red-breasted merganser, Mergus serrator',\n",
      " 99: 'goose',\n",
      " 100: 'black swan, Cygnus atratus',\n",
      " 101: 'tusker',\n",
      " 102: 'echidna, spiny anteater, anteater',\n",
      " 103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n",
      " 104: 'wallaby, brush kangaroo',\n",
      " 105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n",
      " 106: 'wombat',\n",
      " 107: 'jellyfish',\n",
      " 108: 'sea anemone, anemone',\n",
      " 109: 'brain coral',\n",
      " 110: 'flatworm, platyhelminth',\n",
      " 111: 'nematode, nematode worm, roundworm',\n",
      " 112: 'conch',\n",
      " 113: 'snail',\n",
      " 114: 'slug',\n",
      " 115: 'sea slug, nudibranch',\n",
      " 116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore',\n",
      " 117: 'chambered nautilus, pearly nautilus, nautilus',\n",
      " 118: 'Dungeness crab, Cancer magister',\n",
      " 119: 'rock crab, Cancer irroratus',\n",
      " 120: 'fiddler crab',\n",
      " 121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica',\n",
      " 122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus',\n",
      " 123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish',\n",
      " 124: 'crayfish, crawfish, crawdad, crawdaddy',\n",
      " 125: 'hermit crab',\n",
      " 126: 'isopod',\n",
      " 127: 'white stork, Ciconia ciconia',\n",
      " 128: 'black stork, Ciconia nigra',\n",
      " 129: 'spoonbill',\n",
      " 130: 'flamingo',\n",
      " 131: 'little blue heron, Egretta caerulea',\n",
      " 132: 'American egret, great white heron, Egretta albus',\n",
      " 133: 'bittern',\n",
      " 134: 'crane',\n",
      " 135: 'limpkin, Aramus pictus',\n",
      " 136: 'European gallinule, Porphyrio porphyrio',\n",
      " 137: 'American coot, marsh hen, mud hen, water hen, Fulica americana',\n",
      " 138: 'bustard',\n",
      " 139: 'ruddy turnstone, Arenaria interpres',\n",
      " 140: 'red-backed sandpiper, dunlin, Erolia alpina',\n",
      " 141: 'redshank, Tringa totanus',\n",
      " 142: 'dowitcher',\n",
      " 143: 'oystercatcher, oyster catcher',\n",
      " 144: 'pelican',\n",
      " 145: 'king penguin, Aptenodytes patagonica',\n",
      " 146: 'albatross, mollymawk',\n",
      " 147: 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus',\n",
      " 148: 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca',\n",
      " 149: 'dugong, Dugong dugon',\n",
      " 150: 'sea lion',\n",
      " 151: 'Chihuahua',\n",
      " 152: 'Japanese spaniel',\n",
      " 153: 'Maltese dog, Maltese terrier, Maltese',\n",
      " 154: 'Pekinese, Pekingese, Peke',\n",
      " 155: 'Shih-Tzu',\n",
      " 156: 'Blenheim spaniel',\n",
      " 157: 'papillon',\n",
      " 158: 'toy terrier',\n",
      " 159: 'Rhodesian ridgeback',\n",
      " 160: 'Afghan hound, Afghan',\n",
      " 161: 'basset, basset hound',\n",
      " 162: 'beagle',\n",
      " 163: 'bloodhound, sleuthhound',\n",
      " 164: 'bluetick',\n",
      " 165: 'black-and-tan coonhound',\n",
      " 166: 'Walker hound, Walker foxhound',\n",
      " 167: 'English foxhound',\n",
      " 168: 'redbone',\n",
      " 169: 'borzoi, Russian wolfhound',\n",
      " 170: 'Irish wolfhound',\n",
      " 171: 'Italian greyhound',\n",
      " 172: 'whippet',\n",
      " 173: 'Ibizan hound, Ibizan Podenco',\n",
      " 174: 'Norwegian elkhound, elkhound',\n",
      " 175: 'otterhound, otter hound',\n",
      " 176: 'Saluki, gazelle hound',\n",
      " 177: 'Scottish deerhound, deerhound',\n",
      " 178: 'Weimaraner',\n",
      " 179: 'Staffordshire bullterrier, Staffordshire bull terrier',\n",
      " 180: 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier',\n",
      " 181: 'Bedlington terrier',\n",
      " 182: 'Border terrier',\n",
      " 183: 'Kerry blue terrier',\n",
      " 184: 'Irish terrier',\n",
      " 185: 'Norfolk terrier',\n",
      " 186: 'Norwich terrier',\n",
      " 187: 'Yorkshire terrier',\n",
      " 188: 'wire-haired fox terrier',\n",
      " 189: 'Lakeland terrier',\n",
      " 190: 'Sealyham terrier, Sealyham',\n",
      " 191: 'Airedale, Airedale terrier',\n",
      " 192: 'cairn, cairn terrier',\n",
      " 193: 'Australian terrier',\n",
      " 194: 'Dandie Dinmont, Dandie Dinmont terrier',\n",
      " 195: 'Boston bull, Boston terrier',\n",
      " 196: 'miniature schnauzer',\n",
      " 197: 'giant schnauzer',\n",
      " 198: 'standard schnauzer',\n",
      " 199: 'Scotch terrier, Scottish terrier, Scottie',\n",
      " 200: 'Tibetan terrier, chrysanthemum dog',\n",
      " 201: 'silky terrier, Sydney silky',\n",
      " 202: 'soft-coated wheaten terrier',\n",
      " 203: 'West Highland white terrier',\n",
      " 204: 'Lhasa, Lhasa apso',\n",
      " 205: 'flat-coated retriever',\n",
      " 206: 'curly-coated retriever',\n",
      " 207: 'golden retriever',\n",
      " 208: 'Labrador retriever',\n",
      " 209: 'Chesapeake Bay retriever',\n",
      " 210: 'German short-haired pointer',\n",
      " 211: 'vizsla, Hungarian pointer',\n",
      " 212: 'English setter',\n",
      " 213: 'Irish setter, red setter',\n",
      " 214: 'Gordon setter',\n",
      " 215: 'Brittany spaniel',\n",
      " 216: 'clumber, clumber spaniel',\n",
      " 217: 'English springer, English springer spaniel',\n",
      " 218: 'Welsh springer spaniel',\n",
      " 219: 'cocker spaniel, English cocker spaniel, cocker',\n",
      " 220: 'Sussex spaniel',\n",
      " 221: 'Irish water spaniel',\n",
      " 222: 'kuvasz',\n",
      " 223: 'schipperke',\n",
      " 224: 'groenendael',\n",
      " 225: 'malinois',\n",
      " 226: 'briard',\n",
      " 227: 'kelpie',\n",
      " 228: 'komondor',\n",
      " 229: 'Old English sheepdog, bobtail',\n",
      " 230: 'Shetland sheepdog, Shetland sheep dog, Shetland',\n",
      " 231: 'collie',\n",
      " 232: 'Border collie',\n",
      " 233: 'Bouvier des Flandres, Bouviers des Flandres',\n",
      " 234: 'Rottweiler',\n",
      " 235: 'German shepherd, German shepherd dog, German police dog, alsatian',\n",
      " 236: 'Doberman, Doberman pinscher',\n",
      " 237: 'miniature pinscher',\n",
      " 238: 'Greater Swiss Mountain dog',\n",
      " 239: 'Bernese mountain dog',\n",
      " 240: 'Appenzeller',\n",
      " 241: 'EntleBucher',\n",
      " 242: 'boxer',\n",
      " 243: 'bull mastiff',\n",
      " 244: 'Tibetan mastiff',\n",
      " 245: 'French bulldog',\n",
      " 246: 'Great Dane',\n",
      " 247: 'Saint Bernard, St Bernard',\n",
      " 248: 'Eskimo dog, husky',\n",
      " 249: 'malamute, malemute, Alaskan malamute',\n",
      " 250: 'Siberian husky',\n",
      " 251: 'dalmatian, coach dog, carriage dog',\n",
      " 252: 'affenpinscher, monkey pinscher, monkey dog',\n",
      " 253: 'basenji',\n",
      " 254: 'pug, pug-dog',\n",
      " 255: 'Leonberg',\n",
      " 256: 'Newfoundland, Newfoundland dog',\n",
      " 257: 'Great Pyrenees',\n",
      " 258: 'Samoyed, Samoyede',\n",
      " 259: 'Pomeranian',\n",
      " 260: 'chow, chow chow',\n",
      " 261: 'keeshond',\n",
      " 262: 'Brabancon griffon',\n",
      " 263: 'Pembroke, Pembroke Welsh corgi',\n",
      " 264: 'Cardigan, Cardigan Welsh corgi',\n",
      " 265: 'toy poodle',\n",
      " 266: 'miniature poodle',\n",
      " 267: 'standard poodle',\n",
      " 268: 'Mexican hairless',\n",
      " 269: 'timber wolf, grey wolf, gray wolf, Canis lupus',\n",
      " 270: 'white wolf, Arctic wolf, Canis lupus tundrarum',\n",
      " 271: 'red wolf, maned wolf, Canis rufus, Canis niger',\n",
      " 272: 'coyote, prairie wolf, brush wolf, Canis latrans',\n",
      " 273: 'dingo, warrigal, warragal, Canis dingo',\n",
      " 274: 'dhole, Cuon alpinus',\n",
      " 275: 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus',\n",
      " 276: 'hyena, hyaena',\n",
      " 277: 'red fox, Vulpes vulpes',\n",
      " 278: 'kit fox, Vulpes macrotis',\n",
      " 279: 'Arctic fox, white fox, Alopex lagopus',\n",
      " 280: 'grey fox, gray fox, Urocyon cinereoargenteus',\n",
      " 281: 'tabby, tabby cat',\n",
      " 282: 'tiger cat',\n",
      " 283: 'Persian cat',\n",
      " 284: 'Siamese cat, Siamese',\n",
      " 285: 'Egyptian cat',\n",
      " 286: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n",
      " 287: 'lynx, catamount',\n",
      " 288: 'leopard, Panthera pardus',\n",
      " 289: 'snow leopard, ounce, Panthera uncia',\n",
      " 290: 'jaguar, panther, Panthera onca, Felis onca',\n",
      " 291: 'lion, king of beasts, Panthera leo',\n",
      " 292: 'tiger, Panthera tigris',\n",
      " 293: 'cheetah, chetah, Acinonyx jubatus',\n",
      " 294: 'brown bear, bruin, Ursus arctos',\n",
      " 295: 'American black bear, black bear, Ursus americanus, Euarctos americanus',\n",
      " 296: 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus',\n",
      " 297: 'sloth bear, Melursus ursinus, Ursus ursinus',\n",
      " 298: 'mongoose',\n",
      " 299: 'meerkat, mierkat',\n",
      " 300: 'tiger beetle',\n",
      " 301: 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle',\n",
      " 302: 'ground beetle, carabid beetle',\n",
      " 303: 'long-horned beetle, longicorn, longicorn beetle',\n",
      " 304: 'leaf beetle, chrysomelid',\n",
      " 305: 'dung beetle',\n",
      " 306: 'rhinoceros beetle',\n",
      " 307: 'weevil',\n",
      " 308: 'fly',\n",
      " 309: 'bee',\n",
      " 310: 'ant, emmet, pismire',\n",
      " 311: 'grasshopper, hopper',\n",
      " 312: 'cricket',\n",
      " 313: 'walking stick, walkingstick, stick insect',\n",
      " 314: 'cockroach, roach',\n",
      " 315: 'mantis, mantid',\n",
      " 316: 'cicada, cicala',\n",
      " 317: 'leafhopper',\n",
      " 318: 'lacewing, lacewing fly',\n",
      " 319: \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\",\n",
      " 320: 'damselfly',\n",
      " 321: 'admiral',\n",
      " 322: 'ringlet, ringlet butterfly',\n",
      " 323: 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus',\n",
      " 324: 'cabbage butterfly',\n",
      " 325: 'sulphur butterfly, sulfur butterfly',\n",
      " 326: 'lycaenid, lycaenid butterfly',\n",
      " 327: 'starfish, sea star',\n",
      " 328: 'sea urchin',\n",
      " 329: 'sea cucumber, holothurian',\n",
      " 330: 'wood rabbit, cottontail, cottontail rabbit',\n",
      " 331: 'hare',\n",
      " 332: 'Angora, Angora rabbit',\n",
      " 333: 'hamster',\n",
      " 334: 'porcupine, hedgehog',\n",
      " 335: 'fox squirrel, eastern fox squirrel, Sciurus niger',\n",
      " 336: 'marmot',\n",
      " 337: 'beaver',\n",
      " 338: 'guinea pig, Cavia cobaya',\n",
      " 339: 'sorrel',\n",
      " 340: 'zebra',\n",
      " 341: 'hog, pig, grunter, squealer, Sus scrofa',\n",
      " 342: 'wild boar, boar, Sus scrofa',\n",
      " 343: 'warthog',\n",
      " 344: 'hippopotamus, hippo, river horse, Hippopotamus amphibius',\n",
      " 345: 'ox',\n",
      " 346: 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis',\n",
      " 347: 'bison',\n",
      " 348: 'ram, tup',\n",
      " 349: 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis',\n",
      " 350: 'ibex, Capra ibex',\n",
      " 351: 'hartebeest',\n",
      " 352: 'impala, Aepyceros melampus',\n",
      " 353: 'gazelle',\n",
      " 354: 'Arabian camel, dromedary, Camelus dromedarius',\n",
      " 355: 'llama',\n",
      " 356: 'weasel',\n",
      " 357: 'mink',\n",
      " 358: 'polecat, fitch, foulmart, foumart, Mustela putorius',\n",
      " 359: 'black-footed ferret, ferret, Mustela nigripes',\n",
      " 360: 'otter',\n",
      " 361: 'skunk, polecat, wood pussy',\n",
      " 362: 'badger',\n",
      " 363: 'armadillo',\n",
      " 364: 'three-toed sloth, ai, Bradypus tridactylus',\n",
      " 365: 'orangutan, orang, orangutang, Pongo pygmaeus',\n",
      " 366: 'gorilla, Gorilla gorilla',\n",
      " 367: 'chimpanzee, chimp, Pan troglodytes',\n",
      " 368: 'gibbon, Hylobates lar',\n",
      " 369: 'siamang, Hylobates syndactylus, Symphalangus syndactylus',\n",
      " 370: 'guenon, guenon monkey',\n",
      " 371: 'patas, hussar monkey, Erythrocebus patas',\n",
      " 372: 'baboon',\n",
      " 373: 'macaque',\n",
      " 374: 'langur',\n",
      " 375: 'colobus, colobus monkey',\n",
      " 376: 'proboscis monkey, Nasalis larvatus',\n",
      " 377: 'marmoset',\n",
      " 378: 'capuchin, ringtail, Cebus capucinus',\n",
      " 379: 'howler monkey, howler',\n",
      " 380: 'titi, titi monkey',\n",
      " 381: 'spider monkey, Ateles geoffroyi',\n",
      " 382: 'squirrel monkey, Saimiri sciureus',\n",
      " 383: 'Madagascar cat, ring-tailed lemur, Lemur catta',\n",
      " 384: 'indri, indris, Indri indri, Indri brevicaudatus',\n",
      " 385: 'Indian elephant, Elephas maximus',\n",
      " 386: 'African elephant, Loxodonta africana',\n",
      " 387: 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens',\n",
      " 388: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca',\n",
      " 389: 'barracouta, snoek',\n",
      " 390: 'eel',\n",
      " 391: 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch',\n",
      " 392: 'rock beauty, Holocanthus tricolor',\n",
      " 393: 'anemone fish',\n",
      " 394: 'sturgeon',\n",
      " 395: 'gar, garfish, garpike, billfish, Lepisosteus osseus',\n",
      " 396: 'lionfish',\n",
      " 397: 'puffer, pufferfish, blowfish, globefish',\n",
      " 398: 'abacus',\n",
      " 399: 'abaya',\n",
      " 400: \"academic gown, academic robe, judge's robe\",\n",
      " 401: 'accordion, piano accordion, squeeze box',\n",
      " 402: 'acoustic guitar',\n",
      " 403: 'aircraft carrier, carrier, flattop, attack aircraft carrier',\n",
      " 404: 'airliner',\n",
      " 405: 'airship, dirigible',\n",
      " 406: 'altar',\n",
      " 407: 'ambulance',\n",
      " 408: 'amphibian, amphibious vehicle',\n",
      " 409: 'analog clock',\n",
      " 410: 'apiary, bee house',\n",
      " 411: 'apron',\n",
      " 412: 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin',\n",
      " 413: 'assault rifle, assault gun',\n",
      " 414: 'backpack, back pack, knapsack, packsack, rucksack, haversack',\n",
      " 415: 'bakery, bakeshop, bakehouse',\n",
      " 416: 'balance beam, beam',\n",
      " 417: 'balloon',\n",
      " 418: 'ballpoint, ballpoint pen, ballpen, Biro',\n",
      " 419: 'Band Aid',\n",
      " 420: 'banjo',\n",
      " 421: 'bannister, banister, balustrade, balusters, handrail',\n",
      " 422: 'barbell',\n",
      " 423: 'barber chair',\n",
      " 424: 'barbershop',\n",
      " 425: 'barn',\n",
      " 426: 'barometer',\n",
      " 427: 'barrel, cask',\n",
      " 428: 'barrow, garden cart, lawn cart, wheelbarrow',\n",
      " 429: 'baseball',\n",
      " 430: 'basketball',\n",
      " 431: 'bassinet',\n",
      " 432: 'bassoon',\n",
      " 433: 'bathing cap, swimming cap',\n",
      " 434: 'bath towel',\n",
      " 435: 'bathtub, bathing tub, bath, tub',\n",
      " 436: 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon',\n",
      " 437: 'beacon, lighthouse, beacon light, pharos',\n",
      " 438: 'beaker',\n",
      " 439: 'bearskin, busby, shako',\n",
      " 440: 'beer bottle',\n",
      " 441: 'beer glass',\n",
      " 442: 'bell cote, bell cot',\n",
      " 443: 'bib',\n",
      " 444: 'bicycle-built-for-two, tandem bicycle, tandem',\n",
      " 445: 'bikini, two-piece',\n",
      " 446: 'binder, ring-binder',\n",
      " 447: 'binoculars, field glasses, opera glasses',\n",
      " 448: 'birdhouse',\n",
      " 449: 'boathouse',\n",
      " 450: 'bobsled, bobsleigh, bob',\n",
      " 451: 'bolo tie, bolo, bola tie, bola',\n",
      " 452: 'bonnet, poke bonnet',\n",
      " 453: 'bookcase',\n",
      " 454: 'bookshop, bookstore, bookstall',\n",
      " 455: 'bottlecap',\n",
      " 456: 'bow',\n",
      " 457: 'bow tie, bow-tie, bowtie',\n",
      " 458: 'brass, memorial tablet, plaque',\n",
      " 459: 'brassiere, bra, bandeau',\n",
      " 460: 'breakwater, groin, groyne, mole, bulwark, seawall, jetty',\n",
      " 461: 'breastplate, aegis, egis',\n",
      " 462: 'broom',\n",
      " 463: 'bucket, pail',\n",
      " 464: 'buckle',\n",
      " 465: 'bulletproof vest',\n",
      " 466: 'bullet train, bullet',\n",
      " 467: 'butcher shop, meat market',\n",
      " 468: 'cab, hack, taxi, taxicab',\n",
      " 469: 'caldron, cauldron',\n",
      " 470: 'candle, taper, wax light',\n",
      " 471: 'cannon',\n",
      " 472: 'canoe',\n",
      " 473: 'can opener, tin opener',\n",
      " 474: 'cardigan',\n",
      " 475: 'car mirror',\n",
      " 476: 'carousel, carrousel, merry-go-round, roundabout, whirligig',\n",
      " 477: \"carpenter's kit, tool kit\",\n",
      " 478: 'carton',\n",
      " 479: 'car wheel',\n",
      " 480: 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM',\n",
      " 481: 'cassette',\n",
      " 482: 'cassette player',\n",
      " 483: 'castle',\n",
      " 484: 'catamaran',\n",
      " 485: 'CD player',\n",
      " 486: 'cello, violoncello',\n",
      " 487: 'cellular telephone, cellular phone, cellphone, cell, mobile phone',\n",
      " 488: 'chain',\n",
      " 489: 'chainlink fence',\n",
      " 490: 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour',\n",
      " 491: 'chain saw, chainsaw',\n",
      " 492: 'chest',\n",
      " 493: 'chiffonier, commode',\n",
      " 494: 'chime, bell, gong',\n",
      " 495: 'china cabinet, china closet',\n",
      " 496: 'Christmas stocking',\n",
      " 497: 'church, church building',\n",
      " 498: 'cinema, movie theater, movie theatre, movie house, picture palace',\n",
      " 499: 'cleaver, meat cleaver, chopper',\n",
      " 500: 'cliff dwelling',\n",
      " 501: 'cloak',\n",
      " 502: 'clog, geta, patten, sabot',\n",
      " 503: 'cocktail shaker',\n",
      " 504: 'coffee mug',\n",
      " 505: 'coffeepot',\n",
      " 506: 'coil, spiral, volute, whorl, helix',\n",
      " 507: 'combination lock',\n",
      " 508: 'computer keyboard, keypad',\n",
      " 509: 'confectionery, confectionary, candy store',\n",
      " 510: 'container ship, containership, container vessel',\n",
      " 511: 'convertible',\n",
      " 512: 'corkscrew, bottle screw',\n",
      " 513: 'cornet, horn, trumpet, trump',\n",
      " 514: 'cowboy boot',\n",
      " 515: 'cowboy hat, ten-gallon hat',\n",
      " 516: 'cradle',\n",
      " 517: 'crane',\n",
      " 518: 'crash helmet',\n",
      " 519: 'crate',\n",
      " 520: 'crib, cot',\n",
      " 521: 'Crock Pot',\n",
      " 522: 'croquet ball',\n",
      " 523: 'crutch',\n",
      " 524: 'cuirass',\n",
      " 525: 'dam, dike, dyke',\n",
      " 526: 'desk',\n",
      " 527: 'desktop computer',\n",
      " 528: 'dial telephone, dial phone',\n",
      " 529: 'diaper, nappy, napkin',\n",
      " 530: 'digital clock',\n",
      " 531: 'digital watch',\n",
      " 532: 'dining table, board',\n",
      " 533: 'dishrag, dishcloth',\n",
      " 534: 'dishwasher, dish washer, dishwashing machine',\n",
      " 535: 'disk brake, disc brake',\n",
      " 536: 'dock, dockage, docking facility',\n",
      " 537: 'dogsled, dog sled, dog sleigh',\n",
      " 538: 'dome',\n",
      " 539: 'doormat, welcome mat',\n",
      " 540: 'drilling platform, offshore rig',\n",
      " 541: 'drum, membranophone, tympan',\n",
      " 542: 'drumstick',\n",
      " 543: 'dumbbell',\n",
      " 544: 'Dutch oven',\n",
      " 545: 'electric fan, blower',\n",
      " 546: 'electric guitar',\n",
      " 547: 'electric locomotive',\n",
      " 548: 'entertainment center',\n",
      " 549: 'envelope',\n",
      " 550: 'espresso maker',\n",
      " 551: 'face powder',\n",
      " 552: 'feather boa, boa',\n",
      " 553: 'file, file cabinet, filing cabinet',\n",
      " 554: 'fireboat',\n",
      " 555: 'fire engine, fire truck',\n",
      " 556: 'fire screen, fireguard',\n",
      " 557: 'flagpole, flagstaff',\n",
      " 558: 'flute, transverse flute',\n",
      " 559: 'folding chair',\n",
      " 560: 'football helmet',\n",
      " 561: 'forklift',\n",
      " 562: 'fountain',\n",
      " 563: 'fountain pen',\n",
      " 564: 'four-poster',\n",
      " 565: 'freight car',\n",
      " 566: 'French horn, horn',\n",
      " 567: 'frying pan, frypan, skillet',\n",
      " 568: 'fur coat',\n",
      " 569: 'garbage truck, dustcart',\n",
      " 570: 'gasmask, respirator, gas helmet',\n",
      " 571: 'gas pump, gasoline pump, petrol pump, island dispenser',\n",
      " 572: 'goblet',\n",
      " 573: 'go-kart',\n",
      " 574: 'golf ball',\n",
      " 575: 'golfcart, golf cart',\n",
      " 576: 'gondola',\n",
      " 577: 'gong, tam-tam',\n",
      " 578: 'gown',\n",
      " 579: 'grand piano, grand',\n",
      " 580: 'greenhouse, nursery, glasshouse',\n",
      " 581: 'grille, radiator grille',\n",
      " 582: 'grocery store, grocery, food market, market',\n",
      " 583: 'guillotine',\n",
      " 584: 'hair slide',\n",
      " 585: 'hair spray',\n",
      " 586: 'half track',\n",
      " 587: 'hammer',\n",
      " 588: 'hamper',\n",
      " 589: 'hand blower, blow dryer, blow drier, hair dryer, hair drier',\n",
      " 590: 'hand-held computer, hand-held microcomputer',\n",
      " 591: 'handkerchief, hankie, hanky, hankey',\n",
      " 592: 'hard disc, hard disk, fixed disk',\n",
      " 593: 'harmonica, mouth organ, harp, mouth harp',\n",
      " 594: 'harp',\n",
      " 595: 'harvester, reaper',\n",
      " 596: 'hatchet',\n",
      " 597: 'holster',\n",
      " 598: 'home theater, home theatre',\n",
      " 599: 'honeycomb',\n",
      " 600: 'hook, claw',\n",
      " 601: 'hoopskirt, crinoline',\n",
      " 602: 'horizontal bar, high bar',\n",
      " 603: 'horse cart, horse-cart',\n",
      " 604: 'hourglass',\n",
      " 605: 'iPod',\n",
      " 606: 'iron, smoothing iron',\n",
      " 607: \"jack-o'-lantern\",\n",
      " 608: 'jean, blue jean, denim',\n",
      " 609: 'jeep, landrover',\n",
      " 610: 'jersey, T-shirt, tee shirt',\n",
      " 611: 'jigsaw puzzle',\n",
      " 612: 'jinrikisha, ricksha, rickshaw',\n",
      " 613: 'joystick',\n",
      " 614: 'kimono',\n",
      " 615: 'knee pad',\n",
      " 616: 'knot',\n",
      " 617: 'lab coat, laboratory coat',\n",
      " 618: 'ladle',\n",
      " 619: 'lampshade, lamp shade',\n",
      " 620: 'laptop, laptop computer',\n",
      " 621: 'lawn mower, mower',\n",
      " 622: 'lens cap, lens cover',\n",
      " 623: 'letter opener, paper knife, paperknife',\n",
      " 624: 'library',\n",
      " 625: 'lifeboat',\n",
      " 626: 'lighter, light, igniter, ignitor',\n",
      " 627: 'limousine, limo',\n",
      " 628: 'liner, ocean liner',\n",
      " 629: 'lipstick, lip rouge',\n",
      " 630: 'Loafer',\n",
      " 631: 'lotion',\n",
      " 632: 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system',\n",
      " 633: \"loupe, jeweler's loupe\",\n",
      " 634: 'lumbermill, sawmill',\n",
      " 635: 'magnetic compass',\n",
      " 636: 'mailbag, postbag',\n",
      " 637: 'mailbox, letter box',\n",
      " 638: 'maillot',\n",
      " 639: 'maillot, tank suit',\n",
      " 640: 'manhole cover',\n",
      " 641: 'maraca',\n",
      " 642: 'marimba, xylophone',\n",
      " 643: 'mask',\n",
      " 644: 'matchstick',\n",
      " 645: 'maypole',\n",
      " 646: 'maze, labyrinth',\n",
      " 647: 'measuring cup',\n",
      " 648: 'medicine chest, medicine cabinet',\n",
      " 649: 'megalith, megalithic structure',\n",
      " 650: 'microphone, mike',\n",
      " 651: 'microwave, microwave oven',\n",
      " 652: 'military uniform',\n",
      " 653: 'milk can',\n",
      " 654: 'minibus',\n",
      " 655: 'miniskirt, mini',\n",
      " 656: 'minivan',\n",
      " 657: 'missile',\n",
      " 658: 'mitten',\n",
      " 659: 'mixing bowl',\n",
      " 660: 'mobile home, manufactured home',\n",
      " 661: 'Model T',\n",
      " 662: 'modem',\n",
      " 663: 'monastery',\n",
      " 664: 'monitor',\n",
      " 665: 'moped',\n",
      " 666: 'mortar',\n",
      " 667: 'mortarboard',\n",
      " 668: 'mosque',\n",
      " 669: 'mosquito net',\n",
      " 670: 'motor scooter, scooter',\n",
      " 671: 'mountain bike, all-terrain bike, off-roader',\n",
      " 672: 'mountain tent',\n",
      " 673: 'mouse, computer mouse',\n",
      " 674: 'mousetrap',\n",
      " 675: 'moving van',\n",
      " 676: 'muzzle',\n",
      " 677: 'nail',\n",
      " 678: 'neck brace',\n",
      " 679: 'necklace',\n",
      " 680: 'nipple',\n",
      " 681: 'notebook, notebook computer',\n",
      " 682: 'obelisk',\n",
      " 683: 'oboe, hautboy, hautbois',\n",
      " 684: 'ocarina, sweet potato',\n",
      " 685: 'odometer, hodometer, mileometer, milometer',\n",
      " 686: 'oil filter',\n",
      " 687: 'organ, pipe organ',\n",
      " 688: 'oscilloscope, scope, cathode-ray oscilloscope, CRO',\n",
      " 689: 'overskirt',\n",
      " 690: 'oxcart',\n",
      " 691: 'oxygen mask',\n",
      " 692: 'packet',\n",
      " 693: 'paddle, boat paddle',\n",
      " 694: 'paddlewheel, paddle wheel',\n",
      " 695: 'padlock',\n",
      " 696: 'paintbrush',\n",
      " 697: \"pajama, pyjama, pj's, jammies\",\n",
      " 698: 'palace',\n",
      " 699: 'panpipe, pandean pipe, syrinx',\n",
      " 700: 'paper towel',\n",
      " 701: 'parachute, chute',\n",
      " 702: 'parallel bars, bars',\n",
      " 703: 'park bench',\n",
      " 704: 'parking meter',\n",
      " 705: 'passenger car, coach, carriage',\n",
      " 706: 'patio, terrace',\n",
      " 707: 'pay-phone, pay-station',\n",
      " 708: 'pedestal, plinth, footstall',\n",
      " 709: 'pencil box, pencil case',\n",
      " 710: 'pencil sharpener',\n",
      " 711: 'perfume, essence',\n",
      " 712: 'Petri dish',\n",
      " 713: 'photocopier',\n",
      " 714: 'pick, plectrum, plectron',\n",
      " 715: 'pickelhaube',\n",
      " 716: 'picket fence, paling',\n",
      " 717: 'pickup, pickup truck',\n",
      " 718: 'pier',\n",
      " 719: 'piggy bank, penny bank',\n",
      " 720: 'pill bottle',\n",
      " 721: 'pillow',\n",
      " 722: 'ping-pong ball',\n",
      " 723: 'pinwheel',\n",
      " 724: 'pirate, pirate ship',\n",
      " 725: 'pitcher, ewer',\n",
      " 726: \"plane, carpenter's plane, woodworking plane\",\n",
      " 727: 'planetarium',\n",
      " 728: 'plastic bag',\n",
      " 729: 'plate rack',\n",
      " 730: 'plow, plough',\n",
      " 731: \"plunger, plumber's helper\",\n",
      " 732: 'Polaroid camera, Polaroid Land camera',\n",
      " 733: 'pole',\n",
      " 734: 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria',\n",
      " 735: 'poncho',\n",
      " 736: 'pool table, billiard table, snooker table',\n",
      " 737: 'pop bottle, soda bottle',\n",
      " 738: 'pot, flowerpot',\n",
      " 739: \"potter's wheel\",\n",
      " 740: 'power drill',\n",
      " 741: 'prayer rug, prayer mat',\n",
      " 742: 'printer',\n",
      " 743: 'prison, prison house',\n",
      " 744: 'projectile, missile',\n",
      " 745: 'projector',\n",
      " 746: 'puck, hockey puck',\n",
      " 747: 'punching bag, punch bag, punching ball, punchball',\n",
      " 748: 'purse',\n",
      " 749: 'quill, quill pen',\n",
      " 750: 'quilt, comforter, comfort, puff',\n",
      " 751: 'racer, race car, racing car',\n",
      " 752: 'racket, racquet',\n",
      " 753: 'radiator',\n",
      " 754: 'radio, wireless',\n",
      " 755: 'radio telescope, radio reflector',\n",
      " 756: 'rain barrel',\n",
      " 757: 'recreational vehicle, RV, R.V.',\n",
      " 758: 'reel',\n",
      " 759: 'reflex camera',\n",
      " 760: 'refrigerator, icebox',\n",
      " 761: 'remote control, remote',\n",
      " 762: 'restaurant, eating house, eating place, eatery',\n",
      " 763: 'revolver, six-gun, six-shooter',\n",
      " 764: 'rifle',\n",
      " 765: 'rocking chair, rocker',\n",
      " 766: 'rotisserie',\n",
      " 767: 'rubber eraser, rubber, pencil eraser',\n",
      " 768: 'rugby ball',\n",
      " 769: 'rule, ruler',\n",
      " 770: 'running shoe',\n",
      " 771: 'safe',\n",
      " 772: 'safety pin',\n",
      " 773: 'saltshaker, salt shaker',\n",
      " 774: 'sandal',\n",
      " 775: 'sarong',\n",
      " 776: 'sax, saxophone',\n",
      " 777: 'scabbard',\n",
      " 778: 'scale, weighing machine',\n",
      " 779: 'school bus',\n",
      " 780: 'schooner',\n",
      " 781: 'scoreboard',\n",
      " 782: 'screen, CRT screen',\n",
      " 783: 'screw',\n",
      " 784: 'screwdriver',\n",
      " 785: 'seat belt, seatbelt',\n",
      " 786: 'sewing machine',\n",
      " 787: 'shield, buckler',\n",
      " 788: 'shoe shop, shoe-shop, shoe store',\n",
      " 789: 'shoji',\n",
      " 790: 'shopping basket',\n",
      " 791: 'shopping cart',\n",
      " 792: 'shovel',\n",
      " 793: 'shower cap',\n",
      " 794: 'shower curtain',\n",
      " 795: 'ski',\n",
      " 796: 'ski mask',\n",
      " 797: 'sleeping bag',\n",
      " 798: 'slide rule, slipstick',\n",
      " 799: 'sliding door',\n",
      " 800: 'slot, one-armed bandit',\n",
      " 801: 'snorkel',\n",
      " 802: 'snowmobile',\n",
      " 803: 'snowplow, snowplough',\n",
      " 804: 'soap dispenser',\n",
      " 805: 'soccer ball',\n",
      " 806: 'sock',\n",
      " 807: 'solar dish, solar collector, solar furnace',\n",
      " 808: 'sombrero',\n",
      " 809: 'soup bowl',\n",
      " 810: 'space bar',\n",
      " 811: 'space heater',\n",
      " 812: 'space shuttle',\n",
      " 813: 'spatula',\n",
      " 814: 'speedboat',\n",
      " 815: \"spider web, spider's web\",\n",
      " 816: 'spindle',\n",
      " 817: 'sports car, sport car',\n",
      " 818: 'spotlight, spot',\n",
      " 819: 'stage',\n",
      " 820: 'steam locomotive',\n",
      " 821: 'steel arch bridge',\n",
      " 822: 'steel drum',\n",
      " 823: 'stethoscope',\n",
      " 824: 'stole',\n",
      " 825: 'stone wall',\n",
      " 826: 'stopwatch, stop watch',\n",
      " 827: 'stove',\n",
      " 828: 'strainer',\n",
      " 829: 'streetcar, tram, tramcar, trolley, trolley car',\n",
      " 830: 'stretcher',\n",
      " 831: 'studio couch, day bed',\n",
      " 832: 'stupa, tope',\n",
      " 833: 'submarine, pigboat, sub, U-boat',\n",
      " 834: 'suit, suit of clothes',\n",
      " 835: 'sundial',\n",
      " 836: 'sunglass',\n",
      " 837: 'sunglasses, dark glasses, shades',\n",
      " 838: 'sunscreen, sunblock, sun blocker',\n",
      " 839: 'suspension bridge',\n",
      " 840: 'swab, swob, mop',\n",
      " 841: 'sweatshirt',\n",
      " 842: 'swimming trunks, bathing trunks',\n",
      " 843: 'swing',\n",
      " 844: 'switch, electric switch, electrical switch',\n",
      " 845: 'syringe',\n",
      " 846: 'table lamp',\n",
      " 847: 'tank, army tank, armored combat vehicle, armoured combat vehicle',\n",
      " 848: 'tape player',\n",
      " 849: 'teapot',\n",
      " 850: 'teddy, teddy bear',\n",
      " 851: 'television, television system',\n",
      " 852: 'tennis ball',\n",
      " 853: 'thatch, thatched roof',\n",
      " 854: 'theater curtain, theatre curtain',\n",
      " 855: 'thimble',\n",
      " 856: 'thresher, thrasher, threshing machine',\n",
      " 857: 'throne',\n",
      " 858: 'tile roof',\n",
      " 859: 'toaster',\n",
      " 860: 'tobacco shop, tobacconist shop, tobacconist',\n",
      " 861: 'toilet seat',\n",
      " 862: 'torch',\n",
      " 863: 'totem pole',\n",
      " 864: 'tow truck, tow car, wrecker',\n",
      " 865: 'toyshop',\n",
      " 866: 'tractor',\n",
      " 867: 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi',\n",
      " 868: 'tray',\n",
      " 869: 'trench coat',\n",
      " 870: 'tricycle, trike, velocipede',\n",
      " 871: 'trimaran',\n",
      " 872: 'tripod',\n",
      " 873: 'triumphal arch',\n",
      " 874: 'trolleybus, trolley coach, trackless trolley',\n",
      " 875: 'trombone',\n",
      " 876: 'tub, vat',\n",
      " 877: 'turnstile',\n",
      " 878: 'typewriter keyboard',\n",
      " 879: 'umbrella',\n",
      " 880: 'unicycle, monocycle',\n",
      " 881: 'upright, upright piano',\n",
      " 882: 'vacuum, vacuum cleaner',\n",
      " 883: 'vase',\n",
      " 884: 'vault',\n",
      " 885: 'velvet',\n",
      " 886: 'vending machine',\n",
      " 887: 'vestment',\n",
      " 888: 'viaduct',\n",
      " 889: 'violin, fiddle',\n",
      " 890: 'volleyball',\n",
      " 891: 'waffle iron',\n",
      " 892: 'wall clock',\n",
      " 893: 'wallet, billfold, notecase, pocketbook',\n",
      " 894: 'wardrobe, closet, press',\n",
      " 895: 'warplane, military plane',\n",
      " 896: 'washbasin, handbasin, washbowl, lavabo, wash-hand basin',\n",
      " 897: 'washer, automatic washer, washing machine',\n",
      " 898: 'water bottle',\n",
      " 899: 'water jug',\n",
      " 900: 'water tower',\n",
      " 901: 'whiskey jug',\n",
      " 902: 'whistle',\n",
      " 903: 'wig',\n",
      " 904: 'window screen',\n",
      " 905: 'window shade',\n",
      " 906: 'Windsor tie',\n",
      " 907: 'wine bottle',\n",
      " 908: 'wing',\n",
      " 909: 'wok',\n",
      " 910: 'wooden spoon',\n",
      " 911: 'wool, woolen, woollen',\n",
      " 912: 'worm fence, snake fence, snake-rail fence, Virginia fence',\n",
      " 913: 'wreck',\n",
      " 914: 'yawl',\n",
      " 915: 'yurt',\n",
      " 916: 'web site, website, internet site, site',\n",
      " 917: 'comic book',\n",
      " 918: 'crossword puzzle, crossword',\n",
      " 919: 'street sign',\n",
      " 920: 'traffic light, traffic signal, stoplight',\n",
      " 921: 'book jacket, dust cover, dust jacket, dust wrapper',\n",
      " 922: 'menu',\n",
      " 923: 'plate',\n",
      " 924: 'guacamole',\n",
      " 925: 'consomme',\n",
      " 926: 'hot pot, hotpot',\n",
      " 927: 'trifle',\n",
      " 928: 'ice cream, icecream',\n",
      " 929: 'ice lolly, lolly, lollipop, popsicle',\n",
      " 930: 'French loaf',\n",
      " 931: 'bagel, beigel',\n",
      " 932: 'pretzel',\n",
      " 933: 'cheeseburger',\n",
      " 934: 'hotdog, hot dog, red hot',\n",
      " 935: 'mashed potato',\n",
      " 936: 'head cabbage',\n",
      " 937: 'broccoli',\n",
      " 938: 'cauliflower',\n",
      " 939: 'zucchini, courgette',\n",
      " 940: 'spaghetti squash',\n",
      " 941: 'acorn squash',\n",
      " 942: 'butternut squash',\n",
      " 943: 'cucumber, cuke',\n",
      " 944: 'artichoke, globe artichoke',\n",
      " 945: 'bell pepper',\n",
      " 946: 'cardoon',\n",
      " 947: 'mushroom',\n",
      " 948: 'Granny Smith',\n",
      " 949: 'strawberry',\n",
      " 950: 'orange',\n",
      " 951: 'lemon',\n",
      " 952: 'fig',\n",
      " 953: 'pineapple, ananas',\n",
      " 954: 'banana',\n",
      " 955: 'jackfruit, jak, jack',\n",
      " 956: 'custard apple',\n",
      " 957: 'pomegranate',\n",
      " 958: 'hay',\n",
      " 959: 'carbonara',\n",
      " 960: 'chocolate sauce, chocolate syrup',\n",
      " 961: 'dough',\n",
      " 962: 'meat loaf, meatloaf',\n",
      " 963: 'pizza, pizza pie',\n",
      " 964: 'potpie',\n",
      " 965: 'burrito',\n",
      " 966: 'red wine',\n",
      " 967: 'espresso',\n",
      " 968: 'cup',\n",
      " 969: 'eggnog',\n",
      " 970: 'alp',\n",
      " 971: 'bubble',\n",
      " 972: 'cliff, drop, drop-off',\n",
      " 973: 'coral reef',\n",
      " 974: 'geyser',\n",
      " 975: 'lakeside, lakeshore',\n",
      " 976: 'promontory, headland, head, foreland',\n",
      " 977: 'sandbar, sand bar',\n",
      " 978: 'seashore, coast, seacoast, sea-coast',\n",
      " 979: 'valley, vale',\n",
      " 980: 'volcano',\n",
      " 981: 'ballplayer, baseball player',\n",
      " 982: 'groom, bridegroom',\n",
      " 983: 'scuba diver',\n",
      " 984: 'rapeseed',\n",
      " 985: 'daisy',\n",
      " 986: \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\",\n",
      " 987: 'corn',\n",
      " 988: 'acorn',\n",
      " 989: 'hip, rose hip, rosehip',\n",
      " 990: 'buckeye, horse chestnut, conker',\n",
      " 991: 'coral fungus',\n",
      " 992: 'agaric',\n",
      " 993: 'gyromitra',\n",
      " 994: 'stinkhorn, carrion fungus',\n",
      " 995: 'earthstar',\n",
      " 996: 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa',\n",
      " 997: 'bolete',\n",
      " 998: 'ear, spike, capitulum',\n",
      " 999: 'toilet tissue, toilet paper, bathroom tissue'}\n",
      "71/4:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 380\n",
      "\n",
      "def prep_image(image):\n",
      "    res = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "    res = res.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "    return res\n",
      "\n",
      "def predict(image):\n",
      "    cropped_image = center_crop(image)\n",
      "    prediction = (model.predict(prep_image(image)), model.predict(prep_image(cropped_image)))\n",
      "    if np.max(prediction[0]>=prediction[1]):\n",
      "        return {'class': np.argmax(prediction[0], axis=1)[0], 'all_prob': prediction[0]}\n",
      "    else:\n",
      "        return  {'class': np.argmax(prediction[1], axis=1)[1], 'all_prob': prediction[1]}\n",
      "    \n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "plt.imshow(image)\n",
      "\n",
      "print(class_dict[predict(image)])\n",
      "72/1:\n",
      "from tensorflow.keras.applications import EfficientNetB4\n",
      "import tensorflow as tf\n",
      "model = EfficientNetB4(weights='imagenet')\n",
      "\n",
      "def center_crop(img, new_width=None, new_height=None):        \n",
      "\n",
      "    width = img.shape[1]\n",
      "    height = img.shape[0]\n",
      "\n",
      "    if new_width is None:\n",
      "        new_width = min(width, height)\n",
      "\n",
      "    if new_height is None:\n",
      "        new_height = min(width, height)\n",
      "\n",
      "    left = int(np.ceil((width - new_width) / 2))\n",
      "    right = width - int(np.floor((width - new_width) / 2))\n",
      "\n",
      "    top = int(np.ceil((height - new_height) / 2))\n",
      "    bottom = height - int(np.floor((height - new_height) / 2))\n",
      "\n",
      "    if len(img.shape) == 2:\n",
      "        center_cropped_img = img[top:bottom, left:right]\n",
      "    else:\n",
      "        center_cropped_img = img[top:bottom, left:right, ...]\n",
      "\n",
      "    return center_cropped_img\n",
      "\n",
      "class_dict = {0: 'tench, Tinca tinca',\n",
      " 1: 'goldfish, Carassius auratus',\n",
      " 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n",
      " 3: 'tiger shark, Galeocerdo cuvieri',\n",
      " 4: 'hammerhead, hammerhead shark',\n",
      " 5: 'electric ray, crampfish, numbfish, torpedo',\n",
      " 6: 'stingray',\n",
      " 7: 'cock',\n",
      " 8: 'hen',\n",
      " 9: 'ostrich, Struthio camelus',\n",
      " 10: 'brambling, Fringilla montifringilla',\n",
      " 11: 'goldfinch, Carduelis carduelis',\n",
      " 12: 'house finch, linnet, Carpodacus mexicanus',\n",
      " 13: 'junco, snowbird',\n",
      " 14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea',\n",
      " 15: 'robin, American robin, Turdus migratorius',\n",
      " 16: 'bulbul',\n",
      " 17: 'jay',\n",
      " 18: 'magpie',\n",
      " 19: 'chickadee',\n",
      " 20: 'water ouzel, dipper',\n",
      " 21: 'kite',\n",
      " 22: 'bald eagle, American eagle, Haliaeetus leucocephalus',\n",
      " 23: 'vulture',\n",
      " 24: 'great grey owl, great gray owl, Strix nebulosa',\n",
      " 25: 'European fire salamander, Salamandra salamandra',\n",
      " 26: 'common newt, Triturus vulgaris',\n",
      " 27: 'eft',\n",
      " 28: 'spotted salamander, Ambystoma maculatum',\n",
      " 29: 'axolotl, mud puppy, Ambystoma mexicanum',\n",
      " 30: 'bullfrog, Rana catesbeiana',\n",
      " 31: 'tree frog, tree-frog',\n",
      " 32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui',\n",
      " 33: 'loggerhead, loggerhead turtle, Caretta caretta',\n",
      " 34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea',\n",
      " 35: 'mud turtle',\n",
      " 36: 'terrapin',\n",
      " 37: 'box turtle, box tortoise',\n",
      " 38: 'banded gecko',\n",
      " 39: 'common iguana, iguana, Iguana iguana',\n",
      " 40: 'American chameleon, anole, Anolis carolinensis',\n",
      " 41: 'whiptail, whiptail lizard',\n",
      " 42: 'agama',\n",
      " 43: 'frilled lizard, Chlamydosaurus kingi',\n",
      " 44: 'alligator lizard',\n",
      " 45: 'Gila monster, Heloderma suspectum',\n",
      " 46: 'green lizard, Lacerta viridis',\n",
      " 47: 'African chameleon, Chamaeleo chamaeleon',\n",
      " 48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis',\n",
      " 49: 'African crocodile, Nile crocodile, Crocodylus niloticus',\n",
      " 50: 'American alligator, Alligator mississipiensis',\n",
      " 51: 'triceratops',\n",
      " 52: 'thunder snake, worm snake, Carphophis amoenus',\n",
      " 53: 'ringneck snake, ring-necked snake, ring snake',\n",
      " 54: 'hognose snake, puff adder, sand viper',\n",
      " 55: 'green snake, grass snake',\n",
      " 56: 'king snake, kingsnake',\n",
      " 57: 'garter snake, grass snake',\n",
      " 58: 'water snake',\n",
      " 59: 'vine snake',\n",
      " 60: 'night snake, Hypsiglena torquata',\n",
      " 61: 'boa constrictor, Constrictor constrictor',\n",
      " 62: 'rock python, rock snake, Python sebae',\n",
      " 63: 'Indian cobra, Naja naja',\n",
      " 64: 'green mamba',\n",
      " 65: 'sea snake',\n",
      " 66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus',\n",
      " 67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus',\n",
      " 68: 'sidewinder, horned rattlesnake, Crotalus cerastes',\n",
      " 69: 'trilobite',\n",
      " 70: 'harvestman, daddy longlegs, Phalangium opilio',\n",
      " 71: 'scorpion',\n",
      " 72: 'black and gold garden spider, Argiope aurantia',\n",
      " 73: 'barn spider, Araneus cavaticus',\n",
      " 74: 'garden spider, Aranea diademata',\n",
      " 75: 'black widow, Latrodectus mactans',\n",
      " 76: 'tarantula',\n",
      " 77: 'wolf spider, hunting spider',\n",
      " 78: 'tick',\n",
      " 79: 'centipede',\n",
      " 80: 'black grouse',\n",
      " 81: 'ptarmigan',\n",
      " 82: 'ruffed grouse, partridge, Bonasa umbellus',\n",
      " 83: 'prairie chicken, prairie grouse, prairie fowl',\n",
      " 84: 'peacock',\n",
      " 85: 'quail',\n",
      " 86: 'partridge',\n",
      " 87: 'African grey, African gray, Psittacus erithacus',\n",
      " 88: 'macaw',\n",
      " 89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n",
      " 90: 'lorikeet',\n",
      " 91: 'coucal',\n",
      " 92: 'bee eater',\n",
      " 93: 'hornbill',\n",
      " 94: 'hummingbird',\n",
      " 95: 'jacamar',\n",
      " 96: 'toucan',\n",
      " 97: 'drake',\n",
      " 98: 'red-breasted merganser, Mergus serrator',\n",
      " 99: 'goose',\n",
      " 100: 'black swan, Cygnus atratus',\n",
      " 101: 'tusker',\n",
      " 102: 'echidna, spiny anteater, anteater',\n",
      " 103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n",
      " 104: 'wallaby, brush kangaroo',\n",
      " 105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n",
      " 106: 'wombat',\n",
      " 107: 'jellyfish',\n",
      " 108: 'sea anemone, anemone',\n",
      " 109: 'brain coral',\n",
      " 110: 'flatworm, platyhelminth',\n",
      " 111: 'nematode, nematode worm, roundworm',\n",
      " 112: 'conch',\n",
      " 113: 'snail',\n",
      " 114: 'slug',\n",
      " 115: 'sea slug, nudibranch',\n",
      " 116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore',\n",
      " 117: 'chambered nautilus, pearly nautilus, nautilus',\n",
      " 118: 'Dungeness crab, Cancer magister',\n",
      " 119: 'rock crab, Cancer irroratus',\n",
      " 120: 'fiddler crab',\n",
      " 121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica',\n",
      " 122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus',\n",
      " 123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish',\n",
      " 124: 'crayfish, crawfish, crawdad, crawdaddy',\n",
      " 125: 'hermit crab',\n",
      " 126: 'isopod',\n",
      " 127: 'white stork, Ciconia ciconia',\n",
      " 128: 'black stork, Ciconia nigra',\n",
      " 129: 'spoonbill',\n",
      " 130: 'flamingo',\n",
      " 131: 'little blue heron, Egretta caerulea',\n",
      " 132: 'American egret, great white heron, Egretta albus',\n",
      " 133: 'bittern',\n",
      " 134: 'crane',\n",
      " 135: 'limpkin, Aramus pictus',\n",
      " 136: 'European gallinule, Porphyrio porphyrio',\n",
      " 137: 'American coot, marsh hen, mud hen, water hen, Fulica americana',\n",
      " 138: 'bustard',\n",
      " 139: 'ruddy turnstone, Arenaria interpres',\n",
      " 140: 'red-backed sandpiper, dunlin, Erolia alpina',\n",
      " 141: 'redshank, Tringa totanus',\n",
      " 142: 'dowitcher',\n",
      " 143: 'oystercatcher, oyster catcher',\n",
      " 144: 'pelican',\n",
      " 145: 'king penguin, Aptenodytes patagonica',\n",
      " 146: 'albatross, mollymawk',\n",
      " 147: 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus',\n",
      " 148: 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca',\n",
      " 149: 'dugong, Dugong dugon',\n",
      " 150: 'sea lion',\n",
      " 151: 'Chihuahua',\n",
      " 152: 'Japanese spaniel',\n",
      " 153: 'Maltese dog, Maltese terrier, Maltese',\n",
      " 154: 'Pekinese, Pekingese, Peke',\n",
      " 155: 'Shih-Tzu',\n",
      " 156: 'Blenheim spaniel',\n",
      " 157: 'papillon',\n",
      " 158: 'toy terrier',\n",
      " 159: 'Rhodesian ridgeback',\n",
      " 160: 'Afghan hound, Afghan',\n",
      " 161: 'basset, basset hound',\n",
      " 162: 'beagle',\n",
      " 163: 'bloodhound, sleuthhound',\n",
      " 164: 'bluetick',\n",
      " 165: 'black-and-tan coonhound',\n",
      " 166: 'Walker hound, Walker foxhound',\n",
      " 167: 'English foxhound',\n",
      " 168: 'redbone',\n",
      " 169: 'borzoi, Russian wolfhound',\n",
      " 170: 'Irish wolfhound',\n",
      " 171: 'Italian greyhound',\n",
      " 172: 'whippet',\n",
      " 173: 'Ibizan hound, Ibizan Podenco',\n",
      " 174: 'Norwegian elkhound, elkhound',\n",
      " 175: 'otterhound, otter hound',\n",
      " 176: 'Saluki, gazelle hound',\n",
      " 177: 'Scottish deerhound, deerhound',\n",
      " 178: 'Weimaraner',\n",
      " 179: 'Staffordshire bullterrier, Staffordshire bull terrier',\n",
      " 180: 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier',\n",
      " 181: 'Bedlington terrier',\n",
      " 182: 'Border terrier',\n",
      " 183: 'Kerry blue terrier',\n",
      " 184: 'Irish terrier',\n",
      " 185: 'Norfolk terrier',\n",
      " 186: 'Norwich terrier',\n",
      " 187: 'Yorkshire terrier',\n",
      " 188: 'wire-haired fox terrier',\n",
      " 189: 'Lakeland terrier',\n",
      " 190: 'Sealyham terrier, Sealyham',\n",
      " 191: 'Airedale, Airedale terrier',\n",
      " 192: 'cairn, cairn terrier',\n",
      " 193: 'Australian terrier',\n",
      " 194: 'Dandie Dinmont, Dandie Dinmont terrier',\n",
      " 195: 'Boston bull, Boston terrier',\n",
      " 196: 'miniature schnauzer',\n",
      " 197: 'giant schnauzer',\n",
      " 198: 'standard schnauzer',\n",
      " 199: 'Scotch terrier, Scottish terrier, Scottie',\n",
      " 200: 'Tibetan terrier, chrysanthemum dog',\n",
      " 201: 'silky terrier, Sydney silky',\n",
      " 202: 'soft-coated wheaten terrier',\n",
      " 203: 'West Highland white terrier',\n",
      " 204: 'Lhasa, Lhasa apso',\n",
      " 205: 'flat-coated retriever',\n",
      " 206: 'curly-coated retriever',\n",
      " 207: 'golden retriever',\n",
      " 208: 'Labrador retriever',\n",
      " 209: 'Chesapeake Bay retriever',\n",
      " 210: 'German short-haired pointer',\n",
      " 211: 'vizsla, Hungarian pointer',\n",
      " 212: 'English setter',\n",
      " 213: 'Irish setter, red setter',\n",
      " 214: 'Gordon setter',\n",
      " 215: 'Brittany spaniel',\n",
      " 216: 'clumber, clumber spaniel',\n",
      " 217: 'English springer, English springer spaniel',\n",
      " 218: 'Welsh springer spaniel',\n",
      " 219: 'cocker spaniel, English cocker spaniel, cocker',\n",
      " 220: 'Sussex spaniel',\n",
      " 221: 'Irish water spaniel',\n",
      " 222: 'kuvasz',\n",
      " 223: 'schipperke',\n",
      " 224: 'groenendael',\n",
      " 225: 'malinois',\n",
      " 226: 'briard',\n",
      " 227: 'kelpie',\n",
      " 228: 'komondor',\n",
      " 229: 'Old English sheepdog, bobtail',\n",
      " 230: 'Shetland sheepdog, Shetland sheep dog, Shetland',\n",
      " 231: 'collie',\n",
      " 232: 'Border collie',\n",
      " 233: 'Bouvier des Flandres, Bouviers des Flandres',\n",
      " 234: 'Rottweiler',\n",
      " 235: 'German shepherd, German shepherd dog, German police dog, alsatian',\n",
      " 236: 'Doberman, Doberman pinscher',\n",
      " 237: 'miniature pinscher',\n",
      " 238: 'Greater Swiss Mountain dog',\n",
      " 239: 'Bernese mountain dog',\n",
      " 240: 'Appenzeller',\n",
      " 241: 'EntleBucher',\n",
      " 242: 'boxer',\n",
      " 243: 'bull mastiff',\n",
      " 244: 'Tibetan mastiff',\n",
      " 245: 'French bulldog',\n",
      " 246: 'Great Dane',\n",
      " 247: 'Saint Bernard, St Bernard',\n",
      " 248: 'Eskimo dog, husky',\n",
      " 249: 'malamute, malemute, Alaskan malamute',\n",
      " 250: 'Siberian husky',\n",
      " 251: 'dalmatian, coach dog, carriage dog',\n",
      " 252: 'affenpinscher, monkey pinscher, monkey dog',\n",
      " 253: 'basenji',\n",
      " 254: 'pug, pug-dog',\n",
      " 255: 'Leonberg',\n",
      " 256: 'Newfoundland, Newfoundland dog',\n",
      " 257: 'Great Pyrenees',\n",
      " 258: 'Samoyed, Samoyede',\n",
      " 259: 'Pomeranian',\n",
      " 260: 'chow, chow chow',\n",
      " 261: 'keeshond',\n",
      " 262: 'Brabancon griffon',\n",
      " 263: 'Pembroke, Pembroke Welsh corgi',\n",
      " 264: 'Cardigan, Cardigan Welsh corgi',\n",
      " 265: 'toy poodle',\n",
      " 266: 'miniature poodle',\n",
      " 267: 'standard poodle',\n",
      " 268: 'Mexican hairless',\n",
      " 269: 'timber wolf, grey wolf, gray wolf, Canis lupus',\n",
      " 270: 'white wolf, Arctic wolf, Canis lupus tundrarum',\n",
      " 271: 'red wolf, maned wolf, Canis rufus, Canis niger',\n",
      " 272: 'coyote, prairie wolf, brush wolf, Canis latrans',\n",
      " 273: 'dingo, warrigal, warragal, Canis dingo',\n",
      " 274: 'dhole, Cuon alpinus',\n",
      " 275: 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus',\n",
      " 276: 'hyena, hyaena',\n",
      " 277: 'red fox, Vulpes vulpes',\n",
      " 278: 'kit fox, Vulpes macrotis',\n",
      " 279: 'Arctic fox, white fox, Alopex lagopus',\n",
      " 280: 'grey fox, gray fox, Urocyon cinereoargenteus',\n",
      " 281: 'tabby, tabby cat',\n",
      " 282: 'tiger cat',\n",
      " 283: 'Persian cat',\n",
      " 284: 'Siamese cat, Siamese',\n",
      " 285: 'Egyptian cat',\n",
      " 286: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n",
      " 287: 'lynx, catamount',\n",
      " 288: 'leopard, Panthera pardus',\n",
      " 289: 'snow leopard, ounce, Panthera uncia',\n",
      " 290: 'jaguar, panther, Panthera onca, Felis onca',\n",
      " 291: 'lion, king of beasts, Panthera leo',\n",
      " 292: 'tiger, Panthera tigris',\n",
      " 293: 'cheetah, chetah, Acinonyx jubatus',\n",
      " 294: 'brown bear, bruin, Ursus arctos',\n",
      " 295: 'American black bear, black bear, Ursus americanus, Euarctos americanus',\n",
      " 296: 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus',\n",
      " 297: 'sloth bear, Melursus ursinus, Ursus ursinus',\n",
      " 298: 'mongoose',\n",
      " 299: 'meerkat, mierkat',\n",
      " 300: 'tiger beetle',\n",
      " 301: 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle',\n",
      " 302: 'ground beetle, carabid beetle',\n",
      " 303: 'long-horned beetle, longicorn, longicorn beetle',\n",
      " 304: 'leaf beetle, chrysomelid',\n",
      " 305: 'dung beetle',\n",
      " 306: 'rhinoceros beetle',\n",
      " 307: 'weevil',\n",
      " 308: 'fly',\n",
      " 309: 'bee',\n",
      " 310: 'ant, emmet, pismire',\n",
      " 311: 'grasshopper, hopper',\n",
      " 312: 'cricket',\n",
      " 313: 'walking stick, walkingstick, stick insect',\n",
      " 314: 'cockroach, roach',\n",
      " 315: 'mantis, mantid',\n",
      " 316: 'cicada, cicala',\n",
      " 317: 'leafhopper',\n",
      " 318: 'lacewing, lacewing fly',\n",
      " 319: \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\",\n",
      " 320: 'damselfly',\n",
      " 321: 'admiral',\n",
      " 322: 'ringlet, ringlet butterfly',\n",
      " 323: 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus',\n",
      " 324: 'cabbage butterfly',\n",
      " 325: 'sulphur butterfly, sulfur butterfly',\n",
      " 326: 'lycaenid, lycaenid butterfly',\n",
      " 327: 'starfish, sea star',\n",
      " 328: 'sea urchin',\n",
      " 329: 'sea cucumber, holothurian',\n",
      " 330: 'wood rabbit, cottontail, cottontail rabbit',\n",
      " 331: 'hare',\n",
      " 332: 'Angora, Angora rabbit',\n",
      " 333: 'hamster',\n",
      " 334: 'porcupine, hedgehog',\n",
      " 335: 'fox squirrel, eastern fox squirrel, Sciurus niger',\n",
      " 336: 'marmot',\n",
      " 337: 'beaver',\n",
      " 338: 'guinea pig, Cavia cobaya',\n",
      " 339: 'sorrel',\n",
      " 340: 'zebra',\n",
      " 341: 'hog, pig, grunter, squealer, Sus scrofa',\n",
      " 342: 'wild boar, boar, Sus scrofa',\n",
      " 343: 'warthog',\n",
      " 344: 'hippopotamus, hippo, river horse, Hippopotamus amphibius',\n",
      " 345: 'ox',\n",
      " 346: 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis',\n",
      " 347: 'bison',\n",
      " 348: 'ram, tup',\n",
      " 349: 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis',\n",
      " 350: 'ibex, Capra ibex',\n",
      " 351: 'hartebeest',\n",
      " 352: 'impala, Aepyceros melampus',\n",
      " 353: 'gazelle',\n",
      " 354: 'Arabian camel, dromedary, Camelus dromedarius',\n",
      " 355: 'llama',\n",
      " 356: 'weasel',\n",
      " 357: 'mink',\n",
      " 358: 'polecat, fitch, foulmart, foumart, Mustela putorius',\n",
      " 359: 'black-footed ferret, ferret, Mustela nigripes',\n",
      " 360: 'otter',\n",
      " 361: 'skunk, polecat, wood pussy',\n",
      " 362: 'badger',\n",
      " 363: 'armadillo',\n",
      " 364: 'three-toed sloth, ai, Bradypus tridactylus',\n",
      " 365: 'orangutan, orang, orangutang, Pongo pygmaeus',\n",
      " 366: 'gorilla, Gorilla gorilla',\n",
      " 367: 'chimpanzee, chimp, Pan troglodytes',\n",
      " 368: 'gibbon, Hylobates lar',\n",
      " 369: 'siamang, Hylobates syndactylus, Symphalangus syndactylus',\n",
      " 370: 'guenon, guenon monkey',\n",
      " 371: 'patas, hussar monkey, Erythrocebus patas',\n",
      " 372: 'baboon',\n",
      " 373: 'macaque',\n",
      " 374: 'langur',\n",
      " 375: 'colobus, colobus monkey',\n",
      " 376: 'proboscis monkey, Nasalis larvatus',\n",
      " 377: 'marmoset',\n",
      " 378: 'capuchin, ringtail, Cebus capucinus',\n",
      " 379: 'howler monkey, howler',\n",
      " 380: 'titi, titi monkey',\n",
      " 381: 'spider monkey, Ateles geoffroyi',\n",
      " 382: 'squirrel monkey, Saimiri sciureus',\n",
      " 383: 'Madagascar cat, ring-tailed lemur, Lemur catta',\n",
      " 384: 'indri, indris, Indri indri, Indri brevicaudatus',\n",
      " 385: 'Indian elephant, Elephas maximus',\n",
      " 386: 'African elephant, Loxodonta africana',\n",
      " 387: 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens',\n",
      " 388: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca',\n",
      " 389: 'barracouta, snoek',\n",
      " 390: 'eel',\n",
      " 391: 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch',\n",
      " 392: 'rock beauty, Holocanthus tricolor',\n",
      " 393: 'anemone fish',\n",
      " 394: 'sturgeon',\n",
      " 395: 'gar, garfish, garpike, billfish, Lepisosteus osseus',\n",
      " 396: 'lionfish',\n",
      " 397: 'puffer, pufferfish, blowfish, globefish',\n",
      " 398: 'abacus',\n",
      " 399: 'abaya',\n",
      " 400: \"academic gown, academic robe, judge's robe\",\n",
      " 401: 'accordion, piano accordion, squeeze box',\n",
      " 402: 'acoustic guitar',\n",
      " 403: 'aircraft carrier, carrier, flattop, attack aircraft carrier',\n",
      " 404: 'airliner',\n",
      " 405: 'airship, dirigible',\n",
      " 406: 'altar',\n",
      " 407: 'ambulance',\n",
      " 408: 'amphibian, amphibious vehicle',\n",
      " 409: 'analog clock',\n",
      " 410: 'apiary, bee house',\n",
      " 411: 'apron',\n",
      " 412: 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin',\n",
      " 413: 'assault rifle, assault gun',\n",
      " 414: 'backpack, back pack, knapsack, packsack, rucksack, haversack',\n",
      " 415: 'bakery, bakeshop, bakehouse',\n",
      " 416: 'balance beam, beam',\n",
      " 417: 'balloon',\n",
      " 418: 'ballpoint, ballpoint pen, ballpen, Biro',\n",
      " 419: 'Band Aid',\n",
      " 420: 'banjo',\n",
      " 421: 'bannister, banister, balustrade, balusters, handrail',\n",
      " 422: 'barbell',\n",
      " 423: 'barber chair',\n",
      " 424: 'barbershop',\n",
      " 425: 'barn',\n",
      " 426: 'barometer',\n",
      " 427: 'barrel, cask',\n",
      " 428: 'barrow, garden cart, lawn cart, wheelbarrow',\n",
      " 429: 'baseball',\n",
      " 430: 'basketball',\n",
      " 431: 'bassinet',\n",
      " 432: 'bassoon',\n",
      " 433: 'bathing cap, swimming cap',\n",
      " 434: 'bath towel',\n",
      " 435: 'bathtub, bathing tub, bath, tub',\n",
      " 436: 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon',\n",
      " 437: 'beacon, lighthouse, beacon light, pharos',\n",
      " 438: 'beaker',\n",
      " 439: 'bearskin, busby, shako',\n",
      " 440: 'beer bottle',\n",
      " 441: 'beer glass',\n",
      " 442: 'bell cote, bell cot',\n",
      " 443: 'bib',\n",
      " 444: 'bicycle-built-for-two, tandem bicycle, tandem',\n",
      " 445: 'bikini, two-piece',\n",
      " 446: 'binder, ring-binder',\n",
      " 447: 'binoculars, field glasses, opera glasses',\n",
      " 448: 'birdhouse',\n",
      " 449: 'boathouse',\n",
      " 450: 'bobsled, bobsleigh, bob',\n",
      " 451: 'bolo tie, bolo, bola tie, bola',\n",
      " 452: 'bonnet, poke bonnet',\n",
      " 453: 'bookcase',\n",
      " 454: 'bookshop, bookstore, bookstall',\n",
      " 455: 'bottlecap',\n",
      " 456: 'bow',\n",
      " 457: 'bow tie, bow-tie, bowtie',\n",
      " 458: 'brass, memorial tablet, plaque',\n",
      " 459: 'brassiere, bra, bandeau',\n",
      " 460: 'breakwater, groin, groyne, mole, bulwark, seawall, jetty',\n",
      " 461: 'breastplate, aegis, egis',\n",
      " 462: 'broom',\n",
      " 463: 'bucket, pail',\n",
      " 464: 'buckle',\n",
      " 465: 'bulletproof vest',\n",
      " 466: 'bullet train, bullet',\n",
      " 467: 'butcher shop, meat market',\n",
      " 468: 'cab, hack, taxi, taxicab',\n",
      " 469: 'caldron, cauldron',\n",
      " 470: 'candle, taper, wax light',\n",
      " 471: 'cannon',\n",
      " 472: 'canoe',\n",
      " 473: 'can opener, tin opener',\n",
      " 474: 'cardigan',\n",
      " 475: 'car mirror',\n",
      " 476: 'carousel, carrousel, merry-go-round, roundabout, whirligig',\n",
      " 477: \"carpenter's kit, tool kit\",\n",
      " 478: 'carton',\n",
      " 479: 'car wheel',\n",
      " 480: 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM',\n",
      " 481: 'cassette',\n",
      " 482: 'cassette player',\n",
      " 483: 'castle',\n",
      " 484: 'catamaran',\n",
      " 485: 'CD player',\n",
      " 486: 'cello, violoncello',\n",
      " 487: 'cellular telephone, cellular phone, cellphone, cell, mobile phone',\n",
      " 488: 'chain',\n",
      " 489: 'chainlink fence',\n",
      " 490: 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour',\n",
      " 491: 'chain saw, chainsaw',\n",
      " 492: 'chest',\n",
      " 493: 'chiffonier, commode',\n",
      " 494: 'chime, bell, gong',\n",
      " 495: 'china cabinet, china closet',\n",
      " 496: 'Christmas stocking',\n",
      " 497: 'church, church building',\n",
      " 498: 'cinema, movie theater, movie theatre, movie house, picture palace',\n",
      " 499: 'cleaver, meat cleaver, chopper',\n",
      " 500: 'cliff dwelling',\n",
      " 501: 'cloak',\n",
      " 502: 'clog, geta, patten, sabot',\n",
      " 503: 'cocktail shaker',\n",
      " 504: 'coffee mug',\n",
      " 505: 'coffeepot',\n",
      " 506: 'coil, spiral, volute, whorl, helix',\n",
      " 507: 'combination lock',\n",
      " 508: 'computer keyboard, keypad',\n",
      " 509: 'confectionery, confectionary, candy store',\n",
      " 510: 'container ship, containership, container vessel',\n",
      " 511: 'convertible',\n",
      " 512: 'corkscrew, bottle screw',\n",
      " 513: 'cornet, horn, trumpet, trump',\n",
      " 514: 'cowboy boot',\n",
      " 515: 'cowboy hat, ten-gallon hat',\n",
      " 516: 'cradle',\n",
      " 517: 'crane',\n",
      " 518: 'crash helmet',\n",
      " 519: 'crate',\n",
      " 520: 'crib, cot',\n",
      " 521: 'Crock Pot',\n",
      " 522: 'croquet ball',\n",
      " 523: 'crutch',\n",
      " 524: 'cuirass',\n",
      " 525: 'dam, dike, dyke',\n",
      " 526: 'desk',\n",
      " 527: 'desktop computer',\n",
      " 528: 'dial telephone, dial phone',\n",
      " 529: 'diaper, nappy, napkin',\n",
      " 530: 'digital clock',\n",
      " 531: 'digital watch',\n",
      " 532: 'dining table, board',\n",
      " 533: 'dishrag, dishcloth',\n",
      " 534: 'dishwasher, dish washer, dishwashing machine',\n",
      " 535: 'disk brake, disc brake',\n",
      " 536: 'dock, dockage, docking facility',\n",
      " 537: 'dogsled, dog sled, dog sleigh',\n",
      " 538: 'dome',\n",
      " 539: 'doormat, welcome mat',\n",
      " 540: 'drilling platform, offshore rig',\n",
      " 541: 'drum, membranophone, tympan',\n",
      " 542: 'drumstick',\n",
      " 543: 'dumbbell',\n",
      " 544: 'Dutch oven',\n",
      " 545: 'electric fan, blower',\n",
      " 546: 'electric guitar',\n",
      " 547: 'electric locomotive',\n",
      " 548: 'entertainment center',\n",
      " 549: 'envelope',\n",
      " 550: 'espresso maker',\n",
      " 551: 'face powder',\n",
      " 552: 'feather boa, boa',\n",
      " 553: 'file, file cabinet, filing cabinet',\n",
      " 554: 'fireboat',\n",
      " 555: 'fire engine, fire truck',\n",
      " 556: 'fire screen, fireguard',\n",
      " 557: 'flagpole, flagstaff',\n",
      " 558: 'flute, transverse flute',\n",
      " 559: 'folding chair',\n",
      " 560: 'football helmet',\n",
      " 561: 'forklift',\n",
      " 562: 'fountain',\n",
      " 563: 'fountain pen',\n",
      " 564: 'four-poster',\n",
      " 565: 'freight car',\n",
      " 566: 'French horn, horn',\n",
      " 567: 'frying pan, frypan, skillet',\n",
      " 568: 'fur coat',\n",
      " 569: 'garbage truck, dustcart',\n",
      " 570: 'gasmask, respirator, gas helmet',\n",
      " 571: 'gas pump, gasoline pump, petrol pump, island dispenser',\n",
      " 572: 'goblet',\n",
      " 573: 'go-kart',\n",
      " 574: 'golf ball',\n",
      " 575: 'golfcart, golf cart',\n",
      " 576: 'gondola',\n",
      " 577: 'gong, tam-tam',\n",
      " 578: 'gown',\n",
      " 579: 'grand piano, grand',\n",
      " 580: 'greenhouse, nursery, glasshouse',\n",
      " 581: 'grille, radiator grille',\n",
      " 582: 'grocery store, grocery, food market, market',\n",
      " 583: 'guillotine',\n",
      " 584: 'hair slide',\n",
      " 585: 'hair spray',\n",
      " 586: 'half track',\n",
      " 587: 'hammer',\n",
      " 588: 'hamper',\n",
      " 589: 'hand blower, blow dryer, blow drier, hair dryer, hair drier',\n",
      " 590: 'hand-held computer, hand-held microcomputer',\n",
      " 591: 'handkerchief, hankie, hanky, hankey',\n",
      " 592: 'hard disc, hard disk, fixed disk',\n",
      " 593: 'harmonica, mouth organ, harp, mouth harp',\n",
      " 594: 'harp',\n",
      " 595: 'harvester, reaper',\n",
      " 596: 'hatchet',\n",
      " 597: 'holster',\n",
      " 598: 'home theater, home theatre',\n",
      " 599: 'honeycomb',\n",
      " 600: 'hook, claw',\n",
      " 601: 'hoopskirt, crinoline',\n",
      " 602: 'horizontal bar, high bar',\n",
      " 603: 'horse cart, horse-cart',\n",
      " 604: 'hourglass',\n",
      " 605: 'iPod',\n",
      " 606: 'iron, smoothing iron',\n",
      " 607: \"jack-o'-lantern\",\n",
      " 608: 'jean, blue jean, denim',\n",
      " 609: 'jeep, landrover',\n",
      " 610: 'jersey, T-shirt, tee shirt',\n",
      " 611: 'jigsaw puzzle',\n",
      " 612: 'jinrikisha, ricksha, rickshaw',\n",
      " 613: 'joystick',\n",
      " 614: 'kimono',\n",
      " 615: 'knee pad',\n",
      " 616: 'knot',\n",
      " 617: 'lab coat, laboratory coat',\n",
      " 618: 'ladle',\n",
      " 619: 'lampshade, lamp shade',\n",
      " 620: 'laptop, laptop computer',\n",
      " 621: 'lawn mower, mower',\n",
      " 622: 'lens cap, lens cover',\n",
      " 623: 'letter opener, paper knife, paperknife',\n",
      " 624: 'library',\n",
      " 625: 'lifeboat',\n",
      " 626: 'lighter, light, igniter, ignitor',\n",
      " 627: 'limousine, limo',\n",
      " 628: 'liner, ocean liner',\n",
      " 629: 'lipstick, lip rouge',\n",
      " 630: 'Loafer',\n",
      " 631: 'lotion',\n",
      " 632: 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system',\n",
      " 633: \"loupe, jeweler's loupe\",\n",
      " 634: 'lumbermill, sawmill',\n",
      " 635: 'magnetic compass',\n",
      " 636: 'mailbag, postbag',\n",
      " 637: 'mailbox, letter box',\n",
      " 638: 'maillot',\n",
      " 639: 'maillot, tank suit',\n",
      " 640: 'manhole cover',\n",
      " 641: 'maraca',\n",
      " 642: 'marimba, xylophone',\n",
      " 643: 'mask',\n",
      " 644: 'matchstick',\n",
      " 645: 'maypole',\n",
      " 646: 'maze, labyrinth',\n",
      " 647: 'measuring cup',\n",
      " 648: 'medicine chest, medicine cabinet',\n",
      " 649: 'megalith, megalithic structure',\n",
      " 650: 'microphone, mike',\n",
      " 651: 'microwave, microwave oven',\n",
      " 652: 'military uniform',\n",
      " 653: 'milk can',\n",
      " 654: 'minibus',\n",
      " 655: 'miniskirt, mini',\n",
      " 656: 'minivan',\n",
      " 657: 'missile',\n",
      " 658: 'mitten',\n",
      " 659: 'mixing bowl',\n",
      " 660: 'mobile home, manufactured home',\n",
      " 661: 'Model T',\n",
      " 662: 'modem',\n",
      " 663: 'monastery',\n",
      " 664: 'monitor',\n",
      " 665: 'moped',\n",
      " 666: 'mortar',\n",
      " 667: 'mortarboard',\n",
      " 668: 'mosque',\n",
      " 669: 'mosquito net',\n",
      " 670: 'motor scooter, scooter',\n",
      " 671: 'mountain bike, all-terrain bike, off-roader',\n",
      " 672: 'mountain tent',\n",
      " 673: 'mouse, computer mouse',\n",
      " 674: 'mousetrap',\n",
      " 675: 'moving van',\n",
      " 676: 'muzzle',\n",
      " 677: 'nail',\n",
      " 678: 'neck brace',\n",
      " 679: 'necklace',\n",
      " 680: 'nipple',\n",
      " 681: 'notebook, notebook computer',\n",
      " 682: 'obelisk',\n",
      " 683: 'oboe, hautboy, hautbois',\n",
      " 684: 'ocarina, sweet potato',\n",
      " 685: 'odometer, hodometer, mileometer, milometer',\n",
      " 686: 'oil filter',\n",
      " 687: 'organ, pipe organ',\n",
      " 688: 'oscilloscope, scope, cathode-ray oscilloscope, CRO',\n",
      " 689: 'overskirt',\n",
      " 690: 'oxcart',\n",
      " 691: 'oxygen mask',\n",
      " 692: 'packet',\n",
      " 693: 'paddle, boat paddle',\n",
      " 694: 'paddlewheel, paddle wheel',\n",
      " 695: 'padlock',\n",
      " 696: 'paintbrush',\n",
      " 697: \"pajama, pyjama, pj's, jammies\",\n",
      " 698: 'palace',\n",
      " 699: 'panpipe, pandean pipe, syrinx',\n",
      " 700: 'paper towel',\n",
      " 701: 'parachute, chute',\n",
      " 702: 'parallel bars, bars',\n",
      " 703: 'park bench',\n",
      " 704: 'parking meter',\n",
      " 705: 'passenger car, coach, carriage',\n",
      " 706: 'patio, terrace',\n",
      " 707: 'pay-phone, pay-station',\n",
      " 708: 'pedestal, plinth, footstall',\n",
      " 709: 'pencil box, pencil case',\n",
      " 710: 'pencil sharpener',\n",
      " 711: 'perfume, essence',\n",
      " 712: 'Petri dish',\n",
      " 713: 'photocopier',\n",
      " 714: 'pick, plectrum, plectron',\n",
      " 715: 'pickelhaube',\n",
      " 716: 'picket fence, paling',\n",
      " 717: 'pickup, pickup truck',\n",
      " 718: 'pier',\n",
      " 719: 'piggy bank, penny bank',\n",
      " 720: 'pill bottle',\n",
      " 721: 'pillow',\n",
      " 722: 'ping-pong ball',\n",
      " 723: 'pinwheel',\n",
      " 724: 'pirate, pirate ship',\n",
      " 725: 'pitcher, ewer',\n",
      " 726: \"plane, carpenter's plane, woodworking plane\",\n",
      " 727: 'planetarium',\n",
      " 728: 'plastic bag',\n",
      " 729: 'plate rack',\n",
      " 730: 'plow, plough',\n",
      " 731: \"plunger, plumber's helper\",\n",
      " 732: 'Polaroid camera, Polaroid Land camera',\n",
      " 733: 'pole',\n",
      " 734: 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria',\n",
      " 735: 'poncho',\n",
      " 736: 'pool table, billiard table, snooker table',\n",
      " 737: 'pop bottle, soda bottle',\n",
      " 738: 'pot, flowerpot',\n",
      " 739: \"potter's wheel\",\n",
      " 740: 'power drill',\n",
      " 741: 'prayer rug, prayer mat',\n",
      " 742: 'printer',\n",
      " 743: 'prison, prison house',\n",
      " 744: 'projectile, missile',\n",
      " 745: 'projector',\n",
      " 746: 'puck, hockey puck',\n",
      " 747: 'punching bag, punch bag, punching ball, punchball',\n",
      " 748: 'purse',\n",
      " 749: 'quill, quill pen',\n",
      " 750: 'quilt, comforter, comfort, puff',\n",
      " 751: 'racer, race car, racing car',\n",
      " 752: 'racket, racquet',\n",
      " 753: 'radiator',\n",
      " 754: 'radio, wireless',\n",
      " 755: 'radio telescope, radio reflector',\n",
      " 756: 'rain barrel',\n",
      " 757: 'recreational vehicle, RV, R.V.',\n",
      " 758: 'reel',\n",
      " 759: 'reflex camera',\n",
      " 760: 'refrigerator, icebox',\n",
      " 761: 'remote control, remote',\n",
      " 762: 'restaurant, eating house, eating place, eatery',\n",
      " 763: 'revolver, six-gun, six-shooter',\n",
      " 764: 'rifle',\n",
      " 765: 'rocking chair, rocker',\n",
      " 766: 'rotisserie',\n",
      " 767: 'rubber eraser, rubber, pencil eraser',\n",
      " 768: 'rugby ball',\n",
      " 769: 'rule, ruler',\n",
      " 770: 'running shoe',\n",
      " 771: 'safe',\n",
      " 772: 'safety pin',\n",
      " 773: 'saltshaker, salt shaker',\n",
      " 774: 'sandal',\n",
      " 775: 'sarong',\n",
      " 776: 'sax, saxophone',\n",
      " 777: 'scabbard',\n",
      " 778: 'scale, weighing machine',\n",
      " 779: 'school bus',\n",
      " 780: 'schooner',\n",
      " 781: 'scoreboard',\n",
      " 782: 'screen, CRT screen',\n",
      " 783: 'screw',\n",
      " 784: 'screwdriver',\n",
      " 785: 'seat belt, seatbelt',\n",
      " 786: 'sewing machine',\n",
      " 787: 'shield, buckler',\n",
      " 788: 'shoe shop, shoe-shop, shoe store',\n",
      " 789: 'shoji',\n",
      " 790: 'shopping basket',\n",
      " 791: 'shopping cart',\n",
      " 792: 'shovel',\n",
      " 793: 'shower cap',\n",
      " 794: 'shower curtain',\n",
      " 795: 'ski',\n",
      " 796: 'ski mask',\n",
      " 797: 'sleeping bag',\n",
      " 798: 'slide rule, slipstick',\n",
      " 799: 'sliding door',\n",
      " 800: 'slot, one-armed bandit',\n",
      " 801: 'snorkel',\n",
      " 802: 'snowmobile',\n",
      " 803: 'snowplow, snowplough',\n",
      " 804: 'soap dispenser',\n",
      " 805: 'soccer ball',\n",
      " 806: 'sock',\n",
      " 807: 'solar dish, solar collector, solar furnace',\n",
      " 808: 'sombrero',\n",
      " 809: 'soup bowl',\n",
      " 810: 'space bar',\n",
      " 811: 'space heater',\n",
      " 812: 'space shuttle',\n",
      " 813: 'spatula',\n",
      " 814: 'speedboat',\n",
      " 815: \"spider web, spider's web\",\n",
      " 816: 'spindle',\n",
      " 817: 'sports car, sport car',\n",
      " 818: 'spotlight, spot',\n",
      " 819: 'stage',\n",
      " 820: 'steam locomotive',\n",
      " 821: 'steel arch bridge',\n",
      " 822: 'steel drum',\n",
      " 823: 'stethoscope',\n",
      " 824: 'stole',\n",
      " 825: 'stone wall',\n",
      " 826: 'stopwatch, stop watch',\n",
      " 827: 'stove',\n",
      " 828: 'strainer',\n",
      " 829: 'streetcar, tram, tramcar, trolley, trolley car',\n",
      " 830: 'stretcher',\n",
      " 831: 'studio couch, day bed',\n",
      " 832: 'stupa, tope',\n",
      " 833: 'submarine, pigboat, sub, U-boat',\n",
      " 834: 'suit, suit of clothes',\n",
      " 835: 'sundial',\n",
      " 836: 'sunglass',\n",
      " 837: 'sunglasses, dark glasses, shades',\n",
      " 838: 'sunscreen, sunblock, sun blocker',\n",
      " 839: 'suspension bridge',\n",
      " 840: 'swab, swob, mop',\n",
      " 841: 'sweatshirt',\n",
      " 842: 'swimming trunks, bathing trunks',\n",
      " 843: 'swing',\n",
      " 844: 'switch, electric switch, electrical switch',\n",
      " 845: 'syringe',\n",
      " 846: 'table lamp',\n",
      " 847: 'tank, army tank, armored combat vehicle, armoured combat vehicle',\n",
      " 848: 'tape player',\n",
      " 849: 'teapot',\n",
      " 850: 'teddy, teddy bear',\n",
      " 851: 'television, television system',\n",
      " 852: 'tennis ball',\n",
      " 853: 'thatch, thatched roof',\n",
      " 854: 'theater curtain, theatre curtain',\n",
      " 855: 'thimble',\n",
      " 856: 'thresher, thrasher, threshing machine',\n",
      " 857: 'throne',\n",
      " 858: 'tile roof',\n",
      " 859: 'toaster',\n",
      " 860: 'tobacco shop, tobacconist shop, tobacconist',\n",
      " 861: 'toilet seat',\n",
      " 862: 'torch',\n",
      " 863: 'totem pole',\n",
      " 864: 'tow truck, tow car, wrecker',\n",
      " 865: 'toyshop',\n",
      " 866: 'tractor',\n",
      " 867: 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi',\n",
      " 868: 'tray',\n",
      " 869: 'trench coat',\n",
      " 870: 'tricycle, trike, velocipede',\n",
      " 871: 'trimaran',\n",
      " 872: 'tripod',\n",
      " 873: 'triumphal arch',\n",
      " 874: 'trolleybus, trolley coach, trackless trolley',\n",
      " 875: 'trombone',\n",
      " 876: 'tub, vat',\n",
      " 877: 'turnstile',\n",
      " 878: 'typewriter keyboard',\n",
      " 879: 'umbrella',\n",
      " 880: 'unicycle, monocycle',\n",
      " 881: 'upright, upright piano',\n",
      " 882: 'vacuum, vacuum cleaner',\n",
      " 883: 'vase',\n",
      " 884: 'vault',\n",
      " 885: 'velvet',\n",
      " 886: 'vending machine',\n",
      " 887: 'vestment',\n",
      " 888: 'viaduct',\n",
      " 889: 'violin, fiddle',\n",
      " 890: 'volleyball',\n",
      " 891: 'waffle iron',\n",
      " 892: 'wall clock',\n",
      " 893: 'wallet, billfold, notecase, pocketbook',\n",
      " 894: 'wardrobe, closet, press',\n",
      " 895: 'warplane, military plane',\n",
      " 896: 'washbasin, handbasin, washbowl, lavabo, wash-hand basin',\n",
      " 897: 'washer, automatic washer, washing machine',\n",
      " 898: 'water bottle',\n",
      " 899: 'water jug',\n",
      " 900: 'water tower',\n",
      " 901: 'whiskey jug',\n",
      " 902: 'whistle',\n",
      " 903: 'wig',\n",
      " 904: 'window screen',\n",
      " 905: 'window shade',\n",
      " 906: 'Windsor tie',\n",
      " 907: 'wine bottle',\n",
      " 908: 'wing',\n",
      " 909: 'wok',\n",
      " 910: 'wooden spoon',\n",
      " 911: 'wool, woolen, woollen',\n",
      " 912: 'worm fence, snake fence, snake-rail fence, Virginia fence',\n",
      " 913: 'wreck',\n",
      " 914: 'yawl',\n",
      " 915: 'yurt',\n",
      " 916: 'web site, website, internet site, site',\n",
      " 917: 'comic book',\n",
      " 918: 'crossword puzzle, crossword',\n",
      " 919: 'street sign',\n",
      " 920: 'traffic light, traffic signal, stoplight',\n",
      " 921: 'book jacket, dust cover, dust jacket, dust wrapper',\n",
      " 922: 'menu',\n",
      " 923: 'plate',\n",
      " 924: 'guacamole',\n",
      " 925: 'consomme',\n",
      " 926: 'hot pot, hotpot',\n",
      " 927: 'trifle',\n",
      " 928: 'ice cream, icecream',\n",
      " 929: 'ice lolly, lolly, lollipop, popsicle',\n",
      " 930: 'French loaf',\n",
      " 931: 'bagel, beigel',\n",
      " 932: 'pretzel',\n",
      " 933: 'cheeseburger',\n",
      " 934: 'hotdog, hot dog, red hot',\n",
      " 935: 'mashed potato',\n",
      " 936: 'head cabbage',\n",
      " 937: 'broccoli',\n",
      " 938: 'cauliflower',\n",
      " 939: 'zucchini, courgette',\n",
      " 940: 'spaghetti squash',\n",
      " 941: 'acorn squash',\n",
      " 942: 'butternut squash',\n",
      " 943: 'cucumber, cuke',\n",
      " 944: 'artichoke, globe artichoke',\n",
      " 945: 'bell pepper',\n",
      " 946: 'cardoon',\n",
      " 947: 'mushroom',\n",
      " 948: 'Granny Smith',\n",
      " 949: 'strawberry',\n",
      " 950: 'orange',\n",
      " 951: 'lemon',\n",
      " 952: 'fig',\n",
      " 953: 'pineapple, ananas',\n",
      " 954: 'banana',\n",
      " 955: 'jackfruit, jak, jack',\n",
      " 956: 'custard apple',\n",
      " 957: 'pomegranate',\n",
      " 958: 'hay',\n",
      " 959: 'carbonara',\n",
      " 960: 'chocolate sauce, chocolate syrup',\n",
      " 961: 'dough',\n",
      " 962: 'meat loaf, meatloaf',\n",
      " 963: 'pizza, pizza pie',\n",
      " 964: 'potpie',\n",
      " 965: 'burrito',\n",
      " 966: 'red wine',\n",
      " 967: 'espresso',\n",
      " 968: 'cup',\n",
      " 969: 'eggnog',\n",
      " 970: 'alp',\n",
      " 971: 'bubble',\n",
      " 972: 'cliff, drop, drop-off',\n",
      " 973: 'coral reef',\n",
      " 974: 'geyser',\n",
      " 975: 'lakeside, lakeshore',\n",
      " 976: 'promontory, headland, head, foreland',\n",
      " 977: 'sandbar, sand bar',\n",
      " 978: 'seashore, coast, seacoast, sea-coast',\n",
      " 979: 'valley, vale',\n",
      " 980: 'volcano',\n",
      " 981: 'ballplayer, baseball player',\n",
      " 982: 'groom, bridegroom',\n",
      " 983: 'scuba diver',\n",
      " 984: 'rapeseed',\n",
      " 985: 'daisy',\n",
      " 986: \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\",\n",
      " 987: 'corn',\n",
      " 988: 'acorn',\n",
      " 989: 'hip, rose hip, rosehip',\n",
      " 990: 'buckeye, horse chestnut, conker',\n",
      " 991: 'coral fungus',\n",
      " 992: 'agaric',\n",
      " 993: 'gyromitra',\n",
      " 994: 'stinkhorn, carrion fungus',\n",
      " 995: 'earthstar',\n",
      " 996: 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa',\n",
      " 997: 'bolete',\n",
      " 998: 'ear, spike, capitulum',\n",
      " 999: 'toilet tissue, toilet paper, bathroom tissue'}\n",
      "72/2:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 380\n",
      "\n",
      "def prep_image(image):\n",
      "    res = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "    res = res.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "    return res\n",
      "\n",
      "def predict(image):\n",
      "    cropped_image = center_crop(image)\n",
      "    prediction = (model.predict(prep_image(image)), model.predict(prep_image(cropped_image)))\n",
      "    if np.max(prediction[0]>=prediction[1]):\n",
      "        return {'class': np.argmax(prediction[0], axis=1)[0], 'all_prob': prediction[0]}\n",
      "    else:\n",
      "        return  {'class': np.argmax(prediction[1], axis=1)[1], 'all_prob': prediction[1]}\n",
      "    \n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "plt.imshow(image)\n",
      "\n",
      "print(class_dict[predict(image)])\n",
      "73/1:\n",
      "from tensorflow.keras.applications import EfficientNetB4\n",
      "import tensorflow as tf\n",
      "model = EfficientNetB4(weights='imagenet')\n",
      "\n",
      "def center_crop(img, new_width=None, new_height=None):        \n",
      "\n",
      "    width = img.shape[1]\n",
      "    height = img.shape[0]\n",
      "\n",
      "    if new_width is None:\n",
      "        new_width = min(width, height)\n",
      "\n",
      "    if new_height is None:\n",
      "        new_height = min(width, height)\n",
      "\n",
      "    left = int(np.ceil((width - new_width) / 2))\n",
      "    right = width - int(np.floor((width - new_width) / 2))\n",
      "\n",
      "    top = int(np.ceil((height - new_height) / 2))\n",
      "    bottom = height - int(np.floor((height - new_height) / 2))\n",
      "\n",
      "    if len(img.shape) == 2:\n",
      "        center_cropped_img = img[top:bottom, left:right]\n",
      "    else:\n",
      "        center_cropped_img = img[top:bottom, left:right, ...]\n",
      "\n",
      "    return center_cropped_img\n",
      "\n",
      "class_dict = {0: 'tench, Tinca tinca',\n",
      " 1: 'goldfish, Carassius auratus',\n",
      " 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n",
      " 3: 'tiger shark, Galeocerdo cuvieri',\n",
      " 4: 'hammerhead, hammerhead shark',\n",
      " 5: 'electric ray, crampfish, numbfish, torpedo',\n",
      " 6: 'stingray',\n",
      " 7: 'cock',\n",
      " 8: 'hen',\n",
      " 9: 'ostrich, Struthio camelus',\n",
      " 10: 'brambling, Fringilla montifringilla',\n",
      " 11: 'goldfinch, Carduelis carduelis',\n",
      " 12: 'house finch, linnet, Carpodacus mexicanus',\n",
      " 13: 'junco, snowbird',\n",
      " 14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea',\n",
      " 15: 'robin, American robin, Turdus migratorius',\n",
      " 16: 'bulbul',\n",
      " 17: 'jay',\n",
      " 18: 'magpie',\n",
      " 19: 'chickadee',\n",
      " 20: 'water ouzel, dipper',\n",
      " 21: 'kite',\n",
      " 22: 'bald eagle, American eagle, Haliaeetus leucocephalus',\n",
      " 23: 'vulture',\n",
      " 24: 'great grey owl, great gray owl, Strix nebulosa',\n",
      " 25: 'European fire salamander, Salamandra salamandra',\n",
      " 26: 'common newt, Triturus vulgaris',\n",
      " 27: 'eft',\n",
      " 28: 'spotted salamander, Ambystoma maculatum',\n",
      " 29: 'axolotl, mud puppy, Ambystoma mexicanum',\n",
      " 30: 'bullfrog, Rana catesbeiana',\n",
      " 31: 'tree frog, tree-frog',\n",
      " 32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui',\n",
      " 33: 'loggerhead, loggerhead turtle, Caretta caretta',\n",
      " 34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea',\n",
      " 35: 'mud turtle',\n",
      " 36: 'terrapin',\n",
      " 37: 'box turtle, box tortoise',\n",
      " 38: 'banded gecko',\n",
      " 39: 'common iguana, iguana, Iguana iguana',\n",
      " 40: 'American chameleon, anole, Anolis carolinensis',\n",
      " 41: 'whiptail, whiptail lizard',\n",
      " 42: 'agama',\n",
      " 43: 'frilled lizard, Chlamydosaurus kingi',\n",
      " 44: 'alligator lizard',\n",
      " 45: 'Gila monster, Heloderma suspectum',\n",
      " 46: 'green lizard, Lacerta viridis',\n",
      " 47: 'African chameleon, Chamaeleo chamaeleon',\n",
      " 48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis',\n",
      " 49: 'African crocodile, Nile crocodile, Crocodylus niloticus',\n",
      " 50: 'American alligator, Alligator mississipiensis',\n",
      " 51: 'triceratops',\n",
      " 52: 'thunder snake, worm snake, Carphophis amoenus',\n",
      " 53: 'ringneck snake, ring-necked snake, ring snake',\n",
      " 54: 'hognose snake, puff adder, sand viper',\n",
      " 55: 'green snake, grass snake',\n",
      " 56: 'king snake, kingsnake',\n",
      " 57: 'garter snake, grass snake',\n",
      " 58: 'water snake',\n",
      " 59: 'vine snake',\n",
      " 60: 'night snake, Hypsiglena torquata',\n",
      " 61: 'boa constrictor, Constrictor constrictor',\n",
      " 62: 'rock python, rock snake, Python sebae',\n",
      " 63: 'Indian cobra, Naja naja',\n",
      " 64: 'green mamba',\n",
      " 65: 'sea snake',\n",
      " 66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus',\n",
      " 67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus',\n",
      " 68: 'sidewinder, horned rattlesnake, Crotalus cerastes',\n",
      " 69: 'trilobite',\n",
      " 70: 'harvestman, daddy longlegs, Phalangium opilio',\n",
      " 71: 'scorpion',\n",
      " 72: 'black and gold garden spider, Argiope aurantia',\n",
      " 73: 'barn spider, Araneus cavaticus',\n",
      " 74: 'garden spider, Aranea diademata',\n",
      " 75: 'black widow, Latrodectus mactans',\n",
      " 76: 'tarantula',\n",
      " 77: 'wolf spider, hunting spider',\n",
      " 78: 'tick',\n",
      " 79: 'centipede',\n",
      " 80: 'black grouse',\n",
      " 81: 'ptarmigan',\n",
      " 82: 'ruffed grouse, partridge, Bonasa umbellus',\n",
      " 83: 'prairie chicken, prairie grouse, prairie fowl',\n",
      " 84: 'peacock',\n",
      " 85: 'quail',\n",
      " 86: 'partridge',\n",
      " 87: 'African grey, African gray, Psittacus erithacus',\n",
      " 88: 'macaw',\n",
      " 89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n",
      " 90: 'lorikeet',\n",
      " 91: 'coucal',\n",
      " 92: 'bee eater',\n",
      " 93: 'hornbill',\n",
      " 94: 'hummingbird',\n",
      " 95: 'jacamar',\n",
      " 96: 'toucan',\n",
      " 97: 'drake',\n",
      " 98: 'red-breasted merganser, Mergus serrator',\n",
      " 99: 'goose',\n",
      " 100: 'black swan, Cygnus atratus',\n",
      " 101: 'tusker',\n",
      " 102: 'echidna, spiny anteater, anteater',\n",
      " 103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n",
      " 104: 'wallaby, brush kangaroo',\n",
      " 105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n",
      " 106: 'wombat',\n",
      " 107: 'jellyfish',\n",
      " 108: 'sea anemone, anemone',\n",
      " 109: 'brain coral',\n",
      " 110: 'flatworm, platyhelminth',\n",
      " 111: 'nematode, nematode worm, roundworm',\n",
      " 112: 'conch',\n",
      " 113: 'snail',\n",
      " 114: 'slug',\n",
      " 115: 'sea slug, nudibranch',\n",
      " 116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore',\n",
      " 117: 'chambered nautilus, pearly nautilus, nautilus',\n",
      " 118: 'Dungeness crab, Cancer magister',\n",
      " 119: 'rock crab, Cancer irroratus',\n",
      " 120: 'fiddler crab',\n",
      " 121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica',\n",
      " 122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus',\n",
      " 123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish',\n",
      " 124: 'crayfish, crawfish, crawdad, crawdaddy',\n",
      " 125: 'hermit crab',\n",
      " 126: 'isopod',\n",
      " 127: 'white stork, Ciconia ciconia',\n",
      " 128: 'black stork, Ciconia nigra',\n",
      " 129: 'spoonbill',\n",
      " 130: 'flamingo',\n",
      " 131: 'little blue heron, Egretta caerulea',\n",
      " 132: 'American egret, great white heron, Egretta albus',\n",
      " 133: 'bittern',\n",
      " 134: 'crane',\n",
      " 135: 'limpkin, Aramus pictus',\n",
      " 136: 'European gallinule, Porphyrio porphyrio',\n",
      " 137: 'American coot, marsh hen, mud hen, water hen, Fulica americana',\n",
      " 138: 'bustard',\n",
      " 139: 'ruddy turnstone, Arenaria interpres',\n",
      " 140: 'red-backed sandpiper, dunlin, Erolia alpina',\n",
      " 141: 'redshank, Tringa totanus',\n",
      " 142: 'dowitcher',\n",
      " 143: 'oystercatcher, oyster catcher',\n",
      " 144: 'pelican',\n",
      " 145: 'king penguin, Aptenodytes patagonica',\n",
      " 146: 'albatross, mollymawk',\n",
      " 147: 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus',\n",
      " 148: 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca',\n",
      " 149: 'dugong, Dugong dugon',\n",
      " 150: 'sea lion',\n",
      " 151: 'Chihuahua',\n",
      " 152: 'Japanese spaniel',\n",
      " 153: 'Maltese dog, Maltese terrier, Maltese',\n",
      " 154: 'Pekinese, Pekingese, Peke',\n",
      " 155: 'Shih-Tzu',\n",
      " 156: 'Blenheim spaniel',\n",
      " 157: 'papillon',\n",
      " 158: 'toy terrier',\n",
      " 159: 'Rhodesian ridgeback',\n",
      " 160: 'Afghan hound, Afghan',\n",
      " 161: 'basset, basset hound',\n",
      " 162: 'beagle',\n",
      " 163: 'bloodhound, sleuthhound',\n",
      " 164: 'bluetick',\n",
      " 165: 'black-and-tan coonhound',\n",
      " 166: 'Walker hound, Walker foxhound',\n",
      " 167: 'English foxhound',\n",
      " 168: 'redbone',\n",
      " 169: 'borzoi, Russian wolfhound',\n",
      " 170: 'Irish wolfhound',\n",
      " 171: 'Italian greyhound',\n",
      " 172: 'whippet',\n",
      " 173: 'Ibizan hound, Ibizan Podenco',\n",
      " 174: 'Norwegian elkhound, elkhound',\n",
      " 175: 'otterhound, otter hound',\n",
      " 176: 'Saluki, gazelle hound',\n",
      " 177: 'Scottish deerhound, deerhound',\n",
      " 178: 'Weimaraner',\n",
      " 179: 'Staffordshire bullterrier, Staffordshire bull terrier',\n",
      " 180: 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier',\n",
      " 181: 'Bedlington terrier',\n",
      " 182: 'Border terrier',\n",
      " 183: 'Kerry blue terrier',\n",
      " 184: 'Irish terrier',\n",
      " 185: 'Norfolk terrier',\n",
      " 186: 'Norwich terrier',\n",
      " 187: 'Yorkshire terrier',\n",
      " 188: 'wire-haired fox terrier',\n",
      " 189: 'Lakeland terrier',\n",
      " 190: 'Sealyham terrier, Sealyham',\n",
      " 191: 'Airedale, Airedale terrier',\n",
      " 192: 'cairn, cairn terrier',\n",
      " 193: 'Australian terrier',\n",
      " 194: 'Dandie Dinmont, Dandie Dinmont terrier',\n",
      " 195: 'Boston bull, Boston terrier',\n",
      " 196: 'miniature schnauzer',\n",
      " 197: 'giant schnauzer',\n",
      " 198: 'standard schnauzer',\n",
      " 199: 'Scotch terrier, Scottish terrier, Scottie',\n",
      " 200: 'Tibetan terrier, chrysanthemum dog',\n",
      " 201: 'silky terrier, Sydney silky',\n",
      " 202: 'soft-coated wheaten terrier',\n",
      " 203: 'West Highland white terrier',\n",
      " 204: 'Lhasa, Lhasa apso',\n",
      " 205: 'flat-coated retriever',\n",
      " 206: 'curly-coated retriever',\n",
      " 207: 'golden retriever',\n",
      " 208: 'Labrador retriever',\n",
      " 209: 'Chesapeake Bay retriever',\n",
      " 210: 'German short-haired pointer',\n",
      " 211: 'vizsla, Hungarian pointer',\n",
      " 212: 'English setter',\n",
      " 213: 'Irish setter, red setter',\n",
      " 214: 'Gordon setter',\n",
      " 215: 'Brittany spaniel',\n",
      " 216: 'clumber, clumber spaniel',\n",
      " 217: 'English springer, English springer spaniel',\n",
      " 218: 'Welsh springer spaniel',\n",
      " 219: 'cocker spaniel, English cocker spaniel, cocker',\n",
      " 220: 'Sussex spaniel',\n",
      " 221: 'Irish water spaniel',\n",
      " 222: 'kuvasz',\n",
      " 223: 'schipperke',\n",
      " 224: 'groenendael',\n",
      " 225: 'malinois',\n",
      " 226: 'briard',\n",
      " 227: 'kelpie',\n",
      " 228: 'komondor',\n",
      " 229: 'Old English sheepdog, bobtail',\n",
      " 230: 'Shetland sheepdog, Shetland sheep dog, Shetland',\n",
      " 231: 'collie',\n",
      " 232: 'Border collie',\n",
      " 233: 'Bouvier des Flandres, Bouviers des Flandres',\n",
      " 234: 'Rottweiler',\n",
      " 235: 'German shepherd, German shepherd dog, German police dog, alsatian',\n",
      " 236: 'Doberman, Doberman pinscher',\n",
      " 237: 'miniature pinscher',\n",
      " 238: 'Greater Swiss Mountain dog',\n",
      " 239: 'Bernese mountain dog',\n",
      " 240: 'Appenzeller',\n",
      " 241: 'EntleBucher',\n",
      " 242: 'boxer',\n",
      " 243: 'bull mastiff',\n",
      " 244: 'Tibetan mastiff',\n",
      " 245: 'French bulldog',\n",
      " 246: 'Great Dane',\n",
      " 247: 'Saint Bernard, St Bernard',\n",
      " 248: 'Eskimo dog, husky',\n",
      " 249: 'malamute, malemute, Alaskan malamute',\n",
      " 250: 'Siberian husky',\n",
      " 251: 'dalmatian, coach dog, carriage dog',\n",
      " 252: 'affenpinscher, monkey pinscher, monkey dog',\n",
      " 253: 'basenji',\n",
      " 254: 'pug, pug-dog',\n",
      " 255: 'Leonberg',\n",
      " 256: 'Newfoundland, Newfoundland dog',\n",
      " 257: 'Great Pyrenees',\n",
      " 258: 'Samoyed, Samoyede',\n",
      " 259: 'Pomeranian',\n",
      " 260: 'chow, chow chow',\n",
      " 261: 'keeshond',\n",
      " 262: 'Brabancon griffon',\n",
      " 263: 'Pembroke, Pembroke Welsh corgi',\n",
      " 264: 'Cardigan, Cardigan Welsh corgi',\n",
      " 265: 'toy poodle',\n",
      " 266: 'miniature poodle',\n",
      " 267: 'standard poodle',\n",
      " 268: 'Mexican hairless',\n",
      " 269: 'timber wolf, grey wolf, gray wolf, Canis lupus',\n",
      " 270: 'white wolf, Arctic wolf, Canis lupus tundrarum',\n",
      " 271: 'red wolf, maned wolf, Canis rufus, Canis niger',\n",
      " 272: 'coyote, prairie wolf, brush wolf, Canis latrans',\n",
      " 273: 'dingo, warrigal, warragal, Canis dingo',\n",
      " 274: 'dhole, Cuon alpinus',\n",
      " 275: 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus',\n",
      " 276: 'hyena, hyaena',\n",
      " 277: 'red fox, Vulpes vulpes',\n",
      " 278: 'kit fox, Vulpes macrotis',\n",
      " 279: 'Arctic fox, white fox, Alopex lagopus',\n",
      " 280: 'grey fox, gray fox, Urocyon cinereoargenteus',\n",
      " 281: 'tabby, tabby cat',\n",
      " 282: 'tiger cat',\n",
      " 283: 'Persian cat',\n",
      " 284: 'Siamese cat, Siamese',\n",
      " 285: 'Egyptian cat',\n",
      " 286: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n",
      " 287: 'lynx, catamount',\n",
      " 288: 'leopard, Panthera pardus',\n",
      " 289: 'snow leopard, ounce, Panthera uncia',\n",
      " 290: 'jaguar, panther, Panthera onca, Felis onca',\n",
      " 291: 'lion, king of beasts, Panthera leo',\n",
      " 292: 'tiger, Panthera tigris',\n",
      " 293: 'cheetah, chetah, Acinonyx jubatus',\n",
      " 294: 'brown bear, bruin, Ursus arctos',\n",
      " 295: 'American black bear, black bear, Ursus americanus, Euarctos americanus',\n",
      " 296: 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus',\n",
      " 297: 'sloth bear, Melursus ursinus, Ursus ursinus',\n",
      " 298: 'mongoose',\n",
      " 299: 'meerkat, mierkat',\n",
      " 300: 'tiger beetle',\n",
      " 301: 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle',\n",
      " 302: 'ground beetle, carabid beetle',\n",
      " 303: 'long-horned beetle, longicorn, longicorn beetle',\n",
      " 304: 'leaf beetle, chrysomelid',\n",
      " 305: 'dung beetle',\n",
      " 306: 'rhinoceros beetle',\n",
      " 307: 'weevil',\n",
      " 308: 'fly',\n",
      " 309: 'bee',\n",
      " 310: 'ant, emmet, pismire',\n",
      " 311: 'grasshopper, hopper',\n",
      " 312: 'cricket',\n",
      " 313: 'walking stick, walkingstick, stick insect',\n",
      " 314: 'cockroach, roach',\n",
      " 315: 'mantis, mantid',\n",
      " 316: 'cicada, cicala',\n",
      " 317: 'leafhopper',\n",
      " 318: 'lacewing, lacewing fly',\n",
      " 319: \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\",\n",
      " 320: 'damselfly',\n",
      " 321: 'admiral',\n",
      " 322: 'ringlet, ringlet butterfly',\n",
      " 323: 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus',\n",
      " 324: 'cabbage butterfly',\n",
      " 325: 'sulphur butterfly, sulfur butterfly',\n",
      " 326: 'lycaenid, lycaenid butterfly',\n",
      " 327: 'starfish, sea star',\n",
      " 328: 'sea urchin',\n",
      " 329: 'sea cucumber, holothurian',\n",
      " 330: 'wood rabbit, cottontail, cottontail rabbit',\n",
      " 331: 'hare',\n",
      " 332: 'Angora, Angora rabbit',\n",
      " 333: 'hamster',\n",
      " 334: 'porcupine, hedgehog',\n",
      " 335: 'fox squirrel, eastern fox squirrel, Sciurus niger',\n",
      " 336: 'marmot',\n",
      " 337: 'beaver',\n",
      " 338: 'guinea pig, Cavia cobaya',\n",
      " 339: 'sorrel',\n",
      " 340: 'zebra',\n",
      " 341: 'hog, pig, grunter, squealer, Sus scrofa',\n",
      " 342: 'wild boar, boar, Sus scrofa',\n",
      " 343: 'warthog',\n",
      " 344: 'hippopotamus, hippo, river horse, Hippopotamus amphibius',\n",
      " 345: 'ox',\n",
      " 346: 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis',\n",
      " 347: 'bison',\n",
      " 348: 'ram, tup',\n",
      " 349: 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis',\n",
      " 350: 'ibex, Capra ibex',\n",
      " 351: 'hartebeest',\n",
      " 352: 'impala, Aepyceros melampus',\n",
      " 353: 'gazelle',\n",
      " 354: 'Arabian camel, dromedary, Camelus dromedarius',\n",
      " 355: 'llama',\n",
      " 356: 'weasel',\n",
      " 357: 'mink',\n",
      " 358: 'polecat, fitch, foulmart, foumart, Mustela putorius',\n",
      " 359: 'black-footed ferret, ferret, Mustela nigripes',\n",
      " 360: 'otter',\n",
      " 361: 'skunk, polecat, wood pussy',\n",
      " 362: 'badger',\n",
      " 363: 'armadillo',\n",
      " 364: 'three-toed sloth, ai, Bradypus tridactylus',\n",
      " 365: 'orangutan, orang, orangutang, Pongo pygmaeus',\n",
      " 366: 'gorilla, Gorilla gorilla',\n",
      " 367: 'chimpanzee, chimp, Pan troglodytes',\n",
      " 368: 'gibbon, Hylobates lar',\n",
      " 369: 'siamang, Hylobates syndactylus, Symphalangus syndactylus',\n",
      " 370: 'guenon, guenon monkey',\n",
      " 371: 'patas, hussar monkey, Erythrocebus patas',\n",
      " 372: 'baboon',\n",
      " 373: 'macaque',\n",
      " 374: 'langur',\n",
      " 375: 'colobus, colobus monkey',\n",
      " 376: 'proboscis monkey, Nasalis larvatus',\n",
      " 377: 'marmoset',\n",
      " 378: 'capuchin, ringtail, Cebus capucinus',\n",
      " 379: 'howler monkey, howler',\n",
      " 380: 'titi, titi monkey',\n",
      " 381: 'spider monkey, Ateles geoffroyi',\n",
      " 382: 'squirrel monkey, Saimiri sciureus',\n",
      " 383: 'Madagascar cat, ring-tailed lemur, Lemur catta',\n",
      " 384: 'indri, indris, Indri indri, Indri brevicaudatus',\n",
      " 385: 'Indian elephant, Elephas maximus',\n",
      " 386: 'African elephant, Loxodonta africana',\n",
      " 387: 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens',\n",
      " 388: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca',\n",
      " 389: 'barracouta, snoek',\n",
      " 390: 'eel',\n",
      " 391: 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch',\n",
      " 392: 'rock beauty, Holocanthus tricolor',\n",
      " 393: 'anemone fish',\n",
      " 394: 'sturgeon',\n",
      " 395: 'gar, garfish, garpike, billfish, Lepisosteus osseus',\n",
      " 396: 'lionfish',\n",
      " 397: 'puffer, pufferfish, blowfish, globefish',\n",
      " 398: 'abacus',\n",
      " 399: 'abaya',\n",
      " 400: \"academic gown, academic robe, judge's robe\",\n",
      " 401: 'accordion, piano accordion, squeeze box',\n",
      " 402: 'acoustic guitar',\n",
      " 403: 'aircraft carrier, carrier, flattop, attack aircraft carrier',\n",
      " 404: 'airliner',\n",
      " 405: 'airship, dirigible',\n",
      " 406: 'altar',\n",
      " 407: 'ambulance',\n",
      " 408: 'amphibian, amphibious vehicle',\n",
      " 409: 'analog clock',\n",
      " 410: 'apiary, bee house',\n",
      " 411: 'apron',\n",
      " 412: 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin',\n",
      " 413: 'assault rifle, assault gun',\n",
      " 414: 'backpack, back pack, knapsack, packsack, rucksack, haversack',\n",
      " 415: 'bakery, bakeshop, bakehouse',\n",
      " 416: 'balance beam, beam',\n",
      " 417: 'balloon',\n",
      " 418: 'ballpoint, ballpoint pen, ballpen, Biro',\n",
      " 419: 'Band Aid',\n",
      " 420: 'banjo',\n",
      " 421: 'bannister, banister, balustrade, balusters, handrail',\n",
      " 422: 'barbell',\n",
      " 423: 'barber chair',\n",
      " 424: 'barbershop',\n",
      " 425: 'barn',\n",
      " 426: 'barometer',\n",
      " 427: 'barrel, cask',\n",
      " 428: 'barrow, garden cart, lawn cart, wheelbarrow',\n",
      " 429: 'baseball',\n",
      " 430: 'basketball',\n",
      " 431: 'bassinet',\n",
      " 432: 'bassoon',\n",
      " 433: 'bathing cap, swimming cap',\n",
      " 434: 'bath towel',\n",
      " 435: 'bathtub, bathing tub, bath, tub',\n",
      " 436: 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon',\n",
      " 437: 'beacon, lighthouse, beacon light, pharos',\n",
      " 438: 'beaker',\n",
      " 439: 'bearskin, busby, shako',\n",
      " 440: 'beer bottle',\n",
      " 441: 'beer glass',\n",
      " 442: 'bell cote, bell cot',\n",
      " 443: 'bib',\n",
      " 444: 'bicycle-built-for-two, tandem bicycle, tandem',\n",
      " 445: 'bikini, two-piece',\n",
      " 446: 'binder, ring-binder',\n",
      " 447: 'binoculars, field glasses, opera glasses',\n",
      " 448: 'birdhouse',\n",
      " 449: 'boathouse',\n",
      " 450: 'bobsled, bobsleigh, bob',\n",
      " 451: 'bolo tie, bolo, bola tie, bola',\n",
      " 452: 'bonnet, poke bonnet',\n",
      " 453: 'bookcase',\n",
      " 454: 'bookshop, bookstore, bookstall',\n",
      " 455: 'bottlecap',\n",
      " 456: 'bow',\n",
      " 457: 'bow tie, bow-tie, bowtie',\n",
      " 458: 'brass, memorial tablet, plaque',\n",
      " 459: 'brassiere, bra, bandeau',\n",
      " 460: 'breakwater, groin, groyne, mole, bulwark, seawall, jetty',\n",
      " 461: 'breastplate, aegis, egis',\n",
      " 462: 'broom',\n",
      " 463: 'bucket, pail',\n",
      " 464: 'buckle',\n",
      " 465: 'bulletproof vest',\n",
      " 466: 'bullet train, bullet',\n",
      " 467: 'butcher shop, meat market',\n",
      " 468: 'cab, hack, taxi, taxicab',\n",
      " 469: 'caldron, cauldron',\n",
      " 470: 'candle, taper, wax light',\n",
      " 471: 'cannon',\n",
      " 472: 'canoe',\n",
      " 473: 'can opener, tin opener',\n",
      " 474: 'cardigan',\n",
      " 475: 'car mirror',\n",
      " 476: 'carousel, carrousel, merry-go-round, roundabout, whirligig',\n",
      " 477: \"carpenter's kit, tool kit\",\n",
      " 478: 'carton',\n",
      " 479: 'car wheel',\n",
      " 480: 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM',\n",
      " 481: 'cassette',\n",
      " 482: 'cassette player',\n",
      " 483: 'castle',\n",
      " 484: 'catamaran',\n",
      " 485: 'CD player',\n",
      " 486: 'cello, violoncello',\n",
      " 487: 'cellular telephone, cellular phone, cellphone, cell, mobile phone',\n",
      " 488: 'chain',\n",
      " 489: 'chainlink fence',\n",
      " 490: 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour',\n",
      " 491: 'chain saw, chainsaw',\n",
      " 492: 'chest',\n",
      " 493: 'chiffonier, commode',\n",
      " 494: 'chime, bell, gong',\n",
      " 495: 'china cabinet, china closet',\n",
      " 496: 'Christmas stocking',\n",
      " 497: 'church, church building',\n",
      " 498: 'cinema, movie theater, movie theatre, movie house, picture palace',\n",
      " 499: 'cleaver, meat cleaver, chopper',\n",
      " 500: 'cliff dwelling',\n",
      " 501: 'cloak',\n",
      " 502: 'clog, geta, patten, sabot',\n",
      " 503: 'cocktail shaker',\n",
      " 504: 'coffee mug',\n",
      " 505: 'coffeepot',\n",
      " 506: 'coil, spiral, volute, whorl, helix',\n",
      " 507: 'combination lock',\n",
      " 508: 'computer keyboard, keypad',\n",
      " 509: 'confectionery, confectionary, candy store',\n",
      " 510: 'container ship, containership, container vessel',\n",
      " 511: 'convertible',\n",
      " 512: 'corkscrew, bottle screw',\n",
      " 513: 'cornet, horn, trumpet, trump',\n",
      " 514: 'cowboy boot',\n",
      " 515: 'cowboy hat, ten-gallon hat',\n",
      " 516: 'cradle',\n",
      " 517: 'crane',\n",
      " 518: 'crash helmet',\n",
      " 519: 'crate',\n",
      " 520: 'crib, cot',\n",
      " 521: 'Crock Pot',\n",
      " 522: 'croquet ball',\n",
      " 523: 'crutch',\n",
      " 524: 'cuirass',\n",
      " 525: 'dam, dike, dyke',\n",
      " 526: 'desk',\n",
      " 527: 'desktop computer',\n",
      " 528: 'dial telephone, dial phone',\n",
      " 529: 'diaper, nappy, napkin',\n",
      " 530: 'digital clock',\n",
      " 531: 'digital watch',\n",
      " 532: 'dining table, board',\n",
      " 533: 'dishrag, dishcloth',\n",
      " 534: 'dishwasher, dish washer, dishwashing machine',\n",
      " 535: 'disk brake, disc brake',\n",
      " 536: 'dock, dockage, docking facility',\n",
      " 537: 'dogsled, dog sled, dog sleigh',\n",
      " 538: 'dome',\n",
      " 539: 'doormat, welcome mat',\n",
      " 540: 'drilling platform, offshore rig',\n",
      " 541: 'drum, membranophone, tympan',\n",
      " 542: 'drumstick',\n",
      " 543: 'dumbbell',\n",
      " 544: 'Dutch oven',\n",
      " 545: 'electric fan, blower',\n",
      " 546: 'electric guitar',\n",
      " 547: 'electric locomotive',\n",
      " 548: 'entertainment center',\n",
      " 549: 'envelope',\n",
      " 550: 'espresso maker',\n",
      " 551: 'face powder',\n",
      " 552: 'feather boa, boa',\n",
      " 553: 'file, file cabinet, filing cabinet',\n",
      " 554: 'fireboat',\n",
      " 555: 'fire engine, fire truck',\n",
      " 556: 'fire screen, fireguard',\n",
      " 557: 'flagpole, flagstaff',\n",
      " 558: 'flute, transverse flute',\n",
      " 559: 'folding chair',\n",
      " 560: 'football helmet',\n",
      " 561: 'forklift',\n",
      " 562: 'fountain',\n",
      " 563: 'fountain pen',\n",
      " 564: 'four-poster',\n",
      " 565: 'freight car',\n",
      " 566: 'French horn, horn',\n",
      " 567: 'frying pan, frypan, skillet',\n",
      " 568: 'fur coat',\n",
      " 569: 'garbage truck, dustcart',\n",
      " 570: 'gasmask, respirator, gas helmet',\n",
      " 571: 'gas pump, gasoline pump, petrol pump, island dispenser',\n",
      " 572: 'goblet',\n",
      " 573: 'go-kart',\n",
      " 574: 'golf ball',\n",
      " 575: 'golfcart, golf cart',\n",
      " 576: 'gondola',\n",
      " 577: 'gong, tam-tam',\n",
      " 578: 'gown',\n",
      " 579: 'grand piano, grand',\n",
      " 580: 'greenhouse, nursery, glasshouse',\n",
      " 581: 'grille, radiator grille',\n",
      " 582: 'grocery store, grocery, food market, market',\n",
      " 583: 'guillotine',\n",
      " 584: 'hair slide',\n",
      " 585: 'hair spray',\n",
      " 586: 'half track',\n",
      " 587: 'hammer',\n",
      " 588: 'hamper',\n",
      " 589: 'hand blower, blow dryer, blow drier, hair dryer, hair drier',\n",
      " 590: 'hand-held computer, hand-held microcomputer',\n",
      " 591: 'handkerchief, hankie, hanky, hankey',\n",
      " 592: 'hard disc, hard disk, fixed disk',\n",
      " 593: 'harmonica, mouth organ, harp, mouth harp',\n",
      " 594: 'harp',\n",
      " 595: 'harvester, reaper',\n",
      " 596: 'hatchet',\n",
      " 597: 'holster',\n",
      " 598: 'home theater, home theatre',\n",
      " 599: 'honeycomb',\n",
      " 600: 'hook, claw',\n",
      " 601: 'hoopskirt, crinoline',\n",
      " 602: 'horizontal bar, high bar',\n",
      " 603: 'horse cart, horse-cart',\n",
      " 604: 'hourglass',\n",
      " 605: 'iPod',\n",
      " 606: 'iron, smoothing iron',\n",
      " 607: \"jack-o'-lantern\",\n",
      " 608: 'jean, blue jean, denim',\n",
      " 609: 'jeep, landrover',\n",
      " 610: 'jersey, T-shirt, tee shirt',\n",
      " 611: 'jigsaw puzzle',\n",
      " 612: 'jinrikisha, ricksha, rickshaw',\n",
      " 613: 'joystick',\n",
      " 614: 'kimono',\n",
      " 615: 'knee pad',\n",
      " 616: 'knot',\n",
      " 617: 'lab coat, laboratory coat',\n",
      " 618: 'ladle',\n",
      " 619: 'lampshade, lamp shade',\n",
      " 620: 'laptop, laptop computer',\n",
      " 621: 'lawn mower, mower',\n",
      " 622: 'lens cap, lens cover',\n",
      " 623: 'letter opener, paper knife, paperknife',\n",
      " 624: 'library',\n",
      " 625: 'lifeboat',\n",
      " 626: 'lighter, light, igniter, ignitor',\n",
      " 627: 'limousine, limo',\n",
      " 628: 'liner, ocean liner',\n",
      " 629: 'lipstick, lip rouge',\n",
      " 630: 'Loafer',\n",
      " 631: 'lotion',\n",
      " 632: 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system',\n",
      " 633: \"loupe, jeweler's loupe\",\n",
      " 634: 'lumbermill, sawmill',\n",
      " 635: 'magnetic compass',\n",
      " 636: 'mailbag, postbag',\n",
      " 637: 'mailbox, letter box',\n",
      " 638: 'maillot',\n",
      " 639: 'maillot, tank suit',\n",
      " 640: 'manhole cover',\n",
      " 641: 'maraca',\n",
      " 642: 'marimba, xylophone',\n",
      " 643: 'mask',\n",
      " 644: 'matchstick',\n",
      " 645: 'maypole',\n",
      " 646: 'maze, labyrinth',\n",
      " 647: 'measuring cup',\n",
      " 648: 'medicine chest, medicine cabinet',\n",
      " 649: 'megalith, megalithic structure',\n",
      " 650: 'microphone, mike',\n",
      " 651: 'microwave, microwave oven',\n",
      " 652: 'military uniform',\n",
      " 653: 'milk can',\n",
      " 654: 'minibus',\n",
      " 655: 'miniskirt, mini',\n",
      " 656: 'minivan',\n",
      " 657: 'missile',\n",
      " 658: 'mitten',\n",
      " 659: 'mixing bowl',\n",
      " 660: 'mobile home, manufactured home',\n",
      " 661: 'Model T',\n",
      " 662: 'modem',\n",
      " 663: 'monastery',\n",
      " 664: 'monitor',\n",
      " 665: 'moped',\n",
      " 666: 'mortar',\n",
      " 667: 'mortarboard',\n",
      " 668: 'mosque',\n",
      " 669: 'mosquito net',\n",
      " 670: 'motor scooter, scooter',\n",
      " 671: 'mountain bike, all-terrain bike, off-roader',\n",
      " 672: 'mountain tent',\n",
      " 673: 'mouse, computer mouse',\n",
      " 674: 'mousetrap',\n",
      " 675: 'moving van',\n",
      " 676: 'muzzle',\n",
      " 677: 'nail',\n",
      " 678: 'neck brace',\n",
      " 679: 'necklace',\n",
      " 680: 'nipple',\n",
      " 681: 'notebook, notebook computer',\n",
      " 682: 'obelisk',\n",
      " 683: 'oboe, hautboy, hautbois',\n",
      " 684: 'ocarina, sweet potato',\n",
      " 685: 'odometer, hodometer, mileometer, milometer',\n",
      " 686: 'oil filter',\n",
      " 687: 'organ, pipe organ',\n",
      " 688: 'oscilloscope, scope, cathode-ray oscilloscope, CRO',\n",
      " 689: 'overskirt',\n",
      " 690: 'oxcart',\n",
      " 691: 'oxygen mask',\n",
      " 692: 'packet',\n",
      " 693: 'paddle, boat paddle',\n",
      " 694: 'paddlewheel, paddle wheel',\n",
      " 695: 'padlock',\n",
      " 696: 'paintbrush',\n",
      " 697: \"pajama, pyjama, pj's, jammies\",\n",
      " 698: 'palace',\n",
      " 699: 'panpipe, pandean pipe, syrinx',\n",
      " 700: 'paper towel',\n",
      " 701: 'parachute, chute',\n",
      " 702: 'parallel bars, bars',\n",
      " 703: 'park bench',\n",
      " 704: 'parking meter',\n",
      " 705: 'passenger car, coach, carriage',\n",
      " 706: 'patio, terrace',\n",
      " 707: 'pay-phone, pay-station',\n",
      " 708: 'pedestal, plinth, footstall',\n",
      " 709: 'pencil box, pencil case',\n",
      " 710: 'pencil sharpener',\n",
      " 711: 'perfume, essence',\n",
      " 712: 'Petri dish',\n",
      " 713: 'photocopier',\n",
      " 714: 'pick, plectrum, plectron',\n",
      " 715: 'pickelhaube',\n",
      " 716: 'picket fence, paling',\n",
      " 717: 'pickup, pickup truck',\n",
      " 718: 'pier',\n",
      " 719: 'piggy bank, penny bank',\n",
      " 720: 'pill bottle',\n",
      " 721: 'pillow',\n",
      " 722: 'ping-pong ball',\n",
      " 723: 'pinwheel',\n",
      " 724: 'pirate, pirate ship',\n",
      " 725: 'pitcher, ewer',\n",
      " 726: \"plane, carpenter's plane, woodworking plane\",\n",
      " 727: 'planetarium',\n",
      " 728: 'plastic bag',\n",
      " 729: 'plate rack',\n",
      " 730: 'plow, plough',\n",
      " 731: \"plunger, plumber's helper\",\n",
      " 732: 'Polaroid camera, Polaroid Land camera',\n",
      " 733: 'pole',\n",
      " 734: 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria',\n",
      " 735: 'poncho',\n",
      " 736: 'pool table, billiard table, snooker table',\n",
      " 737: 'pop bottle, soda bottle',\n",
      " 738: 'pot, flowerpot',\n",
      " 739: \"potter's wheel\",\n",
      " 740: 'power drill',\n",
      " 741: 'prayer rug, prayer mat',\n",
      " 742: 'printer',\n",
      " 743: 'prison, prison house',\n",
      " 744: 'projectile, missile',\n",
      " 745: 'projector',\n",
      " 746: 'puck, hockey puck',\n",
      " 747: 'punching bag, punch bag, punching ball, punchball',\n",
      " 748: 'purse',\n",
      " 749: 'quill, quill pen',\n",
      " 750: 'quilt, comforter, comfort, puff',\n",
      " 751: 'racer, race car, racing car',\n",
      " 752: 'racket, racquet',\n",
      " 753: 'radiator',\n",
      " 754: 'radio, wireless',\n",
      " 755: 'radio telescope, radio reflector',\n",
      " 756: 'rain barrel',\n",
      " 757: 'recreational vehicle, RV, R.V.',\n",
      " 758: 'reel',\n",
      " 759: 'reflex camera',\n",
      " 760: 'refrigerator, icebox',\n",
      " 761: 'remote control, remote',\n",
      " 762: 'restaurant, eating house, eating place, eatery',\n",
      " 763: 'revolver, six-gun, six-shooter',\n",
      " 764: 'rifle',\n",
      " 765: 'rocking chair, rocker',\n",
      " 766: 'rotisserie',\n",
      " 767: 'rubber eraser, rubber, pencil eraser',\n",
      " 768: 'rugby ball',\n",
      " 769: 'rule, ruler',\n",
      " 770: 'running shoe',\n",
      " 771: 'safe',\n",
      " 772: 'safety pin',\n",
      " 773: 'saltshaker, salt shaker',\n",
      " 774: 'sandal',\n",
      " 775: 'sarong',\n",
      " 776: 'sax, saxophone',\n",
      " 777: 'scabbard',\n",
      " 778: 'scale, weighing machine',\n",
      " 779: 'school bus',\n",
      " 780: 'schooner',\n",
      " 781: 'scoreboard',\n",
      " 782: 'screen, CRT screen',\n",
      " 783: 'screw',\n",
      " 784: 'screwdriver',\n",
      " 785: 'seat belt, seatbelt',\n",
      " 786: 'sewing machine',\n",
      " 787: 'shield, buckler',\n",
      " 788: 'shoe shop, shoe-shop, shoe store',\n",
      " 789: 'shoji',\n",
      " 790: 'shopping basket',\n",
      " 791: 'shopping cart',\n",
      " 792: 'shovel',\n",
      " 793: 'shower cap',\n",
      " 794: 'shower curtain',\n",
      " 795: 'ski',\n",
      " 796: 'ski mask',\n",
      " 797: 'sleeping bag',\n",
      " 798: 'slide rule, slipstick',\n",
      " 799: 'sliding door',\n",
      " 800: 'slot, one-armed bandit',\n",
      " 801: 'snorkel',\n",
      " 802: 'snowmobile',\n",
      " 803: 'snowplow, snowplough',\n",
      " 804: 'soap dispenser',\n",
      " 805: 'soccer ball',\n",
      " 806: 'sock',\n",
      " 807: 'solar dish, solar collector, solar furnace',\n",
      " 808: 'sombrero',\n",
      " 809: 'soup bowl',\n",
      " 810: 'space bar',\n",
      " 811: 'space heater',\n",
      " 812: 'space shuttle',\n",
      " 813: 'spatula',\n",
      " 814: 'speedboat',\n",
      " 815: \"spider web, spider's web\",\n",
      " 816: 'spindle',\n",
      " 817: 'sports car, sport car',\n",
      " 818: 'spotlight, spot',\n",
      " 819: 'stage',\n",
      " 820: 'steam locomotive',\n",
      " 821: 'steel arch bridge',\n",
      " 822: 'steel drum',\n",
      " 823: 'stethoscope',\n",
      " 824: 'stole',\n",
      " 825: 'stone wall',\n",
      " 826: 'stopwatch, stop watch',\n",
      " 827: 'stove',\n",
      " 828: 'strainer',\n",
      " 829: 'streetcar, tram, tramcar, trolley, trolley car',\n",
      " 830: 'stretcher',\n",
      " 831: 'studio couch, day bed',\n",
      " 832: 'stupa, tope',\n",
      " 833: 'submarine, pigboat, sub, U-boat',\n",
      " 834: 'suit, suit of clothes',\n",
      " 835: 'sundial',\n",
      " 836: 'sunglass',\n",
      " 837: 'sunglasses, dark glasses, shades',\n",
      " 838: 'sunscreen, sunblock, sun blocker',\n",
      " 839: 'suspension bridge',\n",
      " 840: 'swab, swob, mop',\n",
      " 841: 'sweatshirt',\n",
      " 842: 'swimming trunks, bathing trunks',\n",
      " 843: 'swing',\n",
      " 844: 'switch, electric switch, electrical switch',\n",
      " 845: 'syringe',\n",
      " 846: 'table lamp',\n",
      " 847: 'tank, army tank, armored combat vehicle, armoured combat vehicle',\n",
      " 848: 'tape player',\n",
      " 849: 'teapot',\n",
      " 850: 'teddy, teddy bear',\n",
      " 851: 'television, television system',\n",
      " 852: 'tennis ball',\n",
      " 853: 'thatch, thatched roof',\n",
      " 854: 'theater curtain, theatre curtain',\n",
      " 855: 'thimble',\n",
      " 856: 'thresher, thrasher, threshing machine',\n",
      " 857: 'throne',\n",
      " 858: 'tile roof',\n",
      " 859: 'toaster',\n",
      " 860: 'tobacco shop, tobacconist shop, tobacconist',\n",
      " 861: 'toilet seat',\n",
      " 862: 'torch',\n",
      " 863: 'totem pole',\n",
      " 864: 'tow truck, tow car, wrecker',\n",
      " 865: 'toyshop',\n",
      " 866: 'tractor',\n",
      " 867: 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi',\n",
      " 868: 'tray',\n",
      " 869: 'trench coat',\n",
      " 870: 'tricycle, trike, velocipede',\n",
      " 871: 'trimaran',\n",
      " 872: 'tripod',\n",
      " 873: 'triumphal arch',\n",
      " 874: 'trolleybus, trolley coach, trackless trolley',\n",
      " 875: 'trombone',\n",
      " 876: 'tub, vat',\n",
      " 877: 'turnstile',\n",
      " 878: 'typewriter keyboard',\n",
      " 879: 'umbrella',\n",
      " 880: 'unicycle, monocycle',\n",
      " 881: 'upright, upright piano',\n",
      " 882: 'vacuum, vacuum cleaner',\n",
      " 883: 'vase',\n",
      " 884: 'vault',\n",
      " 885: 'velvet',\n",
      " 886: 'vending machine',\n",
      " 887: 'vestment',\n",
      " 888: 'viaduct',\n",
      " 889: 'violin, fiddle',\n",
      " 890: 'volleyball',\n",
      " 891: 'waffle iron',\n",
      " 892: 'wall clock',\n",
      " 893: 'wallet, billfold, notecase, pocketbook',\n",
      " 894: 'wardrobe, closet, press',\n",
      " 895: 'warplane, military plane',\n",
      " 896: 'washbasin, handbasin, washbowl, lavabo, wash-hand basin',\n",
      " 897: 'washer, automatic washer, washing machine',\n",
      " 898: 'water bottle',\n",
      " 899: 'water jug',\n",
      " 900: 'water tower',\n",
      " 901: 'whiskey jug',\n",
      " 902: 'whistle',\n",
      " 903: 'wig',\n",
      " 904: 'window screen',\n",
      " 905: 'window shade',\n",
      " 906: 'Windsor tie',\n",
      " 907: 'wine bottle',\n",
      " 908: 'wing',\n",
      " 909: 'wok',\n",
      " 910: 'wooden spoon',\n",
      " 911: 'wool, woolen, woollen',\n",
      " 912: 'worm fence, snake fence, snake-rail fence, Virginia fence',\n",
      " 913: 'wreck',\n",
      " 914: 'yawl',\n",
      " 915: 'yurt',\n",
      " 916: 'web site, website, internet site, site',\n",
      " 917: 'comic book',\n",
      " 918: 'crossword puzzle, crossword',\n",
      " 919: 'street sign',\n",
      " 920: 'traffic light, traffic signal, stoplight',\n",
      " 921: 'book jacket, dust cover, dust jacket, dust wrapper',\n",
      " 922: 'menu',\n",
      " 923: 'plate',\n",
      " 924: 'guacamole',\n",
      " 925: 'consomme',\n",
      " 926: 'hot pot, hotpot',\n",
      " 927: 'trifle',\n",
      " 928: 'ice cream, icecream',\n",
      " 929: 'ice lolly, lolly, lollipop, popsicle',\n",
      " 930: 'French loaf',\n",
      " 931: 'bagel, beigel',\n",
      " 932: 'pretzel',\n",
      " 933: 'cheeseburger',\n",
      " 934: 'hotdog, hot dog, red hot',\n",
      " 935: 'mashed potato',\n",
      " 936: 'head cabbage',\n",
      " 937: 'broccoli',\n",
      " 938: 'cauliflower',\n",
      " 939: 'zucchini, courgette',\n",
      " 940: 'spaghetti squash',\n",
      " 941: 'acorn squash',\n",
      " 942: 'butternut squash',\n",
      " 943: 'cucumber, cuke',\n",
      " 944: 'artichoke, globe artichoke',\n",
      " 945: 'bell pepper',\n",
      " 946: 'cardoon',\n",
      " 947: 'mushroom',\n",
      " 948: 'Granny Smith',\n",
      " 949: 'strawberry',\n",
      " 950: 'orange',\n",
      " 951: 'lemon',\n",
      " 952: 'fig',\n",
      " 953: 'pineapple, ananas',\n",
      " 954: 'banana',\n",
      " 955: 'jackfruit, jak, jack',\n",
      " 956: 'custard apple',\n",
      " 957: 'pomegranate',\n",
      " 958: 'hay',\n",
      " 959: 'carbonara',\n",
      " 960: 'chocolate sauce, chocolate syrup',\n",
      " 961: 'dough',\n",
      " 962: 'meat loaf, meatloaf',\n",
      " 963: 'pizza, pizza pie',\n",
      " 964: 'potpie',\n",
      " 965: 'burrito',\n",
      " 966: 'red wine',\n",
      " 967: 'espresso',\n",
      " 968: 'cup',\n",
      " 969: 'eggnog',\n",
      " 970: 'alp',\n",
      " 971: 'bubble',\n",
      " 972: 'cliff, drop, drop-off',\n",
      " 973: 'coral reef',\n",
      " 974: 'geyser',\n",
      " 975: 'lakeside, lakeshore',\n",
      " 976: 'promontory, headland, head, foreland',\n",
      " 977: 'sandbar, sand bar',\n",
      " 978: 'seashore, coast, seacoast, sea-coast',\n",
      " 979: 'valley, vale',\n",
      " 980: 'volcano',\n",
      " 981: 'ballplayer, baseball player',\n",
      " 982: 'groom, bridegroom',\n",
      " 983: 'scuba diver',\n",
      " 984: 'rapeseed',\n",
      " 985: 'daisy',\n",
      " 986: \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\",\n",
      " 987: 'corn',\n",
      " 988: 'acorn',\n",
      " 989: 'hip, rose hip, rosehip',\n",
      " 990: 'buckeye, horse chestnut, conker',\n",
      " 991: 'coral fungus',\n",
      " 992: 'agaric',\n",
      " 993: 'gyromitra',\n",
      " 994: 'stinkhorn, carrion fungus',\n",
      " 995: 'earthstar',\n",
      " 996: 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa',\n",
      " 997: 'bolete',\n",
      " 998: 'ear, spike, capitulum',\n",
      " 999: 'toilet tissue, toilet paper, bathroom tissue'}\n",
      "73/2:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 380\n",
      "\n",
      "def prep_image(image):\n",
      "    res = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "    res = res.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "    return res\n",
      "\n",
      "def predict(image):\n",
      "    cropped_image = center_crop(image)\n",
      "    prediction = (model.predict(prep_image(image)), model.predict(prep_image(cropped_image)))\n",
      "    if np.max(prediction[0]>=prediction[1]):\n",
      "        return {'class': np.argmax(prediction[0], axis=1)[0], 'all_prob': prediction[0]}\n",
      "    else:\n",
      "        return  {'class': np.argmax(prediction[1], axis=1)[1], 'all_prob': prediction[1]}\n",
      "    \n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "plt.imshow(image)\n",
      "\n",
      "print(class_dict[predict(image)])\n",
      "74/1:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 380\n",
      "\n",
      "def prep_image(image):\n",
      "    res = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "    res = res.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "    return res\n",
      "\n",
      "def predict(image):\n",
      "    cropped_image = center_crop(image)\n",
      "    prediction = (model.predict(prep_image(image)), model.predict(prep_image(cropped_image)))\n",
      "    if np.max(prediction[0]>=prediction[1]):\n",
      "        return {'class': np.argmax(prediction[0], axis=1)[0], 'all_prob': prediction[0]}\n",
      "    else:\n",
      "        return  {'class': np.argmax(prediction[1], axis=1)[1], 'all_prob': prediction[1]}\n",
      "    \n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "plt.imshow(image)\n",
      "\n",
      "print(class_dict[predict(image)])\n",
      "74/2:\n",
      "from tensorflow.keras.applications import EfficientNetB4\n",
      "import tensorflow as tf\n",
      "model = EfficientNetB4(weights='imagenet')\n",
      "\n",
      "def center_crop(img, new_width=None, new_height=None):        \n",
      "\n",
      "    width = img.shape[1]\n",
      "    height = img.shape[0]\n",
      "\n",
      "    if new_width is None:\n",
      "        new_width = min(width, height)\n",
      "\n",
      "    if new_height is None:\n",
      "        new_height = min(width, height)\n",
      "\n",
      "    left = int(np.ceil((width - new_width) / 2))\n",
      "    right = width - int(np.floor((width - new_width) / 2))\n",
      "\n",
      "    top = int(np.ceil((height - new_height) / 2))\n",
      "    bottom = height - int(np.floor((height - new_height) / 2))\n",
      "\n",
      "    if len(img.shape) == 2:\n",
      "        center_cropped_img = img[top:bottom, left:right]\n",
      "    else:\n",
      "        center_cropped_img = img[top:bottom, left:right, ...]\n",
      "\n",
      "    return center_cropped_img\n",
      "\n",
      "class_dict = {0: 'tench, Tinca tinca',\n",
      " 1: 'goldfish, Carassius auratus',\n",
      " 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n",
      " 3: 'tiger shark, Galeocerdo cuvieri',\n",
      " 4: 'hammerhead, hammerhead shark',\n",
      " 5: 'electric ray, crampfish, numbfish, torpedo',\n",
      " 6: 'stingray',\n",
      " 7: 'cock',\n",
      " 8: 'hen',\n",
      " 9: 'ostrich, Struthio camelus',\n",
      " 10: 'brambling, Fringilla montifringilla',\n",
      " 11: 'goldfinch, Carduelis carduelis',\n",
      " 12: 'house finch, linnet, Carpodacus mexicanus',\n",
      " 13: 'junco, snowbird',\n",
      " 14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea',\n",
      " 15: 'robin, American robin, Turdus migratorius',\n",
      " 16: 'bulbul',\n",
      " 17: 'jay',\n",
      " 18: 'magpie',\n",
      " 19: 'chickadee',\n",
      " 20: 'water ouzel, dipper',\n",
      " 21: 'kite',\n",
      " 22: 'bald eagle, American eagle, Haliaeetus leucocephalus',\n",
      " 23: 'vulture',\n",
      " 24: 'great grey owl, great gray owl, Strix nebulosa',\n",
      " 25: 'European fire salamander, Salamandra salamandra',\n",
      " 26: 'common newt, Triturus vulgaris',\n",
      " 27: 'eft',\n",
      " 28: 'spotted salamander, Ambystoma maculatum',\n",
      " 29: 'axolotl, mud puppy, Ambystoma mexicanum',\n",
      " 30: 'bullfrog, Rana catesbeiana',\n",
      " 31: 'tree frog, tree-frog',\n",
      " 32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui',\n",
      " 33: 'loggerhead, loggerhead turtle, Caretta caretta',\n",
      " 34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea',\n",
      " 35: 'mud turtle',\n",
      " 36: 'terrapin',\n",
      " 37: 'box turtle, box tortoise',\n",
      " 38: 'banded gecko',\n",
      " 39: 'common iguana, iguana, Iguana iguana',\n",
      " 40: 'American chameleon, anole, Anolis carolinensis',\n",
      " 41: 'whiptail, whiptail lizard',\n",
      " 42: 'agama',\n",
      " 43: 'frilled lizard, Chlamydosaurus kingi',\n",
      " 44: 'alligator lizard',\n",
      " 45: 'Gila monster, Heloderma suspectum',\n",
      " 46: 'green lizard, Lacerta viridis',\n",
      " 47: 'African chameleon, Chamaeleo chamaeleon',\n",
      " 48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis',\n",
      " 49: 'African crocodile, Nile crocodile, Crocodylus niloticus',\n",
      " 50: 'American alligator, Alligator mississipiensis',\n",
      " 51: 'triceratops',\n",
      " 52: 'thunder snake, worm snake, Carphophis amoenus',\n",
      " 53: 'ringneck snake, ring-necked snake, ring snake',\n",
      " 54: 'hognose snake, puff adder, sand viper',\n",
      " 55: 'green snake, grass snake',\n",
      " 56: 'king snake, kingsnake',\n",
      " 57: 'garter snake, grass snake',\n",
      " 58: 'water snake',\n",
      " 59: 'vine snake',\n",
      " 60: 'night snake, Hypsiglena torquata',\n",
      " 61: 'boa constrictor, Constrictor constrictor',\n",
      " 62: 'rock python, rock snake, Python sebae',\n",
      " 63: 'Indian cobra, Naja naja',\n",
      " 64: 'green mamba',\n",
      " 65: 'sea snake',\n",
      " 66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus',\n",
      " 67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus',\n",
      " 68: 'sidewinder, horned rattlesnake, Crotalus cerastes',\n",
      " 69: 'trilobite',\n",
      " 70: 'harvestman, daddy longlegs, Phalangium opilio',\n",
      " 71: 'scorpion',\n",
      " 72: 'black and gold garden spider, Argiope aurantia',\n",
      " 73: 'barn spider, Araneus cavaticus',\n",
      " 74: 'garden spider, Aranea diademata',\n",
      " 75: 'black widow, Latrodectus mactans',\n",
      " 76: 'tarantula',\n",
      " 77: 'wolf spider, hunting spider',\n",
      " 78: 'tick',\n",
      " 79: 'centipede',\n",
      " 80: 'black grouse',\n",
      " 81: 'ptarmigan',\n",
      " 82: 'ruffed grouse, partridge, Bonasa umbellus',\n",
      " 83: 'prairie chicken, prairie grouse, prairie fowl',\n",
      " 84: 'peacock',\n",
      " 85: 'quail',\n",
      " 86: 'partridge',\n",
      " 87: 'African grey, African gray, Psittacus erithacus',\n",
      " 88: 'macaw',\n",
      " 89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n",
      " 90: 'lorikeet',\n",
      " 91: 'coucal',\n",
      " 92: 'bee eater',\n",
      " 93: 'hornbill',\n",
      " 94: 'hummingbird',\n",
      " 95: 'jacamar',\n",
      " 96: 'toucan',\n",
      " 97: 'drake',\n",
      " 98: 'red-breasted merganser, Mergus serrator',\n",
      " 99: 'goose',\n",
      " 100: 'black swan, Cygnus atratus',\n",
      " 101: 'tusker',\n",
      " 102: 'echidna, spiny anteater, anteater',\n",
      " 103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n",
      " 104: 'wallaby, brush kangaroo',\n",
      " 105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n",
      " 106: 'wombat',\n",
      " 107: 'jellyfish',\n",
      " 108: 'sea anemone, anemone',\n",
      " 109: 'brain coral',\n",
      " 110: 'flatworm, platyhelminth',\n",
      " 111: 'nematode, nematode worm, roundworm',\n",
      " 112: 'conch',\n",
      " 113: 'snail',\n",
      " 114: 'slug',\n",
      " 115: 'sea slug, nudibranch',\n",
      " 116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore',\n",
      " 117: 'chambered nautilus, pearly nautilus, nautilus',\n",
      " 118: 'Dungeness crab, Cancer magister',\n",
      " 119: 'rock crab, Cancer irroratus',\n",
      " 120: 'fiddler crab',\n",
      " 121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica',\n",
      " 122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus',\n",
      " 123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish',\n",
      " 124: 'crayfish, crawfish, crawdad, crawdaddy',\n",
      " 125: 'hermit crab',\n",
      " 126: 'isopod',\n",
      " 127: 'white stork, Ciconia ciconia',\n",
      " 128: 'black stork, Ciconia nigra',\n",
      " 129: 'spoonbill',\n",
      " 130: 'flamingo',\n",
      " 131: 'little blue heron, Egretta caerulea',\n",
      " 132: 'American egret, great white heron, Egretta albus',\n",
      " 133: 'bittern',\n",
      " 134: 'crane',\n",
      " 135: 'limpkin, Aramus pictus',\n",
      " 136: 'European gallinule, Porphyrio porphyrio',\n",
      " 137: 'American coot, marsh hen, mud hen, water hen, Fulica americana',\n",
      " 138: 'bustard',\n",
      " 139: 'ruddy turnstone, Arenaria interpres',\n",
      " 140: 'red-backed sandpiper, dunlin, Erolia alpina',\n",
      " 141: 'redshank, Tringa totanus',\n",
      " 142: 'dowitcher',\n",
      " 143: 'oystercatcher, oyster catcher',\n",
      " 144: 'pelican',\n",
      " 145: 'king penguin, Aptenodytes patagonica',\n",
      " 146: 'albatross, mollymawk',\n",
      " 147: 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus',\n",
      " 148: 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca',\n",
      " 149: 'dugong, Dugong dugon',\n",
      " 150: 'sea lion',\n",
      " 151: 'Chihuahua',\n",
      " 152: 'Japanese spaniel',\n",
      " 153: 'Maltese dog, Maltese terrier, Maltese',\n",
      " 154: 'Pekinese, Pekingese, Peke',\n",
      " 155: 'Shih-Tzu',\n",
      " 156: 'Blenheim spaniel',\n",
      " 157: 'papillon',\n",
      " 158: 'toy terrier',\n",
      " 159: 'Rhodesian ridgeback',\n",
      " 160: 'Afghan hound, Afghan',\n",
      " 161: 'basset, basset hound',\n",
      " 162: 'beagle',\n",
      " 163: 'bloodhound, sleuthhound',\n",
      " 164: 'bluetick',\n",
      " 165: 'black-and-tan coonhound',\n",
      " 166: 'Walker hound, Walker foxhound',\n",
      " 167: 'English foxhound',\n",
      " 168: 'redbone',\n",
      " 169: 'borzoi, Russian wolfhound',\n",
      " 170: 'Irish wolfhound',\n",
      " 171: 'Italian greyhound',\n",
      " 172: 'whippet',\n",
      " 173: 'Ibizan hound, Ibizan Podenco',\n",
      " 174: 'Norwegian elkhound, elkhound',\n",
      " 175: 'otterhound, otter hound',\n",
      " 176: 'Saluki, gazelle hound',\n",
      " 177: 'Scottish deerhound, deerhound',\n",
      " 178: 'Weimaraner',\n",
      " 179: 'Staffordshire bullterrier, Staffordshire bull terrier',\n",
      " 180: 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier',\n",
      " 181: 'Bedlington terrier',\n",
      " 182: 'Border terrier',\n",
      " 183: 'Kerry blue terrier',\n",
      " 184: 'Irish terrier',\n",
      " 185: 'Norfolk terrier',\n",
      " 186: 'Norwich terrier',\n",
      " 187: 'Yorkshire terrier',\n",
      " 188: 'wire-haired fox terrier',\n",
      " 189: 'Lakeland terrier',\n",
      " 190: 'Sealyham terrier, Sealyham',\n",
      " 191: 'Airedale, Airedale terrier',\n",
      " 192: 'cairn, cairn terrier',\n",
      " 193: 'Australian terrier',\n",
      " 194: 'Dandie Dinmont, Dandie Dinmont terrier',\n",
      " 195: 'Boston bull, Boston terrier',\n",
      " 196: 'miniature schnauzer',\n",
      " 197: 'giant schnauzer',\n",
      " 198: 'standard schnauzer',\n",
      " 199: 'Scotch terrier, Scottish terrier, Scottie',\n",
      " 200: 'Tibetan terrier, chrysanthemum dog',\n",
      " 201: 'silky terrier, Sydney silky',\n",
      " 202: 'soft-coated wheaten terrier',\n",
      " 203: 'West Highland white terrier',\n",
      " 204: 'Lhasa, Lhasa apso',\n",
      " 205: 'flat-coated retriever',\n",
      " 206: 'curly-coated retriever',\n",
      " 207: 'golden retriever',\n",
      " 208: 'Labrador retriever',\n",
      " 209: 'Chesapeake Bay retriever',\n",
      " 210: 'German short-haired pointer',\n",
      " 211: 'vizsla, Hungarian pointer',\n",
      " 212: 'English setter',\n",
      " 213: 'Irish setter, red setter',\n",
      " 214: 'Gordon setter',\n",
      " 215: 'Brittany spaniel',\n",
      " 216: 'clumber, clumber spaniel',\n",
      " 217: 'English springer, English springer spaniel',\n",
      " 218: 'Welsh springer spaniel',\n",
      " 219: 'cocker spaniel, English cocker spaniel, cocker',\n",
      " 220: 'Sussex spaniel',\n",
      " 221: 'Irish water spaniel',\n",
      " 222: 'kuvasz',\n",
      " 223: 'schipperke',\n",
      " 224: 'groenendael',\n",
      " 225: 'malinois',\n",
      " 226: 'briard',\n",
      " 227: 'kelpie',\n",
      " 228: 'komondor',\n",
      " 229: 'Old English sheepdog, bobtail',\n",
      " 230: 'Shetland sheepdog, Shetland sheep dog, Shetland',\n",
      " 231: 'collie',\n",
      " 232: 'Border collie',\n",
      " 233: 'Bouvier des Flandres, Bouviers des Flandres',\n",
      " 234: 'Rottweiler',\n",
      " 235: 'German shepherd, German shepherd dog, German police dog, alsatian',\n",
      " 236: 'Doberman, Doberman pinscher',\n",
      " 237: 'miniature pinscher',\n",
      " 238: 'Greater Swiss Mountain dog',\n",
      " 239: 'Bernese mountain dog',\n",
      " 240: 'Appenzeller',\n",
      " 241: 'EntleBucher',\n",
      " 242: 'boxer',\n",
      " 243: 'bull mastiff',\n",
      " 244: 'Tibetan mastiff',\n",
      " 245: 'French bulldog',\n",
      " 246: 'Great Dane',\n",
      " 247: 'Saint Bernard, St Bernard',\n",
      " 248: 'Eskimo dog, husky',\n",
      " 249: 'malamute, malemute, Alaskan malamute',\n",
      " 250: 'Siberian husky',\n",
      " 251: 'dalmatian, coach dog, carriage dog',\n",
      " 252: 'affenpinscher, monkey pinscher, monkey dog',\n",
      " 253: 'basenji',\n",
      " 254: 'pug, pug-dog',\n",
      " 255: 'Leonberg',\n",
      " 256: 'Newfoundland, Newfoundland dog',\n",
      " 257: 'Great Pyrenees',\n",
      " 258: 'Samoyed, Samoyede',\n",
      " 259: 'Pomeranian',\n",
      " 260: 'chow, chow chow',\n",
      " 261: 'keeshond',\n",
      " 262: 'Brabancon griffon',\n",
      " 263: 'Pembroke, Pembroke Welsh corgi',\n",
      " 264: 'Cardigan, Cardigan Welsh corgi',\n",
      " 265: 'toy poodle',\n",
      " 266: 'miniature poodle',\n",
      " 267: 'standard poodle',\n",
      " 268: 'Mexican hairless',\n",
      " 269: 'timber wolf, grey wolf, gray wolf, Canis lupus',\n",
      " 270: 'white wolf, Arctic wolf, Canis lupus tundrarum',\n",
      " 271: 'red wolf, maned wolf, Canis rufus, Canis niger',\n",
      " 272: 'coyote, prairie wolf, brush wolf, Canis latrans',\n",
      " 273: 'dingo, warrigal, warragal, Canis dingo',\n",
      " 274: 'dhole, Cuon alpinus',\n",
      " 275: 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus',\n",
      " 276: 'hyena, hyaena',\n",
      " 277: 'red fox, Vulpes vulpes',\n",
      " 278: 'kit fox, Vulpes macrotis',\n",
      " 279: 'Arctic fox, white fox, Alopex lagopus',\n",
      " 280: 'grey fox, gray fox, Urocyon cinereoargenteus',\n",
      " 281: 'tabby, tabby cat',\n",
      " 282: 'tiger cat',\n",
      " 283: 'Persian cat',\n",
      " 284: 'Siamese cat, Siamese',\n",
      " 285: 'Egyptian cat',\n",
      " 286: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n",
      " 287: 'lynx, catamount',\n",
      " 288: 'leopard, Panthera pardus',\n",
      " 289: 'snow leopard, ounce, Panthera uncia',\n",
      " 290: 'jaguar, panther, Panthera onca, Felis onca',\n",
      " 291: 'lion, king of beasts, Panthera leo',\n",
      " 292: 'tiger, Panthera tigris',\n",
      " 293: 'cheetah, chetah, Acinonyx jubatus',\n",
      " 294: 'brown bear, bruin, Ursus arctos',\n",
      " 295: 'American black bear, black bear, Ursus americanus, Euarctos americanus',\n",
      " 296: 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus',\n",
      " 297: 'sloth bear, Melursus ursinus, Ursus ursinus',\n",
      " 298: 'mongoose',\n",
      " 299: 'meerkat, mierkat',\n",
      " 300: 'tiger beetle',\n",
      " 301: 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle',\n",
      " 302: 'ground beetle, carabid beetle',\n",
      " 303: 'long-horned beetle, longicorn, longicorn beetle',\n",
      " 304: 'leaf beetle, chrysomelid',\n",
      " 305: 'dung beetle',\n",
      " 306: 'rhinoceros beetle',\n",
      " 307: 'weevil',\n",
      " 308: 'fly',\n",
      " 309: 'bee',\n",
      " 310: 'ant, emmet, pismire',\n",
      " 311: 'grasshopper, hopper',\n",
      " 312: 'cricket',\n",
      " 313: 'walking stick, walkingstick, stick insect',\n",
      " 314: 'cockroach, roach',\n",
      " 315: 'mantis, mantid',\n",
      " 316: 'cicada, cicala',\n",
      " 317: 'leafhopper',\n",
      " 318: 'lacewing, lacewing fly',\n",
      " 319: \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\",\n",
      " 320: 'damselfly',\n",
      " 321: 'admiral',\n",
      " 322: 'ringlet, ringlet butterfly',\n",
      " 323: 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus',\n",
      " 324: 'cabbage butterfly',\n",
      " 325: 'sulphur butterfly, sulfur butterfly',\n",
      " 326: 'lycaenid, lycaenid butterfly',\n",
      " 327: 'starfish, sea star',\n",
      " 328: 'sea urchin',\n",
      " 329: 'sea cucumber, holothurian',\n",
      " 330: 'wood rabbit, cottontail, cottontail rabbit',\n",
      " 331: 'hare',\n",
      " 332: 'Angora, Angora rabbit',\n",
      " 333: 'hamster',\n",
      " 334: 'porcupine, hedgehog',\n",
      " 335: 'fox squirrel, eastern fox squirrel, Sciurus niger',\n",
      " 336: 'marmot',\n",
      " 337: 'beaver',\n",
      " 338: 'guinea pig, Cavia cobaya',\n",
      " 339: 'sorrel',\n",
      " 340: 'zebra',\n",
      " 341: 'hog, pig, grunter, squealer, Sus scrofa',\n",
      " 342: 'wild boar, boar, Sus scrofa',\n",
      " 343: 'warthog',\n",
      " 344: 'hippopotamus, hippo, river horse, Hippopotamus amphibius',\n",
      " 345: 'ox',\n",
      " 346: 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis',\n",
      " 347: 'bison',\n",
      " 348: 'ram, tup',\n",
      " 349: 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis',\n",
      " 350: 'ibex, Capra ibex',\n",
      " 351: 'hartebeest',\n",
      " 352: 'impala, Aepyceros melampus',\n",
      " 353: 'gazelle',\n",
      " 354: 'Arabian camel, dromedary, Camelus dromedarius',\n",
      " 355: 'llama',\n",
      " 356: 'weasel',\n",
      " 357: 'mink',\n",
      " 358: 'polecat, fitch, foulmart, foumart, Mustela putorius',\n",
      " 359: 'black-footed ferret, ferret, Mustela nigripes',\n",
      " 360: 'otter',\n",
      " 361: 'skunk, polecat, wood pussy',\n",
      " 362: 'badger',\n",
      " 363: 'armadillo',\n",
      " 364: 'three-toed sloth, ai, Bradypus tridactylus',\n",
      " 365: 'orangutan, orang, orangutang, Pongo pygmaeus',\n",
      " 366: 'gorilla, Gorilla gorilla',\n",
      " 367: 'chimpanzee, chimp, Pan troglodytes',\n",
      " 368: 'gibbon, Hylobates lar',\n",
      " 369: 'siamang, Hylobates syndactylus, Symphalangus syndactylus',\n",
      " 370: 'guenon, guenon monkey',\n",
      " 371: 'patas, hussar monkey, Erythrocebus patas',\n",
      " 372: 'baboon',\n",
      " 373: 'macaque',\n",
      " 374: 'langur',\n",
      " 375: 'colobus, colobus monkey',\n",
      " 376: 'proboscis monkey, Nasalis larvatus',\n",
      " 377: 'marmoset',\n",
      " 378: 'capuchin, ringtail, Cebus capucinus',\n",
      " 379: 'howler monkey, howler',\n",
      " 380: 'titi, titi monkey',\n",
      " 381: 'spider monkey, Ateles geoffroyi',\n",
      " 382: 'squirrel monkey, Saimiri sciureus',\n",
      " 383: 'Madagascar cat, ring-tailed lemur, Lemur catta',\n",
      " 384: 'indri, indris, Indri indri, Indri brevicaudatus',\n",
      " 385: 'Indian elephant, Elephas maximus',\n",
      " 386: 'African elephant, Loxodonta africana',\n",
      " 387: 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens',\n",
      " 388: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca',\n",
      " 389: 'barracouta, snoek',\n",
      " 390: 'eel',\n",
      " 391: 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch',\n",
      " 392: 'rock beauty, Holocanthus tricolor',\n",
      " 393: 'anemone fish',\n",
      " 394: 'sturgeon',\n",
      " 395: 'gar, garfish, garpike, billfish, Lepisosteus osseus',\n",
      " 396: 'lionfish',\n",
      " 397: 'puffer, pufferfish, blowfish, globefish',\n",
      " 398: 'abacus',\n",
      " 399: 'abaya',\n",
      " 400: \"academic gown, academic robe, judge's robe\",\n",
      " 401: 'accordion, piano accordion, squeeze box',\n",
      " 402: 'acoustic guitar',\n",
      " 403: 'aircraft carrier, carrier, flattop, attack aircraft carrier',\n",
      " 404: 'airliner',\n",
      " 405: 'airship, dirigible',\n",
      " 406: 'altar',\n",
      " 407: 'ambulance',\n",
      " 408: 'amphibian, amphibious vehicle',\n",
      " 409: 'analog clock',\n",
      " 410: 'apiary, bee house',\n",
      " 411: 'apron',\n",
      " 412: 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin',\n",
      " 413: 'assault rifle, assault gun',\n",
      " 414: 'backpack, back pack, knapsack, packsack, rucksack, haversack',\n",
      " 415: 'bakery, bakeshop, bakehouse',\n",
      " 416: 'balance beam, beam',\n",
      " 417: 'balloon',\n",
      " 418: 'ballpoint, ballpoint pen, ballpen, Biro',\n",
      " 419: 'Band Aid',\n",
      " 420: 'banjo',\n",
      " 421: 'bannister, banister, balustrade, balusters, handrail',\n",
      " 422: 'barbell',\n",
      " 423: 'barber chair',\n",
      " 424: 'barbershop',\n",
      " 425: 'barn',\n",
      " 426: 'barometer',\n",
      " 427: 'barrel, cask',\n",
      " 428: 'barrow, garden cart, lawn cart, wheelbarrow',\n",
      " 429: 'baseball',\n",
      " 430: 'basketball',\n",
      " 431: 'bassinet',\n",
      " 432: 'bassoon',\n",
      " 433: 'bathing cap, swimming cap',\n",
      " 434: 'bath towel',\n",
      " 435: 'bathtub, bathing tub, bath, tub',\n",
      " 436: 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon',\n",
      " 437: 'beacon, lighthouse, beacon light, pharos',\n",
      " 438: 'beaker',\n",
      " 439: 'bearskin, busby, shako',\n",
      " 440: 'beer bottle',\n",
      " 441: 'beer glass',\n",
      " 442: 'bell cote, bell cot',\n",
      " 443: 'bib',\n",
      " 444: 'bicycle-built-for-two, tandem bicycle, tandem',\n",
      " 445: 'bikini, two-piece',\n",
      " 446: 'binder, ring-binder',\n",
      " 447: 'binoculars, field glasses, opera glasses',\n",
      " 448: 'birdhouse',\n",
      " 449: 'boathouse',\n",
      " 450: 'bobsled, bobsleigh, bob',\n",
      " 451: 'bolo tie, bolo, bola tie, bola',\n",
      " 452: 'bonnet, poke bonnet',\n",
      " 453: 'bookcase',\n",
      " 454: 'bookshop, bookstore, bookstall',\n",
      " 455: 'bottlecap',\n",
      " 456: 'bow',\n",
      " 457: 'bow tie, bow-tie, bowtie',\n",
      " 458: 'brass, memorial tablet, plaque',\n",
      " 459: 'brassiere, bra, bandeau',\n",
      " 460: 'breakwater, groin, groyne, mole, bulwark, seawall, jetty',\n",
      " 461: 'breastplate, aegis, egis',\n",
      " 462: 'broom',\n",
      " 463: 'bucket, pail',\n",
      " 464: 'buckle',\n",
      " 465: 'bulletproof vest',\n",
      " 466: 'bullet train, bullet',\n",
      " 467: 'butcher shop, meat market',\n",
      " 468: 'cab, hack, taxi, taxicab',\n",
      " 469: 'caldron, cauldron',\n",
      " 470: 'candle, taper, wax light',\n",
      " 471: 'cannon',\n",
      " 472: 'canoe',\n",
      " 473: 'can opener, tin opener',\n",
      " 474: 'cardigan',\n",
      " 475: 'car mirror',\n",
      " 476: 'carousel, carrousel, merry-go-round, roundabout, whirligig',\n",
      " 477: \"carpenter's kit, tool kit\",\n",
      " 478: 'carton',\n",
      " 479: 'car wheel',\n",
      " 480: 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM',\n",
      " 481: 'cassette',\n",
      " 482: 'cassette player',\n",
      " 483: 'castle',\n",
      " 484: 'catamaran',\n",
      " 485: 'CD player',\n",
      " 486: 'cello, violoncello',\n",
      " 487: 'cellular telephone, cellular phone, cellphone, cell, mobile phone',\n",
      " 488: 'chain',\n",
      " 489: 'chainlink fence',\n",
      " 490: 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour',\n",
      " 491: 'chain saw, chainsaw',\n",
      " 492: 'chest',\n",
      " 493: 'chiffonier, commode',\n",
      " 494: 'chime, bell, gong',\n",
      " 495: 'china cabinet, china closet',\n",
      " 496: 'Christmas stocking',\n",
      " 497: 'church, church building',\n",
      " 498: 'cinema, movie theater, movie theatre, movie house, picture palace',\n",
      " 499: 'cleaver, meat cleaver, chopper',\n",
      " 500: 'cliff dwelling',\n",
      " 501: 'cloak',\n",
      " 502: 'clog, geta, patten, sabot',\n",
      " 503: 'cocktail shaker',\n",
      " 504: 'coffee mug',\n",
      " 505: 'coffeepot',\n",
      " 506: 'coil, spiral, volute, whorl, helix',\n",
      " 507: 'combination lock',\n",
      " 508: 'computer keyboard, keypad',\n",
      " 509: 'confectionery, confectionary, candy store',\n",
      " 510: 'container ship, containership, container vessel',\n",
      " 511: 'convertible',\n",
      " 512: 'corkscrew, bottle screw',\n",
      " 513: 'cornet, horn, trumpet, trump',\n",
      " 514: 'cowboy boot',\n",
      " 515: 'cowboy hat, ten-gallon hat',\n",
      " 516: 'cradle',\n",
      " 517: 'crane',\n",
      " 518: 'crash helmet',\n",
      " 519: 'crate',\n",
      " 520: 'crib, cot',\n",
      " 521: 'Crock Pot',\n",
      " 522: 'croquet ball',\n",
      " 523: 'crutch',\n",
      " 524: 'cuirass',\n",
      " 525: 'dam, dike, dyke',\n",
      " 526: 'desk',\n",
      " 527: 'desktop computer',\n",
      " 528: 'dial telephone, dial phone',\n",
      " 529: 'diaper, nappy, napkin',\n",
      " 530: 'digital clock',\n",
      " 531: 'digital watch',\n",
      " 532: 'dining table, board',\n",
      " 533: 'dishrag, dishcloth',\n",
      " 534: 'dishwasher, dish washer, dishwashing machine',\n",
      " 535: 'disk brake, disc brake',\n",
      " 536: 'dock, dockage, docking facility',\n",
      " 537: 'dogsled, dog sled, dog sleigh',\n",
      " 538: 'dome',\n",
      " 539: 'doormat, welcome mat',\n",
      " 540: 'drilling platform, offshore rig',\n",
      " 541: 'drum, membranophone, tympan',\n",
      " 542: 'drumstick',\n",
      " 543: 'dumbbell',\n",
      " 544: 'Dutch oven',\n",
      " 545: 'electric fan, blower',\n",
      " 546: 'electric guitar',\n",
      " 547: 'electric locomotive',\n",
      " 548: 'entertainment center',\n",
      " 549: 'envelope',\n",
      " 550: 'espresso maker',\n",
      " 551: 'face powder',\n",
      " 552: 'feather boa, boa',\n",
      " 553: 'file, file cabinet, filing cabinet',\n",
      " 554: 'fireboat',\n",
      " 555: 'fire engine, fire truck',\n",
      " 556: 'fire screen, fireguard',\n",
      " 557: 'flagpole, flagstaff',\n",
      " 558: 'flute, transverse flute',\n",
      " 559: 'folding chair',\n",
      " 560: 'football helmet',\n",
      " 561: 'forklift',\n",
      " 562: 'fountain',\n",
      " 563: 'fountain pen',\n",
      " 564: 'four-poster',\n",
      " 565: 'freight car',\n",
      " 566: 'French horn, horn',\n",
      " 567: 'frying pan, frypan, skillet',\n",
      " 568: 'fur coat',\n",
      " 569: 'garbage truck, dustcart',\n",
      " 570: 'gasmask, respirator, gas helmet',\n",
      " 571: 'gas pump, gasoline pump, petrol pump, island dispenser',\n",
      " 572: 'goblet',\n",
      " 573: 'go-kart',\n",
      " 574: 'golf ball',\n",
      " 575: 'golfcart, golf cart',\n",
      " 576: 'gondola',\n",
      " 577: 'gong, tam-tam',\n",
      " 578: 'gown',\n",
      " 579: 'grand piano, grand',\n",
      " 580: 'greenhouse, nursery, glasshouse',\n",
      " 581: 'grille, radiator grille',\n",
      " 582: 'grocery store, grocery, food market, market',\n",
      " 583: 'guillotine',\n",
      " 584: 'hair slide',\n",
      " 585: 'hair spray',\n",
      " 586: 'half track',\n",
      " 587: 'hammer',\n",
      " 588: 'hamper',\n",
      " 589: 'hand blower, blow dryer, blow drier, hair dryer, hair drier',\n",
      " 590: 'hand-held computer, hand-held microcomputer',\n",
      " 591: 'handkerchief, hankie, hanky, hankey',\n",
      " 592: 'hard disc, hard disk, fixed disk',\n",
      " 593: 'harmonica, mouth organ, harp, mouth harp',\n",
      " 594: 'harp',\n",
      " 595: 'harvester, reaper',\n",
      " 596: 'hatchet',\n",
      " 597: 'holster',\n",
      " 598: 'home theater, home theatre',\n",
      " 599: 'honeycomb',\n",
      " 600: 'hook, claw',\n",
      " 601: 'hoopskirt, crinoline',\n",
      " 602: 'horizontal bar, high bar',\n",
      " 603: 'horse cart, horse-cart',\n",
      " 604: 'hourglass',\n",
      " 605: 'iPod',\n",
      " 606: 'iron, smoothing iron',\n",
      " 607: \"jack-o'-lantern\",\n",
      " 608: 'jean, blue jean, denim',\n",
      " 609: 'jeep, landrover',\n",
      " 610: 'jersey, T-shirt, tee shirt',\n",
      " 611: 'jigsaw puzzle',\n",
      " 612: 'jinrikisha, ricksha, rickshaw',\n",
      " 613: 'joystick',\n",
      " 614: 'kimono',\n",
      " 615: 'knee pad',\n",
      " 616: 'knot',\n",
      " 617: 'lab coat, laboratory coat',\n",
      " 618: 'ladle',\n",
      " 619: 'lampshade, lamp shade',\n",
      " 620: 'laptop, laptop computer',\n",
      " 621: 'lawn mower, mower',\n",
      " 622: 'lens cap, lens cover',\n",
      " 623: 'letter opener, paper knife, paperknife',\n",
      " 624: 'library',\n",
      " 625: 'lifeboat',\n",
      " 626: 'lighter, light, igniter, ignitor',\n",
      " 627: 'limousine, limo',\n",
      " 628: 'liner, ocean liner',\n",
      " 629: 'lipstick, lip rouge',\n",
      " 630: 'Loafer',\n",
      " 631: 'lotion',\n",
      " 632: 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system',\n",
      " 633: \"loupe, jeweler's loupe\",\n",
      " 634: 'lumbermill, sawmill',\n",
      " 635: 'magnetic compass',\n",
      " 636: 'mailbag, postbag',\n",
      " 637: 'mailbox, letter box',\n",
      " 638: 'maillot',\n",
      " 639: 'maillot, tank suit',\n",
      " 640: 'manhole cover',\n",
      " 641: 'maraca',\n",
      " 642: 'marimba, xylophone',\n",
      " 643: 'mask',\n",
      " 644: 'matchstick',\n",
      " 645: 'maypole',\n",
      " 646: 'maze, labyrinth',\n",
      " 647: 'measuring cup',\n",
      " 648: 'medicine chest, medicine cabinet',\n",
      " 649: 'megalith, megalithic structure',\n",
      " 650: 'microphone, mike',\n",
      " 651: 'microwave, microwave oven',\n",
      " 652: 'military uniform',\n",
      " 653: 'milk can',\n",
      " 654: 'minibus',\n",
      " 655: 'miniskirt, mini',\n",
      " 656: 'minivan',\n",
      " 657: 'missile',\n",
      " 658: 'mitten',\n",
      " 659: 'mixing bowl',\n",
      " 660: 'mobile home, manufactured home',\n",
      " 661: 'Model T',\n",
      " 662: 'modem',\n",
      " 663: 'monastery',\n",
      " 664: 'monitor',\n",
      " 665: 'moped',\n",
      " 666: 'mortar',\n",
      " 667: 'mortarboard',\n",
      " 668: 'mosque',\n",
      " 669: 'mosquito net',\n",
      " 670: 'motor scooter, scooter',\n",
      " 671: 'mountain bike, all-terrain bike, off-roader',\n",
      " 672: 'mountain tent',\n",
      " 673: 'mouse, computer mouse',\n",
      " 674: 'mousetrap',\n",
      " 675: 'moving van',\n",
      " 676: 'muzzle',\n",
      " 677: 'nail',\n",
      " 678: 'neck brace',\n",
      " 679: 'necklace',\n",
      " 680: 'nipple',\n",
      " 681: 'notebook, notebook computer',\n",
      " 682: 'obelisk',\n",
      " 683: 'oboe, hautboy, hautbois',\n",
      " 684: 'ocarina, sweet potato',\n",
      " 685: 'odometer, hodometer, mileometer, milometer',\n",
      " 686: 'oil filter',\n",
      " 687: 'organ, pipe organ',\n",
      " 688: 'oscilloscope, scope, cathode-ray oscilloscope, CRO',\n",
      " 689: 'overskirt',\n",
      " 690: 'oxcart',\n",
      " 691: 'oxygen mask',\n",
      " 692: 'packet',\n",
      " 693: 'paddle, boat paddle',\n",
      " 694: 'paddlewheel, paddle wheel',\n",
      " 695: 'padlock',\n",
      " 696: 'paintbrush',\n",
      " 697: \"pajama, pyjama, pj's, jammies\",\n",
      " 698: 'palace',\n",
      " 699: 'panpipe, pandean pipe, syrinx',\n",
      " 700: 'paper towel',\n",
      " 701: 'parachute, chute',\n",
      " 702: 'parallel bars, bars',\n",
      " 703: 'park bench',\n",
      " 704: 'parking meter',\n",
      " 705: 'passenger car, coach, carriage',\n",
      " 706: 'patio, terrace',\n",
      " 707: 'pay-phone, pay-station',\n",
      " 708: 'pedestal, plinth, footstall',\n",
      " 709: 'pencil box, pencil case',\n",
      " 710: 'pencil sharpener',\n",
      " 711: 'perfume, essence',\n",
      " 712: 'Petri dish',\n",
      " 713: 'photocopier',\n",
      " 714: 'pick, plectrum, plectron',\n",
      " 715: 'pickelhaube',\n",
      " 716: 'picket fence, paling',\n",
      " 717: 'pickup, pickup truck',\n",
      " 718: 'pier',\n",
      " 719: 'piggy bank, penny bank',\n",
      " 720: 'pill bottle',\n",
      " 721: 'pillow',\n",
      " 722: 'ping-pong ball',\n",
      " 723: 'pinwheel',\n",
      " 724: 'pirate, pirate ship',\n",
      " 725: 'pitcher, ewer',\n",
      " 726: \"plane, carpenter's plane, woodworking plane\",\n",
      " 727: 'planetarium',\n",
      " 728: 'plastic bag',\n",
      " 729: 'plate rack',\n",
      " 730: 'plow, plough',\n",
      " 731: \"plunger, plumber's helper\",\n",
      " 732: 'Polaroid camera, Polaroid Land camera',\n",
      " 733: 'pole',\n",
      " 734: 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria',\n",
      " 735: 'poncho',\n",
      " 736: 'pool table, billiard table, snooker table',\n",
      " 737: 'pop bottle, soda bottle',\n",
      " 738: 'pot, flowerpot',\n",
      " 739: \"potter's wheel\",\n",
      " 740: 'power drill',\n",
      " 741: 'prayer rug, prayer mat',\n",
      " 742: 'printer',\n",
      " 743: 'prison, prison house',\n",
      " 744: 'projectile, missile',\n",
      " 745: 'projector',\n",
      " 746: 'puck, hockey puck',\n",
      " 747: 'punching bag, punch bag, punching ball, punchball',\n",
      " 748: 'purse',\n",
      " 749: 'quill, quill pen',\n",
      " 750: 'quilt, comforter, comfort, puff',\n",
      " 751: 'racer, race car, racing car',\n",
      " 752: 'racket, racquet',\n",
      " 753: 'radiator',\n",
      " 754: 'radio, wireless',\n",
      " 755: 'radio telescope, radio reflector',\n",
      " 756: 'rain barrel',\n",
      " 757: 'recreational vehicle, RV, R.V.',\n",
      " 758: 'reel',\n",
      " 759: 'reflex camera',\n",
      " 760: 'refrigerator, icebox',\n",
      " 761: 'remote control, remote',\n",
      " 762: 'restaurant, eating house, eating place, eatery',\n",
      " 763: 'revolver, six-gun, six-shooter',\n",
      " 764: 'rifle',\n",
      " 765: 'rocking chair, rocker',\n",
      " 766: 'rotisserie',\n",
      " 767: 'rubber eraser, rubber, pencil eraser',\n",
      " 768: 'rugby ball',\n",
      " 769: 'rule, ruler',\n",
      " 770: 'running shoe',\n",
      " 771: 'safe',\n",
      " 772: 'safety pin',\n",
      " 773: 'saltshaker, salt shaker',\n",
      " 774: 'sandal',\n",
      " 775: 'sarong',\n",
      " 776: 'sax, saxophone',\n",
      " 777: 'scabbard',\n",
      " 778: 'scale, weighing machine',\n",
      " 779: 'school bus',\n",
      " 780: 'schooner',\n",
      " 781: 'scoreboard',\n",
      " 782: 'screen, CRT screen',\n",
      " 783: 'screw',\n",
      " 784: 'screwdriver',\n",
      " 785: 'seat belt, seatbelt',\n",
      " 786: 'sewing machine',\n",
      " 787: 'shield, buckler',\n",
      " 788: 'shoe shop, shoe-shop, shoe store',\n",
      " 789: 'shoji',\n",
      " 790: 'shopping basket',\n",
      " 791: 'shopping cart',\n",
      " 792: 'shovel',\n",
      " 793: 'shower cap',\n",
      " 794: 'shower curtain',\n",
      " 795: 'ski',\n",
      " 796: 'ski mask',\n",
      " 797: 'sleeping bag',\n",
      " 798: 'slide rule, slipstick',\n",
      " 799: 'sliding door',\n",
      " 800: 'slot, one-armed bandit',\n",
      " 801: 'snorkel',\n",
      " 802: 'snowmobile',\n",
      " 803: 'snowplow, snowplough',\n",
      " 804: 'soap dispenser',\n",
      " 805: 'soccer ball',\n",
      " 806: 'sock',\n",
      " 807: 'solar dish, solar collector, solar furnace',\n",
      " 808: 'sombrero',\n",
      " 809: 'soup bowl',\n",
      " 810: 'space bar',\n",
      " 811: 'space heater',\n",
      " 812: 'space shuttle',\n",
      " 813: 'spatula',\n",
      " 814: 'speedboat',\n",
      " 815: \"spider web, spider's web\",\n",
      " 816: 'spindle',\n",
      " 817: 'sports car, sport car',\n",
      " 818: 'spotlight, spot',\n",
      " 819: 'stage',\n",
      " 820: 'steam locomotive',\n",
      " 821: 'steel arch bridge',\n",
      " 822: 'steel drum',\n",
      " 823: 'stethoscope',\n",
      " 824: 'stole',\n",
      " 825: 'stone wall',\n",
      " 826: 'stopwatch, stop watch',\n",
      " 827: 'stove',\n",
      " 828: 'strainer',\n",
      " 829: 'streetcar, tram, tramcar, trolley, trolley car',\n",
      " 830: 'stretcher',\n",
      " 831: 'studio couch, day bed',\n",
      " 832: 'stupa, tope',\n",
      " 833: 'submarine, pigboat, sub, U-boat',\n",
      " 834: 'suit, suit of clothes',\n",
      " 835: 'sundial',\n",
      " 836: 'sunglass',\n",
      " 837: 'sunglasses, dark glasses, shades',\n",
      " 838: 'sunscreen, sunblock, sun blocker',\n",
      " 839: 'suspension bridge',\n",
      " 840: 'swab, swob, mop',\n",
      " 841: 'sweatshirt',\n",
      " 842: 'swimming trunks, bathing trunks',\n",
      " 843: 'swing',\n",
      " 844: 'switch, electric switch, electrical switch',\n",
      " 845: 'syringe',\n",
      " 846: 'table lamp',\n",
      " 847: 'tank, army tank, armored combat vehicle, armoured combat vehicle',\n",
      " 848: 'tape player',\n",
      " 849: 'teapot',\n",
      " 850: 'teddy, teddy bear',\n",
      " 851: 'television, television system',\n",
      " 852: 'tennis ball',\n",
      " 853: 'thatch, thatched roof',\n",
      " 854: 'theater curtain, theatre curtain',\n",
      " 855: 'thimble',\n",
      " 856: 'thresher, thrasher, threshing machine',\n",
      " 857: 'throne',\n",
      " 858: 'tile roof',\n",
      " 859: 'toaster',\n",
      " 860: 'tobacco shop, tobacconist shop, tobacconist',\n",
      " 861: 'toilet seat',\n",
      " 862: 'torch',\n",
      " 863: 'totem pole',\n",
      " 864: 'tow truck, tow car, wrecker',\n",
      " 865: 'toyshop',\n",
      " 866: 'tractor',\n",
      " 867: 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi',\n",
      " 868: 'tray',\n",
      " 869: 'trench coat',\n",
      " 870: 'tricycle, trike, velocipede',\n",
      " 871: 'trimaran',\n",
      " 872: 'tripod',\n",
      " 873: 'triumphal arch',\n",
      " 874: 'trolleybus, trolley coach, trackless trolley',\n",
      " 875: 'trombone',\n",
      " 876: 'tub, vat',\n",
      " 877: 'turnstile',\n",
      " 878: 'typewriter keyboard',\n",
      " 879: 'umbrella',\n",
      " 880: 'unicycle, monocycle',\n",
      " 881: 'upright, upright piano',\n",
      " 882: 'vacuum, vacuum cleaner',\n",
      " 883: 'vase',\n",
      " 884: 'vault',\n",
      " 885: 'velvet',\n",
      " 886: 'vending machine',\n",
      " 887: 'vestment',\n",
      " 888: 'viaduct',\n",
      " 889: 'violin, fiddle',\n",
      " 890: 'volleyball',\n",
      " 891: 'waffle iron',\n",
      " 892: 'wall clock',\n",
      " 893: 'wallet, billfold, notecase, pocketbook',\n",
      " 894: 'wardrobe, closet, press',\n",
      " 895: 'warplane, military plane',\n",
      " 896: 'washbasin, handbasin, washbowl, lavabo, wash-hand basin',\n",
      " 897: 'washer, automatic washer, washing machine',\n",
      " 898: 'water bottle',\n",
      " 899: 'water jug',\n",
      " 900: 'water tower',\n",
      " 901: 'whiskey jug',\n",
      " 902: 'whistle',\n",
      " 903: 'wig',\n",
      " 904: 'window screen',\n",
      " 905: 'window shade',\n",
      " 906: 'Windsor tie',\n",
      " 907: 'wine bottle',\n",
      " 908: 'wing',\n",
      " 909: 'wok',\n",
      " 910: 'wooden spoon',\n",
      " 911: 'wool, woolen, woollen',\n",
      " 912: 'worm fence, snake fence, snake-rail fence, Virginia fence',\n",
      " 913: 'wreck',\n",
      " 914: 'yawl',\n",
      " 915: 'yurt',\n",
      " 916: 'web site, website, internet site, site',\n",
      " 917: 'comic book',\n",
      " 918: 'crossword puzzle, crossword',\n",
      " 919: 'street sign',\n",
      " 920: 'traffic light, traffic signal, stoplight',\n",
      " 921: 'book jacket, dust cover, dust jacket, dust wrapper',\n",
      " 922: 'menu',\n",
      " 923: 'plate',\n",
      " 924: 'guacamole',\n",
      " 925: 'consomme',\n",
      " 926: 'hot pot, hotpot',\n",
      " 927: 'trifle',\n",
      " 928: 'ice cream, icecream',\n",
      " 929: 'ice lolly, lolly, lollipop, popsicle',\n",
      " 930: 'French loaf',\n",
      " 931: 'bagel, beigel',\n",
      " 932: 'pretzel',\n",
      " 933: 'cheeseburger',\n",
      " 934: 'hotdog, hot dog, red hot',\n",
      " 935: 'mashed potato',\n",
      " 936: 'head cabbage',\n",
      " 937: 'broccoli',\n",
      " 938: 'cauliflower',\n",
      " 939: 'zucchini, courgette',\n",
      " 940: 'spaghetti squash',\n",
      " 941: 'acorn squash',\n",
      " 942: 'butternut squash',\n",
      " 943: 'cucumber, cuke',\n",
      " 944: 'artichoke, globe artichoke',\n",
      " 945: 'bell pepper',\n",
      " 946: 'cardoon',\n",
      " 947: 'mushroom',\n",
      " 948: 'Granny Smith',\n",
      " 949: 'strawberry',\n",
      " 950: 'orange',\n",
      " 951: 'lemon',\n",
      " 952: 'fig',\n",
      " 953: 'pineapple, ananas',\n",
      " 954: 'banana',\n",
      " 955: 'jackfruit, jak, jack',\n",
      " 956: 'custard apple',\n",
      " 957: 'pomegranate',\n",
      " 958: 'hay',\n",
      " 959: 'carbonara',\n",
      " 960: 'chocolate sauce, chocolate syrup',\n",
      " 961: 'dough',\n",
      " 962: 'meat loaf, meatloaf',\n",
      " 963: 'pizza, pizza pie',\n",
      " 964: 'potpie',\n",
      " 965: 'burrito',\n",
      " 966: 'red wine',\n",
      " 967: 'espresso',\n",
      " 968: 'cup',\n",
      " 969: 'eggnog',\n",
      " 970: 'alp',\n",
      " 971: 'bubble',\n",
      " 972: 'cliff, drop, drop-off',\n",
      " 973: 'coral reef',\n",
      " 974: 'geyser',\n",
      " 975: 'lakeside, lakeshore',\n",
      " 976: 'promontory, headland, head, foreland',\n",
      " 977: 'sandbar, sand bar',\n",
      " 978: 'seashore, coast, seacoast, sea-coast',\n",
      " 979: 'valley, vale',\n",
      " 980: 'volcano',\n",
      " 981: 'ballplayer, baseball player',\n",
      " 982: 'groom, bridegroom',\n",
      " 983: 'scuba diver',\n",
      " 984: 'rapeseed',\n",
      " 985: 'daisy',\n",
      " 986: \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\",\n",
      " 987: 'corn',\n",
      " 988: 'acorn',\n",
      " 989: 'hip, rose hip, rosehip',\n",
      " 990: 'buckeye, horse chestnut, conker',\n",
      " 991: 'coral fungus',\n",
      " 992: 'agaric',\n",
      " 993: 'gyromitra',\n",
      " 994: 'stinkhorn, carrion fungus',\n",
      " 995: 'earthstar',\n",
      " 996: 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa',\n",
      " 997: 'bolete',\n",
      " 998: 'ear, spike, capitulum',\n",
      " 999: 'toilet tissue, toilet paper, bathroom tissue'}\n",
      "74/3:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 380\n",
      "\n",
      "def prep_image(image):\n",
      "    res = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "    res = res.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "    return res\n",
      "\n",
      "def predict(image):\n",
      "    cropped_image = center_crop(image)\n",
      "    prediction = (model.predict(prep_image(image)), model.predict(prep_image(cropped_image)))\n",
      "    if np.max(prediction[0]>=prediction[1]):\n",
      "        return {'class': np.argmax(prediction[0], axis=1)[0], 'all_prob': prediction[0]}\n",
      "    else:\n",
      "        return  {'class': np.argmax(prediction[1], axis=1)[1], 'all_prob': prediction[1]}\n",
      "    \n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "plt.imshow(image)\n",
      "\n",
      "print(class_dict[predict(image)])\n",
      "76/1:\n",
      "from tensorflow.keras.applications import EfficientNetB4\n",
      "import tensorflow as tf\n",
      "model = EfficientNetB4(weights='imagenet')\n",
      "\n",
      "def center_crop(img, new_width=None, new_height=None):        \n",
      "\n",
      "    width = img.shape[1]\n",
      "    height = img.shape[0]\n",
      "\n",
      "    if new_width is None:\n",
      "        new_width = min(width, height)\n",
      "\n",
      "    if new_height is None:\n",
      "        new_height = min(width, height)\n",
      "\n",
      "    left = int(np.ceil((width - new_width) / 2))\n",
      "    right = width - int(np.floor((width - new_width) / 2))\n",
      "\n",
      "    top = int(np.ceil((height - new_height) / 2))\n",
      "    bottom = height - int(np.floor((height - new_height) / 2))\n",
      "\n",
      "    if len(img.shape) == 2:\n",
      "        center_cropped_img = img[top:bottom, left:right]\n",
      "    else:\n",
      "        center_cropped_img = img[top:bottom, left:right, ...]\n",
      "\n",
      "    return center_cropped_img\n",
      "\n",
      "class_dict = {0: 'tench, Tinca tinca',\n",
      " 1: 'goldfish, Carassius auratus',\n",
      " 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n",
      " 3: 'tiger shark, Galeocerdo cuvieri',\n",
      " 4: 'hammerhead, hammerhead shark',\n",
      " 5: 'electric ray, crampfish, numbfish, torpedo',\n",
      " 6: 'stingray',\n",
      " 7: 'cock',\n",
      " 8: 'hen',\n",
      " 9: 'ostrich, Struthio camelus',\n",
      " 10: 'brambling, Fringilla montifringilla',\n",
      " 11: 'goldfinch, Carduelis carduelis',\n",
      " 12: 'house finch, linnet, Carpodacus mexicanus',\n",
      " 13: 'junco, snowbird',\n",
      " 14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea',\n",
      " 15: 'robin, American robin, Turdus migratorius',\n",
      " 16: 'bulbul',\n",
      " 17: 'jay',\n",
      " 18: 'magpie',\n",
      " 19: 'chickadee',\n",
      " 20: 'water ouzel, dipper',\n",
      " 21: 'kite',\n",
      " 22: 'bald eagle, American eagle, Haliaeetus leucocephalus',\n",
      " 23: 'vulture',\n",
      " 24: 'great grey owl, great gray owl, Strix nebulosa',\n",
      " 25: 'European fire salamander, Salamandra salamandra',\n",
      " 26: 'common newt, Triturus vulgaris',\n",
      " 27: 'eft',\n",
      " 28: 'spotted salamander, Ambystoma maculatum',\n",
      " 29: 'axolotl, mud puppy, Ambystoma mexicanum',\n",
      " 30: 'bullfrog, Rana catesbeiana',\n",
      " 31: 'tree frog, tree-frog',\n",
      " 32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui',\n",
      " 33: 'loggerhead, loggerhead turtle, Caretta caretta',\n",
      " 34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea',\n",
      " 35: 'mud turtle',\n",
      " 36: 'terrapin',\n",
      " 37: 'box turtle, box tortoise',\n",
      " 38: 'banded gecko',\n",
      " 39: 'common iguana, iguana, Iguana iguana',\n",
      " 40: 'American chameleon, anole, Anolis carolinensis',\n",
      " 41: 'whiptail, whiptail lizard',\n",
      " 42: 'agama',\n",
      " 43: 'frilled lizard, Chlamydosaurus kingi',\n",
      " 44: 'alligator lizard',\n",
      " 45: 'Gila monster, Heloderma suspectum',\n",
      " 46: 'green lizard, Lacerta viridis',\n",
      " 47: 'African chameleon, Chamaeleo chamaeleon',\n",
      " 48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis',\n",
      " 49: 'African crocodile, Nile crocodile, Crocodylus niloticus',\n",
      " 50: 'American alligator, Alligator mississipiensis',\n",
      " 51: 'triceratops',\n",
      " 52: 'thunder snake, worm snake, Carphophis amoenus',\n",
      " 53: 'ringneck snake, ring-necked snake, ring snake',\n",
      " 54: 'hognose snake, puff adder, sand viper',\n",
      " 55: 'green snake, grass snake',\n",
      " 56: 'king snake, kingsnake',\n",
      " 57: 'garter snake, grass snake',\n",
      " 58: 'water snake',\n",
      " 59: 'vine snake',\n",
      " 60: 'night snake, Hypsiglena torquata',\n",
      " 61: 'boa constrictor, Constrictor constrictor',\n",
      " 62: 'rock python, rock snake, Python sebae',\n",
      " 63: 'Indian cobra, Naja naja',\n",
      " 64: 'green mamba',\n",
      " 65: 'sea snake',\n",
      " 66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus',\n",
      " 67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus',\n",
      " 68: 'sidewinder, horned rattlesnake, Crotalus cerastes',\n",
      " 69: 'trilobite',\n",
      " 70: 'harvestman, daddy longlegs, Phalangium opilio',\n",
      " 71: 'scorpion',\n",
      " 72: 'black and gold garden spider, Argiope aurantia',\n",
      " 73: 'barn spider, Araneus cavaticus',\n",
      " 74: 'garden spider, Aranea diademata',\n",
      " 75: 'black widow, Latrodectus mactans',\n",
      " 76: 'tarantula',\n",
      " 77: 'wolf spider, hunting spider',\n",
      " 78: 'tick',\n",
      " 79: 'centipede',\n",
      " 80: 'black grouse',\n",
      " 81: 'ptarmigan',\n",
      " 82: 'ruffed grouse, partridge, Bonasa umbellus',\n",
      " 83: 'prairie chicken, prairie grouse, prairie fowl',\n",
      " 84: 'peacock',\n",
      " 85: 'quail',\n",
      " 86: 'partridge',\n",
      " 87: 'African grey, African gray, Psittacus erithacus',\n",
      " 88: 'macaw',\n",
      " 89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n",
      " 90: 'lorikeet',\n",
      " 91: 'coucal',\n",
      " 92: 'bee eater',\n",
      " 93: 'hornbill',\n",
      " 94: 'hummingbird',\n",
      " 95: 'jacamar',\n",
      " 96: 'toucan',\n",
      " 97: 'drake',\n",
      " 98: 'red-breasted merganser, Mergus serrator',\n",
      " 99: 'goose',\n",
      " 100: 'black swan, Cygnus atratus',\n",
      " 101: 'tusker',\n",
      " 102: 'echidna, spiny anteater, anteater',\n",
      " 103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n",
      " 104: 'wallaby, brush kangaroo',\n",
      " 105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n",
      " 106: 'wombat',\n",
      " 107: 'jellyfish',\n",
      " 108: 'sea anemone, anemone',\n",
      " 109: 'brain coral',\n",
      " 110: 'flatworm, platyhelminth',\n",
      " 111: 'nematode, nematode worm, roundworm',\n",
      " 112: 'conch',\n",
      " 113: 'snail',\n",
      " 114: 'slug',\n",
      " 115: 'sea slug, nudibranch',\n",
      " 116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore',\n",
      " 117: 'chambered nautilus, pearly nautilus, nautilus',\n",
      " 118: 'Dungeness crab, Cancer magister',\n",
      " 119: 'rock crab, Cancer irroratus',\n",
      " 120: 'fiddler crab',\n",
      " 121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica',\n",
      " 122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus',\n",
      " 123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish',\n",
      " 124: 'crayfish, crawfish, crawdad, crawdaddy',\n",
      " 125: 'hermit crab',\n",
      " 126: 'isopod',\n",
      " 127: 'white stork, Ciconia ciconia',\n",
      " 128: 'black stork, Ciconia nigra',\n",
      " 129: 'spoonbill',\n",
      " 130: 'flamingo',\n",
      " 131: 'little blue heron, Egretta caerulea',\n",
      " 132: 'American egret, great white heron, Egretta albus',\n",
      " 133: 'bittern',\n",
      " 134: 'crane',\n",
      " 135: 'limpkin, Aramus pictus',\n",
      " 136: 'European gallinule, Porphyrio porphyrio',\n",
      " 137: 'American coot, marsh hen, mud hen, water hen, Fulica americana',\n",
      " 138: 'bustard',\n",
      " 139: 'ruddy turnstone, Arenaria interpres',\n",
      " 140: 'red-backed sandpiper, dunlin, Erolia alpina',\n",
      " 141: 'redshank, Tringa totanus',\n",
      " 142: 'dowitcher',\n",
      " 143: 'oystercatcher, oyster catcher',\n",
      " 144: 'pelican',\n",
      " 145: 'king penguin, Aptenodytes patagonica',\n",
      " 146: 'albatross, mollymawk',\n",
      " 147: 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus',\n",
      " 148: 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca',\n",
      " 149: 'dugong, Dugong dugon',\n",
      " 150: 'sea lion',\n",
      " 151: 'Chihuahua',\n",
      " 152: 'Japanese spaniel',\n",
      " 153: 'Maltese dog, Maltese terrier, Maltese',\n",
      " 154: 'Pekinese, Pekingese, Peke',\n",
      " 155: 'Shih-Tzu',\n",
      " 156: 'Blenheim spaniel',\n",
      " 157: 'papillon',\n",
      " 158: 'toy terrier',\n",
      " 159: 'Rhodesian ridgeback',\n",
      " 160: 'Afghan hound, Afghan',\n",
      " 161: 'basset, basset hound',\n",
      " 162: 'beagle',\n",
      " 163: 'bloodhound, sleuthhound',\n",
      " 164: 'bluetick',\n",
      " 165: 'black-and-tan coonhound',\n",
      " 166: 'Walker hound, Walker foxhound',\n",
      " 167: 'English foxhound',\n",
      " 168: 'redbone',\n",
      " 169: 'borzoi, Russian wolfhound',\n",
      " 170: 'Irish wolfhound',\n",
      " 171: 'Italian greyhound',\n",
      " 172: 'whippet',\n",
      " 173: 'Ibizan hound, Ibizan Podenco',\n",
      " 174: 'Norwegian elkhound, elkhound',\n",
      " 175: 'otterhound, otter hound',\n",
      " 176: 'Saluki, gazelle hound',\n",
      " 177: 'Scottish deerhound, deerhound',\n",
      " 178: 'Weimaraner',\n",
      " 179: 'Staffordshire bullterrier, Staffordshire bull terrier',\n",
      " 180: 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier',\n",
      " 181: 'Bedlington terrier',\n",
      " 182: 'Border terrier',\n",
      " 183: 'Kerry blue terrier',\n",
      " 184: 'Irish terrier',\n",
      " 185: 'Norfolk terrier',\n",
      " 186: 'Norwich terrier',\n",
      " 187: 'Yorkshire terrier',\n",
      " 188: 'wire-haired fox terrier',\n",
      " 189: 'Lakeland terrier',\n",
      " 190: 'Sealyham terrier, Sealyham',\n",
      " 191: 'Airedale, Airedale terrier',\n",
      " 192: 'cairn, cairn terrier',\n",
      " 193: 'Australian terrier',\n",
      " 194: 'Dandie Dinmont, Dandie Dinmont terrier',\n",
      " 195: 'Boston bull, Boston terrier',\n",
      " 196: 'miniature schnauzer',\n",
      " 197: 'giant schnauzer',\n",
      " 198: 'standard schnauzer',\n",
      " 199: 'Scotch terrier, Scottish terrier, Scottie',\n",
      " 200: 'Tibetan terrier, chrysanthemum dog',\n",
      " 201: 'silky terrier, Sydney silky',\n",
      " 202: 'soft-coated wheaten terrier',\n",
      " 203: 'West Highland white terrier',\n",
      " 204: 'Lhasa, Lhasa apso',\n",
      " 205: 'flat-coated retriever',\n",
      " 206: 'curly-coated retriever',\n",
      " 207: 'golden retriever',\n",
      " 208: 'Labrador retriever',\n",
      " 209: 'Chesapeake Bay retriever',\n",
      " 210: 'German short-haired pointer',\n",
      " 211: 'vizsla, Hungarian pointer',\n",
      " 212: 'English setter',\n",
      " 213: 'Irish setter, red setter',\n",
      " 214: 'Gordon setter',\n",
      " 215: 'Brittany spaniel',\n",
      " 216: 'clumber, clumber spaniel',\n",
      " 217: 'English springer, English springer spaniel',\n",
      " 218: 'Welsh springer spaniel',\n",
      " 219: 'cocker spaniel, English cocker spaniel, cocker',\n",
      " 220: 'Sussex spaniel',\n",
      " 221: 'Irish water spaniel',\n",
      " 222: 'kuvasz',\n",
      " 223: 'schipperke',\n",
      " 224: 'groenendael',\n",
      " 225: 'malinois',\n",
      " 226: 'briard',\n",
      " 227: 'kelpie',\n",
      " 228: 'komondor',\n",
      " 229: 'Old English sheepdog, bobtail',\n",
      " 230: 'Shetland sheepdog, Shetland sheep dog, Shetland',\n",
      " 231: 'collie',\n",
      " 232: 'Border collie',\n",
      " 233: 'Bouvier des Flandres, Bouviers des Flandres',\n",
      " 234: 'Rottweiler',\n",
      " 235: 'German shepherd, German shepherd dog, German police dog, alsatian',\n",
      " 236: 'Doberman, Doberman pinscher',\n",
      " 237: 'miniature pinscher',\n",
      " 238: 'Greater Swiss Mountain dog',\n",
      " 239: 'Bernese mountain dog',\n",
      " 240: 'Appenzeller',\n",
      " 241: 'EntleBucher',\n",
      " 242: 'boxer',\n",
      " 243: 'bull mastiff',\n",
      " 244: 'Tibetan mastiff',\n",
      " 245: 'French bulldog',\n",
      " 246: 'Great Dane',\n",
      " 247: 'Saint Bernard, St Bernard',\n",
      " 248: 'Eskimo dog, husky',\n",
      " 249: 'malamute, malemute, Alaskan malamute',\n",
      " 250: 'Siberian husky',\n",
      " 251: 'dalmatian, coach dog, carriage dog',\n",
      " 252: 'affenpinscher, monkey pinscher, monkey dog',\n",
      " 253: 'basenji',\n",
      " 254: 'pug, pug-dog',\n",
      " 255: 'Leonberg',\n",
      " 256: 'Newfoundland, Newfoundland dog',\n",
      " 257: 'Great Pyrenees',\n",
      " 258: 'Samoyed, Samoyede',\n",
      " 259: 'Pomeranian',\n",
      " 260: 'chow, chow chow',\n",
      " 261: 'keeshond',\n",
      " 262: 'Brabancon griffon',\n",
      " 263: 'Pembroke, Pembroke Welsh corgi',\n",
      " 264: 'Cardigan, Cardigan Welsh corgi',\n",
      " 265: 'toy poodle',\n",
      " 266: 'miniature poodle',\n",
      " 267: 'standard poodle',\n",
      " 268: 'Mexican hairless',\n",
      " 269: 'timber wolf, grey wolf, gray wolf, Canis lupus',\n",
      " 270: 'white wolf, Arctic wolf, Canis lupus tundrarum',\n",
      " 271: 'red wolf, maned wolf, Canis rufus, Canis niger',\n",
      " 272: 'coyote, prairie wolf, brush wolf, Canis latrans',\n",
      " 273: 'dingo, warrigal, warragal, Canis dingo',\n",
      " 274: 'dhole, Cuon alpinus',\n",
      " 275: 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus',\n",
      " 276: 'hyena, hyaena',\n",
      " 277: 'red fox, Vulpes vulpes',\n",
      " 278: 'kit fox, Vulpes macrotis',\n",
      " 279: 'Arctic fox, white fox, Alopex lagopus',\n",
      " 280: 'grey fox, gray fox, Urocyon cinereoargenteus',\n",
      " 281: 'tabby, tabby cat',\n",
      " 282: 'tiger cat',\n",
      " 283: 'Persian cat',\n",
      " 284: 'Siamese cat, Siamese',\n",
      " 285: 'Egyptian cat',\n",
      " 286: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n",
      " 287: 'lynx, catamount',\n",
      " 288: 'leopard, Panthera pardus',\n",
      " 289: 'snow leopard, ounce, Panthera uncia',\n",
      " 290: 'jaguar, panther, Panthera onca, Felis onca',\n",
      " 291: 'lion, king of beasts, Panthera leo',\n",
      " 292: 'tiger, Panthera tigris',\n",
      " 293: 'cheetah, chetah, Acinonyx jubatus',\n",
      " 294: 'brown bear, bruin, Ursus arctos',\n",
      " 295: 'American black bear, black bear, Ursus americanus, Euarctos americanus',\n",
      " 296: 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus',\n",
      " 297: 'sloth bear, Melursus ursinus, Ursus ursinus',\n",
      " 298: 'mongoose',\n",
      " 299: 'meerkat, mierkat',\n",
      " 300: 'tiger beetle',\n",
      " 301: 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle',\n",
      " 302: 'ground beetle, carabid beetle',\n",
      " 303: 'long-horned beetle, longicorn, longicorn beetle',\n",
      " 304: 'leaf beetle, chrysomelid',\n",
      " 305: 'dung beetle',\n",
      " 306: 'rhinoceros beetle',\n",
      " 307: 'weevil',\n",
      " 308: 'fly',\n",
      " 309: 'bee',\n",
      " 310: 'ant, emmet, pismire',\n",
      " 311: 'grasshopper, hopper',\n",
      " 312: 'cricket',\n",
      " 313: 'walking stick, walkingstick, stick insect',\n",
      " 314: 'cockroach, roach',\n",
      " 315: 'mantis, mantid',\n",
      " 316: 'cicada, cicala',\n",
      " 317: 'leafhopper',\n",
      " 318: 'lacewing, lacewing fly',\n",
      " 319: \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\",\n",
      " 320: 'damselfly',\n",
      " 321: 'admiral',\n",
      " 322: 'ringlet, ringlet butterfly',\n",
      " 323: 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus',\n",
      " 324: 'cabbage butterfly',\n",
      " 325: 'sulphur butterfly, sulfur butterfly',\n",
      " 326: 'lycaenid, lycaenid butterfly',\n",
      " 327: 'starfish, sea star',\n",
      " 328: 'sea urchin',\n",
      " 329: 'sea cucumber, holothurian',\n",
      " 330: 'wood rabbit, cottontail, cottontail rabbit',\n",
      " 331: 'hare',\n",
      " 332: 'Angora, Angora rabbit',\n",
      " 333: 'hamster',\n",
      " 334: 'porcupine, hedgehog',\n",
      " 335: 'fox squirrel, eastern fox squirrel, Sciurus niger',\n",
      " 336: 'marmot',\n",
      " 337: 'beaver',\n",
      " 338: 'guinea pig, Cavia cobaya',\n",
      " 339: 'sorrel',\n",
      " 340: 'zebra',\n",
      " 341: 'hog, pig, grunter, squealer, Sus scrofa',\n",
      " 342: 'wild boar, boar, Sus scrofa',\n",
      " 343: 'warthog',\n",
      " 344: 'hippopotamus, hippo, river horse, Hippopotamus amphibius',\n",
      " 345: 'ox',\n",
      " 346: 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis',\n",
      " 347: 'bison',\n",
      " 348: 'ram, tup',\n",
      " 349: 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis',\n",
      " 350: 'ibex, Capra ibex',\n",
      " 351: 'hartebeest',\n",
      " 352: 'impala, Aepyceros melampus',\n",
      " 353: 'gazelle',\n",
      " 354: 'Arabian camel, dromedary, Camelus dromedarius',\n",
      " 355: 'llama',\n",
      " 356: 'weasel',\n",
      " 357: 'mink',\n",
      " 358: 'polecat, fitch, foulmart, foumart, Mustela putorius',\n",
      " 359: 'black-footed ferret, ferret, Mustela nigripes',\n",
      " 360: 'otter',\n",
      " 361: 'skunk, polecat, wood pussy',\n",
      " 362: 'badger',\n",
      " 363: 'armadillo',\n",
      " 364: 'three-toed sloth, ai, Bradypus tridactylus',\n",
      " 365: 'orangutan, orang, orangutang, Pongo pygmaeus',\n",
      " 366: 'gorilla, Gorilla gorilla',\n",
      " 367: 'chimpanzee, chimp, Pan troglodytes',\n",
      " 368: 'gibbon, Hylobates lar',\n",
      " 369: 'siamang, Hylobates syndactylus, Symphalangus syndactylus',\n",
      " 370: 'guenon, guenon monkey',\n",
      " 371: 'patas, hussar monkey, Erythrocebus patas',\n",
      " 372: 'baboon',\n",
      " 373: 'macaque',\n",
      " 374: 'langur',\n",
      " 375: 'colobus, colobus monkey',\n",
      " 376: 'proboscis monkey, Nasalis larvatus',\n",
      " 377: 'marmoset',\n",
      " 378: 'capuchin, ringtail, Cebus capucinus',\n",
      " 379: 'howler monkey, howler',\n",
      " 380: 'titi, titi monkey',\n",
      " 381: 'spider monkey, Ateles geoffroyi',\n",
      " 382: 'squirrel monkey, Saimiri sciureus',\n",
      " 383: 'Madagascar cat, ring-tailed lemur, Lemur catta',\n",
      " 384: 'indri, indris, Indri indri, Indri brevicaudatus',\n",
      " 385: 'Indian elephant, Elephas maximus',\n",
      " 386: 'African elephant, Loxodonta africana',\n",
      " 387: 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens',\n",
      " 388: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca',\n",
      " 389: 'barracouta, snoek',\n",
      " 390: 'eel',\n",
      " 391: 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch',\n",
      " 392: 'rock beauty, Holocanthus tricolor',\n",
      " 393: 'anemone fish',\n",
      " 394: 'sturgeon',\n",
      " 395: 'gar, garfish, garpike, billfish, Lepisosteus osseus',\n",
      " 396: 'lionfish',\n",
      " 397: 'puffer, pufferfish, blowfish, globefish',\n",
      " 398: 'abacus',\n",
      " 399: 'abaya',\n",
      " 400: \"academic gown, academic robe, judge's robe\",\n",
      " 401: 'accordion, piano accordion, squeeze box',\n",
      " 402: 'acoustic guitar',\n",
      " 403: 'aircraft carrier, carrier, flattop, attack aircraft carrier',\n",
      " 404: 'airliner',\n",
      " 405: 'airship, dirigible',\n",
      " 406: 'altar',\n",
      " 407: 'ambulance',\n",
      " 408: 'amphibian, amphibious vehicle',\n",
      " 409: 'analog clock',\n",
      " 410: 'apiary, bee house',\n",
      " 411: 'apron',\n",
      " 412: 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin',\n",
      " 413: 'assault rifle, assault gun',\n",
      " 414: 'backpack, back pack, knapsack, packsack, rucksack, haversack',\n",
      " 415: 'bakery, bakeshop, bakehouse',\n",
      " 416: 'balance beam, beam',\n",
      " 417: 'balloon',\n",
      " 418: 'ballpoint, ballpoint pen, ballpen, Biro',\n",
      " 419: 'Band Aid',\n",
      " 420: 'banjo',\n",
      " 421: 'bannister, banister, balustrade, balusters, handrail',\n",
      " 422: 'barbell',\n",
      " 423: 'barber chair',\n",
      " 424: 'barbershop',\n",
      " 425: 'barn',\n",
      " 426: 'barometer',\n",
      " 427: 'barrel, cask',\n",
      " 428: 'barrow, garden cart, lawn cart, wheelbarrow',\n",
      " 429: 'baseball',\n",
      " 430: 'basketball',\n",
      " 431: 'bassinet',\n",
      " 432: 'bassoon',\n",
      " 433: 'bathing cap, swimming cap',\n",
      " 434: 'bath towel',\n",
      " 435: 'bathtub, bathing tub, bath, tub',\n",
      " 436: 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon',\n",
      " 437: 'beacon, lighthouse, beacon light, pharos',\n",
      " 438: 'beaker',\n",
      " 439: 'bearskin, busby, shako',\n",
      " 440: 'beer bottle',\n",
      " 441: 'beer glass',\n",
      " 442: 'bell cote, bell cot',\n",
      " 443: 'bib',\n",
      " 444: 'bicycle-built-for-two, tandem bicycle, tandem',\n",
      " 445: 'bikini, two-piece',\n",
      " 446: 'binder, ring-binder',\n",
      " 447: 'binoculars, field glasses, opera glasses',\n",
      " 448: 'birdhouse',\n",
      " 449: 'boathouse',\n",
      " 450: 'bobsled, bobsleigh, bob',\n",
      " 451: 'bolo tie, bolo, bola tie, bola',\n",
      " 452: 'bonnet, poke bonnet',\n",
      " 453: 'bookcase',\n",
      " 454: 'bookshop, bookstore, bookstall',\n",
      " 455: 'bottlecap',\n",
      " 456: 'bow',\n",
      " 457: 'bow tie, bow-tie, bowtie',\n",
      " 458: 'brass, memorial tablet, plaque',\n",
      " 459: 'brassiere, bra, bandeau',\n",
      " 460: 'breakwater, groin, groyne, mole, bulwark, seawall, jetty',\n",
      " 461: 'breastplate, aegis, egis',\n",
      " 462: 'broom',\n",
      " 463: 'bucket, pail',\n",
      " 464: 'buckle',\n",
      " 465: 'bulletproof vest',\n",
      " 466: 'bullet train, bullet',\n",
      " 467: 'butcher shop, meat market',\n",
      " 468: 'cab, hack, taxi, taxicab',\n",
      " 469: 'caldron, cauldron',\n",
      " 470: 'candle, taper, wax light',\n",
      " 471: 'cannon',\n",
      " 472: 'canoe',\n",
      " 473: 'can opener, tin opener',\n",
      " 474: 'cardigan',\n",
      " 475: 'car mirror',\n",
      " 476: 'carousel, carrousel, merry-go-round, roundabout, whirligig',\n",
      " 477: \"carpenter's kit, tool kit\",\n",
      " 478: 'carton',\n",
      " 479: 'car wheel',\n",
      " 480: 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM',\n",
      " 481: 'cassette',\n",
      " 482: 'cassette player',\n",
      " 483: 'castle',\n",
      " 484: 'catamaran',\n",
      " 485: 'CD player',\n",
      " 486: 'cello, violoncello',\n",
      " 487: 'cellular telephone, cellular phone, cellphone, cell, mobile phone',\n",
      " 488: 'chain',\n",
      " 489: 'chainlink fence',\n",
      " 490: 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour',\n",
      " 491: 'chain saw, chainsaw',\n",
      " 492: 'chest',\n",
      " 493: 'chiffonier, commode',\n",
      " 494: 'chime, bell, gong',\n",
      " 495: 'china cabinet, china closet',\n",
      " 496: 'Christmas stocking',\n",
      " 497: 'church, church building',\n",
      " 498: 'cinema, movie theater, movie theatre, movie house, picture palace',\n",
      " 499: 'cleaver, meat cleaver, chopper',\n",
      " 500: 'cliff dwelling',\n",
      " 501: 'cloak',\n",
      " 502: 'clog, geta, patten, sabot',\n",
      " 503: 'cocktail shaker',\n",
      " 504: 'coffee mug',\n",
      " 505: 'coffeepot',\n",
      " 506: 'coil, spiral, volute, whorl, helix',\n",
      " 507: 'combination lock',\n",
      " 508: 'computer keyboard, keypad',\n",
      " 509: 'confectionery, confectionary, candy store',\n",
      " 510: 'container ship, containership, container vessel',\n",
      " 511: 'convertible',\n",
      " 512: 'corkscrew, bottle screw',\n",
      " 513: 'cornet, horn, trumpet, trump',\n",
      " 514: 'cowboy boot',\n",
      " 515: 'cowboy hat, ten-gallon hat',\n",
      " 516: 'cradle',\n",
      " 517: 'crane',\n",
      " 518: 'crash helmet',\n",
      " 519: 'crate',\n",
      " 520: 'crib, cot',\n",
      " 521: 'Crock Pot',\n",
      " 522: 'croquet ball',\n",
      " 523: 'crutch',\n",
      " 524: 'cuirass',\n",
      " 525: 'dam, dike, dyke',\n",
      " 526: 'desk',\n",
      " 527: 'desktop computer',\n",
      " 528: 'dial telephone, dial phone',\n",
      " 529: 'diaper, nappy, napkin',\n",
      " 530: 'digital clock',\n",
      " 531: 'digital watch',\n",
      " 532: 'dining table, board',\n",
      " 533: 'dishrag, dishcloth',\n",
      " 534: 'dishwasher, dish washer, dishwashing machine',\n",
      " 535: 'disk brake, disc brake',\n",
      " 536: 'dock, dockage, docking facility',\n",
      " 537: 'dogsled, dog sled, dog sleigh',\n",
      " 538: 'dome',\n",
      " 539: 'doormat, welcome mat',\n",
      " 540: 'drilling platform, offshore rig',\n",
      " 541: 'drum, membranophone, tympan',\n",
      " 542: 'drumstick',\n",
      " 543: 'dumbbell',\n",
      " 544: 'Dutch oven',\n",
      " 545: 'electric fan, blower',\n",
      " 546: 'electric guitar',\n",
      " 547: 'electric locomotive',\n",
      " 548: 'entertainment center',\n",
      " 549: 'envelope',\n",
      " 550: 'espresso maker',\n",
      " 551: 'face powder',\n",
      " 552: 'feather boa, boa',\n",
      " 553: 'file, file cabinet, filing cabinet',\n",
      " 554: 'fireboat',\n",
      " 555: 'fire engine, fire truck',\n",
      " 556: 'fire screen, fireguard',\n",
      " 557: 'flagpole, flagstaff',\n",
      " 558: 'flute, transverse flute',\n",
      " 559: 'folding chair',\n",
      " 560: 'football helmet',\n",
      " 561: 'forklift',\n",
      " 562: 'fountain',\n",
      " 563: 'fountain pen',\n",
      " 564: 'four-poster',\n",
      " 565: 'freight car',\n",
      " 566: 'French horn, horn',\n",
      " 567: 'frying pan, frypan, skillet',\n",
      " 568: 'fur coat',\n",
      " 569: 'garbage truck, dustcart',\n",
      " 570: 'gasmask, respirator, gas helmet',\n",
      " 571: 'gas pump, gasoline pump, petrol pump, island dispenser',\n",
      " 572: 'goblet',\n",
      " 573: 'go-kart',\n",
      " 574: 'golf ball',\n",
      " 575: 'golfcart, golf cart',\n",
      " 576: 'gondola',\n",
      " 577: 'gong, tam-tam',\n",
      " 578: 'gown',\n",
      " 579: 'grand piano, grand',\n",
      " 580: 'greenhouse, nursery, glasshouse',\n",
      " 581: 'grille, radiator grille',\n",
      " 582: 'grocery store, grocery, food market, market',\n",
      " 583: 'guillotine',\n",
      " 584: 'hair slide',\n",
      " 585: 'hair spray',\n",
      " 586: 'half track',\n",
      " 587: 'hammer',\n",
      " 588: 'hamper',\n",
      " 589: 'hand blower, blow dryer, blow drier, hair dryer, hair drier',\n",
      " 590: 'hand-held computer, hand-held microcomputer',\n",
      " 591: 'handkerchief, hankie, hanky, hankey',\n",
      " 592: 'hard disc, hard disk, fixed disk',\n",
      " 593: 'harmonica, mouth organ, harp, mouth harp',\n",
      " 594: 'harp',\n",
      " 595: 'harvester, reaper',\n",
      " 596: 'hatchet',\n",
      " 597: 'holster',\n",
      " 598: 'home theater, home theatre',\n",
      " 599: 'honeycomb',\n",
      " 600: 'hook, claw',\n",
      " 601: 'hoopskirt, crinoline',\n",
      " 602: 'horizontal bar, high bar',\n",
      " 603: 'horse cart, horse-cart',\n",
      " 604: 'hourglass',\n",
      " 605: 'iPod',\n",
      " 606: 'iron, smoothing iron',\n",
      " 607: \"jack-o'-lantern\",\n",
      " 608: 'jean, blue jean, denim',\n",
      " 609: 'jeep, landrover',\n",
      " 610: 'jersey, T-shirt, tee shirt',\n",
      " 611: 'jigsaw puzzle',\n",
      " 612: 'jinrikisha, ricksha, rickshaw',\n",
      " 613: 'joystick',\n",
      " 614: 'kimono',\n",
      " 615: 'knee pad',\n",
      " 616: 'knot',\n",
      " 617: 'lab coat, laboratory coat',\n",
      " 618: 'ladle',\n",
      " 619: 'lampshade, lamp shade',\n",
      " 620: 'laptop, laptop computer',\n",
      " 621: 'lawn mower, mower',\n",
      " 622: 'lens cap, lens cover',\n",
      " 623: 'letter opener, paper knife, paperknife',\n",
      " 624: 'library',\n",
      " 625: 'lifeboat',\n",
      " 626: 'lighter, light, igniter, ignitor',\n",
      " 627: 'limousine, limo',\n",
      " 628: 'liner, ocean liner',\n",
      " 629: 'lipstick, lip rouge',\n",
      " 630: 'Loafer',\n",
      " 631: 'lotion',\n",
      " 632: 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system',\n",
      " 633: \"loupe, jeweler's loupe\",\n",
      " 634: 'lumbermill, sawmill',\n",
      " 635: 'magnetic compass',\n",
      " 636: 'mailbag, postbag',\n",
      " 637: 'mailbox, letter box',\n",
      " 638: 'maillot',\n",
      " 639: 'maillot, tank suit',\n",
      " 640: 'manhole cover',\n",
      " 641: 'maraca',\n",
      " 642: 'marimba, xylophone',\n",
      " 643: 'mask',\n",
      " 644: 'matchstick',\n",
      " 645: 'maypole',\n",
      " 646: 'maze, labyrinth',\n",
      " 647: 'measuring cup',\n",
      " 648: 'medicine chest, medicine cabinet',\n",
      " 649: 'megalith, megalithic structure',\n",
      " 650: 'microphone, mike',\n",
      " 651: 'microwave, microwave oven',\n",
      " 652: 'military uniform',\n",
      " 653: 'milk can',\n",
      " 654: 'minibus',\n",
      " 655: 'miniskirt, mini',\n",
      " 656: 'minivan',\n",
      " 657: 'missile',\n",
      " 658: 'mitten',\n",
      " 659: 'mixing bowl',\n",
      " 660: 'mobile home, manufactured home',\n",
      " 661: 'Model T',\n",
      " 662: 'modem',\n",
      " 663: 'monastery',\n",
      " 664: 'monitor',\n",
      " 665: 'moped',\n",
      " 666: 'mortar',\n",
      " 667: 'mortarboard',\n",
      " 668: 'mosque',\n",
      " 669: 'mosquito net',\n",
      " 670: 'motor scooter, scooter',\n",
      " 671: 'mountain bike, all-terrain bike, off-roader',\n",
      " 672: 'mountain tent',\n",
      " 673: 'mouse, computer mouse',\n",
      " 674: 'mousetrap',\n",
      " 675: 'moving van',\n",
      " 676: 'muzzle',\n",
      " 677: 'nail',\n",
      " 678: 'neck brace',\n",
      " 679: 'necklace',\n",
      " 680: 'nipple',\n",
      " 681: 'notebook, notebook computer',\n",
      " 682: 'obelisk',\n",
      " 683: 'oboe, hautboy, hautbois',\n",
      " 684: 'ocarina, sweet potato',\n",
      " 685: 'odometer, hodometer, mileometer, milometer',\n",
      " 686: 'oil filter',\n",
      " 687: 'organ, pipe organ',\n",
      " 688: 'oscilloscope, scope, cathode-ray oscilloscope, CRO',\n",
      " 689: 'overskirt',\n",
      " 690: 'oxcart',\n",
      " 691: 'oxygen mask',\n",
      " 692: 'packet',\n",
      " 693: 'paddle, boat paddle',\n",
      " 694: 'paddlewheel, paddle wheel',\n",
      " 695: 'padlock',\n",
      " 696: 'paintbrush',\n",
      " 697: \"pajama, pyjama, pj's, jammies\",\n",
      " 698: 'palace',\n",
      " 699: 'panpipe, pandean pipe, syrinx',\n",
      " 700: 'paper towel',\n",
      " 701: 'parachute, chute',\n",
      " 702: 'parallel bars, bars',\n",
      " 703: 'park bench',\n",
      " 704: 'parking meter',\n",
      " 705: 'passenger car, coach, carriage',\n",
      " 706: 'patio, terrace',\n",
      " 707: 'pay-phone, pay-station',\n",
      " 708: 'pedestal, plinth, footstall',\n",
      " 709: 'pencil box, pencil case',\n",
      " 710: 'pencil sharpener',\n",
      " 711: 'perfume, essence',\n",
      " 712: 'Petri dish',\n",
      " 713: 'photocopier',\n",
      " 714: 'pick, plectrum, plectron',\n",
      " 715: 'pickelhaube',\n",
      " 716: 'picket fence, paling',\n",
      " 717: 'pickup, pickup truck',\n",
      " 718: 'pier',\n",
      " 719: 'piggy bank, penny bank',\n",
      " 720: 'pill bottle',\n",
      " 721: 'pillow',\n",
      " 722: 'ping-pong ball',\n",
      " 723: 'pinwheel',\n",
      " 724: 'pirate, pirate ship',\n",
      " 725: 'pitcher, ewer',\n",
      " 726: \"plane, carpenter's plane, woodworking plane\",\n",
      " 727: 'planetarium',\n",
      " 728: 'plastic bag',\n",
      " 729: 'plate rack',\n",
      " 730: 'plow, plough',\n",
      " 731: \"plunger, plumber's helper\",\n",
      " 732: 'Polaroid camera, Polaroid Land camera',\n",
      " 733: 'pole',\n",
      " 734: 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria',\n",
      " 735: 'poncho',\n",
      " 736: 'pool table, billiard table, snooker table',\n",
      " 737: 'pop bottle, soda bottle',\n",
      " 738: 'pot, flowerpot',\n",
      " 739: \"potter's wheel\",\n",
      " 740: 'power drill',\n",
      " 741: 'prayer rug, prayer mat',\n",
      " 742: 'printer',\n",
      " 743: 'prison, prison house',\n",
      " 744: 'projectile, missile',\n",
      " 745: 'projector',\n",
      " 746: 'puck, hockey puck',\n",
      " 747: 'punching bag, punch bag, punching ball, punchball',\n",
      " 748: 'purse',\n",
      " 749: 'quill, quill pen',\n",
      " 750: 'quilt, comforter, comfort, puff',\n",
      " 751: 'racer, race car, racing car',\n",
      " 752: 'racket, racquet',\n",
      " 753: 'radiator',\n",
      " 754: 'radio, wireless',\n",
      " 755: 'radio telescope, radio reflector',\n",
      " 756: 'rain barrel',\n",
      " 757: 'recreational vehicle, RV, R.V.',\n",
      " 758: 'reel',\n",
      " 759: 'reflex camera',\n",
      " 760: 'refrigerator, icebox',\n",
      " 761: 'remote control, remote',\n",
      " 762: 'restaurant, eating house, eating place, eatery',\n",
      " 763: 'revolver, six-gun, six-shooter',\n",
      " 764: 'rifle',\n",
      " 765: 'rocking chair, rocker',\n",
      " 766: 'rotisserie',\n",
      " 767: 'rubber eraser, rubber, pencil eraser',\n",
      " 768: 'rugby ball',\n",
      " 769: 'rule, ruler',\n",
      " 770: 'running shoe',\n",
      " 771: 'safe',\n",
      " 772: 'safety pin',\n",
      " 773: 'saltshaker, salt shaker',\n",
      " 774: 'sandal',\n",
      " 775: 'sarong',\n",
      " 776: 'sax, saxophone',\n",
      " 777: 'scabbard',\n",
      " 778: 'scale, weighing machine',\n",
      " 779: 'school bus',\n",
      " 780: 'schooner',\n",
      " 781: 'scoreboard',\n",
      " 782: 'screen, CRT screen',\n",
      " 783: 'screw',\n",
      " 784: 'screwdriver',\n",
      " 785: 'seat belt, seatbelt',\n",
      " 786: 'sewing machine',\n",
      " 787: 'shield, buckler',\n",
      " 788: 'shoe shop, shoe-shop, shoe store',\n",
      " 789: 'shoji',\n",
      " 790: 'shopping basket',\n",
      " 791: 'shopping cart',\n",
      " 792: 'shovel',\n",
      " 793: 'shower cap',\n",
      " 794: 'shower curtain',\n",
      " 795: 'ski',\n",
      " 796: 'ski mask',\n",
      " 797: 'sleeping bag',\n",
      " 798: 'slide rule, slipstick',\n",
      " 799: 'sliding door',\n",
      " 800: 'slot, one-armed bandit',\n",
      " 801: 'snorkel',\n",
      " 802: 'snowmobile',\n",
      " 803: 'snowplow, snowplough',\n",
      " 804: 'soap dispenser',\n",
      " 805: 'soccer ball',\n",
      " 806: 'sock',\n",
      " 807: 'solar dish, solar collector, solar furnace',\n",
      " 808: 'sombrero',\n",
      " 809: 'soup bowl',\n",
      " 810: 'space bar',\n",
      " 811: 'space heater',\n",
      " 812: 'space shuttle',\n",
      " 813: 'spatula',\n",
      " 814: 'speedboat',\n",
      " 815: \"spider web, spider's web\",\n",
      " 816: 'spindle',\n",
      " 817: 'sports car, sport car',\n",
      " 818: 'spotlight, spot',\n",
      " 819: 'stage',\n",
      " 820: 'steam locomotive',\n",
      " 821: 'steel arch bridge',\n",
      " 822: 'steel drum',\n",
      " 823: 'stethoscope',\n",
      " 824: 'stole',\n",
      " 825: 'stone wall',\n",
      " 826: 'stopwatch, stop watch',\n",
      " 827: 'stove',\n",
      " 828: 'strainer',\n",
      " 829: 'streetcar, tram, tramcar, trolley, trolley car',\n",
      " 830: 'stretcher',\n",
      " 831: 'studio couch, day bed',\n",
      " 832: 'stupa, tope',\n",
      " 833: 'submarine, pigboat, sub, U-boat',\n",
      " 834: 'suit, suit of clothes',\n",
      " 835: 'sundial',\n",
      " 836: 'sunglass',\n",
      " 837: 'sunglasses, dark glasses, shades',\n",
      " 838: 'sunscreen, sunblock, sun blocker',\n",
      " 839: 'suspension bridge',\n",
      " 840: 'swab, swob, mop',\n",
      " 841: 'sweatshirt',\n",
      " 842: 'swimming trunks, bathing trunks',\n",
      " 843: 'swing',\n",
      " 844: 'switch, electric switch, electrical switch',\n",
      " 845: 'syringe',\n",
      " 846: 'table lamp',\n",
      " 847: 'tank, army tank, armored combat vehicle, armoured combat vehicle',\n",
      " 848: 'tape player',\n",
      " 849: 'teapot',\n",
      " 850: 'teddy, teddy bear',\n",
      " 851: 'television, television system',\n",
      " 852: 'tennis ball',\n",
      " 853: 'thatch, thatched roof',\n",
      " 854: 'theater curtain, theatre curtain',\n",
      " 855: 'thimble',\n",
      " 856: 'thresher, thrasher, threshing machine',\n",
      " 857: 'throne',\n",
      " 858: 'tile roof',\n",
      " 859: 'toaster',\n",
      " 860: 'tobacco shop, tobacconist shop, tobacconist',\n",
      " 861: 'toilet seat',\n",
      " 862: 'torch',\n",
      " 863: 'totem pole',\n",
      " 864: 'tow truck, tow car, wrecker',\n",
      " 865: 'toyshop',\n",
      " 866: 'tractor',\n",
      " 867: 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi',\n",
      " 868: 'tray',\n",
      " 869: 'trench coat',\n",
      " 870: 'tricycle, trike, velocipede',\n",
      " 871: 'trimaran',\n",
      " 872: 'tripod',\n",
      " 873: 'triumphal arch',\n",
      " 874: 'trolleybus, trolley coach, trackless trolley',\n",
      " 875: 'trombone',\n",
      " 876: 'tub, vat',\n",
      " 877: 'turnstile',\n",
      " 878: 'typewriter keyboard',\n",
      " 879: 'umbrella',\n",
      " 880: 'unicycle, monocycle',\n",
      " 881: 'upright, upright piano',\n",
      " 882: 'vacuum, vacuum cleaner',\n",
      " 883: 'vase',\n",
      " 884: 'vault',\n",
      " 885: 'velvet',\n",
      " 886: 'vending machine',\n",
      " 887: 'vestment',\n",
      " 888: 'viaduct',\n",
      " 889: 'violin, fiddle',\n",
      " 890: 'volleyball',\n",
      " 891: 'waffle iron',\n",
      " 892: 'wall clock',\n",
      " 893: 'wallet, billfold, notecase, pocketbook',\n",
      " 894: 'wardrobe, closet, press',\n",
      " 895: 'warplane, military plane',\n",
      " 896: 'washbasin, handbasin, washbowl, lavabo, wash-hand basin',\n",
      " 897: 'washer, automatic washer, washing machine',\n",
      " 898: 'water bottle',\n",
      " 899: 'water jug',\n",
      " 900: 'water tower',\n",
      " 901: 'whiskey jug',\n",
      " 902: 'whistle',\n",
      " 903: 'wig',\n",
      " 904: 'window screen',\n",
      " 905: 'window shade',\n",
      " 906: 'Windsor tie',\n",
      " 907: 'wine bottle',\n",
      " 908: 'wing',\n",
      " 909: 'wok',\n",
      " 910: 'wooden spoon',\n",
      " 911: 'wool, woolen, woollen',\n",
      " 912: 'worm fence, snake fence, snake-rail fence, Virginia fence',\n",
      " 913: 'wreck',\n",
      " 914: 'yawl',\n",
      " 915: 'yurt',\n",
      " 916: 'web site, website, internet site, site',\n",
      " 917: 'comic book',\n",
      " 918: 'crossword puzzle, crossword',\n",
      " 919: 'street sign',\n",
      " 920: 'traffic light, traffic signal, stoplight',\n",
      " 921: 'book jacket, dust cover, dust jacket, dust wrapper',\n",
      " 922: 'menu',\n",
      " 923: 'plate',\n",
      " 924: 'guacamole',\n",
      " 925: 'consomme',\n",
      " 926: 'hot pot, hotpot',\n",
      " 927: 'trifle',\n",
      " 928: 'ice cream, icecream',\n",
      " 929: 'ice lolly, lolly, lollipop, popsicle',\n",
      " 930: 'French loaf',\n",
      " 931: 'bagel, beigel',\n",
      " 932: 'pretzel',\n",
      " 933: 'cheeseburger',\n",
      " 934: 'hotdog, hot dog, red hot',\n",
      " 935: 'mashed potato',\n",
      " 936: 'head cabbage',\n",
      " 937: 'broccoli',\n",
      " 938: 'cauliflower',\n",
      " 939: 'zucchini, courgette',\n",
      " 940: 'spaghetti squash',\n",
      " 941: 'acorn squash',\n",
      " 942: 'butternut squash',\n",
      " 943: 'cucumber, cuke',\n",
      " 944: 'artichoke, globe artichoke',\n",
      " 945: 'bell pepper',\n",
      " 946: 'cardoon',\n",
      " 947: 'mushroom',\n",
      " 948: 'Granny Smith',\n",
      " 949: 'strawberry',\n",
      " 950: 'orange',\n",
      " 951: 'lemon',\n",
      " 952: 'fig',\n",
      " 953: 'pineapple, ananas',\n",
      " 954: 'banana',\n",
      " 955: 'jackfruit, jak, jack',\n",
      " 956: 'custard apple',\n",
      " 957: 'pomegranate',\n",
      " 958: 'hay',\n",
      " 959: 'carbonara',\n",
      " 960: 'chocolate sauce, chocolate syrup',\n",
      " 961: 'dough',\n",
      " 962: 'meat loaf, meatloaf',\n",
      " 963: 'pizza, pizza pie',\n",
      " 964: 'potpie',\n",
      " 965: 'burrito',\n",
      " 966: 'red wine',\n",
      " 967: 'espresso',\n",
      " 968: 'cup',\n",
      " 969: 'eggnog',\n",
      " 970: 'alp',\n",
      " 971: 'bubble',\n",
      " 972: 'cliff, drop, drop-off',\n",
      " 973: 'coral reef',\n",
      " 974: 'geyser',\n",
      " 975: 'lakeside, lakeshore',\n",
      " 976: 'promontory, headland, head, foreland',\n",
      " 977: 'sandbar, sand bar',\n",
      " 978: 'seashore, coast, seacoast, sea-coast',\n",
      " 979: 'valley, vale',\n",
      " 980: 'volcano',\n",
      " 981: 'ballplayer, baseball player',\n",
      " 982: 'groom, bridegroom',\n",
      " 983: 'scuba diver',\n",
      " 984: 'rapeseed',\n",
      " 985: 'daisy',\n",
      " 986: \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\",\n",
      " 987: 'corn',\n",
      " 988: 'acorn',\n",
      " 989: 'hip, rose hip, rosehip',\n",
      " 990: 'buckeye, horse chestnut, conker',\n",
      " 991: 'coral fungus',\n",
      " 992: 'agaric',\n",
      " 993: 'gyromitra',\n",
      " 994: 'stinkhorn, carrion fungus',\n",
      " 995: 'earthstar',\n",
      " 996: 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa',\n",
      " 997: 'bolete',\n",
      " 998: 'ear, spike, capitulum',\n",
      " 999: 'toilet tissue, toilet paper, bathroom tissue'}\n",
      "76/2:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 380\n",
      "\n",
      "def prep_image(image):\n",
      "    res = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "    res = res.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "    return res\n",
      "\n",
      "def predict(image):\n",
      "    cropped_image = center_crop(image)\n",
      "    prediction = (model.predict(prep_image(image)), model.predict(prep_image(cropped_image)))\n",
      "    if np.max(prediction[0]>=prediction[1]):\n",
      "        return {'class': np.argmax(prediction[0], axis=1)[0], 'all_prob': prediction[0]}\n",
      "    else:\n",
      "        return  {'class': np.argmax(prediction[1], axis=1)[1], 'all_prob': prediction[1]}\n",
      "    \n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "plt.imshow(image)\n",
      "\n",
      "print(class_dict[predict(image)])\n",
      "78/1:\n",
      "from tensorflow.keras.applications import EfficientNetB4\n",
      "import tensorflow as tf\n",
      "model = EfficientNetB4(weights='imagenet')\n",
      "\n",
      "def center_crop(img, new_width=None, new_height=None):        \n",
      "\n",
      "    width = img.shape[1]\n",
      "    height = img.shape[0]\n",
      "\n",
      "    if new_width is None:\n",
      "        new_width = min(width, height)\n",
      "\n",
      "    if new_height is None:\n",
      "        new_height = min(width, height)\n",
      "\n",
      "    left = int(np.ceil((width - new_width) / 2))\n",
      "    right = width - int(np.floor((width - new_width) / 2))\n",
      "\n",
      "    top = int(np.ceil((height - new_height) / 2))\n",
      "    bottom = height - int(np.floor((height - new_height) / 2))\n",
      "\n",
      "    if len(img.shape) == 2:\n",
      "        center_cropped_img = img[top:bottom, left:right]\n",
      "    else:\n",
      "        center_cropped_img = img[top:bottom, left:right, ...]\n",
      "\n",
      "    return center_cropped_img\n",
      "\n",
      "class_dict = {0: 'tench, Tinca tinca',\n",
      " 1: 'goldfish, Carassius auratus',\n",
      " 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n",
      " 3: 'tiger shark, Galeocerdo cuvieri',\n",
      " 4: 'hammerhead, hammerhead shark',\n",
      " 5: 'electric ray, crampfish, numbfish, torpedo',\n",
      " 6: 'stingray',\n",
      " 7: 'cock',\n",
      " 8: 'hen',\n",
      " 9: 'ostrich, Struthio camelus',\n",
      " 10: 'brambling, Fringilla montifringilla',\n",
      " 11: 'goldfinch, Carduelis carduelis',\n",
      " 12: 'house finch, linnet, Carpodacus mexicanus',\n",
      " 13: 'junco, snowbird',\n",
      " 14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea',\n",
      " 15: 'robin, American robin, Turdus migratorius',\n",
      " 16: 'bulbul',\n",
      " 17: 'jay',\n",
      " 18: 'magpie',\n",
      " 19: 'chickadee',\n",
      " 20: 'water ouzel, dipper',\n",
      " 21: 'kite',\n",
      " 22: 'bald eagle, American eagle, Haliaeetus leucocephalus',\n",
      " 23: 'vulture',\n",
      " 24: 'great grey owl, great gray owl, Strix nebulosa',\n",
      " 25: 'European fire salamander, Salamandra salamandra',\n",
      " 26: 'common newt, Triturus vulgaris',\n",
      " 27: 'eft',\n",
      " 28: 'spotted salamander, Ambystoma maculatum',\n",
      " 29: 'axolotl, mud puppy, Ambystoma mexicanum',\n",
      " 30: 'bullfrog, Rana catesbeiana',\n",
      " 31: 'tree frog, tree-frog',\n",
      " 32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui',\n",
      " 33: 'loggerhead, loggerhead turtle, Caretta caretta',\n",
      " 34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea',\n",
      " 35: 'mud turtle',\n",
      " 36: 'terrapin',\n",
      " 37: 'box turtle, box tortoise',\n",
      " 38: 'banded gecko',\n",
      " 39: 'common iguana, iguana, Iguana iguana',\n",
      " 40: 'American chameleon, anole, Anolis carolinensis',\n",
      " 41: 'whiptail, whiptail lizard',\n",
      " 42: 'agama',\n",
      " 43: 'frilled lizard, Chlamydosaurus kingi',\n",
      " 44: 'alligator lizard',\n",
      " 45: 'Gila monster, Heloderma suspectum',\n",
      " 46: 'green lizard, Lacerta viridis',\n",
      " 47: 'African chameleon, Chamaeleo chamaeleon',\n",
      " 48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis',\n",
      " 49: 'African crocodile, Nile crocodile, Crocodylus niloticus',\n",
      " 50: 'American alligator, Alligator mississipiensis',\n",
      " 51: 'triceratops',\n",
      " 52: 'thunder snake, worm snake, Carphophis amoenus',\n",
      " 53: 'ringneck snake, ring-necked snake, ring snake',\n",
      " 54: 'hognose snake, puff adder, sand viper',\n",
      " 55: 'green snake, grass snake',\n",
      " 56: 'king snake, kingsnake',\n",
      " 57: 'garter snake, grass snake',\n",
      " 58: 'water snake',\n",
      " 59: 'vine snake',\n",
      " 60: 'night snake, Hypsiglena torquata',\n",
      " 61: 'boa constrictor, Constrictor constrictor',\n",
      " 62: 'rock python, rock snake, Python sebae',\n",
      " 63: 'Indian cobra, Naja naja',\n",
      " 64: 'green mamba',\n",
      " 65: 'sea snake',\n",
      " 66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus',\n",
      " 67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus',\n",
      " 68: 'sidewinder, horned rattlesnake, Crotalus cerastes',\n",
      " 69: 'trilobite',\n",
      " 70: 'harvestman, daddy longlegs, Phalangium opilio',\n",
      " 71: 'scorpion',\n",
      " 72: 'black and gold garden spider, Argiope aurantia',\n",
      " 73: 'barn spider, Araneus cavaticus',\n",
      " 74: 'garden spider, Aranea diademata',\n",
      " 75: 'black widow, Latrodectus mactans',\n",
      " 76: 'tarantula',\n",
      " 77: 'wolf spider, hunting spider',\n",
      " 78: 'tick',\n",
      " 79: 'centipede',\n",
      " 80: 'black grouse',\n",
      " 81: 'ptarmigan',\n",
      " 82: 'ruffed grouse, partridge, Bonasa umbellus',\n",
      " 83: 'prairie chicken, prairie grouse, prairie fowl',\n",
      " 84: 'peacock',\n",
      " 85: 'quail',\n",
      " 86: 'partridge',\n",
      " 87: 'African grey, African gray, Psittacus erithacus',\n",
      " 88: 'macaw',\n",
      " 89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n",
      " 90: 'lorikeet',\n",
      " 91: 'coucal',\n",
      " 92: 'bee eater',\n",
      " 93: 'hornbill',\n",
      " 94: 'hummingbird',\n",
      " 95: 'jacamar',\n",
      " 96: 'toucan',\n",
      " 97: 'drake',\n",
      " 98: 'red-breasted merganser, Mergus serrator',\n",
      " 99: 'goose',\n",
      " 100: 'black swan, Cygnus atratus',\n",
      " 101: 'tusker',\n",
      " 102: 'echidna, spiny anteater, anteater',\n",
      " 103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n",
      " 104: 'wallaby, brush kangaroo',\n",
      " 105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n",
      " 106: 'wombat',\n",
      " 107: 'jellyfish',\n",
      " 108: 'sea anemone, anemone',\n",
      " 109: 'brain coral',\n",
      " 110: 'flatworm, platyhelminth',\n",
      " 111: 'nematode, nematode worm, roundworm',\n",
      " 112: 'conch',\n",
      " 113: 'snail',\n",
      " 114: 'slug',\n",
      " 115: 'sea slug, nudibranch',\n",
      " 116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore',\n",
      " 117: 'chambered nautilus, pearly nautilus, nautilus',\n",
      " 118: 'Dungeness crab, Cancer magister',\n",
      " 119: 'rock crab, Cancer irroratus',\n",
      " 120: 'fiddler crab',\n",
      " 121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica',\n",
      " 122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus',\n",
      " 123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish',\n",
      " 124: 'crayfish, crawfish, crawdad, crawdaddy',\n",
      " 125: 'hermit crab',\n",
      " 126: 'isopod',\n",
      " 127: 'white stork, Ciconia ciconia',\n",
      " 128: 'black stork, Ciconia nigra',\n",
      " 129: 'spoonbill',\n",
      " 130: 'flamingo',\n",
      " 131: 'little blue heron, Egretta caerulea',\n",
      " 132: 'American egret, great white heron, Egretta albus',\n",
      " 133: 'bittern',\n",
      " 134: 'crane',\n",
      " 135: 'limpkin, Aramus pictus',\n",
      " 136: 'European gallinule, Porphyrio porphyrio',\n",
      " 137: 'American coot, marsh hen, mud hen, water hen, Fulica americana',\n",
      " 138: 'bustard',\n",
      " 139: 'ruddy turnstone, Arenaria interpres',\n",
      " 140: 'red-backed sandpiper, dunlin, Erolia alpina',\n",
      " 141: 'redshank, Tringa totanus',\n",
      " 142: 'dowitcher',\n",
      " 143: 'oystercatcher, oyster catcher',\n",
      " 144: 'pelican',\n",
      " 145: 'king penguin, Aptenodytes patagonica',\n",
      " 146: 'albatross, mollymawk',\n",
      " 147: 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus',\n",
      " 148: 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca',\n",
      " 149: 'dugong, Dugong dugon',\n",
      " 150: 'sea lion',\n",
      " 151: 'Chihuahua',\n",
      " 152: 'Japanese spaniel',\n",
      " 153: 'Maltese dog, Maltese terrier, Maltese',\n",
      " 154: 'Pekinese, Pekingese, Peke',\n",
      " 155: 'Shih-Tzu',\n",
      " 156: 'Blenheim spaniel',\n",
      " 157: 'papillon',\n",
      " 158: 'toy terrier',\n",
      " 159: 'Rhodesian ridgeback',\n",
      " 160: 'Afghan hound, Afghan',\n",
      " 161: 'basset, basset hound',\n",
      " 162: 'beagle',\n",
      " 163: 'bloodhound, sleuthhound',\n",
      " 164: 'bluetick',\n",
      " 165: 'black-and-tan coonhound',\n",
      " 166: 'Walker hound, Walker foxhound',\n",
      " 167: 'English foxhound',\n",
      " 168: 'redbone',\n",
      " 169: 'borzoi, Russian wolfhound',\n",
      " 170: 'Irish wolfhound',\n",
      " 171: 'Italian greyhound',\n",
      " 172: 'whippet',\n",
      " 173: 'Ibizan hound, Ibizan Podenco',\n",
      " 174: 'Norwegian elkhound, elkhound',\n",
      " 175: 'otterhound, otter hound',\n",
      " 176: 'Saluki, gazelle hound',\n",
      " 177: 'Scottish deerhound, deerhound',\n",
      " 178: 'Weimaraner',\n",
      " 179: 'Staffordshire bullterrier, Staffordshire bull terrier',\n",
      " 180: 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier',\n",
      " 181: 'Bedlington terrier',\n",
      " 182: 'Border terrier',\n",
      " 183: 'Kerry blue terrier',\n",
      " 184: 'Irish terrier',\n",
      " 185: 'Norfolk terrier',\n",
      " 186: 'Norwich terrier',\n",
      " 187: 'Yorkshire terrier',\n",
      " 188: 'wire-haired fox terrier',\n",
      " 189: 'Lakeland terrier',\n",
      " 190: 'Sealyham terrier, Sealyham',\n",
      " 191: 'Airedale, Airedale terrier',\n",
      " 192: 'cairn, cairn terrier',\n",
      " 193: 'Australian terrier',\n",
      " 194: 'Dandie Dinmont, Dandie Dinmont terrier',\n",
      " 195: 'Boston bull, Boston terrier',\n",
      " 196: 'miniature schnauzer',\n",
      " 197: 'giant schnauzer',\n",
      " 198: 'standard schnauzer',\n",
      " 199: 'Scotch terrier, Scottish terrier, Scottie',\n",
      " 200: 'Tibetan terrier, chrysanthemum dog',\n",
      " 201: 'silky terrier, Sydney silky',\n",
      " 202: 'soft-coated wheaten terrier',\n",
      " 203: 'West Highland white terrier',\n",
      " 204: 'Lhasa, Lhasa apso',\n",
      " 205: 'flat-coated retriever',\n",
      " 206: 'curly-coated retriever',\n",
      " 207: 'golden retriever',\n",
      " 208: 'Labrador retriever',\n",
      " 209: 'Chesapeake Bay retriever',\n",
      " 210: 'German short-haired pointer',\n",
      " 211: 'vizsla, Hungarian pointer',\n",
      " 212: 'English setter',\n",
      " 213: 'Irish setter, red setter',\n",
      " 214: 'Gordon setter',\n",
      " 215: 'Brittany spaniel',\n",
      " 216: 'clumber, clumber spaniel',\n",
      " 217: 'English springer, English springer spaniel',\n",
      " 218: 'Welsh springer spaniel',\n",
      " 219: 'cocker spaniel, English cocker spaniel, cocker',\n",
      " 220: 'Sussex spaniel',\n",
      " 221: 'Irish water spaniel',\n",
      " 222: 'kuvasz',\n",
      " 223: 'schipperke',\n",
      " 224: 'groenendael',\n",
      " 225: 'malinois',\n",
      " 226: 'briard',\n",
      " 227: 'kelpie',\n",
      " 228: 'komondor',\n",
      " 229: 'Old English sheepdog, bobtail',\n",
      " 230: 'Shetland sheepdog, Shetland sheep dog, Shetland',\n",
      " 231: 'collie',\n",
      " 232: 'Border collie',\n",
      " 233: 'Bouvier des Flandres, Bouviers des Flandres',\n",
      " 234: 'Rottweiler',\n",
      " 235: 'German shepherd, German shepherd dog, German police dog, alsatian',\n",
      " 236: 'Doberman, Doberman pinscher',\n",
      " 237: 'miniature pinscher',\n",
      " 238: 'Greater Swiss Mountain dog',\n",
      " 239: 'Bernese mountain dog',\n",
      " 240: 'Appenzeller',\n",
      " 241: 'EntleBucher',\n",
      " 242: 'boxer',\n",
      " 243: 'bull mastiff',\n",
      " 244: 'Tibetan mastiff',\n",
      " 245: 'French bulldog',\n",
      " 246: 'Great Dane',\n",
      " 247: 'Saint Bernard, St Bernard',\n",
      " 248: 'Eskimo dog, husky',\n",
      " 249: 'malamute, malemute, Alaskan malamute',\n",
      " 250: 'Siberian husky',\n",
      " 251: 'dalmatian, coach dog, carriage dog',\n",
      " 252: 'affenpinscher, monkey pinscher, monkey dog',\n",
      " 253: 'basenji',\n",
      " 254: 'pug, pug-dog',\n",
      " 255: 'Leonberg',\n",
      " 256: 'Newfoundland, Newfoundland dog',\n",
      " 257: 'Great Pyrenees',\n",
      " 258: 'Samoyed, Samoyede',\n",
      " 259: 'Pomeranian',\n",
      " 260: 'chow, chow chow',\n",
      " 261: 'keeshond',\n",
      " 262: 'Brabancon griffon',\n",
      " 263: 'Pembroke, Pembroke Welsh corgi',\n",
      " 264: 'Cardigan, Cardigan Welsh corgi',\n",
      " 265: 'toy poodle',\n",
      " 266: 'miniature poodle',\n",
      " 267: 'standard poodle',\n",
      " 268: 'Mexican hairless',\n",
      " 269: 'timber wolf, grey wolf, gray wolf, Canis lupus',\n",
      " 270: 'white wolf, Arctic wolf, Canis lupus tundrarum',\n",
      " 271: 'red wolf, maned wolf, Canis rufus, Canis niger',\n",
      " 272: 'coyote, prairie wolf, brush wolf, Canis latrans',\n",
      " 273: 'dingo, warrigal, warragal, Canis dingo',\n",
      " 274: 'dhole, Cuon alpinus',\n",
      " 275: 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus',\n",
      " 276: 'hyena, hyaena',\n",
      " 277: 'red fox, Vulpes vulpes',\n",
      " 278: 'kit fox, Vulpes macrotis',\n",
      " 279: 'Arctic fox, white fox, Alopex lagopus',\n",
      " 280: 'grey fox, gray fox, Urocyon cinereoargenteus',\n",
      " 281: 'tabby, tabby cat',\n",
      " 282: 'tiger cat',\n",
      " 283: 'Persian cat',\n",
      " 284: 'Siamese cat, Siamese',\n",
      " 285: 'Egyptian cat',\n",
      " 286: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n",
      " 287: 'lynx, catamount',\n",
      " 288: 'leopard, Panthera pardus',\n",
      " 289: 'snow leopard, ounce, Panthera uncia',\n",
      " 290: 'jaguar, panther, Panthera onca, Felis onca',\n",
      " 291: 'lion, king of beasts, Panthera leo',\n",
      " 292: 'tiger, Panthera tigris',\n",
      " 293: 'cheetah, chetah, Acinonyx jubatus',\n",
      " 294: 'brown bear, bruin, Ursus arctos',\n",
      " 295: 'American black bear, black bear, Ursus americanus, Euarctos americanus',\n",
      " 296: 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus',\n",
      " 297: 'sloth bear, Melursus ursinus, Ursus ursinus',\n",
      " 298: 'mongoose',\n",
      " 299: 'meerkat, mierkat',\n",
      " 300: 'tiger beetle',\n",
      " 301: 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle',\n",
      " 302: 'ground beetle, carabid beetle',\n",
      " 303: 'long-horned beetle, longicorn, longicorn beetle',\n",
      " 304: 'leaf beetle, chrysomelid',\n",
      " 305: 'dung beetle',\n",
      " 306: 'rhinoceros beetle',\n",
      " 307: 'weevil',\n",
      " 308: 'fly',\n",
      " 309: 'bee',\n",
      " 310: 'ant, emmet, pismire',\n",
      " 311: 'grasshopper, hopper',\n",
      " 312: 'cricket',\n",
      " 313: 'walking stick, walkingstick, stick insect',\n",
      " 314: 'cockroach, roach',\n",
      " 315: 'mantis, mantid',\n",
      " 316: 'cicada, cicala',\n",
      " 317: 'leafhopper',\n",
      " 318: 'lacewing, lacewing fly',\n",
      " 319: \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\",\n",
      " 320: 'damselfly',\n",
      " 321: 'admiral',\n",
      " 322: 'ringlet, ringlet butterfly',\n",
      " 323: 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus',\n",
      " 324: 'cabbage butterfly',\n",
      " 325: 'sulphur butterfly, sulfur butterfly',\n",
      " 326: 'lycaenid, lycaenid butterfly',\n",
      " 327: 'starfish, sea star',\n",
      " 328: 'sea urchin',\n",
      " 329: 'sea cucumber, holothurian',\n",
      " 330: 'wood rabbit, cottontail, cottontail rabbit',\n",
      " 331: 'hare',\n",
      " 332: 'Angora, Angora rabbit',\n",
      " 333: 'hamster',\n",
      " 334: 'porcupine, hedgehog',\n",
      " 335: 'fox squirrel, eastern fox squirrel, Sciurus niger',\n",
      " 336: 'marmot',\n",
      " 337: 'beaver',\n",
      " 338: 'guinea pig, Cavia cobaya',\n",
      " 339: 'sorrel',\n",
      " 340: 'zebra',\n",
      " 341: 'hog, pig, grunter, squealer, Sus scrofa',\n",
      " 342: 'wild boar, boar, Sus scrofa',\n",
      " 343: 'warthog',\n",
      " 344: 'hippopotamus, hippo, river horse, Hippopotamus amphibius',\n",
      " 345: 'ox',\n",
      " 346: 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis',\n",
      " 347: 'bison',\n",
      " 348: 'ram, tup',\n",
      " 349: 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis',\n",
      " 350: 'ibex, Capra ibex',\n",
      " 351: 'hartebeest',\n",
      " 352: 'impala, Aepyceros melampus',\n",
      " 353: 'gazelle',\n",
      " 354: 'Arabian camel, dromedary, Camelus dromedarius',\n",
      " 355: 'llama',\n",
      " 356: 'weasel',\n",
      " 357: 'mink',\n",
      " 358: 'polecat, fitch, foulmart, foumart, Mustela putorius',\n",
      " 359: 'black-footed ferret, ferret, Mustela nigripes',\n",
      " 360: 'otter',\n",
      " 361: 'skunk, polecat, wood pussy',\n",
      " 362: 'badger',\n",
      " 363: 'armadillo',\n",
      " 364: 'three-toed sloth, ai, Bradypus tridactylus',\n",
      " 365: 'orangutan, orang, orangutang, Pongo pygmaeus',\n",
      " 366: 'gorilla, Gorilla gorilla',\n",
      " 367: 'chimpanzee, chimp, Pan troglodytes',\n",
      " 368: 'gibbon, Hylobates lar',\n",
      " 369: 'siamang, Hylobates syndactylus, Symphalangus syndactylus',\n",
      " 370: 'guenon, guenon monkey',\n",
      " 371: 'patas, hussar monkey, Erythrocebus patas',\n",
      " 372: 'baboon',\n",
      " 373: 'macaque',\n",
      " 374: 'langur',\n",
      " 375: 'colobus, colobus monkey',\n",
      " 376: 'proboscis monkey, Nasalis larvatus',\n",
      " 377: 'marmoset',\n",
      " 378: 'capuchin, ringtail, Cebus capucinus',\n",
      " 379: 'howler monkey, howler',\n",
      " 380: 'titi, titi monkey',\n",
      " 381: 'spider monkey, Ateles geoffroyi',\n",
      " 382: 'squirrel monkey, Saimiri sciureus',\n",
      " 383: 'Madagascar cat, ring-tailed lemur, Lemur catta',\n",
      " 384: 'indri, indris, Indri indri, Indri brevicaudatus',\n",
      " 385: 'Indian elephant, Elephas maximus',\n",
      " 386: 'African elephant, Loxodonta africana',\n",
      " 387: 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens',\n",
      " 388: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca',\n",
      " 389: 'barracouta, snoek',\n",
      " 390: 'eel',\n",
      " 391: 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch',\n",
      " 392: 'rock beauty, Holocanthus tricolor',\n",
      " 393: 'anemone fish',\n",
      " 394: 'sturgeon',\n",
      " 395: 'gar, garfish, garpike, billfish, Lepisosteus osseus',\n",
      " 396: 'lionfish',\n",
      " 397: 'puffer, pufferfish, blowfish, globefish',\n",
      " 398: 'abacus',\n",
      " 399: 'abaya',\n",
      " 400: \"academic gown, academic robe, judge's robe\",\n",
      " 401: 'accordion, piano accordion, squeeze box',\n",
      " 402: 'acoustic guitar',\n",
      " 403: 'aircraft carrier, carrier, flattop, attack aircraft carrier',\n",
      " 404: 'airliner',\n",
      " 405: 'airship, dirigible',\n",
      " 406: 'altar',\n",
      " 407: 'ambulance',\n",
      " 408: 'amphibian, amphibious vehicle',\n",
      " 409: 'analog clock',\n",
      " 410: 'apiary, bee house',\n",
      " 411: 'apron',\n",
      " 412: 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin',\n",
      " 413: 'assault rifle, assault gun',\n",
      " 414: 'backpack, back pack, knapsack, packsack, rucksack, haversack',\n",
      " 415: 'bakery, bakeshop, bakehouse',\n",
      " 416: 'balance beam, beam',\n",
      " 417: 'balloon',\n",
      " 418: 'ballpoint, ballpoint pen, ballpen, Biro',\n",
      " 419: 'Band Aid',\n",
      " 420: 'banjo',\n",
      " 421: 'bannister, banister, balustrade, balusters, handrail',\n",
      " 422: 'barbell',\n",
      " 423: 'barber chair',\n",
      " 424: 'barbershop',\n",
      " 425: 'barn',\n",
      " 426: 'barometer',\n",
      " 427: 'barrel, cask',\n",
      " 428: 'barrow, garden cart, lawn cart, wheelbarrow',\n",
      " 429: 'baseball',\n",
      " 430: 'basketball',\n",
      " 431: 'bassinet',\n",
      " 432: 'bassoon',\n",
      " 433: 'bathing cap, swimming cap',\n",
      " 434: 'bath towel',\n",
      " 435: 'bathtub, bathing tub, bath, tub',\n",
      " 436: 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon',\n",
      " 437: 'beacon, lighthouse, beacon light, pharos',\n",
      " 438: 'beaker',\n",
      " 439: 'bearskin, busby, shako',\n",
      " 440: 'beer bottle',\n",
      " 441: 'beer glass',\n",
      " 442: 'bell cote, bell cot',\n",
      " 443: 'bib',\n",
      " 444: 'bicycle-built-for-two, tandem bicycle, tandem',\n",
      " 445: 'bikini, two-piece',\n",
      " 446: 'binder, ring-binder',\n",
      " 447: 'binoculars, field glasses, opera glasses',\n",
      " 448: 'birdhouse',\n",
      " 449: 'boathouse',\n",
      " 450: 'bobsled, bobsleigh, bob',\n",
      " 451: 'bolo tie, bolo, bola tie, bola',\n",
      " 452: 'bonnet, poke bonnet',\n",
      " 453: 'bookcase',\n",
      " 454: 'bookshop, bookstore, bookstall',\n",
      " 455: 'bottlecap',\n",
      " 456: 'bow',\n",
      " 457: 'bow tie, bow-tie, bowtie',\n",
      " 458: 'brass, memorial tablet, plaque',\n",
      " 459: 'brassiere, bra, bandeau',\n",
      " 460: 'breakwater, groin, groyne, mole, bulwark, seawall, jetty',\n",
      " 461: 'breastplate, aegis, egis',\n",
      " 462: 'broom',\n",
      " 463: 'bucket, pail',\n",
      " 464: 'buckle',\n",
      " 465: 'bulletproof vest',\n",
      " 466: 'bullet train, bullet',\n",
      " 467: 'butcher shop, meat market',\n",
      " 468: 'cab, hack, taxi, taxicab',\n",
      " 469: 'caldron, cauldron',\n",
      " 470: 'candle, taper, wax light',\n",
      " 471: 'cannon',\n",
      " 472: 'canoe',\n",
      " 473: 'can opener, tin opener',\n",
      " 474: 'cardigan',\n",
      " 475: 'car mirror',\n",
      " 476: 'carousel, carrousel, merry-go-round, roundabout, whirligig',\n",
      " 477: \"carpenter's kit, tool kit\",\n",
      " 478: 'carton',\n",
      " 479: 'car wheel',\n",
      " 480: 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM',\n",
      " 481: 'cassette',\n",
      " 482: 'cassette player',\n",
      " 483: 'castle',\n",
      " 484: 'catamaran',\n",
      " 485: 'CD player',\n",
      " 486: 'cello, violoncello',\n",
      " 487: 'cellular telephone, cellular phone, cellphone, cell, mobile phone',\n",
      " 488: 'chain',\n",
      " 489: 'chainlink fence',\n",
      " 490: 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour',\n",
      " 491: 'chain saw, chainsaw',\n",
      " 492: 'chest',\n",
      " 493: 'chiffonier, commode',\n",
      " 494: 'chime, bell, gong',\n",
      " 495: 'china cabinet, china closet',\n",
      " 496: 'Christmas stocking',\n",
      " 497: 'church, church building',\n",
      " 498: 'cinema, movie theater, movie theatre, movie house, picture palace',\n",
      " 499: 'cleaver, meat cleaver, chopper',\n",
      " 500: 'cliff dwelling',\n",
      " 501: 'cloak',\n",
      " 502: 'clog, geta, patten, sabot',\n",
      " 503: 'cocktail shaker',\n",
      " 504: 'coffee mug',\n",
      " 505: 'coffeepot',\n",
      " 506: 'coil, spiral, volute, whorl, helix',\n",
      " 507: 'combination lock',\n",
      " 508: 'computer keyboard, keypad',\n",
      " 509: 'confectionery, confectionary, candy store',\n",
      " 510: 'container ship, containership, container vessel',\n",
      " 511: 'convertible',\n",
      " 512: 'corkscrew, bottle screw',\n",
      " 513: 'cornet, horn, trumpet, trump',\n",
      " 514: 'cowboy boot',\n",
      " 515: 'cowboy hat, ten-gallon hat',\n",
      " 516: 'cradle',\n",
      " 517: 'crane',\n",
      " 518: 'crash helmet',\n",
      " 519: 'crate',\n",
      " 520: 'crib, cot',\n",
      " 521: 'Crock Pot',\n",
      " 522: 'croquet ball',\n",
      " 523: 'crutch',\n",
      " 524: 'cuirass',\n",
      " 525: 'dam, dike, dyke',\n",
      " 526: 'desk',\n",
      " 527: 'desktop computer',\n",
      " 528: 'dial telephone, dial phone',\n",
      " 529: 'diaper, nappy, napkin',\n",
      " 530: 'digital clock',\n",
      " 531: 'digital watch',\n",
      " 532: 'dining table, board',\n",
      " 533: 'dishrag, dishcloth',\n",
      " 534: 'dishwasher, dish washer, dishwashing machine',\n",
      " 535: 'disk brake, disc brake',\n",
      " 536: 'dock, dockage, docking facility',\n",
      " 537: 'dogsled, dog sled, dog sleigh',\n",
      " 538: 'dome',\n",
      " 539: 'doormat, welcome mat',\n",
      " 540: 'drilling platform, offshore rig',\n",
      " 541: 'drum, membranophone, tympan',\n",
      " 542: 'drumstick',\n",
      " 543: 'dumbbell',\n",
      " 544: 'Dutch oven',\n",
      " 545: 'electric fan, blower',\n",
      " 546: 'electric guitar',\n",
      " 547: 'electric locomotive',\n",
      " 548: 'entertainment center',\n",
      " 549: 'envelope',\n",
      " 550: 'espresso maker',\n",
      " 551: 'face powder',\n",
      " 552: 'feather boa, boa',\n",
      " 553: 'file, file cabinet, filing cabinet',\n",
      " 554: 'fireboat',\n",
      " 555: 'fire engine, fire truck',\n",
      " 556: 'fire screen, fireguard',\n",
      " 557: 'flagpole, flagstaff',\n",
      " 558: 'flute, transverse flute',\n",
      " 559: 'folding chair',\n",
      " 560: 'football helmet',\n",
      " 561: 'forklift',\n",
      " 562: 'fountain',\n",
      " 563: 'fountain pen',\n",
      " 564: 'four-poster',\n",
      " 565: 'freight car',\n",
      " 566: 'French horn, horn',\n",
      " 567: 'frying pan, frypan, skillet',\n",
      " 568: 'fur coat',\n",
      " 569: 'garbage truck, dustcart',\n",
      " 570: 'gasmask, respirator, gas helmet',\n",
      " 571: 'gas pump, gasoline pump, petrol pump, island dispenser',\n",
      " 572: 'goblet',\n",
      " 573: 'go-kart',\n",
      " 574: 'golf ball',\n",
      " 575: 'golfcart, golf cart',\n",
      " 576: 'gondola',\n",
      " 577: 'gong, tam-tam',\n",
      " 578: 'gown',\n",
      " 579: 'grand piano, grand',\n",
      " 580: 'greenhouse, nursery, glasshouse',\n",
      " 581: 'grille, radiator grille',\n",
      " 582: 'grocery store, grocery, food market, market',\n",
      " 583: 'guillotine',\n",
      " 584: 'hair slide',\n",
      " 585: 'hair spray',\n",
      " 586: 'half track',\n",
      " 587: 'hammer',\n",
      " 588: 'hamper',\n",
      " 589: 'hand blower, blow dryer, blow drier, hair dryer, hair drier',\n",
      " 590: 'hand-held computer, hand-held microcomputer',\n",
      " 591: 'handkerchief, hankie, hanky, hankey',\n",
      " 592: 'hard disc, hard disk, fixed disk',\n",
      " 593: 'harmonica, mouth organ, harp, mouth harp',\n",
      " 594: 'harp',\n",
      " 595: 'harvester, reaper',\n",
      " 596: 'hatchet',\n",
      " 597: 'holster',\n",
      " 598: 'home theater, home theatre',\n",
      " 599: 'honeycomb',\n",
      " 600: 'hook, claw',\n",
      " 601: 'hoopskirt, crinoline',\n",
      " 602: 'horizontal bar, high bar',\n",
      " 603: 'horse cart, horse-cart',\n",
      " 604: 'hourglass',\n",
      " 605: 'iPod',\n",
      " 606: 'iron, smoothing iron',\n",
      " 607: \"jack-o'-lantern\",\n",
      " 608: 'jean, blue jean, denim',\n",
      " 609: 'jeep, landrover',\n",
      " 610: 'jersey, T-shirt, tee shirt',\n",
      " 611: 'jigsaw puzzle',\n",
      " 612: 'jinrikisha, ricksha, rickshaw',\n",
      " 613: 'joystick',\n",
      " 614: 'kimono',\n",
      " 615: 'knee pad',\n",
      " 616: 'knot',\n",
      " 617: 'lab coat, laboratory coat',\n",
      " 618: 'ladle',\n",
      " 619: 'lampshade, lamp shade',\n",
      " 620: 'laptop, laptop computer',\n",
      " 621: 'lawn mower, mower',\n",
      " 622: 'lens cap, lens cover',\n",
      " 623: 'letter opener, paper knife, paperknife',\n",
      " 624: 'library',\n",
      " 625: 'lifeboat',\n",
      " 626: 'lighter, light, igniter, ignitor',\n",
      " 627: 'limousine, limo',\n",
      " 628: 'liner, ocean liner',\n",
      " 629: 'lipstick, lip rouge',\n",
      " 630: 'Loafer',\n",
      " 631: 'lotion',\n",
      " 632: 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system',\n",
      " 633: \"loupe, jeweler's loupe\",\n",
      " 634: 'lumbermill, sawmill',\n",
      " 635: 'magnetic compass',\n",
      " 636: 'mailbag, postbag',\n",
      " 637: 'mailbox, letter box',\n",
      " 638: 'maillot',\n",
      " 639: 'maillot, tank suit',\n",
      " 640: 'manhole cover',\n",
      " 641: 'maraca',\n",
      " 642: 'marimba, xylophone',\n",
      " 643: 'mask',\n",
      " 644: 'matchstick',\n",
      " 645: 'maypole',\n",
      " 646: 'maze, labyrinth',\n",
      " 647: 'measuring cup',\n",
      " 648: 'medicine chest, medicine cabinet',\n",
      " 649: 'megalith, megalithic structure',\n",
      " 650: 'microphone, mike',\n",
      " 651: 'microwave, microwave oven',\n",
      " 652: 'military uniform',\n",
      " 653: 'milk can',\n",
      " 654: 'minibus',\n",
      " 655: 'miniskirt, mini',\n",
      " 656: 'minivan',\n",
      " 657: 'missile',\n",
      " 658: 'mitten',\n",
      " 659: 'mixing bowl',\n",
      " 660: 'mobile home, manufactured home',\n",
      " 661: 'Model T',\n",
      " 662: 'modem',\n",
      " 663: 'monastery',\n",
      " 664: 'monitor',\n",
      " 665: 'moped',\n",
      " 666: 'mortar',\n",
      " 667: 'mortarboard',\n",
      " 668: 'mosque',\n",
      " 669: 'mosquito net',\n",
      " 670: 'motor scooter, scooter',\n",
      " 671: 'mountain bike, all-terrain bike, off-roader',\n",
      " 672: 'mountain tent',\n",
      " 673: 'mouse, computer mouse',\n",
      " 674: 'mousetrap',\n",
      " 675: 'moving van',\n",
      " 676: 'muzzle',\n",
      " 677: 'nail',\n",
      " 678: 'neck brace',\n",
      " 679: 'necklace',\n",
      " 680: 'nipple',\n",
      " 681: 'notebook, notebook computer',\n",
      " 682: 'obelisk',\n",
      " 683: 'oboe, hautboy, hautbois',\n",
      " 684: 'ocarina, sweet potato',\n",
      " 685: 'odometer, hodometer, mileometer, milometer',\n",
      " 686: 'oil filter',\n",
      " 687: 'organ, pipe organ',\n",
      " 688: 'oscilloscope, scope, cathode-ray oscilloscope, CRO',\n",
      " 689: 'overskirt',\n",
      " 690: 'oxcart',\n",
      " 691: 'oxygen mask',\n",
      " 692: 'packet',\n",
      " 693: 'paddle, boat paddle',\n",
      " 694: 'paddlewheel, paddle wheel',\n",
      " 695: 'padlock',\n",
      " 696: 'paintbrush',\n",
      " 697: \"pajama, pyjama, pj's, jammies\",\n",
      " 698: 'palace',\n",
      " 699: 'panpipe, pandean pipe, syrinx',\n",
      " 700: 'paper towel',\n",
      " 701: 'parachute, chute',\n",
      " 702: 'parallel bars, bars',\n",
      " 703: 'park bench',\n",
      " 704: 'parking meter',\n",
      " 705: 'passenger car, coach, carriage',\n",
      " 706: 'patio, terrace',\n",
      " 707: 'pay-phone, pay-station',\n",
      " 708: 'pedestal, plinth, footstall',\n",
      " 709: 'pencil box, pencil case',\n",
      " 710: 'pencil sharpener',\n",
      " 711: 'perfume, essence',\n",
      " 712: 'Petri dish',\n",
      " 713: 'photocopier',\n",
      " 714: 'pick, plectrum, plectron',\n",
      " 715: 'pickelhaube',\n",
      " 716: 'picket fence, paling',\n",
      " 717: 'pickup, pickup truck',\n",
      " 718: 'pier',\n",
      " 719: 'piggy bank, penny bank',\n",
      " 720: 'pill bottle',\n",
      " 721: 'pillow',\n",
      " 722: 'ping-pong ball',\n",
      " 723: 'pinwheel',\n",
      " 724: 'pirate, pirate ship',\n",
      " 725: 'pitcher, ewer',\n",
      " 726: \"plane, carpenter's plane, woodworking plane\",\n",
      " 727: 'planetarium',\n",
      " 728: 'plastic bag',\n",
      " 729: 'plate rack',\n",
      " 730: 'plow, plough',\n",
      " 731: \"plunger, plumber's helper\",\n",
      " 732: 'Polaroid camera, Polaroid Land camera',\n",
      " 733: 'pole',\n",
      " 734: 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria',\n",
      " 735: 'poncho',\n",
      " 736: 'pool table, billiard table, snooker table',\n",
      " 737: 'pop bottle, soda bottle',\n",
      " 738: 'pot, flowerpot',\n",
      " 739: \"potter's wheel\",\n",
      " 740: 'power drill',\n",
      " 741: 'prayer rug, prayer mat',\n",
      " 742: 'printer',\n",
      " 743: 'prison, prison house',\n",
      " 744: 'projectile, missile',\n",
      " 745: 'projector',\n",
      " 746: 'puck, hockey puck',\n",
      " 747: 'punching bag, punch bag, punching ball, punchball',\n",
      " 748: 'purse',\n",
      " 749: 'quill, quill pen',\n",
      " 750: 'quilt, comforter, comfort, puff',\n",
      " 751: 'racer, race car, racing car',\n",
      " 752: 'racket, racquet',\n",
      " 753: 'radiator',\n",
      " 754: 'radio, wireless',\n",
      " 755: 'radio telescope, radio reflector',\n",
      " 756: 'rain barrel',\n",
      " 757: 'recreational vehicle, RV, R.V.',\n",
      " 758: 'reel',\n",
      " 759: 'reflex camera',\n",
      " 760: 'refrigerator, icebox',\n",
      " 761: 'remote control, remote',\n",
      " 762: 'restaurant, eating house, eating place, eatery',\n",
      " 763: 'revolver, six-gun, six-shooter',\n",
      " 764: 'rifle',\n",
      " 765: 'rocking chair, rocker',\n",
      " 766: 'rotisserie',\n",
      " 767: 'rubber eraser, rubber, pencil eraser',\n",
      " 768: 'rugby ball',\n",
      " 769: 'rule, ruler',\n",
      " 770: 'running shoe',\n",
      " 771: 'safe',\n",
      " 772: 'safety pin',\n",
      " 773: 'saltshaker, salt shaker',\n",
      " 774: 'sandal',\n",
      " 775: 'sarong',\n",
      " 776: 'sax, saxophone',\n",
      " 777: 'scabbard',\n",
      " 778: 'scale, weighing machine',\n",
      " 779: 'school bus',\n",
      " 780: 'schooner',\n",
      " 781: 'scoreboard',\n",
      " 782: 'screen, CRT screen',\n",
      " 783: 'screw',\n",
      " 784: 'screwdriver',\n",
      " 785: 'seat belt, seatbelt',\n",
      " 786: 'sewing machine',\n",
      " 787: 'shield, buckler',\n",
      " 788: 'shoe shop, shoe-shop, shoe store',\n",
      " 789: 'shoji',\n",
      " 790: 'shopping basket',\n",
      " 791: 'shopping cart',\n",
      " 792: 'shovel',\n",
      " 793: 'shower cap',\n",
      " 794: 'shower curtain',\n",
      " 795: 'ski',\n",
      " 796: 'ski mask',\n",
      " 797: 'sleeping bag',\n",
      " 798: 'slide rule, slipstick',\n",
      " 799: 'sliding door',\n",
      " 800: 'slot, one-armed bandit',\n",
      " 801: 'snorkel',\n",
      " 802: 'snowmobile',\n",
      " 803: 'snowplow, snowplough',\n",
      " 804: 'soap dispenser',\n",
      " 805: 'soccer ball',\n",
      " 806: 'sock',\n",
      " 807: 'solar dish, solar collector, solar furnace',\n",
      " 808: 'sombrero',\n",
      " 809: 'soup bowl',\n",
      " 810: 'space bar',\n",
      " 811: 'space heater',\n",
      " 812: 'space shuttle',\n",
      " 813: 'spatula',\n",
      " 814: 'speedboat',\n",
      " 815: \"spider web, spider's web\",\n",
      " 816: 'spindle',\n",
      " 817: 'sports car, sport car',\n",
      " 818: 'spotlight, spot',\n",
      " 819: 'stage',\n",
      " 820: 'steam locomotive',\n",
      " 821: 'steel arch bridge',\n",
      " 822: 'steel drum',\n",
      " 823: 'stethoscope',\n",
      " 824: 'stole',\n",
      " 825: 'stone wall',\n",
      " 826: 'stopwatch, stop watch',\n",
      " 827: 'stove',\n",
      " 828: 'strainer',\n",
      " 829: 'streetcar, tram, tramcar, trolley, trolley car',\n",
      " 830: 'stretcher',\n",
      " 831: 'studio couch, day bed',\n",
      " 832: 'stupa, tope',\n",
      " 833: 'submarine, pigboat, sub, U-boat',\n",
      " 834: 'suit, suit of clothes',\n",
      " 835: 'sundial',\n",
      " 836: 'sunglass',\n",
      " 837: 'sunglasses, dark glasses, shades',\n",
      " 838: 'sunscreen, sunblock, sun blocker',\n",
      " 839: 'suspension bridge',\n",
      " 840: 'swab, swob, mop',\n",
      " 841: 'sweatshirt',\n",
      " 842: 'swimming trunks, bathing trunks',\n",
      " 843: 'swing',\n",
      " 844: 'switch, electric switch, electrical switch',\n",
      " 845: 'syringe',\n",
      " 846: 'table lamp',\n",
      " 847: 'tank, army tank, armored combat vehicle, armoured combat vehicle',\n",
      " 848: 'tape player',\n",
      " 849: 'teapot',\n",
      " 850: 'teddy, teddy bear',\n",
      " 851: 'television, television system',\n",
      " 852: 'tennis ball',\n",
      " 853: 'thatch, thatched roof',\n",
      " 854: 'theater curtain, theatre curtain',\n",
      " 855: 'thimble',\n",
      " 856: 'thresher, thrasher, threshing machine',\n",
      " 857: 'throne',\n",
      " 858: 'tile roof',\n",
      " 859: 'toaster',\n",
      " 860: 'tobacco shop, tobacconist shop, tobacconist',\n",
      " 861: 'toilet seat',\n",
      " 862: 'torch',\n",
      " 863: 'totem pole',\n",
      " 864: 'tow truck, tow car, wrecker',\n",
      " 865: 'toyshop',\n",
      " 866: 'tractor',\n",
      " 867: 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi',\n",
      " 868: 'tray',\n",
      " 869: 'trench coat',\n",
      " 870: 'tricycle, trike, velocipede',\n",
      " 871: 'trimaran',\n",
      " 872: 'tripod',\n",
      " 873: 'triumphal arch',\n",
      " 874: 'trolleybus, trolley coach, trackless trolley',\n",
      " 875: 'trombone',\n",
      " 876: 'tub, vat',\n",
      " 877: 'turnstile',\n",
      " 878: 'typewriter keyboard',\n",
      " 879: 'umbrella',\n",
      " 880: 'unicycle, monocycle',\n",
      " 881: 'upright, upright piano',\n",
      " 882: 'vacuum, vacuum cleaner',\n",
      " 883: 'vase',\n",
      " 884: 'vault',\n",
      " 885: 'velvet',\n",
      " 886: 'vending machine',\n",
      " 887: 'vestment',\n",
      " 888: 'viaduct',\n",
      " 889: 'violin, fiddle',\n",
      " 890: 'volleyball',\n",
      " 891: 'waffle iron',\n",
      " 892: 'wall clock',\n",
      " 893: 'wallet, billfold, notecase, pocketbook',\n",
      " 894: 'wardrobe, closet, press',\n",
      " 895: 'warplane, military plane',\n",
      " 896: 'washbasin, handbasin, washbowl, lavabo, wash-hand basin',\n",
      " 897: 'washer, automatic washer, washing machine',\n",
      " 898: 'water bottle',\n",
      " 899: 'water jug',\n",
      " 900: 'water tower',\n",
      " 901: 'whiskey jug',\n",
      " 902: 'whistle',\n",
      " 903: 'wig',\n",
      " 904: 'window screen',\n",
      " 905: 'window shade',\n",
      " 906: 'Windsor tie',\n",
      " 907: 'wine bottle',\n",
      " 908: 'wing',\n",
      " 909: 'wok',\n",
      " 910: 'wooden spoon',\n",
      " 911: 'wool, woolen, woollen',\n",
      " 912: 'worm fence, snake fence, snake-rail fence, Virginia fence',\n",
      " 913: 'wreck',\n",
      " 914: 'yawl',\n",
      " 915: 'yurt',\n",
      " 916: 'web site, website, internet site, site',\n",
      " 917: 'comic book',\n",
      " 918: 'crossword puzzle, crossword',\n",
      " 919: 'street sign',\n",
      " 920: 'traffic light, traffic signal, stoplight',\n",
      " 921: 'book jacket, dust cover, dust jacket, dust wrapper',\n",
      " 922: 'menu',\n",
      " 923: 'plate',\n",
      " 924: 'guacamole',\n",
      " 925: 'consomme',\n",
      " 926: 'hot pot, hotpot',\n",
      " 927: 'trifle',\n",
      " 928: 'ice cream, icecream',\n",
      " 929: 'ice lolly, lolly, lollipop, popsicle',\n",
      " 930: 'French loaf',\n",
      " 931: 'bagel, beigel',\n",
      " 932: 'pretzel',\n",
      " 933: 'cheeseburger',\n",
      " 934: 'hotdog, hot dog, red hot',\n",
      " 935: 'mashed potato',\n",
      " 936: 'head cabbage',\n",
      " 937: 'broccoli',\n",
      " 938: 'cauliflower',\n",
      " 939: 'zucchini, courgette',\n",
      " 940: 'spaghetti squash',\n",
      " 941: 'acorn squash',\n",
      " 942: 'butternut squash',\n",
      " 943: 'cucumber, cuke',\n",
      " 944: 'artichoke, globe artichoke',\n",
      " 945: 'bell pepper',\n",
      " 946: 'cardoon',\n",
      " 947: 'mushroom',\n",
      " 948: 'Granny Smith',\n",
      " 949: 'strawberry',\n",
      " 950: 'orange',\n",
      " 951: 'lemon',\n",
      " 952: 'fig',\n",
      " 953: 'pineapple, ananas',\n",
      " 954: 'banana',\n",
      " 955: 'jackfruit, jak, jack',\n",
      " 956: 'custard apple',\n",
      " 957: 'pomegranate',\n",
      " 958: 'hay',\n",
      " 959: 'carbonara',\n",
      " 960: 'chocolate sauce, chocolate syrup',\n",
      " 961: 'dough',\n",
      " 962: 'meat loaf, meatloaf',\n",
      " 963: 'pizza, pizza pie',\n",
      " 964: 'potpie',\n",
      " 965: 'burrito',\n",
      " 966: 'red wine',\n",
      " 967: 'espresso',\n",
      " 968: 'cup',\n",
      " 969: 'eggnog',\n",
      " 970: 'alp',\n",
      " 971: 'bubble',\n",
      " 972: 'cliff, drop, drop-off',\n",
      " 973: 'coral reef',\n",
      " 974: 'geyser',\n",
      " 975: 'lakeside, lakeshore',\n",
      " 976: 'promontory, headland, head, foreland',\n",
      " 977: 'sandbar, sand bar',\n",
      " 978: 'seashore, coast, seacoast, sea-coast',\n",
      " 979: 'valley, vale',\n",
      " 980: 'volcano',\n",
      " 981: 'ballplayer, baseball player',\n",
      " 982: 'groom, bridegroom',\n",
      " 983: 'scuba diver',\n",
      " 984: 'rapeseed',\n",
      " 985: 'daisy',\n",
      " 986: \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\",\n",
      " 987: 'corn',\n",
      " 988: 'acorn',\n",
      " 989: 'hip, rose hip, rosehip',\n",
      " 990: 'buckeye, horse chestnut, conker',\n",
      " 991: 'coral fungus',\n",
      " 992: 'agaric',\n",
      " 993: 'gyromitra',\n",
      " 994: 'stinkhorn, carrion fungus',\n",
      " 995: 'earthstar',\n",
      " 996: 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa',\n",
      " 997: 'bolete',\n",
      " 998: 'ear, spike, capitulum',\n",
      " 999: 'toilet tissue, toilet paper, bathroom tissue'}\n",
      "78/2:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 380\n",
      "\n",
      "def prep_image(image):\n",
      "    res = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "    res = res.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "    return res\n",
      "\n",
      "def predict(image):\n",
      "    cropped_image = center_crop(image)\n",
      "    prediction = (model.predict(prep_image(image)), model.predict(prep_image(cropped_image)))\n",
      "    if np.max(prediction[0]>=prediction[1]):\n",
      "        return {'class': np.argmax(prediction[0], axis=1)[0], 'all_prob': prediction[0]}\n",
      "    else:\n",
      "        return  {'class': np.argmax(prediction[1], axis=1)[1], 'all_prob': prediction[1]}\n",
      "    \n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "plt.imshow(image)\n",
      "\n",
      "print(class_dict[predict(image)])\n",
      "78/3:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 380\n",
      "\n",
      "def prep_image(image):\n",
      "    res = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "    res = res.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "    return res\n",
      "\n",
      "def predict(image):\n",
      "    cropped_image = center_crop(image)\n",
      "    prediction = (model.predict(prep_image(image)), model.predict(prep_image(cropped_image)))\n",
      "    if np.max(prediction[0]>=prediction[1]):\n",
      "        return {'class': np.argmax(prediction[0], axis=1)[0], 'all_prob': prediction[0]}\n",
      "    else:\n",
      "        return  {'class': np.argmax(prediction[1], axis=1)[1], 'all_prob': prediction[1]}\n",
      "    \n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "plt.imshow(image)\n",
      "\n",
      "print(class_dict[predict(image)])\n",
      "78/4:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 380\n",
      "\n",
      "def prep_image(image):\n",
      "    res = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "    res = res.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "    return res\n",
      "\n",
      "def predict(image):\n",
      "    cropped_image = center_crop(image)\n",
      "    prediction = (model.predict(prep_image(image)), model.predict(prep_image(cropped_image)))\n",
      "    if np.max(prediction[0]>=prediction[1]):\n",
      "        return {'class': np.argmax(prediction[0], axis=1)[0], 'all_prob': prediction[0]}\n",
      "    else:\n",
      "        return  {'class': np.argmax(prediction[1], axis=1)[1], 'all_prob': prediction[1]}\n",
      "    \n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "plt.imshow(image)\n",
      "\n",
      "print(class_dict[predict(image)])\n",
      "78/5:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 380\n",
      "\n",
      "def prep_image(image):\n",
      "    res = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "    res = res.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "    return res\n",
      "\n",
      "def predict(image):\n",
      "    cropped_image = center_crop(image)\n",
      "    prediction = (model.predict(prep_image(image)), model.predict(prep_image(cropped_image)))\n",
      "    if np.max(prediction[0]>=prediction[1]):\n",
      "        return {'class': np.argmax(prediction[0], axis=1)[0], 'all_prob': prediction[0]}\n",
      "    else:\n",
      "        return  {'class': np.argmax(prediction[1], axis=1)[1], 'all_prob': prediction[1]}\n",
      "    \n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "plt.imshow(image)\n",
      "\n",
      "print(class_dict[predict(image)['class']])\n",
      "78/6:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 380\n",
      "\n",
      "def prep_image(image):\n",
      "    res = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "    res = res.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "    return res\n",
      "\n",
      "def predict(image):\n",
      "    cropped_image = center_crop(image)\n",
      "    prediction = (model.predict(prep_image(image)), model.predict(prep_image(cropped_image)))\n",
      "    if np.max(prediction[0]>=prediction[1]):\n",
      "        return {'class': np.argmax(prediction[0], axis=1)[0], 'all_prob': prediction[0]}\n",
      "    else:\n",
      "        return  {'class': np.argmax(prediction[1], axis=1)[1], 'all_prob': prediction[1]}\n",
      "    \n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "plt.imshow(image)\n",
      "\n",
      "print(class_dict[predict(image)['all_prob']])\n",
      "78/7:\n",
      "import os\n",
      "import cv2\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "IMG_SIZE = 380\n",
      "\n",
      "def prep_image(image):\n",
      "    res = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "    res = res.reshape(1,IMG_SIZE,IMG_SIZE,3)\n",
      "    return res\n",
      "\n",
      "def predict(image):\n",
      "    cropped_image = center_crop(image)\n",
      "    prediction = (model.predict(prep_image(image)), model.predict(prep_image(cropped_image)))\n",
      "    if np.max(prediction[0]>=prediction[1]):\n",
      "        return {'class': np.argmax(prediction[0], axis=1)[0], 'all_prob': prediction[0]}\n",
      "    else:\n",
      "        return  {'class': np.argmax(prediction[1], axis=1)[1], 'all_prob': prediction[1]}\n",
      "    \n",
      "\n",
      "image = cv2.imread(\"../data/images/\" + \"l0j2uy.jpg\")\n",
      "plt.imshow(image)\n",
      "\n",
      "print(class_dict[predict(image)['class']])\n",
      "\n",
      "print(predict(image)['all_prob'])\n",
      "78/8:\n",
      "import pickle\n",
      "\n",
      "result = {}\n",
      "for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "        print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "        image = cv2.imread(\"../data/images/\" + file)\n",
      "\n",
      "        result[file] = predict(image)\n",
      "        \n",
      "pickle.dump(result, open( \"../pickle/image_classes.pkl\", \"wb\" ))\n",
      "\n",
      "image_classes = pickle.load(open( \"../pickle/image_classes.pkl\", \"rb\" ))\n",
      "image_classes\n",
      "78/9:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "def visualize_colors(cluster, centroids):\n",
      "    # Get the number of different clusters, create histogram, and normalize\n",
      "    labels = np.arange(0, len(np.unique(cluster.labels_)) + 1)\n",
      "    (hist, _) = np.histogram(cluster.labels_, bins = labels)\n",
      "    hist = hist.astype(\"float\")\n",
      "    hist /= hist.sum()\n",
      "\n",
      "    # Create frequency rect and iterate through each cluster's color and percentage\n",
      "    rect = np.zeros((50, 300, 3), dtype=np.uint8)\n",
      "    colors = sorted([(percent, color) for (percent, color) in zip(hist, centroids)])\n",
      "    start = 0\n",
      "    for (percent, color) in colors:\n",
      "        print(color, \"{:0.2f}%\".format(percent * 100))\n",
      "        end = start + (percent * 300)\n",
      "        cv2.rectangle(rect, (int(start), 0), (int(end), 50), \\\n",
      "                      color.astype(\"uint8\").tolist(), -1)\n",
      "        start = end\n",
      "    return rect\n",
      "\n",
      "image = cv2.imread('../data/images/l146p3.jpg')\n",
      "plt.figure()\n",
      "plt.imshow(image)\n",
      "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "visualize = visualize_colors(cluster, cluster.cluster_centers_)\n",
      "plt.figure()\n",
      "plt.imshow(visualize)\n",
      "\n",
      "image = cv2.imread('../data/images/l31llr.png')\n",
      "plt.figure()\n",
      "plt.imshow(image)\n",
      "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "visualize = visualize_colors(cluster, cluster.cluster_centers_)\n",
      "plt.figure()\n",
      "plt.imshow(visualize)\n",
      "\n",
      "result = {}\n",
      "for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "        print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "        image = cv2.imread(\"../data/images/\" + file)\n",
      "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "        reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "        cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "        print(cluster.cluster_centers_)\n",
      "        result[file] = cluster.cluster_centers_\n",
      "        \n",
      "pickle.dump(result, open( \"../pickle/image_color_clusters.pkl\", \"wb\" ))\n",
      "78/10:\n",
      "import pickle\n",
      "\n",
      "# result = {}\n",
      "# for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "#     if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "#         print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "#         image = cv2.imread(\"../data/images/\" + file)\n",
      "\n",
      "#         result[file] = predict(image)\n",
      "        \n",
      "# pickle.dump(result, open( \"../pickle/image_classes.pkl\", \"wb\" ))\n",
      "\n",
      "image_classes = pickle.load(open( \"../pickle/image_classes.pkl\", \"rb\" ))\n",
      "image_classes\n",
      "78/11:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "def visualize_colors(cluster, centroids):\n",
      "    # Get the number of different clusters, create histogram, and normalize\n",
      "    labels = np.arange(0, len(np.unique(cluster.labels_)) + 1)\n",
      "    (hist, _) = np.histogram(cluster.labels_, bins = labels)\n",
      "    hist = hist.astype(\"float\")\n",
      "    hist /= hist.sum()\n",
      "\n",
      "    # Create frequency rect and iterate through each cluster's color and percentage\n",
      "    rect = np.zeros((50, 300, 3), dtype=np.uint8)\n",
      "    colors = sorted([(percent, color) for (percent, color) in zip(hist, centroids)])\n",
      "    start = 0\n",
      "    for (percent, color) in colors:\n",
      "        print(color, \"{:0.2f}%\".format(percent * 100))\n",
      "        end = start + (percent * 300)\n",
      "        cv2.rectangle(rect, (int(start), 0), (int(end), 50), \\\n",
      "                      color.astype(\"uint8\").tolist(), -1)\n",
      "        start = end\n",
      "    return rect\n",
      "\n",
      "image = cv2.imread('../data/images/l146p3.jpg')\n",
      "plt.figure()\n",
      "plt.imshow(image)\n",
      "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "visualize = visualize_colors(cluster, cluster.cluster_centers_)\n",
      "plt.figure()\n",
      "plt.imshow(visualize)\n",
      "\n",
      "image = cv2.imread('../data/images/l31llr.png')\n",
      "plt.figure()\n",
      "plt.imshow(image)\n",
      "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "visualize = visualize_colors(cluster, cluster.cluster_centers_)\n",
      "plt.figure()\n",
      "plt.imshow(visualize)\n",
      "\n",
      "result = {}\n",
      "for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "        print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "        image = cv2.imread(\"../data/images/\" + file)\n",
      "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "        reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "        cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "        print(cluster.cluster_centers_)\n",
      "        \n",
      "        labels = np.arange(0, len(np.unique(cluster.labels_)) + 1)\n",
      "        (hist, _) = np.histogram(cluster.labels_, bins = labels)\n",
      "        hist = hist.astype(\"float\")\n",
      "        hist /= hist.sum()\n",
      "        \n",
      "        result[file]['color'] = cluster.cluster_centers_\n",
      "        result[file]['percentage'] = hist\n",
      "        \n",
      "pickle.dump(result, open( \"../pickle/image_color_clusters.pkl\", \"wb\" ))\n",
      "78/12:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "def visualize_colors(cluster, centroids):\n",
      "    # Get the number of different clusters, create histogram, and normalize\n",
      "    labels = np.arange(0, len(np.unique(cluster.labels_)) + 1)\n",
      "    (hist, _) = np.histogram(cluster.labels_, bins = labels)\n",
      "    hist = hist.astype(\"float\")\n",
      "    hist /= hist.sum()\n",
      "\n",
      "    # Create frequency rect and iterate through each cluster's color and percentage\n",
      "    rect = np.zeros((50, 300, 3), dtype=np.uint8)\n",
      "    colors = sorted([(percent, color) for (percent, color) in zip(hist, centroids)])\n",
      "    start = 0\n",
      "    for (percent, color) in colors:\n",
      "        print(color, \"{:0.2f}%\".format(percent * 100))\n",
      "        end = start + (percent * 300)\n",
      "        cv2.rectangle(rect, (int(start), 0), (int(end), 50), \\\n",
      "                      color.astype(\"uint8\").tolist(), -1)\n",
      "        start = end\n",
      "    return rect\n",
      "\n",
      "image = cv2.imread('../data/images/l146p3.jpg')\n",
      "plt.figure()\n",
      "plt.imshow(image)\n",
      "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "visualize = visualize_colors(cluster, cluster.cluster_centers_)\n",
      "plt.figure()\n",
      "plt.imshow(visualize)\n",
      "\n",
      "image = cv2.imread('../data/images/l31llr.png')\n",
      "plt.figure()\n",
      "plt.imshow(image)\n",
      "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "visualize = visualize_colors(cluster, cluster.cluster_centers_)\n",
      "plt.figure()\n",
      "plt.imshow(visualize)\n",
      "\n",
      "result = {}\n",
      "for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "        print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "        image = cv2.imread(\"../data/images/\" + file)\n",
      "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "        reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "        cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "        print(cluster.cluster_centers_)\n",
      "        \n",
      "        labels = np.arange(0, len(np.unique(cluster.labels_)) + 1)\n",
      "        (hist, _) = np.histogram(cluster.labels_, bins = labels)\n",
      "        hist = hist.astype(\"float\")\n",
      "        hist /= hist.sum()\n",
      "        \n",
      "        result[file] = {}\n",
      "        \n",
      "        result[file]['color'] = cluster.cluster_centers_\n",
      "        result[file]['percentage'] = hist\n",
      "        \n",
      "pickle.dump(result, open( \"../pickle/image_color_clusters.pkl\", \"wb\" ))\n",
      "78/13:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "def visualize_colors(cluster, centroids):\n",
      "    # Get the number of different clusters, create histogram, and normalize\n",
      "    labels = np.arange(0, len(np.unique(cluster.labels_)) + 1)\n",
      "    (hist, _) = np.histogram(cluster.labels_, bins = labels)\n",
      "    hist = hist.astype(\"float\")\n",
      "    hist /= hist.sum()\n",
      "\n",
      "    # Create frequency rect and iterate through each cluster's color and percentage\n",
      "    rect = np.zeros((50, 300, 3), dtype=np.uint8)\n",
      "    colors = sorted([(percent, color) for (percent, color) in zip(hist, centroids)])\n",
      "    start = 0\n",
      "    for (percent, color) in colors:\n",
      "        print(color, \"{:0.2f}%\".format(percent * 100))\n",
      "        end = start + (percent * 300)\n",
      "        cv2.rectangle(rect, (int(start), 0), (int(end), 50), \\\n",
      "                      color.astype(\"uint8\").tolist(), -1)\n",
      "        start = end\n",
      "    return rect\n",
      "\n",
      "image = cv2.imread('../data/images/l146p3.jpg')\n",
      "plt.figure()\n",
      "plt.imshow(image)\n",
      "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "visualize = visualize_colors(cluster, cluster.cluster_centers_)\n",
      "plt.figure()\n",
      "plt.imshow(visualize)\n",
      "\n",
      "image = cv2.imread('../data/images/l31llr.png')\n",
      "plt.figure()\n",
      "plt.imshow(image)\n",
      "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "visualize = visualize_colors(cluster, cluster.cluster_centers_)\n",
      "plt.figure()\n",
      "plt.imshow(visualize)\n",
      "\n",
      "result = {}\n",
      "for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "        print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "        image = cv2.imread(\"../data/images/\" + file)\n",
      "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "        reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "        cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "        \n",
      "        labels = np.arange(0, len(np.unique(cluster.labels_)) + 1)\n",
      "        (hist, _) = np.histogram(cluster.labels_, bins = labels)\n",
      "        hist = hist.astype(\"float\")\n",
      "        hist /= hist.sum()\n",
      "        \n",
      "        result[file] = {}\n",
      "        \n",
      "        result[file]['color'] = cluster.cluster_centers_\n",
      "        result[file]['percentage'] = hist\n",
      "        \n",
      "        print( result[file])\n",
      "        \n",
      "pickle.dump(result, open( \"../pickle/image_color_clusters.pkl\", \"wb\" ))\n",
      "78/14:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "def visualize_colors(cluster, centroids):\n",
      "    # Get the number of different clusters, create histogram, and normalize\n",
      "    labels = np.arange(0, len(np.unique(cluster.labels_)) + 1)\n",
      "    (hist, _) = np.histogram(cluster.labels_, bins = labels)\n",
      "    hist = hist.astype(\"float\")\n",
      "    hist /= hist.sum()\n",
      "\n",
      "    # Create frequency rect and iterate through each cluster's color and percentage\n",
      "    rect = np.zeros((50, 300, 3), dtype=np.uint8)\n",
      "    colors = sorted([(percent, color) for (percent, color) in zip(hist, centroids)])\n",
      "    start = 0\n",
      "    for (percent, color) in colors:\n",
      "        print(color, \"{:0.2f}%\".format(percent * 100))\n",
      "        end = start + (percent * 300)\n",
      "        cv2.rectangle(rect, (int(start), 0), (int(end), 50), \\\n",
      "                      color.astype(\"uint8\").tolist(), -1)\n",
      "        start = end\n",
      "    return rect\n",
      "\n",
      "image = cv2.imread('../data/images/l146p3.jpg')\n",
      "image = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "plt.figure()\n",
      "plt.imshow(image)\n",
      "# image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "# reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "# cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "# visualize = visualize_colors(cluster, cluster.cluster_centers_)\n",
      "# plt.figure()\n",
      "# plt.imshow(visualize)\n",
      "\n",
      "# image = cv2.imread('../data/images/l31llr.png')\n",
      "# image = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "# plt.figure()\n",
      "# plt.imshow(image)\n",
      "# image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "# reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "# cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "# visualize = visualize_colors(cluster, cluster.cluster_centers_)\n",
      "# plt.figure()\n",
      "# plt.imshow(visualize)\n",
      "\n",
      "# result = {}\n",
      "# for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "#     if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "#         print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "#         image = cv2.imread(\"../data/images/\" + file)\n",
      "#         image = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "#         reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "#         cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "        \n",
      "#         labels = np.arange(0, len(np.unique(cluster.labels_)) + 1)\n",
      "#         (hist, _) = np.histogram(cluster.labels_, bins = labels)\n",
      "#         hist = hist.astype(\"float\")\n",
      "#         hist /= hist.sum()\n",
      "        \n",
      "#         result[file] = {}\n",
      "        \n",
      "#         result[file]['color'] = cluster.cluster_centers_\n",
      "#         result[file]['percentage'] = hist\n",
      "        \n",
      "#         print( result[file])\n",
      "        \n",
      "# pickle.dump(result, open( \"../pickle/image_color_clusters.pkl\", \"wb\" ))\n",
      "78/15:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "def visualize_colors(cluster, centroids):\n",
      "    # Get the number of different clusters, create histogram, and normalize\n",
      "    labels = np.arange(0, len(np.unique(cluster.labels_)) + 1)\n",
      "    (hist, _) = np.histogram(cluster.labels_, bins = labels)\n",
      "    hist = hist.astype(\"float\")\n",
      "    hist /= hist.sum()\n",
      "\n",
      "    # Create frequency rect and iterate through each cluster's color and percentage\n",
      "    rect = np.zeros((50, 300, 3), dtype=np.uint8)\n",
      "    colors = sorted([(percent, color) for (percent, color) in zip(hist, centroids)])\n",
      "    start = 0\n",
      "    for (percent, color) in colors:\n",
      "        print(color, \"{:0.2f}%\".format(percent * 100))\n",
      "        end = start + (percent * 300)\n",
      "        cv2.rectangle(rect, (int(start), 0), (int(end), 50), \\\n",
      "                      color.astype(\"uint8\").tolist(), -1)\n",
      "        start = end\n",
      "    return rect\n",
      "\n",
      "image = cv2.imread('../data/images/l146p3.jpg')\n",
      "image = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "plt.figure()\n",
      "plt.imshow(image)\n",
      "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "visualize = visualize_colors(cluster, cluster.cluster_centers_)\n",
      "plt.figure()\n",
      "plt.imshow(visualize)\n",
      "\n",
      "# image = cv2.imread('../data/images/l31llr.png')\n",
      "# image = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "# plt.figure()\n",
      "# plt.imshow(image)\n",
      "# image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "# reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "# cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "# visualize = visualize_colors(cluster, cluster.cluster_centers_)\n",
      "# plt.figure()\n",
      "# plt.imshow(visualize)\n",
      "\n",
      "# result = {}\n",
      "# for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "#     if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "#         print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "#         image = cv2.imread(\"../data/images/\" + file)\n",
      "#         image = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "#         reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "#         cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "        \n",
      "#         labels = np.arange(0, len(np.unique(cluster.labels_)) + 1)\n",
      "#         (hist, _) = np.histogram(cluster.labels_, bins = labels)\n",
      "#         hist = hist.astype(\"float\")\n",
      "#         hist /= hist.sum()\n",
      "        \n",
      "#         result[file] = {}\n",
      "        \n",
      "#         result[file]['color'] = cluster.cluster_centers_\n",
      "#         result[file]['percentage'] = hist\n",
      "        \n",
      "#         print( result[file])\n",
      "        \n",
      "# pickle.dump(result, open( \"../pickle/image_color_clusters.pkl\", \"wb\" ))\n",
      "78/16:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "def visualize_colors(cluster, centroids):\n",
      "    # Get the number of different clusters, create histogram, and normalize\n",
      "    labels = np.arange(0, len(np.unique(cluster.labels_)) + 1)\n",
      "    (hist, _) = np.histogram(cluster.labels_, bins = labels)\n",
      "    hist = hist.astype(\"float\")\n",
      "    hist /= hist.sum()\n",
      "\n",
      "    # Create frequency rect and iterate through each cluster's color and percentage\n",
      "    rect = np.zeros((50, 300, 3), dtype=np.uint8)\n",
      "    colors = sorted([(percent, color) for (percent, color) in zip(hist, centroids)])\n",
      "    start = 0\n",
      "    for (percent, color) in colors:\n",
      "        print(color, \"{:0.2f}%\".format(percent * 100))\n",
      "        end = start + (percent * 300)\n",
      "        cv2.rectangle(rect, (int(start), 0), (int(end), 50), \\\n",
      "                      color.astype(\"uint8\").tolist(), -1)\n",
      "        start = end\n",
      "    return rect\n",
      "\n",
      "image = cv2.imread('../data/images/l146p3.jpg')\n",
      "image = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "plt.figure()\n",
      "plt.imshow(image)\n",
      "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "visualize = visualize_colors(cluster, cluster.cluster_centers_)\n",
      "plt.figure()\n",
      "plt.imshow(visualize)\n",
      "\n",
      "image = cv2.imread('../data/images/l31llr.png')\n",
      "image = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "plt.figure()\n",
      "plt.imshow(image)\n",
      "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "visualize = visualize_colors(cluster, cluster.cluster_centers_)\n",
      "plt.figure()\n",
      "plt.imshow(visualize)\n",
      "\n",
      "# result = {}\n",
      "# for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "#     if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "#         print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "#         image = cv2.imread(\"../data/images/\" + file)\n",
      "#         image = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "#         reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "#         cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "        \n",
      "#         labels = np.arange(0, len(np.unique(cluster.labels_)) + 1)\n",
      "#         (hist, _) = np.histogram(cluster.labels_, bins = labels)\n",
      "#         hist = hist.astype(\"float\")\n",
      "#         hist /= hist.sum()\n",
      "        \n",
      "#         result[file] = {}\n",
      "        \n",
      "#         result[file]['color'] = cluster.cluster_centers_\n",
      "#         result[file]['percentage'] = hist\n",
      "        \n",
      "#         print( result[file])\n",
      "        \n",
      "# pickle.dump(result, open( \"../pickle/image_color_clusters.pkl\", \"wb\" ))\n",
      "78/17:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "def visualize_colors(cluster, centroids):\n",
      "    # Get the number of different clusters, create histogram, and normalize\n",
      "    labels = np.arange(0, len(np.unique(cluster.labels_)) + 1)\n",
      "    (hist, _) = np.histogram(cluster.labels_, bins = labels)\n",
      "    hist = hist.astype(\"float\")\n",
      "    hist /= hist.sum()\n",
      "\n",
      "    # Create frequency rect and iterate through each cluster's color and percentage\n",
      "    rect = np.zeros((50, 300, 3), dtype=np.uint8)\n",
      "    colors = sorted([(percent, color) for (percent, color) in zip(hist, centroids)])\n",
      "    start = 0\n",
      "    for (percent, color) in colors:\n",
      "        print(color, \"{:0.2f}%\".format(percent * 100))\n",
      "        end = start + (percent * 300)\n",
      "        cv2.rectangle(rect, (int(start), 0), (int(end), 50), \\\n",
      "                      color.astype(\"uint8\").tolist(), -1)\n",
      "        start = end\n",
      "    return rect\n",
      "\n",
      "image = cv2.imread('../data/images/l146p3.jpg')\n",
      "image = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "plt.figure()\n",
      "plt.imshow(image)\n",
      "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "visualize = visualize_colors(cluster, cluster.cluster_centers_)\n",
      "plt.figure()\n",
      "plt.imshow(visualize)\n",
      "\n",
      "image = cv2.imread('../data/images/l31llr.png')\n",
      "image = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "plt.figure()\n",
      "plt.imshow(image)\n",
      "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "visualize = visualize_colors(cluster, cluster.cluster_centers_)\n",
      "plt.figure()\n",
      "plt.imshow(visualize)\n",
      "\n",
      "result = {}\n",
      "for index, file in enumerate(os.listdir(\"../data/images/\")):\n",
      "    if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
      "        print(str(index) + \"/\" + str(len(os.listdir(\"../data/images/\"))))\n",
      "        image = cv2.imread(\"../data/images/\" + file)\n",
      "        image = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
      "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
      "        reshape = image.reshape((image.shape[0] * image.shape[1], 3))\n",
      "        cluster = KMeans(n_clusters=5).fit(reshape)\n",
      "        \n",
      "        labels = np.arange(0, len(np.unique(cluster.labels_)) + 1)\n",
      "        (hist, _) = np.histogram(cluster.labels_, bins = labels)\n",
      "        hist = hist.astype(\"float\")\n",
      "        hist /= hist.sum()\n",
      "        \n",
      "        result[file] = {}\n",
      "        \n",
      "        result[file]['color'] = cluster.cluster_centers_\n",
      "        result[file]['percentage'] = hist\n",
      "        \n",
      "pickle.dump(result, open( \"../pickle/image_color_clusters.pkl\", \"wb\" ))\n",
      "80/1:\n",
      "print('PyDev console: using IPython 7.19.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['C:\\\\Users\\\\luker\\\\Documents\\\\Github\\\\SkaiWiD', 'C:/Users/luker/Documents/Github/SkaiWiD'])\n",
      "81/1:\n",
      "import pandas\n",
      "from collections import Counter\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import nltk\n",
      "nltk.download('punkt')\n",
      "%matplotlib inline\n",
      "\n",
      "df = pandas.read_csv('../data/reddit_wsb.csv')\n",
      "df[\"body\"] = df[\"body\"].fillna('')\n",
      "df[\"title\"] = df[\"title\"].fillna('')\n",
      "\n",
      "df\n",
      "82/1:\n",
      "import pandas\n",
      "from collections import Counter\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import nltk\n",
      "nltk.download('punkt')\n",
      "%matplotlib inline\n",
      "\n",
      "df = pandas.read_csv('../data/reddit_wsb.csv')\n",
      "df[\"body\"] = df[\"body\"].fillna('')\n",
      "df[\"title\"] = df[\"title\"].fillna('')\n",
      "\n",
      "df\n",
      "82/2:\n",
      "import pickle\n",
      "img_text = pickle.load(open(\"../pickle/texts.pkl\", \"rb\"))\n",
      "print(img_text.keys())\n",
      "83/1:\n",
      "# from google.colab import drive\n",
      "\n",
      "# drive.mount('/content/gdrive')\n",
      "83/2: # !ls gdrive/MyDrive/PED\n",
      "83/3:\n",
      "import pandas\n",
      "from collections import Counter\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import nltk\n",
      "nltk.download('punkt')\n",
      "%matplotlib inline\n",
      "\n",
      "df = pandas.read_csv('../data/reddit_wsb.csv')\n",
      "df[\"body\"] = df[\"body\"].fillna('')\n",
      "df[\"title\"] = df[\"title\"].fillna('')\n",
      "\n",
      "df\n",
      "83/4:\n",
      "# from google.colab import drive\n",
      "\n",
      "# drive.mount('/content/gdrive')\n",
      "83/5: # !ls gdrive/MyDrive/PED\n",
      "83/6:\n",
      "import pandas\n",
      "from collections import Counter\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import nltk\n",
      "nltk.download('punkt')\n",
      "%matplotlib inline\n",
      "\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "df[\"body\"] = df[\"body\"].fillna('')\n",
      "df[\"title\"] = df[\"title\"].fillna('')\n",
      "\n",
      "df\n",
      "83/7:\n",
      "import pickle\n",
      "img_text = pickle.load(open(\"../pickle/texts.pkl\", \"rb\"))\n",
      "print(img_text.keys())\n",
      "83/8:\n",
      "# from google.colab import drive\n",
      "\n",
      "# drive.mount('/content/gdrive')\n",
      "83/9: # !ls gdrive/MyDrive/PED\n",
      "83/10:\n",
      "import pandas\n",
      "from collections import Counter\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import nltk\n",
      "nltk.download('punkt')\n",
      "%matplotlib inline\n",
      "\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "df[\"body\"] = df[\"body\"].fillna('')\n",
      "df[\"title\"] = df[\"title\"].fillna('')\n",
      "\n",
      "df\n",
      "83/11:\n",
      "import pickle\n",
      "img_text = pickle.load(open(\"../../pickle/texts.pkl\", \"rb\"))\n",
      "print(img_text.keys())\n",
      "83/12:\n",
      "seconds_in_day = 60 * 60 * 24.\n",
      "df['sin_time'] = np.sin(2*np.pi*(df.created % seconds_in_day)/seconds_in_day)\n",
      "df['cos_time'] = np.cos(2*np.pi*(df.created % seconds_in_day)/seconds_in_day)\n",
      "83/13: df.describe()\n",
      "83/14: df.sample(5000).plot.scatter('sin_time','cos_time').set_aspect('equal')\n",
      "83/15: df.columns\n",
      "83/16:\n",
      "plt.hist(df['score'], bins=30)\n",
      "plt.ylabel('Count')\n",
      "plt.yscale('log')\n",
      "plt.xlabel('Data');\n",
      "83/17:\n",
      "plt.hist(df['created'], bins=30)\n",
      "plt.ylabel('Count')\n",
      "plt.yscale('log')\n",
      "plt.xlabel('Data');\n",
      "83/18:\n",
      "!pip install emoji\n",
      "from collections import Counter\n",
      "from nltk import word_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
      "\n",
      "nltk.download('stopwords')\n",
      "# tokenizer = RegexpTokenizer(r'\\w+')\n",
      "from nltk.corpus import stopwords\n",
      "from collections import Counter\n",
      "83/19: len(\" \".join(df[\"body\"]))\n",
      "83/20: ' '.join([' '.join(x) for x in img_text.values()])\n",
      "83/21: len(img_text.keys())\n",
      "83/22:\n",
      "text_lens_no_0 = [len(x) for x in img_text.values() if len(x) > 0]\n",
      "print('count', len(img_text))\n",
      "print('no text in', len([len(x) for x in img_text.values() if len(x) == 0]), 'images')\n",
      "print('min', np.min([len(x) for x in img_text.values()]))\n",
      "print('max', np.max([len(x) for x in img_text.values()]))\n",
      "\n",
      "print('q1', np.quantile(text_lens_no_0, 0.25))\n",
      "print('median', np.median(text_lens_no_0))\n",
      "print('q3', np.quantile(text_lens_no_0, 0.75))\n",
      "83/23:\n",
      "words_separate = [' '.join(x).split(' ') for x in img_text.values()]\n",
      "\n",
      "text_lens_no_0 = [len(x) for x in img_text.values() if len(x) > 0]\n",
      "print('min', np.min([len(x) for x in words_separate]))\n",
      "print('max', np.max([len(x) for x in words_separate]))\n",
      "print('q1', np.quantile([len(x) for x in words_separate], 0.25))\n",
      "print('median', np.median([len(x) for x in words_separate]))\n",
      "print('q3', np.quantile([len(x) for x in words_separate], 0.75))\n",
      "83/24:\n",
      "img_most_common = Counter(word_tokenize(' '.join([' '.join(x) for x in img_text.values()])))\n",
      "for key, cnts in list(img_most_common.items()):\n",
      "  if not key in emoji.UNICODE_EMOJI and (key in stopwords.words('english') or not re.match(r'\\w+',key)):\n",
      "    del img_most_common[key]\n",
      "img_most_common = img_most_common.most_common(100)\n",
      "img_most_common\n",
      "83/25:\n",
      "body_most_common = Counter(word_tokenize(\" \".join(df[\"body\"])))\n",
      "for key, cnts in list(body_most_common.items()):\n",
      "  if not key in emoji.UNICODE_EMOJI and (key in stopwords.words('english') or not re.match(r'\\w+',key)):\n",
      "    del body_most_common[key]\n",
      "body_most_common = body_most_common.most_common(100)\n",
      "body_most_common\n",
      "83/26:\n",
      "title_most_common = Counter(word_tokenize(\" \".join(df[\"title\"])))\n",
      "for key, cnts in list(title_most_common.items()):\n",
      "  if not key in emoji.UNICODE_EMOJI and (key in stopwords.words('english') or not re.match(r'\\w+',key)):\n",
      "    del title_most_common[key]\n",
      "title_most_common = title_most_common.most_common(100)\n",
      "title_most_common\n",
      "83/27:\n",
      "additional_keywords = ['gme', 'hold', 'buy', 'retard', 'wife', 'hedgies', '', 'moon','','', '', '','','', '', '']\n",
      "count = lambda l1,l2: sum([1 for x in l1 if x in l2])\n",
      "text_attributes = {}\n",
      "for key in additional_keywords: \n",
      "  text_attributes['title_' + key] = df[\"title\"].str.lower().str.count(key)\n",
      "  text_attributes['body_' + key] = df[\"body\"].str.lower().str.count(key)\n",
      "83/28:\n",
      "for key, cnts in body_most_common: \n",
      "  text_attributes['body_' + key.lower()] = df[\"body\"].str.lower().str.count(key.lower())\n",
      "\n",
      "\n",
      "for key, cnts in title_most_common: \n",
      "  text_attributes['title_' + key.lower()] = df[\"title\"].str.lower().str.count(key.lower())\n",
      "83/29:\n",
      "\n",
      "text_attributes['body_punctuation'] = df['body'].apply(lambda s: count(s, string.punctuation))\n",
      "text_attributes['title_punctuation'] = df['title'].apply(lambda s: count(s, string.punctuation))\n",
      "\n",
      "text_attributes['body_cap_ratio'] = df['body'].str.count(r'[A-Z]')/df['body'].str.count(r'[a-zA-Z]')\n",
      "text_attributes['title_cap_ratio'] = df['title'].str.count(r'[A-Z]')/df['title'].str.count(r'[a-zA-Z]')\n",
      "\n",
      "text_attributes['time_of_day'] = pandas.to_datetime(df['timestamp']).dt.hour + pandas.to_datetime(df['timestamp']).dt.minute/60\n",
      "\n",
      "text_attributes['title_length'] = df['title'].apply(len)\n",
      "text_attributes['title_non_alphanumeric_ratio'] = df['title'].str.count(r'[^A-Za-z0-9]')/df['title'].apply(len)\n",
      "83/30:\n",
      "def regex_count_in_url_and_body(regex):\n",
      "    return df['body'].apply(lambda s: len(re.findall(regex, s))) + df['url'].apply(lambda s: len(re.findall(regex, s)))\n",
      "83/31:\n",
      "text_attributes['link'] = regex_count_in_url_and_body(r'(https?://[^\\s]+)')\n",
      "text_attributes['reddit_link'] = regex_count_in_url_and_body(r'(https?:\\/\\/w?w?w?\\.?redd\\.?it[^\\s]+)')\n",
      "text_attributes['yt_link'] = regex_count_in_url_and_body(r'(https?:\\/\\/w?w?w?\\.?youtu\\.?be[^\\s]+)')\n",
      "text_attributes['tweet_link'] = regex_count_in_url_and_body(r'(https?:\\/\\/w?w?w?\\.?twitter[^\\s]+)')\n",
      "text_attributes['facebook_link'] = regex_count_in_url_and_body(r'(https?:\\/\\/w?w?w?\\.?facebook[^\\s]+)')\n",
      "\n",
      "text_attributes['gif'] = regex_count_in_url_and_body(r'(https?:\\/\\/w?w?w?\\.?giphy[^\\s]+)') + regex_count_in_url_and_body(r'(https?:\\/\\/w?w?w?\\.?gifyu[^\\s]+)') + regex_count_in_url_and_body(r'(https?:\\/\\/w?w?w?\\.?gfycat[^\\s]+)') + df['url'].apply(lambda s: len(re.findall(r'(\\.gif)', s)))\n",
      "\n",
      "text_attributes = pandas.DataFrame(text_attributes)\n",
      "text_attributes\n",
      "df[text_attributes['gif']>=1]\n",
      "83/32: text_attributes.describe()\n",
      "83/33: df.describe()\n",
      "83/34:\n",
      "for i in ['score', 'sin_time', 'cos_time']:\n",
      "  text_attributes[i] = df[i]  \n",
      "text_attributes['day'] = pandas.to_datetime(df['timestamp']).dt.day\n",
      "text_attributes['month'] = pandas.to_datetime(df['timestamp']).dt.month\n",
      "text_attributes.describe()\n",
      "83/35:\n",
      "print(pandas.to_datetime(df['timestamp'].head(10)))\n",
      "print(pandas.to_datetime(df['timestamp'].head(10)).dt.day)\n",
      "print(pandas.to_datetime(df['timestamp'].head(10)).dt.month)\n",
      "83/36:\n",
      "print(re.findall('http','ahttpjkshdoifjdshttp'))\n",
      "print()\n",
      "print(re.findall(r\"https?://(?:(?:[^\\s()])|(?:\\(\\S*\\)]*\\)))+\", 'https://www.benzinga.com/government/21/01/19337399/something-systemically-wrong-with-gamestop-options-trading-says-massachusetts-securities-regulator(dupa=dupa()))'))\n",
      "83/37:\n",
      "text_attributes['body_'].describe()\n",
      "text_attributes['score'].min() == text_attributes['score'].max()\n",
      "83/38:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "\n",
      "img_clusters = pickle.load(open(\"gdrive/MyDrive/PED/image_color_clusters.pkl\", \"rb\"))\n",
      "img_means = pickle.load(open(\"gdrive/MyDrive/PED/image_hsv_means.pkl\", \"rb\"))\n",
      "\n",
      "pic_data = {}\n",
      "pic_data['id'] = []\n",
      "pic_data['text'] = []\n",
      "pic_data['clusters'] = []\n",
      "pic_data['means'] = []\n",
      "pic_data['score'] = []\n",
      "\n",
      "for key in img_means.keys():\n",
      "  pic_data['id'].append(key)\n",
      "  pic_data['text'].append(\" \".join(img_text[key[:-4]]))\n",
      "  pic_data['clusters'].append(img_clusters[key])\n",
      "  pic_data['means'].append(img_means[key])\n",
      "  pic_data['score'].append(df.loc[df['id'] == key[:-4]]['score'].values[0])\n",
      "\n",
      "print(pic_data['score'])\n",
      "pic_data = pandas.DataFrame(pic_data)\n",
      "\n",
      "pic_attributes = {'id': pic_data['id']}\n",
      "for key, cnts in img_most_common: \n",
      "  pic_attributes['text_' + key] =  pic_data['text'].str.lower().str.count(key.lower())\n",
      "\n",
      "pic_attributes['h_sin'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['h_cos'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['s'] = pic_data['means'].apply(lambda s: s[1]/255)\n",
      "pic_attributes['v'] = pic_data['means'].apply(lambda s: s[2]/255)\n",
      "pic_attributes['score'] = pic_data['score']\n",
      "for j in range(5):\n",
      "  for i in range(3):\n",
      "    pic_attributes['color_'+str(j)+'_'+'rgb'[i]] = pic_data['clusters'].apply(lambda s: s['color'][j][i])\n",
      "  pic_attributes['color_'+str(j)+'_%'] = pic_data['clusters'].apply(lambda s: s['percentage'][j])\n",
      "\n",
      "pic_attributes = pandas.DataFrame(pic_attributes)\n",
      "print(pic_attributes)\n",
      "\n",
      "corr_image_matrix = pic_attributes.corr(method='spearman')\n",
      "\n",
      "correlations = []\n",
      "for i,column in enumerate(corr_image_matrix):\n",
      "  x = corr_image_matrix[column][i+1:]\n",
      "  for row,v in x.items():\n",
      "    correlations.append([v,column,row])\n",
      "correlations.sort(key = lambda x: -abs(x[0]))\n",
      "\n",
      "score_corr = corr_image_matrix['score'][corr_image_matrix['score']<1]\n",
      "\n",
      "\n",
      "rf = RandomForestRegressor()\n",
      "rf.fit(pic_attributes[pic_attributes.columns.difference(['id', 'score'])], pic_attributes['score'])\n",
      "print(sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), pic_attributes.head()), \n",
      "             reverse=True))\n",
      "84/1:\n",
      "# from sklearn.decomposition import PCA\n",
      "# img_class = pickle.load(open(\"gdrive/MyDrive/PED/image_classes.pkl\", \"rb\"))\n",
      "\n",
      "# class_table = {}\n",
      "# # class_table['id'] = []\n",
      "\n",
      "# for i in range(1000):\n",
      "#   class_table[str(i)] = []\n",
      "# for key in img_class.keys():\n",
      "#   # class_table['id'].append(key)\n",
      "#   for i in range(1000):\n",
      "#     class_table[str(i)].append(img_class[key]['all_prob'][0][i])\n",
      "\n",
      "# class_table = pandas.DataFrame(class_table)\n",
      "# pca = PCA(n_components=10)\n",
      "# pca.fit(class_table)\n",
      "# print(pca.components_ )\n",
      "\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "\n",
      "img_clusters = pickle.load(open(\"gdrive/MyDrive/PED/image_color_clusters.pkl\", \"rb\"))\n",
      "img_means = pickle.load(open(\"gdrive/MyDrive/PED/image_hsv_means.pkl\", \"rb\"))\n",
      "\n",
      "pic_data = {}\n",
      "pic_data['id'] = []\n",
      "pic_data['text'] = []\n",
      "pic_data['clusters'] = []\n",
      "pic_data['means'] = []\n",
      "pic_data['score'] = []\n",
      "\n",
      "for key in img_means.keys():\n",
      "  pic_data['id'].append(key)\n",
      "  pic_data['text'].append(\" \".join(img_text[key[:-4]]))\n",
      "  pic_data['clusters'].append(img_clusters[key])\n",
      "  pic_data['means'].append(img_means[key])\n",
      "  pic_data['score'].append(df.loc[df['id'] == key[:-4]]['score'].values[0])\n",
      "\n",
      "print(pic_data['score'])\n",
      "pic_data = pandas.DataFrame(pic_data)\n",
      "\n",
      "pic_attributes = {'id': pic_data['id']}\n",
      "for key, cnts in img_most_common: \n",
      "  pic_attributes['text_' + key] =  pic_data['text'].str.lower().str.count(key.lower())\n",
      "\n",
      "pic_attributes['h_sin'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['h_cos'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['s'] = pic_data['means'].apply(lambda s: s[1]/255)\n",
      "pic_attributes['v'] = pic_data['means'].apply(lambda s: s[2]/255)\n",
      "pic_attributes['score'] = pic_data['score']\n",
      "for j in range(5):\n",
      "  for i in range(3):\n",
      "    pic_attributes['color_'+str(j)+'_'+'rgb'[i]] = pic_data['clusters'].apply(lambda s: s['color'][j][i])\n",
      "  pic_attributes['color_'+str(j)+'_%'] = pic_data['clusters'].apply(lambda s: s['percentage'][j])\n",
      "\n",
      "pic_attributes = pandas.DataFrame(pic_attributes)\n",
      "print(pic_attributes)\n",
      "\n",
      "corr_image_matrix = pic_attributes.corr(method='spearman')\n",
      "\n",
      "correlations = []\n",
      "for i,column in enumerate(corr_image_matrix):\n",
      "  x = corr_image_matrix[column][i+1:]\n",
      "  for row,v in x.items():\n",
      "    correlations.append([v,column,row])\n",
      "correlations.sort(key = lambda x: -abs(x[0]))\n",
      "\n",
      "score_corr = corr_image_matrix['score'][corr_image_matrix['score']<1]\n",
      "\n",
      "\n",
      "rf = RandomForestRegressor()\n",
      "rf.fit(pic_attributes[pic_attributes.columns.difference(['id', 'score'])], pic_attributes['score'])\n",
      "print(sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), pic_attributes.head()), \n",
      "             reverse=True))\n",
      "84/2:\n",
      "# from sklearn.decomposition import PCA\n",
      "# img_class = pickle.load(open(\"gdrive/MyDrive/PED/image_classes.pkl\", \"rb\"))\n",
      "\n",
      "# class_table = {}\n",
      "# # class_table['id'] = []\n",
      "\n",
      "# for i in range(1000):\n",
      "#   class_table[str(i)] = []\n",
      "# for key in img_class.keys():\n",
      "#   # class_table['id'].append(key)\n",
      "#   for i in range(1000):\n",
      "#     class_table[str(i)].append(img_class[key]['all_prob'][0][i])\n",
      "\n",
      "# class_table = pandas.DataFrame(class_table)\n",
      "# pca = PCA(n_components=10)\n",
      "# pca.fit(class_table)\n",
      "# print(pca.components_ )\n",
      "import pandas\n",
      "import pickle\n",
      "import numpy as np\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "\n",
      "img_clusters = pickle.load(open(\"gdrive/MyDrive/PED/image_color_clusters.pkl\", \"rb\"))\n",
      "img_means = pickle.load(open(\"gdrive/MyDrive/PED/image_hsv_means.pkl\", \"rb\"))\n",
      "\n",
      "pic_data = {}\n",
      "pic_data['id'] = []\n",
      "pic_data['text'] = []\n",
      "pic_data['clusters'] = []\n",
      "pic_data['means'] = []\n",
      "pic_data['score'] = []\n",
      "\n",
      "for key in img_means.keys():\n",
      "  pic_data['id'].append(key)\n",
      "  pic_data['text'].append(\" \".join(img_text[key[:-4]]))\n",
      "  pic_data['clusters'].append(img_clusters[key])\n",
      "  pic_data['means'].append(img_means[key])\n",
      "  pic_data['score'].append(df.loc[df['id'] == key[:-4]]['score'].values[0])\n",
      "\n",
      "print(pic_data['score'])\n",
      "pic_data = pandas.DataFrame(pic_data)\n",
      "\n",
      "pic_attributes = {'id': pic_data['id']}\n",
      "for key, cnts in img_most_common: \n",
      "  pic_attributes['text_' + key] =  pic_data['text'].str.lower().str.count(key.lower())\n",
      "\n",
      "pic_attributes['h_sin'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['h_cos'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['s'] = pic_data['means'].apply(lambda s: s[1]/255)\n",
      "pic_attributes['v'] = pic_data['means'].apply(lambda s: s[2]/255)\n",
      "pic_attributes['score'] = pic_data['score']\n",
      "for j in range(5):\n",
      "  for i in range(3):\n",
      "    pic_attributes['color_'+str(j)+'_'+'rgb'[i]] = pic_data['clusters'].apply(lambda s: s['color'][j][i])\n",
      "  pic_attributes['color_'+str(j)+'_%'] = pic_data['clusters'].apply(lambda s: s['percentage'][j])\n",
      "\n",
      "pic_attributes = pandas.DataFrame(pic_attributes)\n",
      "print(pic_attributes)\n",
      "\n",
      "corr_image_matrix = pic_attributes.corr(method='spearman')\n",
      "\n",
      "correlations = []\n",
      "for i,column in enumerate(corr_image_matrix):\n",
      "  x = corr_image_matrix[column][i+1:]\n",
      "  for row,v in x.items():\n",
      "    correlations.append([v,column,row])\n",
      "correlations.sort(key = lambda x: -abs(x[0]))\n",
      "\n",
      "score_corr = corr_image_matrix['score'][corr_image_matrix['score']<1]\n",
      "\n",
      "\n",
      "rf = RandomForestRegressor()\n",
      "rf.fit(pic_attributes[pic_attributes.columns.difference(['id', 'score'])], pic_attributes['score'])\n",
      "print(sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), pic_attributes.head()), \n",
      "             reverse=True))\n",
      "84/3:\n",
      "# from sklearn.decomposition import PCA\n",
      "# img_class = pickle.load(open(\"gdrive/MyDrive/PED/image_classes.pkl\", \"rb\"))\n",
      "\n",
      "# class_table = {}\n",
      "# # class_table['id'] = []\n",
      "\n",
      "# for i in range(1000):\n",
      "#   class_table[str(i)] = []\n",
      "# for key in img_class.keys():\n",
      "#   # class_table['id'].append(key)\n",
      "#   for i in range(1000):\n",
      "#     class_table[str(i)].append(img_class[key]['all_prob'][0][i])\n",
      "\n",
      "# class_table = pandas.DataFrame(class_table)\n",
      "# pca = PCA(n_components=10)\n",
      "# pca.fit(class_table)\n",
      "# print(pca.components_ )\n",
      "import pandas\n",
      "import pickle\n",
      "import numpy as np\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "\n",
      "img_clusters = pickle.load(open(\"../../data/image_color_clusters.pkl\", \"rb\"))\n",
      "img_means = pickle.load(open(\"../../data/image_hsv_means.pkl\", \"rb\"))\n",
      "\n",
      "pic_data = {}\n",
      "pic_data['id'] = []\n",
      "pic_data['text'] = []\n",
      "pic_data['clusters'] = []\n",
      "pic_data['means'] = []\n",
      "pic_data['score'] = []\n",
      "\n",
      "for key in img_means.keys():\n",
      "  pic_data['id'].append(key)\n",
      "  pic_data['text'].append(\" \".join(img_text[key[:-4]]))\n",
      "  pic_data['clusters'].append(img_clusters[key])\n",
      "  pic_data['means'].append(img_means[key])\n",
      "  pic_data['score'].append(df.loc[df['id'] == key[:-4]]['score'].values[0])\n",
      "\n",
      "print(pic_data['score'])\n",
      "pic_data = pandas.DataFrame(pic_data)\n",
      "\n",
      "pic_attributes = {'id': pic_data['id']}\n",
      "for key, cnts in img_most_common: \n",
      "  pic_attributes['text_' + key] =  pic_data['text'].str.lower().str.count(key.lower())\n",
      "\n",
      "pic_attributes['h_sin'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['h_cos'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['s'] = pic_data['means'].apply(lambda s: s[1]/255)\n",
      "pic_attributes['v'] = pic_data['means'].apply(lambda s: s[2]/255)\n",
      "pic_attributes['score'] = pic_data['score']\n",
      "for j in range(5):\n",
      "  for i in range(3):\n",
      "    pic_attributes['color_'+str(j)+'_'+'rgb'[i]] = pic_data['clusters'].apply(lambda s: s['color'][j][i])\n",
      "  pic_attributes['color_'+str(j)+'_%'] = pic_data['clusters'].apply(lambda s: s['percentage'][j])\n",
      "\n",
      "pic_attributes = pandas.DataFrame(pic_attributes)\n",
      "print(pic_attributes)\n",
      "\n",
      "corr_image_matrix = pic_attributes.corr(method='spearman')\n",
      "\n",
      "correlations = []\n",
      "for i,column in enumerate(corr_image_matrix):\n",
      "  x = corr_image_matrix[column][i+1:]\n",
      "  for row,v in x.items():\n",
      "    correlations.append([v,column,row])\n",
      "correlations.sort(key = lambda x: -abs(x[0]))\n",
      "\n",
      "score_corr = corr_image_matrix['score'][corr_image_matrix['score']<1]\n",
      "\n",
      "\n",
      "rf = RandomForestRegressor()\n",
      "rf.fit(pic_attributes[pic_attributes.columns.difference(['id', 'score'])], pic_attributes['score'])\n",
      "print(sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), pic_attributes.head()), \n",
      "             reverse=True))\n",
      "84/4:\n",
      "# from sklearn.decomposition import PCA\n",
      "# img_class = pickle.load(open(\"gdrive/MyDrive/PED/image_classes.pkl\", \"rb\"))\n",
      "\n",
      "# class_table = {}\n",
      "# # class_table['id'] = []\n",
      "\n",
      "# for i in range(1000):\n",
      "#   class_table[str(i)] = []\n",
      "# for key in img_class.keys():\n",
      "#   # class_table['id'].append(key)\n",
      "#   for i in range(1000):\n",
      "#     class_table[str(i)].append(img_class[key]['all_prob'][0][i])\n",
      "\n",
      "# class_table = pandas.DataFrame(class_table)\n",
      "# pca = PCA(n_components=10)\n",
      "# pca.fit(class_table)\n",
      "# print(pca.components_ )\n",
      "import pandas\n",
      "import pickle\n",
      "import numpy as np\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "\n",
      "img_clusters = pickle.load(open(\"../../pickle/image_color_clusters.pkl\", \"rb\"))\n",
      "img_means = pickle.load(open(\"../../pickle/image_hsv_means.pkl\", \"rb\"))\n",
      "\n",
      "pic_data = {}\n",
      "pic_data['id'] = []\n",
      "pic_data['text'] = []\n",
      "pic_data['clusters'] = []\n",
      "pic_data['means'] = []\n",
      "pic_data['score'] = []\n",
      "\n",
      "for key in img_means.keys():\n",
      "  pic_data['id'].append(key)\n",
      "  pic_data['text'].append(\" \".join(img_text[key[:-4]]))\n",
      "  pic_data['clusters'].append(img_clusters[key])\n",
      "  pic_data['means'].append(img_means[key])\n",
      "  pic_data['score'].append(df.loc[df['id'] == key[:-4]]['score'].values[0])\n",
      "\n",
      "print(pic_data['score'])\n",
      "pic_data = pandas.DataFrame(pic_data)\n",
      "\n",
      "pic_attributes = {'id': pic_data['id']}\n",
      "for key, cnts in img_most_common: \n",
      "  pic_attributes['text_' + key] =  pic_data['text'].str.lower().str.count(key.lower())\n",
      "\n",
      "pic_attributes['h_sin'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['h_cos'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['s'] = pic_data['means'].apply(lambda s: s[1]/255)\n",
      "pic_attributes['v'] = pic_data['means'].apply(lambda s: s[2]/255)\n",
      "pic_attributes['score'] = pic_data['score']\n",
      "for j in range(5):\n",
      "  for i in range(3):\n",
      "    pic_attributes['color_'+str(j)+'_'+'rgb'[i]] = pic_data['clusters'].apply(lambda s: s['color'][j][i])\n",
      "  pic_attributes['color_'+str(j)+'_%'] = pic_data['clusters'].apply(lambda s: s['percentage'][j])\n",
      "\n",
      "pic_attributes = pandas.DataFrame(pic_attributes)\n",
      "print(pic_attributes)\n",
      "\n",
      "corr_image_matrix = pic_attributes.corr(method='spearman')\n",
      "\n",
      "correlations = []\n",
      "for i,column in enumerate(corr_image_matrix):\n",
      "  x = corr_image_matrix[column][i+1:]\n",
      "  for row,v in x.items():\n",
      "    correlations.append([v,column,row])\n",
      "correlations.sort(key = lambda x: -abs(x[0]))\n",
      "\n",
      "score_corr = corr_image_matrix['score'][corr_image_matrix['score']<1]\n",
      "\n",
      "\n",
      "rf = RandomForestRegressor()\n",
      "rf.fit(pic_attributes[pic_attributes.columns.difference(['id', 'score'])], pic_attributes['score'])\n",
      "print(sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), pic_attributes.head()), \n",
      "             reverse=True))\n",
      "84/5:\n",
      "# from sklearn.decomposition import PCA\n",
      "# img_class = pickle.load(open(\"gdrive/MyDrive/PED/image_classes.pkl\", \"rb\"))\n",
      "\n",
      "# class_table = {}\n",
      "# # class_table['id'] = []\n",
      "\n",
      "# for i in range(1000):\n",
      "#   class_table[str(i)] = []\n",
      "# for key in img_class.keys():\n",
      "#   # class_table['id'].append(key)\n",
      "#   for i in range(1000):\n",
      "#     class_table[str(i)].append(img_class[key]['all_prob'][0][i])\n",
      "\n",
      "# class_table = pandas.DataFrame(class_table)\n",
      "# pca = PCA(n_components=10)\n",
      "# pca.fit(class_table)\n",
      "# print(pca.components_ )\n",
      "import pandas\n",
      "import pickle\n",
      "import numpy as np\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "\n",
      "img_clusters = pickle.load(open(\"../../pickle/image_color_clusters.pkl\", \"rb\"))\n",
      "img_means = pickle.load(open(\"../../pickle/image_hsv_means.pkl\", \"rb\"))\n",
      "img_text = pickle.load(open(\"../../pickle/texts.pkl\", \"rb\"))\n",
      "\n",
      "pic_data = {}\n",
      "pic_data['id'] = []\n",
      "pic_data['text'] = []\n",
      "pic_data['clusters'] = []\n",
      "pic_data['means'] = []\n",
      "pic_data['score'] = []\n",
      "\n",
      "for key in img_means.keys():\n",
      "  pic_data['id'].append(key)\n",
      "  pic_data['text'].append(\" \".join(img_text[key[:-4]]))\n",
      "  pic_data['clusters'].append(img_clusters[key])\n",
      "  pic_data['means'].append(img_means[key])\n",
      "  pic_data['score'].append(df.loc[df['id'] == key[:-4]]['score'].values[0])\n",
      "\n",
      "print(pic_data['score'])\n",
      "pic_data = pandas.DataFrame(pic_data)\n",
      "\n",
      "pic_attributes = {'id': pic_data['id']}\n",
      "for key, cnts in img_most_common: \n",
      "  pic_attributes['text_' + key] =  pic_data['text'].str.lower().str.count(key.lower())\n",
      "\n",
      "pic_attributes['h_sin'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['h_cos'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['s'] = pic_data['means'].apply(lambda s: s[1]/255)\n",
      "pic_attributes['v'] = pic_data['means'].apply(lambda s: s[2]/255)\n",
      "pic_attributes['score'] = pic_data['score']\n",
      "for j in range(5):\n",
      "  for i in range(3):\n",
      "    pic_attributes['color_'+str(j)+'_'+'rgb'[i]] = pic_data['clusters'].apply(lambda s: s['color'][j][i])\n",
      "  pic_attributes['color_'+str(j)+'_%'] = pic_data['clusters'].apply(lambda s: s['percentage'][j])\n",
      "\n",
      "pic_attributes = pandas.DataFrame(pic_attributes)\n",
      "print(pic_attributes)\n",
      "\n",
      "corr_image_matrix = pic_attributes.corr(method='spearman')\n",
      "\n",
      "correlations = []\n",
      "for i,column in enumerate(corr_image_matrix):\n",
      "  x = corr_image_matrix[column][i+1:]\n",
      "  for row,v in x.items():\n",
      "    correlations.append([v,column,row])\n",
      "correlations.sort(key = lambda x: -abs(x[0]))\n",
      "\n",
      "score_corr = corr_image_matrix['score'][corr_image_matrix['score']<1]\n",
      "\n",
      "\n",
      "rf = RandomForestRegressor()\n",
      "rf.fit(pic_attributes[pic_attributes.columns.difference(['id', 'score'])], pic_attributes['score'])\n",
      "print(sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), pic_attributes.head()), \n",
      "             reverse=True))\n",
      "84/6:\n",
      "# from sklearn.decomposition import PCA\n",
      "# img_class = pickle.load(open(\"gdrive/MyDrive/PED/image_classes.pkl\", \"rb\"))\n",
      "\n",
      "# class_table = {}\n",
      "# # class_table['id'] = []\n",
      "\n",
      "# for i in range(1000):\n",
      "#   class_table[str(i)] = []\n",
      "# for key in img_class.keys():\n",
      "#   # class_table['id'].append(key)\n",
      "#   for i in range(1000):\n",
      "#     class_table[str(i)].append(img_class[key]['all_prob'][0][i])\n",
      "\n",
      "# class_table = pandas.DataFrame(class_table)\n",
      "# pca = PCA(n_components=10)\n",
      "# pca.fit(class_table)\n",
      "# print(pca.components_ )\n",
      "import pandas\n",
      "import pickle\n",
      "import numpy as np\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "df[\"body\"] = df[\"body\"].fillna('')\n",
      "df[\"title\"] = df[\"title\"].fillna('')\n",
      "img_clusters = pickle.load(open(\"../../pickle/image_color_clusters.pkl\", \"rb\"))\n",
      "img_means = pickle.load(open(\"../../pickle/image_hsv_means.pkl\", \"rb\"))\n",
      "img_text = pickle.load(open(\"../../pickle/texts.pkl\", \"rb\"))\n",
      "\n",
      "pic_data = {}\n",
      "pic_data['id'] = []\n",
      "pic_data['text'] = []\n",
      "pic_data['clusters'] = []\n",
      "pic_data['means'] = []\n",
      "pic_data['score'] = []\n",
      "\n",
      "for key in img_means.keys():\n",
      "  pic_data['id'].append(key)\n",
      "  pic_data['text'].append(\" \".join(img_text[key[:-4]]))\n",
      "  pic_data['clusters'].append(img_clusters[key])\n",
      "  pic_data['means'].append(img_means[key])\n",
      "  pic_data['score'].append(df.loc[df['id'] == key[:-4]]['score'].values[0])\n",
      "\n",
      "print(pic_data['score'])\n",
      "pic_data = pandas.DataFrame(pic_data)\n",
      "\n",
      "pic_attributes = {'id': pic_data['id']}\n",
      "for key, cnts in img_most_common: \n",
      "  pic_attributes['text_' + key] =  pic_data['text'].str.lower().str.count(key.lower())\n",
      "\n",
      "pic_attributes['h_sin'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['h_cos'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['s'] = pic_data['means'].apply(lambda s: s[1]/255)\n",
      "pic_attributes['v'] = pic_data['means'].apply(lambda s: s[2]/255)\n",
      "pic_attributes['score'] = pic_data['score']\n",
      "for j in range(5):\n",
      "  for i in range(3):\n",
      "    pic_attributes['color_'+str(j)+'_'+'rgb'[i]] = pic_data['clusters'].apply(lambda s: s['color'][j][i])\n",
      "  pic_attributes['color_'+str(j)+'_%'] = pic_data['clusters'].apply(lambda s: s['percentage'][j])\n",
      "\n",
      "pic_attributes = pandas.DataFrame(pic_attributes)\n",
      "print(pic_attributes)\n",
      "\n",
      "corr_image_matrix = pic_attributes.corr(method='spearman')\n",
      "\n",
      "correlations = []\n",
      "for i,column in enumerate(corr_image_matrix):\n",
      "  x = corr_image_matrix[column][i+1:]\n",
      "  for row,v in x.items():\n",
      "    correlations.append([v,column,row])\n",
      "correlations.sort(key = lambda x: -abs(x[0]))\n",
      "\n",
      "score_corr = corr_image_matrix['score'][corr_image_matrix['score']<1]\n",
      "\n",
      "\n",
      "rf = RandomForestRegressor()\n",
      "rf.fit(pic_attributes[pic_attributes.columns.difference(['id', 'score'])], pic_attributes['score'])\n",
      "print(sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), pic_attributes.head()), \n",
      "             reverse=True))\n",
      "84/7:\n",
      "# from sklearn.decomposition import PCA\n",
      "# img_class = pickle.load(open(\"gdrive/MyDrive/PED/image_classes.pkl\", \"rb\"))\n",
      "\n",
      "# class_table = {}\n",
      "# # class_table['id'] = []\n",
      "\n",
      "# for i in range(1000):\n",
      "#   class_table[str(i)] = []\n",
      "# for key in img_class.keys():\n",
      "#   # class_table['id'].append(key)\n",
      "#   for i in range(1000):\n",
      "#     class_table[str(i)].append(img_class[key]['all_prob'][0][i])\n",
      "\n",
      "# class_table = pandas.DataFrame(class_table)\n",
      "# pca = PCA(n_components=10)\n",
      "# pca.fit(class_table)\n",
      "# print(pca.components_ )\n",
      "import pandas\n",
      "import pickle\n",
      "import numpy as np\n",
      "from collections import Counter\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "df[\"body\"] = df[\"body\"].fillna('')\n",
      "df[\"title\"] = df[\"title\"].fillna('')\n",
      "img_clusters = pickle.load(open(\"../../pickle/image_color_clusters.pkl\", \"rb\"))\n",
      "img_means = pickle.load(open(\"../../pickle/image_hsv_means.pkl\", \"rb\"))\n",
      "img_text = pickle.load(open(\"../../pickle/texts.pkl\", \"rb\"))\n",
      "\n",
      "img_most_common = Counter(word_tokenize(' '.join([' '.join(x) for x in img_text.values()])))\n",
      "for key, cnts in list(img_most_common.items()):\n",
      "  if not key in emoji.UNICODE_EMOJI and (key in stopwords.words('english') or not re.match(r'\\w+',key)):\n",
      "    del img_most_common[key]\n",
      "img_most_common = img_most_common.most_common(100)\n",
      "img_most_common\n",
      "\n",
      "\n",
      "pic_data = {}\n",
      "pic_data['id'] = []\n",
      "pic_data['text'] = []\n",
      "pic_data['clusters'] = []\n",
      "pic_data['means'] = []\n",
      "pic_data['score'] = []\n",
      "\n",
      "for key in img_means.keys():\n",
      "  pic_data['id'].append(key)\n",
      "  pic_data['text'].append(\" \".join(img_text[key[:-4]]))\n",
      "  pic_data['clusters'].append(img_clusters[key])\n",
      "  pic_data['means'].append(img_means[key])\n",
      "  pic_data['score'].append(df.loc[df['id'] == key[:-4]]['score'].values[0])\n",
      "\n",
      "print(pic_data['score'])\n",
      "pic_data = pandas.DataFrame(pic_data)\n",
      "\n",
      "pic_attributes = {'id': pic_data['id']}\n",
      "for key, cnts in img_most_common: \n",
      "  pic_attributes['text_' + key] =  pic_data['text'].str.lower().str.count(key.lower())\n",
      "\n",
      "pic_attributes['h_sin'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['h_cos'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['s'] = pic_data['means'].apply(lambda s: s[1]/255)\n",
      "pic_attributes['v'] = pic_data['means'].apply(lambda s: s[2]/255)\n",
      "pic_attributes['score'] = pic_data['score']\n",
      "for j in range(5):\n",
      "  for i in range(3):\n",
      "    pic_attributes['color_'+str(j)+'_'+'rgb'[i]] = pic_data['clusters'].apply(lambda s: s['color'][j][i])\n",
      "  pic_attributes['color_'+str(j)+'_%'] = pic_data['clusters'].apply(lambda s: s['percentage'][j])\n",
      "\n",
      "pic_attributes = pandas.DataFrame(pic_attributes)\n",
      "print(pic_attributes)\n",
      "\n",
      "corr_image_matrix = pic_attributes.corr(method='spearman')\n",
      "\n",
      "correlations = []\n",
      "for i,column in enumerate(corr_image_matrix):\n",
      "  x = corr_image_matrix[column][i+1:]\n",
      "  for row,v in x.items():\n",
      "    correlations.append([v,column,row])\n",
      "correlations.sort(key = lambda x: -abs(x[0]))\n",
      "\n",
      "score_corr = corr_image_matrix['score'][corr_image_matrix['score']<1]\n",
      "\n",
      "\n",
      "rf = RandomForestRegressor()\n",
      "rf.fit(pic_attributes[pic_attributes.columns.difference(['id', 'score'])], pic_attributes['score'])\n",
      "print(sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), pic_attributes.head()), \n",
      "             reverse=True))\n",
      "84/8:\n",
      "# from sklearn.decomposition import PCA\n",
      "# img_class = pickle.load(open(\"gdrive/MyDrive/PED/image_classes.pkl\", \"rb\"))\n",
      "\n",
      "# class_table = {}\n",
      "# # class_table['id'] = []\n",
      "\n",
      "# for i in range(1000):\n",
      "#   class_table[str(i)] = []\n",
      "# for key in img_class.keys():\n",
      "#   # class_table['id'].append(key)\n",
      "#   for i in range(1000):\n",
      "#     class_table[str(i)].append(img_class[key]['all_prob'][0][i])\n",
      "\n",
      "# class_table = pandas.DataFrame(class_table)\n",
      "# pca = PCA(n_components=10)\n",
      "# pca.fit(class_table)\n",
      "# print(pca.components_ )\n",
      "import pandas\n",
      "import pickle\n",
      "import numpy as np\n",
      "from collections import Counter\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "import nltk\n",
      "nltk.download('punkt')\n",
      "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
      "\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "df[\"body\"] = df[\"body\"].fillna('')\n",
      "df[\"title\"] = df[\"title\"].fillna('')\n",
      "img_clusters = pickle.load(open(\"../../pickle/image_color_clusters.pkl\", \"rb\"))\n",
      "img_means = pickle.load(open(\"../../pickle/image_hsv_means.pkl\", \"rb\"))\n",
      "img_text = pickle.load(open(\"../../pickle/texts.pkl\", \"rb\"))\n",
      "\n",
      "img_most_common = Counter(word_tokenize(' '.join([' '.join(x) for x in img_text.values()])))\n",
      "for key, cnts in list(img_most_common.items()):\n",
      "  if not key in emoji.UNICODE_EMOJI and (key in stopwords.words('english') or not re.match(r'\\w+',key)):\n",
      "    del img_most_common[key]\n",
      "img_most_common = img_most_common.most_common(100)\n",
      "img_most_common\n",
      "\n",
      "\n",
      "pic_data = {}\n",
      "pic_data['id'] = []\n",
      "pic_data['text'] = []\n",
      "pic_data['clusters'] = []\n",
      "pic_data['means'] = []\n",
      "pic_data['score'] = []\n",
      "\n",
      "for key in img_means.keys():\n",
      "  pic_data['id'].append(key)\n",
      "  pic_data['text'].append(\" \".join(img_text[key[:-4]]))\n",
      "  pic_data['clusters'].append(img_clusters[key])\n",
      "  pic_data['means'].append(img_means[key])\n",
      "  pic_data['score'].append(df.loc[df['id'] == key[:-4]]['score'].values[0])\n",
      "\n",
      "print(pic_data['score'])\n",
      "pic_data = pandas.DataFrame(pic_data)\n",
      "\n",
      "pic_attributes = {'id': pic_data['id']}\n",
      "for key, cnts in img_most_common: \n",
      "  pic_attributes['text_' + key] =  pic_data['text'].str.lower().str.count(key.lower())\n",
      "\n",
      "pic_attributes['h_sin'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['h_cos'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['s'] = pic_data['means'].apply(lambda s: s[1]/255)\n",
      "pic_attributes['v'] = pic_data['means'].apply(lambda s: s[2]/255)\n",
      "pic_attributes['score'] = pic_data['score']\n",
      "for j in range(5):\n",
      "  for i in range(3):\n",
      "    pic_attributes['color_'+str(j)+'_'+'rgb'[i]] = pic_data['clusters'].apply(lambda s: s['color'][j][i])\n",
      "  pic_attributes['color_'+str(j)+'_%'] = pic_data['clusters'].apply(lambda s: s['percentage'][j])\n",
      "\n",
      "pic_attributes = pandas.DataFrame(pic_attributes)\n",
      "print(pic_attributes)\n",
      "\n",
      "corr_image_matrix = pic_attributes.corr(method='spearman')\n",
      "\n",
      "correlations = []\n",
      "for i,column in enumerate(corr_image_matrix):\n",
      "  x = corr_image_matrix[column][i+1:]\n",
      "  for row,v in x.items():\n",
      "    correlations.append([v,column,row])\n",
      "correlations.sort(key = lambda x: -abs(x[0]))\n",
      "\n",
      "score_corr = corr_image_matrix['score'][corr_image_matrix['score']<1]\n",
      "\n",
      "\n",
      "rf = RandomForestRegressor()\n",
      "rf.fit(pic_attributes[pic_attributes.columns.difference(['id', 'score'])], pic_attributes['score'])\n",
      "print(sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), pic_attributes.head()), \n",
      "             reverse=True))\n",
      "84/9:\n",
      "# from sklearn.decomposition import PCA\n",
      "# img_class = pickle.load(open(\"gdrive/MyDrive/PED/image_classes.pkl\", \"rb\"))\n",
      "\n",
      "# class_table = {}\n",
      "# # class_table['id'] = []\n",
      "\n",
      "# for i in range(1000):\n",
      "#   class_table[str(i)] = []\n",
      "# for key in img_class.keys():\n",
      "#   # class_table['id'].append(key)\n",
      "#   for i in range(1000):\n",
      "#     class_table[str(i)].append(img_class[key]['all_prob'][0][i])\n",
      "\n",
      "# class_table = pandas.DataFrame(class_table)\n",
      "# pca = PCA(n_components=10)\n",
      "# pca.fit(class_table)\n",
      "# print(pca.components_ )\n",
      "import pandas\n",
      "import pickle\n",
      "import numpy as np\n",
      "from collections import Counter\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "import nltk\n",
      "nltk.download('punkt')\n",
      "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
      "from nltk import word_tokenize\n",
      "\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "df[\"body\"] = df[\"body\"].fillna('')\n",
      "df[\"title\"] = df[\"title\"].fillna('')\n",
      "img_clusters = pickle.load(open(\"../../pickle/image_color_clusters.pkl\", \"rb\"))\n",
      "img_means = pickle.load(open(\"../../pickle/image_hsv_means.pkl\", \"rb\"))\n",
      "img_text = pickle.load(open(\"../../pickle/texts.pkl\", \"rb\"))\n",
      "\n",
      "img_most_common = Counter(word_tokenize(' '.join([' '.join(x) for x in img_text.values()])))\n",
      "for key, cnts in list(img_most_common.items()):\n",
      "  if not key in emoji.UNICODE_EMOJI and (key in stopwords.words('english') or not re.match(r'\\w+',key)):\n",
      "    del img_most_common[key]\n",
      "img_most_common = img_most_common.most_common(100)\n",
      "img_most_common\n",
      "\n",
      "\n",
      "pic_data = {}\n",
      "pic_data['id'] = []\n",
      "pic_data['text'] = []\n",
      "pic_data['clusters'] = []\n",
      "pic_data['means'] = []\n",
      "pic_data['score'] = []\n",
      "\n",
      "for key in img_means.keys():\n",
      "  pic_data['id'].append(key)\n",
      "  pic_data['text'].append(\" \".join(img_text[key[:-4]]))\n",
      "  pic_data['clusters'].append(img_clusters[key])\n",
      "  pic_data['means'].append(img_means[key])\n",
      "  pic_data['score'].append(df.loc[df['id'] == key[:-4]]['score'].values[0])\n",
      "\n",
      "print(pic_data['score'])\n",
      "pic_data = pandas.DataFrame(pic_data)\n",
      "\n",
      "pic_attributes = {'id': pic_data['id']}\n",
      "for key, cnts in img_most_common: \n",
      "  pic_attributes['text_' + key] =  pic_data['text'].str.lower().str.count(key.lower())\n",
      "\n",
      "pic_attributes['h_sin'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['h_cos'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['s'] = pic_data['means'].apply(lambda s: s[1]/255)\n",
      "pic_attributes['v'] = pic_data['means'].apply(lambda s: s[2]/255)\n",
      "pic_attributes['score'] = pic_data['score']\n",
      "for j in range(5):\n",
      "  for i in range(3):\n",
      "    pic_attributes['color_'+str(j)+'_'+'rgb'[i]] = pic_data['clusters'].apply(lambda s: s['color'][j][i])\n",
      "  pic_attributes['color_'+str(j)+'_%'] = pic_data['clusters'].apply(lambda s: s['percentage'][j])\n",
      "\n",
      "pic_attributes = pandas.DataFrame(pic_attributes)\n",
      "print(pic_attributes)\n",
      "\n",
      "corr_image_matrix = pic_attributes.corr(method='spearman')\n",
      "\n",
      "correlations = []\n",
      "for i,column in enumerate(corr_image_matrix):\n",
      "  x = corr_image_matrix[column][i+1:]\n",
      "  for row,v in x.items():\n",
      "    correlations.append([v,column,row])\n",
      "correlations.sort(key = lambda x: -abs(x[0]))\n",
      "\n",
      "score_corr = corr_image_matrix['score'][corr_image_matrix['score']<1]\n",
      "\n",
      "\n",
      "rf = RandomForestRegressor()\n",
      "rf.fit(pic_attributes[pic_attributes.columns.difference(['id', 'score'])], pic_attributes['score'])\n",
      "print(sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), pic_attributes.head()), \n",
      "             reverse=True))\n",
      "84/10:\n",
      "# from sklearn.decomposition import PCA\n",
      "# img_class = pickle.load(open(\"gdrive/MyDrive/PED/image_classes.pkl\", \"rb\"))\n",
      "\n",
      "# class_table = {}\n",
      "# # class_table['id'] = []\n",
      "\n",
      "# for i in range(1000):\n",
      "#   class_table[str(i)] = []\n",
      "# for key in img_class.keys():\n",
      "#   # class_table['id'].append(key)\n",
      "#   for i in range(1000):\n",
      "#     class_table[str(i)].append(img_class[key]['all_prob'][0][i])\n",
      "\n",
      "# class_table = pandas.DataFrame(class_table)\n",
      "# pca = PCA(n_components=10)\n",
      "# pca.fit(class_table)\n",
      "# print(pca.components_ )\n",
      "import pandas\n",
      "import pickle\n",
      "import numpy as np\n",
      "from collections import Counter\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "import nltk\n",
      "nltk.download('punkt')\n",
      "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
      "from nltk import word_tokenize\n",
      "from collections import Counter\n",
      "from nltk import word_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
      "\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "df[\"body\"] = df[\"body\"].fillna('')\n",
      "df[\"title\"] = df[\"title\"].fillna('')\n",
      "img_clusters = pickle.load(open(\"../../pickle/image_color_clusters.pkl\", \"rb\"))\n",
      "img_means = pickle.load(open(\"../../pickle/image_hsv_means.pkl\", \"rb\"))\n",
      "img_text = pickle.load(open(\"../../pickle/texts.pkl\", \"rb\"))\n",
      "\n",
      "img_most_common = Counter(word_tokenize(' '.join([' '.join(x) for x in img_text.values()])))\n",
      "for key, cnts in list(img_most_common.items()):\n",
      "  if not key in emoji.UNICODE_EMOJI and (key in stopwords.words('english') or not re.match(r'\\w+',key)):\n",
      "    del img_most_common[key]\n",
      "img_most_common = img_most_common.most_common(100)\n",
      "img_most_common\n",
      "\n",
      "\n",
      "pic_data = {}\n",
      "pic_data['id'] = []\n",
      "pic_data['text'] = []\n",
      "pic_data['clusters'] = []\n",
      "pic_data['means'] = []\n",
      "pic_data['score'] = []\n",
      "\n",
      "for key in img_means.keys():\n",
      "  pic_data['id'].append(key)\n",
      "  pic_data['text'].append(\" \".join(img_text[key[:-4]]))\n",
      "  pic_data['clusters'].append(img_clusters[key])\n",
      "  pic_data['means'].append(img_means[key])\n",
      "  pic_data['score'].append(df.loc[df['id'] == key[:-4]]['score'].values[0])\n",
      "\n",
      "print(pic_data['score'])\n",
      "pic_data = pandas.DataFrame(pic_data)\n",
      "\n",
      "pic_attributes = {'id': pic_data['id']}\n",
      "for key, cnts in img_most_common: \n",
      "  pic_attributes['text_' + key] =  pic_data['text'].str.lower().str.count(key.lower())\n",
      "\n",
      "pic_attributes['h_sin'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['h_cos'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['s'] = pic_data['means'].apply(lambda s: s[1]/255)\n",
      "pic_attributes['v'] = pic_data['means'].apply(lambda s: s[2]/255)\n",
      "pic_attributes['score'] = pic_data['score']\n",
      "for j in range(5):\n",
      "  for i in range(3):\n",
      "    pic_attributes['color_'+str(j)+'_'+'rgb'[i]] = pic_data['clusters'].apply(lambda s: s['color'][j][i])\n",
      "  pic_attributes['color_'+str(j)+'_%'] = pic_data['clusters'].apply(lambda s: s['percentage'][j])\n",
      "\n",
      "pic_attributes = pandas.DataFrame(pic_attributes)\n",
      "print(pic_attributes)\n",
      "\n",
      "corr_image_matrix = pic_attributes.corr(method='spearman')\n",
      "\n",
      "correlations = []\n",
      "for i,column in enumerate(corr_image_matrix):\n",
      "  x = corr_image_matrix[column][i+1:]\n",
      "  for row,v in x.items():\n",
      "    correlations.append([v,column,row])\n",
      "correlations.sort(key = lambda x: -abs(x[0]))\n",
      "\n",
      "score_corr = corr_image_matrix['score'][corr_image_matrix['score']<1]\n",
      "\n",
      "\n",
      "rf = RandomForestRegressor()\n",
      "rf.fit(pic_attributes[pic_attributes.columns.difference(['id', 'score'])], pic_attributes['score'])\n",
      "print(sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), pic_attributes.head()), \n",
      "             reverse=True))\n",
      "84/11:\n",
      "# from sklearn.decomposition import PCA\n",
      "# img_class = pickle.load(open(\"gdrive/MyDrive/PED/image_classes.pkl\", \"rb\"))\n",
      "\n",
      "# class_table = {}\n",
      "# # class_table['id'] = []\n",
      "\n",
      "# for i in range(1000):\n",
      "#   class_table[str(i)] = []\n",
      "# for key in img_class.keys():\n",
      "#   # class_table['id'].append(key)\n",
      "#   for i in range(1000):\n",
      "#     class_table[str(i)].append(img_class[key]['all_prob'][0][i])\n",
      "\n",
      "# class_table = pandas.DataFrame(class_table)\n",
      "# pca = PCA(n_components=10)\n",
      "# pca.fit(class_table)\n",
      "# print(pca.components_ )\n",
      "import pandas\n",
      "import pickle\n",
      "import numpy as np\n",
      "from collections import Counter\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "import nltk\n",
      "nltk.download('punkt')\n",
      "from nltk import word_tokenize\n",
      "from collections import Counter\n",
      "from nltk import word_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
      "84/12:\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "df[\"body\"] = df[\"body\"].fillna('')\n",
      "df[\"title\"] = df[\"title\"].fillna('')\n",
      "img_clusters = pickle.load(open(\"../../pickle/image_color_clusters.pkl\", \"rb\"))\n",
      "img_means = pickle.load(open(\"../../pickle/image_hsv_means.pkl\", \"rb\"))\n",
      "img_text = pickle.load(open(\"../../pickle/texts.pkl\", \"rb\"))\n",
      "\n",
      "img_most_common = Counter(word_tokenize(' '.join([' '.join(x) for x in img_text.values()])))\n",
      "for key, cnts in list(img_most_common.items()):\n",
      "  if not key in emoji.UNICODE_EMOJI and (key in stopwords.words('english') or not re.match(r'\\w+',key)):\n",
      "    del img_most_common[key]\n",
      "img_most_common = img_most_common.most_common(100)\n",
      "img_most_common\n",
      "84/13:\n",
      "pic_data = {}\n",
      "pic_data['id'] = []\n",
      "pic_data['text'] = []\n",
      "pic_data['clusters'] = []\n",
      "pic_data['means'] = []\n",
      "pic_data['score'] = []\n",
      "\n",
      "for key in img_means.keys():\n",
      "  pic_data['id'].append(key)\n",
      "  pic_data['text'].append(\" \".join(img_text[key[:-4]]))\n",
      "  pic_data['clusters'].append(img_clusters[key])\n",
      "  pic_data['means'].append(img_means[key])\n",
      "  pic_data['score'].append(df.loc[df['id'] == key[:-4]]['score'].values[0])\n",
      "\n",
      "print(pic_data['score'])\n",
      "pic_data = pandas.DataFrame(pic_data)\n",
      "\n",
      "pic_attributes = {'id': pic_data['id']}\n",
      "for key, cnts in img_most_common: \n",
      "  pic_attributes['text_' + key] =  pic_data['text'].str.lower().str.count(key.lower())\n",
      "\n",
      "pic_attributes['h_sin'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['h_cos'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['s'] = pic_data['means'].apply(lambda s: s[1]/255)\n",
      "pic_attributes['v'] = pic_data['means'].apply(lambda s: s[2]/255)\n",
      "pic_attributes['score'] = pic_data['score']\n",
      "for j in range(5):\n",
      "  for i in range(3):\n",
      "    pic_attributes['color_'+str(j)+'_'+'rgb'[i]] = pic_data['clusters'].apply(lambda s: s['color'][j][i])\n",
      "  pic_attributes['color_'+str(j)+'_%'] = pic_data['clusters'].apply(lambda s: s['percentage'][j])\n",
      "\n",
      "pic_attributes = pandas.DataFrame(pic_attributes)\n",
      "print(pic_attributes)\n",
      "84/14:\n",
      "corr_image_matrix = pic_attributes.corr(method='spearman')\n",
      "\n",
      "correlations = []\n",
      "for i,column in enumerate(corr_image_matrix):\n",
      "  x = corr_image_matrix[column][i+1:]\n",
      "  for row,v in x.items():\n",
      "    correlations.append([v,column,row])\n",
      "correlations.sort(key = lambda x: -abs(x[0]))\n",
      "\n",
      "score_corr = corr_image_matrix['score'][corr_image_matrix['score']<1]\n",
      "84/15:\n",
      "rf = RandomForestRegressor()\n",
      "rf.fit(pic_attributes[pic_attributes.columns.difference(['id', 'score'])], pic_attributes['score'])\n",
      "print(sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), pic_attributes.head()), \n",
      "             reverse=True))\n",
      "84/16:\n",
      "pic_data = {}\n",
      "pic_data['id'] = []\n",
      "pic_data['text'] = []\n",
      "pic_data['clusters'] = []\n",
      "pic_data['means'] = []\n",
      "pic_data['score'] = []\n",
      "\n",
      "for key in img_means.keys():\n",
      "  pic_data['id'].append(key)\n",
      "  pic_data['text'].append(\" \".join(img_text[key[:-4]]))\n",
      "  pic_data['clusters'].append(img_clusters[key])\n",
      "  pic_data['means'].append(img_means[key])\n",
      "  pic_data['score'].append(df.loc[df['id'] == key[:-4]]['score'].values[0])\n",
      "\n",
      "pic_data = pandas.DataFrame(pic_data)\n",
      "\n",
      "pic_attributes = {'id': pic_data['id']}\n",
      "for key, cnts in img_most_common: \n",
      "  pic_attributes['text_' + key] =  pic_data['text'].str.lower().str.count(key.lower())\n",
      "\n",
      "pic_attributes['h_sin'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['h_cos'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['s'] = pic_data['means'].apply(lambda s: s[1]/255)\n",
      "pic_attributes['v'] = pic_data['means'].apply(lambda s: s[2]/255)\n",
      "pic_attributes['score'] = pic_data['score']\n",
      "for j in range(5):\n",
      "  for i in range(3):\n",
      "    pic_attributes['color_'+str(j)+'_'+'rgb'[i]] = pic_data['clusters'].apply(lambda s: s['color'][j][i])\n",
      "  pic_attributes['color_'+str(j)+'_%'] = pic_data['clusters'].apply(lambda s: s['percentage'][j])\n",
      "\n",
      "pic_attributes = pandas.DataFrame(pic_attributes)\n",
      "print(pic_attributes)\n",
      "83/39:\n",
      "# from google.colab import drive\n",
      "\n",
      "# drive.mount('/content/gdrive')\n",
      "83/40: # !ls gdrive/MyDrive/PED\n",
      "83/41:\n",
      "import pandas\n",
      "from collections import Counter\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import nltk\n",
      "nltk.download('punkt')\n",
      "%matplotlib inline\n",
      "\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "df[\"body\"] = df[\"body\"].fillna('')\n",
      "df[\"title\"] = df[\"title\"].fillna('')\n",
      "\n",
      "df\n",
      "83/42:\n",
      "seconds_in_day = 60 * 60 * 24.\n",
      "df['sin_time'] = np.sin(2*np.pi*(df.created % seconds_in_day)/seconds_in_day)\n",
      "df['cos_time'] = np.cos(2*np.pi*(df.created % seconds_in_day)/seconds_in_day)\n",
      "83/43: df.describe()\n",
      "83/44: df.sample(5000).plot.scatter('sin_time','cos_time').set_aspect('equal')\n",
      "83/45: df.columns\n",
      "83/46:\n",
      "plt.hist(df['score'], bins=30)\n",
      "plt.ylabel('Count')\n",
      "plt.yscale('log')\n",
      "plt.xlabel('Data');\n",
      "83/47:\n",
      "plt.hist(df['created'], bins=30)\n",
      "plt.ylabel('Count')\n",
      "plt.yscale('log')\n",
      "plt.xlabel('Data');\n",
      "83/48:\n",
      "!pip install emoji\n",
      "from collections import Counter\n",
      "from nltk import word_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
      "\n",
      "nltk.download('stopwords')\n",
      "# tokenizer = RegexpTokenizer(r'\\w+')\n",
      "from nltk.corpus import stopwords\n",
      "from collections import Counter\n",
      "83/49: len(\" \".join(df[\"body\"]))\n",
      "83/50:\n",
      "body_most_common = Counter(word_tokenize(\" \".join(df[\"body\"])))\n",
      "for key, cnts in list(body_most_common.items()):\n",
      "  if not key in emoji.UNICODE_EMOJI and (key in stopwords.words('english') or not re.match(r'\\w+',key)):\n",
      "    del body_most_common[key]\n",
      "body_most_common = body_most_common.most_common(100)\n",
      "body_most_common\n",
      "83/51:\n",
      "title_most_common = Counter(word_tokenize(\" \".join(df[\"title\"])))\n",
      "for key, cnts in list(title_most_common.items()):\n",
      "  if not key in emoji.UNICODE_EMOJI and (key in stopwords.words('english') or not re.match(r'\\w+',key)):\n",
      "    del title_most_common[key]\n",
      "title_most_common = title_most_common.most_common(100)\n",
      "title_most_common\n",
      "83/52:\n",
      "additional_keywords = ['gme', 'hold', 'buy', 'retard', 'wife', 'hedgies', '', 'moon','','', '', '','','', '', '']\n",
      "count = lambda l1,l2: sum([1 for x in l1 if x in l2])\n",
      "text_attributes = {}\n",
      "for key in additional_keywords: \n",
      "  text_attributes['title_' + key] = df[\"title\"].str.lower().str.count(key)\n",
      "  text_attributes['body_' + key] = df[\"body\"].str.lower().str.count(key)\n",
      "83/53:\n",
      "for key, cnts in body_most_common: \n",
      "  text_attributes['body_' + key.lower()] = df[\"body\"].str.lower().str.count(key.lower())\n",
      "\n",
      "\n",
      "for key, cnts in title_most_common: \n",
      "  text_attributes['title_' + key.lower()] = df[\"title\"].str.lower().str.count(key.lower())\n",
      "83/54:\n",
      "\n",
      "text_attributes['body_punctuation'] = df['body'].apply(lambda s: count(s, string.punctuation))\n",
      "text_attributes['title_punctuation'] = df['title'].apply(lambda s: count(s, string.punctuation))\n",
      "\n",
      "text_attributes['body_cap_ratio'] = df['body'].str.count(r'[A-Z]')/df['body'].str.count(r'[a-zA-Z]')\n",
      "text_attributes['title_cap_ratio'] = df['title'].str.count(r'[A-Z]')/df['title'].str.count(r'[a-zA-Z]')\n",
      "\n",
      "text_attributes['time_of_day'] = pandas.to_datetime(df['timestamp']).dt.hour + pandas.to_datetime(df['timestamp']).dt.minute/60\n",
      "\n",
      "text_attributes['title_length'] = df['title'].apply(len)\n",
      "text_attributes['title_non_alphanumeric_ratio'] = df['title'].str.count(r'[^A-Za-z0-9]')/df['title'].apply(len)\n",
      "83/55:\n",
      "def regex_count_in_url_and_body(regex):\n",
      "    return df['body'].apply(lambda s: len(re.findall(regex, s))) + df['url'].apply(lambda s: len(re.findall(regex, s)))\n",
      "83/56:\n",
      "text_attributes['link'] = regex_count_in_url_and_body(r'(https?://[^\\s]+)')\n",
      "text_attributes['reddit_link'] = regex_count_in_url_and_body(r'(https?:\\/\\/w?w?w?\\.?redd\\.?it[^\\s]+)')\n",
      "text_attributes['yt_link'] = regex_count_in_url_and_body(r'(https?:\\/\\/w?w?w?\\.?youtu\\.?be[^\\s]+)')\n",
      "text_attributes['tweet_link'] = regex_count_in_url_and_body(r'(https?:\\/\\/w?w?w?\\.?twitter[^\\s]+)')\n",
      "text_attributes['facebook_link'] = regex_count_in_url_and_body(r'(https?:\\/\\/w?w?w?\\.?facebook[^\\s]+)')\n",
      "\n",
      "text_attributes['gif'] = regex_count_in_url_and_body(r'(https?:\\/\\/w?w?w?\\.?giphy[^\\s]+)') + regex_count_in_url_and_body(r'(https?:\\/\\/w?w?w?\\.?gifyu[^\\s]+)') + regex_count_in_url_and_body(r'(https?:\\/\\/w?w?w?\\.?gfycat[^\\s]+)') + df['url'].apply(lambda s: len(re.findall(r'(\\.gif)', s)))\n",
      "\n",
      "text_attributes = pandas.DataFrame(text_attributes)\n",
      "text_attributes\n",
      "df[text_attributes['gif']>=1]\n",
      "83/57: text_attributes.describe()\n",
      "83/58: df.describe()\n",
      "83/59:\n",
      "for i in ['score', 'sin_time', 'cos_time']:\n",
      "  text_attributes[i] = df[i]  \n",
      "text_attributes['day'] = pandas.to_datetime(df['timestamp']).dt.day\n",
      "text_attributes['month'] = pandas.to_datetime(df['timestamp']).dt.month\n",
      "text_attributes.describe()\n",
      "83/60:\n",
      "print(pandas.to_datetime(df['timestamp'].head(10)))\n",
      "print(pandas.to_datetime(df['timestamp'].head(10)).dt.day)\n",
      "print(pandas.to_datetime(df['timestamp'].head(10)).dt.month)\n",
      "83/61:\n",
      "print(re.findall('http','ahttpjkshdoifjdshttp'))\n",
      "print()\n",
      "print(re.findall(r\"https?://(?:(?:[^\\s()])|(?:\\(\\S*\\)]*\\)))+\", 'https://www.benzinga.com/government/21/01/19337399/something-systemically-wrong-with-gamestop-options-trading-says-massachusetts-securities-regulator(dupa=dupa()))'))\n",
      "83/62:\n",
      "text_attributes['body_'].describe()\n",
      "text_attributes['score'].min() == text_attributes['score'].max()\n",
      "83/63:\n",
      "to_be_removed = []\n",
      "for column in text_attributes:\n",
      "  if text_attributes[column].min() == text_attributes[column].max():\n",
      "    to_be_removed.append(column)\n",
      "len(to_be_removed) # 11\n",
      "83/64:\n",
      "for i in to_be_removed:\n",
      "  del text_attributes[i]\n",
      "83/65:\n",
      "# for column in text_attributes:\n",
      "#     print(column, text_attributes[column].corr(df['score']))\n",
      "83/66: corrMatrix = text_attributes.corr()\n",
      "83/67:\n",
      "x = corrMatrix['score'][corrMatrix['score']<1]\n",
      "print(x.min(), x.keys()[x.argmin()])\n",
      "print(x.max(), x.keys()[x.argmax()])\n",
      "with pandas.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
      "  print(x.sort_values())\n",
      "83/68:\n",
      "treshold = .5\n",
      "for i,column in enumerate(corrMatrix):\n",
      "  x = corrMatrix[column][i+1:]\n",
      "  \n",
      "  if x.min() < -treshold:\n",
      "    print(x.min(), column, x.keys()[x.argmin()])\n",
      "  if x.max() > treshold:\n",
      "    print(x.max(), column, x.keys()[x.argmax()])\n",
      "87/1:\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "df[\"body\"] = df[\"body\"].fillna('')\n",
      "df[\"title\"] = df[\"title\"].fillna('')\n",
      "img_clusters = pickle.load(open(\"../../pickle/image_color_clusters.pkl\", \"rb\"))\n",
      "img_means = pickle.load(open(\"../../pickle/image_hsv_means.pkl\", \"rb\"))\n",
      "img_text = pickle.load(open(\"../../pickle/texts.pkl\", \"rb\"))\n",
      "87/2:\n",
      "# from sklearn.decomposition import PCA\n",
      "# img_class = pickle.load(open(\"gdrive/MyDrive/PED/image_classes.pkl\", \"rb\"))\n",
      "\n",
      "# class_table = {}\n",
      "# # class_table['id'] = []\n",
      "\n",
      "# for i in range(1000):\n",
      "#   class_table[str(i)] = []\n",
      "# for key in img_class.keys():\n",
      "#   # class_table['id'].append(key)\n",
      "#   for i in range(1000):\n",
      "#     class_table[str(i)].append(img_class[key]['all_prob'][0][i])\n",
      "\n",
      "# class_table = pandas.DataFrame(class_table)\n",
      "# pca = PCA(n_components=10)\n",
      "# pca.fit(class_table)\n",
      "# print(pca.components_ )\n",
      "import pandas\n",
      "import pickle\n",
      "import numpy as np\n",
      "from collections import Counter\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "import nltk\n",
      "nltk.download('punkt')\n",
      "from nltk import word_tokenize\n",
      "from collections import Counter\n",
      "from nltk import word_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
      "87/3:\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "df[\"body\"] = df[\"body\"].fillna('')\n",
      "df[\"title\"] = df[\"title\"].fillna('')\n",
      "img_clusters = pickle.load(open(\"../../pickle/image_color_clusters.pkl\", \"rb\"))\n",
      "img_means = pickle.load(open(\"../../pickle/image_hsv_means.pkl\", \"rb\"))\n",
      "img_text = pickle.load(open(\"../../pickle/texts.pkl\", \"rb\"))\n",
      "img_classes = pickle.load(open(\"../../pickle/image_classes.pkl\", \"rb\"))\n",
      "87/4:\n",
      "img_most_common = Counter(word_tokenize(' '.join([' '.join(x) for x in img_text.values()])))\n",
      "for key, cnts in list(img_most_common.items()):\n",
      "  if not key in emoji.UNICODE_EMOJI and (key in stopwords.words('english') or not re.match(r'\\w+',key)):\n",
      "    del img_most_common[key]\n",
      "img_most_common = img_most_common.most_common(100)\n",
      "87/5:\n",
      "pic_data = {}\n",
      "pic_data['id'] = []\n",
      "pic_data['text'] = []\n",
      "pic_data['clusters'] = []\n",
      "pic_data['means'] = []\n",
      "pic_data['score'] = []\n",
      "pic_data['classses'] = []\n",
      "\n",
      "for key in img_means.keys():\n",
      "  pic_data['id'].append(key)\n",
      "  pic_data['text'].append(\" \".join(img_text[key[:-4]]))\n",
      "  pic_data['clusters'].append(img_clusters[key])\n",
      "  pic_data['means'].append(img_means[key])\n",
      "  pic_data['score'].append(df.loc[df['id'] == key[:-4]]['score'].values[0])\n",
      "  pic_data['classses'].append(img_classes[key]['prob'])\n",
      "\n",
      "pic_data = pandas.DataFrame(pic_data)\n",
      "\n",
      "pic_attributes = {'id': pic_data['id']}\n",
      "for key, cnts in img_most_common: \n",
      "  pic_attributes['text_' + key] =  pic_data['text'].str.lower().str.count(key.lower())\n",
      "\n",
      "pic_attributes['h_sin'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['h_cos'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['s'] = pic_data['means'].apply(lambda s: s[1]/255)\n",
      "pic_attributes['v'] = pic_data['means'].apply(lambda s: s[2]/255)\n",
      "pic_attributes['score'] = pic_data['score']\n",
      "for j in range(5):\n",
      "  for i in range(3):\n",
      "    pic_attributes['color_'+str(j)+'_'+'rgb'[i]] = pic_data['clusters'].apply(lambda s: s['color'][j][i])\n",
      "  pic_attributes['color_'+str(j)+'_%'] = pic_data['clusters'].apply(lambda s: s['percentage'][j])\n",
      "\n",
      "pic_attributes = pandas.DataFrame(pic_attributes)\n",
      "print(pic_attributes)\n",
      "87/6:\n",
      "pic_data = {}\n",
      "pic_data['id'] = []\n",
      "pic_data['text'] = []\n",
      "pic_data['clusters'] = []\n",
      "pic_data['means'] = []\n",
      "pic_data['score'] = []\n",
      "pic_data['classses'] = []\n",
      "\n",
      "for key in img_means.keys():\n",
      "  print(img_classes[key])\n",
      "  pic_data['id'].append(key)\n",
      "  pic_data['text'].append(\" \".join(img_text[key[:-4]]))\n",
      "  pic_data['clusters'].append(img_clusters[key])\n",
      "  pic_data['means'].append(img_means[key])\n",
      "  pic_data['score'].append(df.loc[df['id'] == key[:-4]]['score'].values[0])\n",
      "  pic_data['classses'].append(img_classes[key]['prob'])\n",
      "\n",
      "pic_data = pandas.DataFrame(pic_data)\n",
      "\n",
      "pic_attributes = {'id': pic_data['id']}\n",
      "for key, cnts in img_most_common: \n",
      "  pic_attributes['text_' + key] =  pic_data['text'].str.lower().str.count(key.lower())\n",
      "\n",
      "pic_attributes['h_sin'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['h_cos'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['s'] = pic_data['means'].apply(lambda s: s[1]/255)\n",
      "pic_attributes['v'] = pic_data['means'].apply(lambda s: s[2]/255)\n",
      "pic_attributes['score'] = pic_data['score']\n",
      "for j in range(5):\n",
      "  for i in range(3):\n",
      "    pic_attributes['color_'+str(j)+'_'+'rgb'[i]] = pic_data['clusters'].apply(lambda s: s['color'][j][i])\n",
      "  pic_attributes['color_'+str(j)+'_%'] = pic_data['clusters'].apply(lambda s: s['percentage'][j])\n",
      "\n",
      "pic_attributes = pandas.DataFrame(pic_attributes)\n",
      "print(pic_attributes)\n",
      "87/7:\n",
      "pic_data = {}\n",
      "pic_data['id'] = []\n",
      "pic_data['text'] = []\n",
      "pic_data['clusters'] = []\n",
      "pic_data['means'] = []\n",
      "pic_data['score'] = []\n",
      "pic_data['classses'] = []\n",
      "\n",
      "for key in img_means.keys():\n",
      "  pic_data['id'].append(key)\n",
      "  pic_data['text'].append(\" \".join(img_text[key[:-4]]))\n",
      "  pic_data['clusters'].append(img_clusters[key])\n",
      "  pic_data['means'].append(img_means[key])\n",
      "  pic_data['score'].append(df.loc[df['id'] == key[:-4]]['score'].values[0])\n",
      "  pic_data['classses'].append(img_classes[key]['all_prob'])\n",
      "\n",
      "pic_data = pandas.DataFrame(pic_data)\n",
      "\n",
      "pic_attributes = {'id': pic_data['id']}\n",
      "for key, cnts in img_most_common: \n",
      "  pic_attributes['text_' + key] =  pic_data['text'].str.lower().str.count(key.lower())\n",
      "\n",
      "pic_attributes['h_sin'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['h_cos'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['s'] = pic_data['means'].apply(lambda s: s[1]/255)\n",
      "pic_attributes['v'] = pic_data['means'].apply(lambda s: s[2]/255)\n",
      "pic_attributes['score'] = pic_data['score']\n",
      "for j in range(5):\n",
      "  for i in range(3):\n",
      "    pic_attributes['color_'+str(j)+'_'+'rgb'[i]] = pic_data['clusters'].apply(lambda s: s['color'][j][i])\n",
      "  pic_attributes['color_'+str(j)+'_%'] = pic_data['clusters'].apply(lambda s: s['percentage'][j])\n",
      "\n",
      "for i in range(1000):\n",
      "    pic_attributes['class_'+str(i)] = pic_data['classses'].apply(lambda s: s[i])\n",
      "\n",
      "pic_attributes = pandas.DataFrame(pic_attributes)\n",
      "print(pic_attributes)\n",
      "87/8:\n",
      "pic_data = {}\n",
      "pic_data['id'] = []\n",
      "pic_data['text'] = []\n",
      "pic_data['clusters'] = []\n",
      "pic_data['means'] = []\n",
      "pic_data['score'] = []\n",
      "pic_data['classses'] = []\n",
      "\n",
      "for key in img_means.keys():\n",
      "  pic_data['id'].append(key)\n",
      "  pic_data['text'].append(\" \".join(img_text[key[:-4]]))\n",
      "  pic_data['clusters'].append(img_clusters[key])\n",
      "  pic_data['means'].append(img_means[key])\n",
      "  pic_data['score'].append(df.loc[df['id'] == key[:-4]]['score'].values[0])\n",
      "  pic_data['classses'].append(img_classes[key]['all_prob'])\n",
      "\n",
      "pic_data = pandas.DataFrame(pic_data)\n",
      "\n",
      "pic_attributes = {'id': pic_data['id']}\n",
      "for key, cnts in img_most_common: \n",
      "  pic_attributes['text_' + key] =  pic_data['text'].str.lower().str.count(key.lower())\n",
      "\n",
      "pic_attributes['h_sin'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['h_cos'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['s'] = pic_data['means'].apply(lambda s: s[1]/255)\n",
      "pic_attributes['v'] = pic_data['means'].apply(lambda s: s[2]/255)\n",
      "pic_attributes['score'] = pic_data['score']\n",
      "for j in range(5):\n",
      "  for i in range(3):\n",
      "    pic_attributes['color_'+str(j)+'_'+'rgb'[i]] = pic_data['clusters'].apply(lambda s: s['color'][j][i])\n",
      "  pic_attributes['color_'+str(j)+'_%'] = pic_data['clusters'].apply(lambda s: s['percentage'][j])\n",
      "\n",
      "for i in range(1000):\n",
      "    pic_data['classses']\n",
      "    pic_attributes['class_'+str(i)] = pic_data['classses'].apply(lambda s: s[i])\n",
      "\n",
      "pic_attributes = pandas.DataFrame(pic_attributes)\n",
      "print(pic_attributes)\n",
      "87/9:\n",
      "pic_data = {}\n",
      "pic_data['id'] = []\n",
      "pic_data['text'] = []\n",
      "pic_data['clusters'] = []\n",
      "pic_data['means'] = []\n",
      "pic_data['score'] = []\n",
      "pic_data['classses'] = []\n",
      "\n",
      "for key in img_means.keys():\n",
      "  pic_data['id'].append(key)\n",
      "  pic_data['text'].append(\" \".join(img_text[key[:-4]]))\n",
      "  pic_data['clusters'].append(img_clusters[key])\n",
      "  pic_data['means'].append(img_means[key])\n",
      "  pic_data['score'].append(df.loc[df['id'] == key[:-4]]['score'].values[0])\n",
      "  pic_data['classses'].append(img_classes[key]['all_prob'])\n",
      "\n",
      "pic_data = pandas.DataFrame(pic_data)\n",
      "\n",
      "pic_attributes = {'id': pic_data['id']}\n",
      "for key, cnts in img_most_common: \n",
      "  pic_attributes['text_' + key] =  pic_data['text'].str.lower().str.count(key.lower())\n",
      "\n",
      "pic_attributes['h_sin'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['h_cos'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['s'] = pic_data['means'].apply(lambda s: s[1]/255)\n",
      "pic_attributes['v'] = pic_data['means'].apply(lambda s: s[2]/255)\n",
      "pic_attributes['score'] = pic_data['score']\n",
      "for j in range(5):\n",
      "  for i in range(3):\n",
      "    pic_attributes['color_'+str(j)+'_'+'rgb'[i]] = pic_data['clusters'].apply(lambda s: s['color'][j][i])\n",
      "  pic_attributes['color_'+str(j)+'_%'] = pic_data['clusters'].apply(lambda s: s['percentage'][j])\n",
      "\n",
      "for i in range(1000):\n",
      "    print(pic_data['classses'])\n",
      "    pic_attributes['class_'+str(i)] = pic_data['classses'].apply(lambda s: s[i])\n",
      "\n",
      "pic_attributes = pandas.DataFrame(pic_attributes)\n",
      "print(pic_attributes)\n",
      "87/10:\n",
      "pic_data = {}\n",
      "pic_data['id'] = []\n",
      "pic_data['text'] = []\n",
      "pic_data['clusters'] = []\n",
      "pic_data['means'] = []\n",
      "pic_data['score'] = []\n",
      "pic_data['classses'] = []\n",
      "\n",
      "for key in img_means.keys():\n",
      "  pic_data['id'].append(key)\n",
      "  pic_data['text'].append(\" \".join(img_text[key[:-4]]))\n",
      "  pic_data['clusters'].append(img_clusters[key])\n",
      "  pic_data['means'].append(img_means[key])\n",
      "  pic_data['score'].append(df.loc[df['id'] == key[:-4]]['score'].values[0])\n",
      "  pic_data['classses'].append(img_classes[key]['all_prob'])\n",
      "\n",
      "pic_data = pandas.DataFrame(pic_data)\n",
      "\n",
      "pic_attributes = {'id': pic_data['id']}\n",
      "for key, cnts in img_most_common: \n",
      "  pic_attributes['text_' + key] =  pic_data['text'].str.lower().str.count(key.lower())\n",
      "\n",
      "pic_attributes['h_sin'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['h_cos'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['s'] = pic_data['means'].apply(lambda s: s[1]/255)\n",
      "pic_attributes['v'] = pic_data['means'].apply(lambda s: s[2]/255)\n",
      "pic_attributes['score'] = pic_data['score']\n",
      "for j in range(5):\n",
      "  for i in range(3):\n",
      "    pic_attributes['color_'+str(j)+'_'+'rgb'[i]] = pic_data['clusters'].apply(lambda s: s['color'][j][i])\n",
      "  pic_attributes['color_'+str(j)+'_%'] = pic_data['clusters'].apply(lambda s: s['percentage'][j])\n",
      "\n",
      "for i in range(1000):\n",
      "    pic_attributes['class_'+str(i)] = pic_data['classses'].apply(lambda s: s[0][i])\n",
      "\n",
      "pic_attributes = pandas.DataFrame(pic_attributes)\n",
      "print(pic_attributes)\n",
      "87/11:\n",
      "corr_image_matrix = pic_attributes.corr(method='spearman')\n",
      "\n",
      "correlations = []\n",
      "for i,column in enumerate(corr_image_matrix):\n",
      "  x = corr_image_matrix[column][i+1:]\n",
      "  for row,v in x.items():\n",
      "    correlations.append([v,column,row])\n",
      "correlations.sort(key = lambda x: -abs(x[0]))\n",
      "\n",
      "score_corr = corr_image_matrix['score'][corr_image_matrix['score']<1]\n",
      "87/12: corr_image_matrix = pic_attributes.corr(method='spearman')\n",
      "87/13: corr_image_matrix = pic_attributes.corr(method='spearman')\n",
      "87/14:\n",
      "rf = RandomForestRegressor()\n",
      "rf.fit(pic_attributes[pic_attributes.columns.difference(['id', 'score'])], pic_attributes['score'])\n",
      "print(sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), pic_attributes.head()), \n",
      "             reverse=True))\n",
      "87/15:\n",
      "pic_data = {}\n",
      "pic_data['id'] = []\n",
      "pic_data['text'] = []\n",
      "pic_data['clusters'] = []\n",
      "pic_data['means'] = []\n",
      "pic_data['score'] = []\n",
      "pic_data['classses'] = []\n",
      "\n",
      "for key in img_means.keys():\n",
      "  pic_data['id'].append(key)\n",
      "  pic_data['text'].append(\" \".join(img_text[key[:-4]]))\n",
      "  pic_data['clusters'].append(img_clusters[key])\n",
      "  pic_data['means'].append(img_means[key])\n",
      "  pic_data['score'].append(df.loc[df['id'] == key[:-4]]['score'].values[0])\n",
      "  pic_data['classses'].append(img_classes[key]['all_prob'][0])\n",
      "\n",
      "pic_data = pandas.DataFrame(pic_data)\n",
      "87/16: print(pic_data['classses'])\n",
      "87/17:\n",
      "temp = {}\n",
      "for i in range(1000):\n",
      "    temp['class_'+str(i)] = pic_data['classses'].apply(lambda s: s[i])\n",
      "    \n",
      "pic_data['classes'] = temp\n",
      "print(pic_data['classses'])\n",
      "87/18:\n",
      "temp = {}\n",
      "for i in range(1000):\n",
      "    temp['class_'+str(i)] = pic_data['classses'].apply(lambda s: s[i])\n",
      "    \n",
      "pic_data['classes'] = pandas.DataFrame(temp)\n",
      "print(pic_data['classses'])\n",
      "87/19:\n",
      "pic_data = {}\n",
      "pic_data['id'] = []\n",
      "pic_data['text'] = []\n",
      "pic_data['clusters'] = []\n",
      "pic_data['means'] = []\n",
      "pic_data['score'] = []\n",
      "pic_data['classses'] = []\n",
      "\n",
      "for key in img_means.keys():\n",
      "  pic_data['id'].append(key)\n",
      "  pic_data['text'].append(\" \".join(img_text[key[:-4]]))\n",
      "  pic_data['clusters'].append(img_clusters[key])\n",
      "  pic_data['means'].append(img_means[key])\n",
      "  pic_data['score'].append(df.loc[df['id'] == key[:-4]]['score'].values[0])\n",
      "  pic_data['classses'].append(img_classes[key]['all_prob'][0])\n",
      "\n",
      "pic_data = pandas.DataFrame(pic_data)\n",
      "87/20:\n",
      "temp = {}\n",
      "for i in range(1000):\n",
      "    temp['class_'+str(i)] = pic_data['classses'].apply(lambda s: s[i])\n",
      "    \n",
      "pic_data['classes'] = pandas.DataFrame(temp)\n",
      "print(pic_data['classses'])\n",
      "87/21:\n",
      "temp = {}\n",
      "for i in range(1000):\n",
      "    temp['class_'+str(i)] = pic_data['classses'].apply(lambda s: s[i])\n",
      "    \n",
      "pic_data['test'] = pandas.DataFrame(temp)\n",
      "print(pic_data['test'])\n",
      "87/22:\n",
      "pic_data = {}\n",
      "pic_data['id'] = []\n",
      "pic_data['text'] = []\n",
      "pic_data['clusters'] = []\n",
      "pic_data['means'] = []\n",
      "pic_data['score'] = []\n",
      "pic_data['classses'] = []\n",
      "\n",
      "for key in img_means.keys():\n",
      "  pic_data['id'].append(key)\n",
      "  pic_data['text'].append(\" \".join(img_text[key[:-4]]))\n",
      "  pic_data['clusters'].append(img_clusters[key])\n",
      "  pic_data['means'].append(img_means[key])\n",
      "  pic_data['score'].append(df.loc[df['id'] == key[:-4]]['score'].values[0])\n",
      "  pic_data['classses'].append(img_classes[key]['all_prob'][0])\n",
      "\n",
      "pic_data = pandas.DataFrame(pic_data)\n",
      "87/23:\n",
      "temp = {}\n",
      "for i in range(1000):\n",
      "    temp['class_'+str(i)] = pic_data['classses'].apply(lambda s: s[i])\n",
      "    \n",
      "pic_data['test'] = pandas.DataFrame(temp)\n",
      "print(pic_data['test'])\n",
      "87/24:\n",
      "temp = {}\n",
      "for i in range(1000):\n",
      "    temp['class_'+str(i)] = pic_data['classses'].apply(lambda s: s[i])\n",
      "    \n",
      "print(pandas.DataFrame(temp))\n",
      "87/25:\n",
      "temp = {}\n",
      "for i in range(1000):\n",
      "    temp['class_'+str(i)] = pic_data['classses'].apply(lambda s: s[i])\n",
      "    \n",
      "temp = pandas.DataFrame(temp)\n",
      "for column in temp:\n",
      "    if np.mean(temp[column])<0.1:\n",
      "        del temp[column]\n",
      "87/26:\n",
      "temp = {}\n",
      "for i in range(1000):\n",
      "    temp['class_'+str(i)] = pic_data['classses'].apply(lambda s: s[i])\n",
      "    \n",
      "temp = pandas.DataFrame(temp)\n",
      "for column in temp:\n",
      "    if np.mean(temp[column])<0.1:\n",
      "        del temp[column]\n",
      "\n",
      "print(temp)\n",
      "87/27:\n",
      "temp = {}\n",
      "for i in range(1000):\n",
      "    temp['class_'+str(i)] = pic_data['classses'].apply(lambda s: s[i])\n",
      "    \n",
      "temp = pandas.DataFrame(temp)\n",
      "for column in temp:\n",
      "    if np.mean(temp[column])<0.001:\n",
      "        del temp[column]\n",
      "\n",
      "print(len(temp.head()))\n",
      "87/28:\n",
      "temp = {}\n",
      "for i in range(1000):\n",
      "    temp['class_'+str(i)] = pic_data['classses'].apply(lambda s: s[i])\n",
      "    \n",
      "temp = pandas.DataFrame(temp)\n",
      "for column in temp:\n",
      "    if np.mean(temp[column])<0.0001:\n",
      "        del temp[column]\n",
      "\n",
      "print(len(temp.head()))\n",
      "87/29:\n",
      "temp = {}\n",
      "for i in range(1000):\n",
      "    temp['class_'+str(i)] = pic_data['classses'].apply(lambda s: s[i])\n",
      "    \n",
      "temp = pandas.DataFrame(temp)\n",
      "for column in temp:\n",
      "    if np.mean(temp[column])<0.00001:\n",
      "        del temp[column]\n",
      "\n",
      "print(len(temp.head()))\n",
      "87/30:\n",
      "temp = {}\n",
      "for i in range(1000):\n",
      "    temp['class_'+str(i)] = pic_data['classses'].apply(lambda s: s[i])\n",
      "    \n",
      "temp = pandas.DataFrame(temp)\n",
      "for column in temp:\n",
      "    if np.mean(temp[column])<0.000001:\n",
      "        del temp[column]\n",
      "\n",
      "print(len(temp.head()))\n",
      "87/31:\n",
      "temp = {}\n",
      "for i in range(1000):\n",
      "    temp['class_'+str(i)] = pic_data['classses'].apply(lambda s: s[i])\n",
      "    \n",
      "temp = pandas.DataFrame(temp)\n",
      "for column in temp:\n",
      "    if np.max(temp[column])<0.001:\n",
      "        del temp[column]\n",
      "\n",
      "print(len(temp.head()))\n",
      "87/32:\n",
      "temp = {}\n",
      "for i in range(1000):\n",
      "    temp['class_'+str(i)] = pic_data['classses'].apply(lambda s: s[i])\n",
      "    \n",
      "temp = pandas.DataFrame(temp)\n",
      "print(len(temp.head()))\n",
      "for column in temp:\n",
      "    if np.max(temp[column])<0.001:\n",
      "        del temp[column]\n",
      "\n",
      "print(len(temp.head()))\n",
      "87/33:\n",
      "pic_data = {}\n",
      "pic_data['id'] = []\n",
      "pic_data['text'] = []\n",
      "pic_data['clusters'] = []\n",
      "pic_data['means'] = []\n",
      "pic_data['score'] = []\n",
      "pic_data['classses'] = []\n",
      "\n",
      "for key in img_means.keys():\n",
      "  pic_data['id'].append(key)\n",
      "  pic_data['text'].append(\" \".join(img_text[key[:-4]]))\n",
      "  pic_data['clusters'].append(img_clusters[key])\n",
      "  pic_data['means'].append(img_means[key])\n",
      "  pic_data['score'].append(df.loc[df['id'] == key[:-4]]['score'].values[0])\n",
      "  pic_data['classses'].append(img_classes[key]['all_prob'][0])\n",
      "\n",
      "pic_data = pandas.DataFrame(pic_data)\n",
      "87/34:\n",
      "temp = {}\n",
      "for i in range(1000):\n",
      "    temp['class_'+str(i)] = pic_data['classses'].apply(lambda s: s[i])\n",
      "    \n",
      "temp = pandas.DataFrame(temp)\n",
      "print(len(temp.head()))\n",
      "for column in temp:\n",
      "    if np.max(temp[column])<0.001:\n",
      "        del temp[column]\n",
      "\n",
      "print(len(temp.head()))\n",
      "87/35:\n",
      "temp = {}\n",
      "for i in range(1000):\n",
      "    temp['class_'+str(i)] = pic_data['classses'].apply(lambda s: s[i])\n",
      "    \n",
      "temp = pandas.DataFrame(temp)\n",
      "print(len(temp.columns))\n",
      "for column in temp:\n",
      "    if np.max(temp[column])<0.001:\n",
      "        del temp[column]\n",
      "\n",
      "print(len(temp.columns))\n",
      "87/36:\n",
      "temp = {}\n",
      "for i in range(1000):\n",
      "    temp['class_'+str(i)] = pic_data['classses'].apply(lambda s: s[i])\n",
      "    \n",
      "temp = pandas.DataFrame(temp)\n",
      "print(len(temp.columns))\n",
      "for column in temp:\n",
      "    if np.max(temp[column])<0.01:\n",
      "        del temp[column]\n",
      "\n",
      "print(len(temp.columns))\n",
      "87/37:\n",
      "temp = {}\n",
      "for i in range(1000):\n",
      "    temp['class_'+str(i)] = pic_data['classses'].apply(lambda s: s[i])\n",
      "    \n",
      "temp = pandas.DataFrame(temp)\n",
      "print(len(temp.columns))\n",
      "for column in temp:\n",
      "    if np.max(temp[column])<0.1:\n",
      "        del temp[column]\n",
      "\n",
      "print(len(temp.columns))\n",
      "87/38:\n",
      "temp = {}\n",
      "for i in range(1000):\n",
      "    temp['class_'+str(i)] = pic_data['classses'].apply(lambda s: s[i])\n",
      "    \n",
      "temp = pandas.DataFrame(temp)\n",
      "print(len(temp.columns))\n",
      "for column in temp:\n",
      "    if np.max(temp[column])<0.2:\n",
      "        del temp[column]\n",
      "\n",
      "print(len(temp.columns))\n",
      "87/39:\n",
      "pic_attributes = {'id': pic_data['id']}\n",
      "for key, cnts in img_most_common: \n",
      "  pic_attributes['text_' + key] =  pic_data['text'].str.lower().str.count(key.lower())\n",
      "\n",
      "pic_attributes['h_sin'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['h_cos'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['s'] = pic_data['means'].apply(lambda s: s[1]/255)\n",
      "pic_attributes['v'] = pic_data['means'].apply(lambda s: s[2]/255)\n",
      "pic_attributes['score'] = pic_data['score']\n",
      "for j in range(5):\n",
      "  for i in range(3):\n",
      "    pic_attributes['color_'+str(j)+'_'+'rgb'[i]] = pic_data['clusters'].apply(lambda s: s['color'][j][i])\n",
      "  pic_attributes['color_'+str(j)+'_%'] = pic_data['clusters'].apply(lambda s: s['percentage'][j])\n",
      "\n",
      "for i in temp.columns:\n",
      "    pic_attributes[i] = temp[i]\n",
      "\n",
      "pic_attributes = pandas.DataFrame(pic_attributes)\n",
      "print(pic_attributes)\n",
      "87/40:\n",
      "rf = RandomForestRegressor()\n",
      "rf.fit(pic_attributes[pic_attributes.columns.difference(['id', 'score'])], pic_attributes['score'])\n",
      "print(sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), pic_attributes.head()), \n",
      "             reverse=True))\n",
      "87/41: corr_image_matrix = pic_attributes.corr(method='spearman')\n",
      "87/42:\n",
      "treshold = .5\n",
      "correlations = []\n",
      "for i,column in enumerate(corr_image_matrix):\n",
      "  x = corrMatrix[column][i+1:]\n",
      "  for row,v in x.items():\n",
      "    correlations.append([v,column,row])\n",
      "correlations.sort(key = lambda x: -abs(x[0]))\n",
      "87/43:\n",
      "treshold = .5\n",
      "correlations = []\n",
      "for i,column in enumerate(corr_image_matrix):\n",
      "  x = corr_image_matrix[column][i+1:]\n",
      "  for row,v in x.items():\n",
      "    correlations.append([v,column,row])\n",
      "correlations.sort(key = lambda x: -abs(x[0]))\n",
      "87/44:\n",
      "treshold = .5\n",
      "correlations = []\n",
      "for i,column in enumerate(corr_image_matrix):\n",
      "  x = corr_image_matrix[column][i+1:]\n",
      "  for row,v in x.items():\n",
      "    correlations.append([v,column,row])\n",
      "correlations.sort(key = lambda x: -abs(x[0]))\n",
      "print(correlations)\n",
      "87/45:\n",
      "score_corr = corr_image_matrix['score'][corr_image_matrix['score']<1]\n",
      "for corr in correlations:\n",
      "  if len(pic_attributes.columns) <= 100: break\n",
      "  c,a1,a2 = corr\n",
      "  if 'score' in [a1,a2]: continue\n",
      "  if a1 in text_attributes and a2 in pic_attributes:\n",
      "    to_be_removed = a1 if score_corr[a1] < score_corr[a2] else a2\n",
      "    del pic_attributes[to_be_removed]\n",
      "87/46:\n",
      "score_corr = corr_image_matrix['score'][corr_image_matrix['score']<1]\n",
      "for corr in correlations:\n",
      "  if len(pic_attributes.columns) <= 100: break\n",
      "  c,a1,a2 = corr\n",
      "  if 'score' in [a1,a2]: continue\n",
      "  if a1 in pic_attributes and a2 in pic_attributes:\n",
      "    to_be_removed = a1 if score_corr[a1] < score_corr[a2] else a2\n",
      "    del pic_attributes[to_be_removed]\n",
      "87/47:\n",
      "rf = RandomForestRegressor()\n",
      "rf.fit(pic_attributes[pic_attributes.columns.difference(['id', 'score'])], pic_attributes['score'])\n",
      "print(sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), pic_attributes.head()), \n",
      "             reverse=True))\n",
      "87/48:\n",
      "temp = {}\n",
      "for i in range(1000):\n",
      "    temp['class_'+str(i)] = pic_data['classses'].apply(lambda s: s[i])\n",
      "    \n",
      "temp = pandas.DataFrame(temp)\n",
      "print(len(temp.columns))\n",
      "for column in temp:\n",
      "    if np.max(temp[column])<0.1:\n",
      "        del temp[column]\n",
      "\n",
      "print(len(temp.columns))\n",
      "87/49:\n",
      "pic_attributes = {'id': pic_data['id']}\n",
      "for key, cnts in img_most_common: \n",
      "  pic_attributes['text_' + key] =  pic_data['text'].str.lower().str.count(key.lower())\n",
      "\n",
      "pic_attributes['h_sin'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['h_cos'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['s'] = pic_data['means'].apply(lambda s: s[1]/255)\n",
      "pic_attributes['v'] = pic_data['means'].apply(lambda s: s[2]/255)\n",
      "pic_attributes['score'] = pic_data['score']\n",
      "for j in range(5):\n",
      "  for i in range(3):\n",
      "    pic_attributes['color_'+str(j)+'_'+'rgb'[i]] = pic_data['clusters'].apply(lambda s: s['color'][j][i])\n",
      "  pic_attributes['color_'+str(j)+'_%'] = pic_data['clusters'].apply(lambda s: s['percentage'][j])\n",
      "\n",
      "for i in temp.columns:\n",
      "    pic_attributes[i] = temp[i]\n",
      "\n",
      "pic_attributes = pandas.DataFrame(pic_attributes)\n",
      "print(pic_attributes)\n",
      "87/50: corr_image_matrix = pic_attributes.corr(method='spearman')\n",
      "87/51:\n",
      "correlations = []\n",
      "for i,column in enumerate(corr_image_matrix):\n",
      "  x = corr_image_matrix[column][i+1:]\n",
      "  for row,v in x.items():\n",
      "    correlations.append([v,column,row])\n",
      "correlations.sort(key = lambda x: -abs(x[0]))\n",
      "87/52:\n",
      "score_corr = corr_image_matrix['score'][corr_image_matrix['score']<1]\n",
      "for corr in correlations:\n",
      "  if len(pic_attributes.columns) <= 200: break\n",
      "  c,a1,a2 = corr\n",
      "  if 'score' in [a1,a2]: continue\n",
      "  if a1 in pic_attributes and a2 in pic_attributes:\n",
      "    to_be_removed = a1 if score_corr[a1] < score_corr[a2] else a2\n",
      "    del pic_attributes[to_be_removed]\n",
      "87/53:\n",
      "rf = RandomForestRegressor()\n",
      "rf.fit(pic_attributes[pic_attributes.columns.difference(['id', 'score'])], pic_attributes['score'])\n",
      "print(sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), pic_attributes.head()), \n",
      "             reverse=True))\n",
      "87/54:\n",
      "img_text = img_text.map(lambda x: [i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0])\n",
      "\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(df[\"body_lemmatized\"].sum())\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "\n",
      "for key, cnts in list(img_most_common.items()):\n",
      "  if not key in emoji.UNICODE_EMOJI and (key in stopwords.words('english') or not re.match(r'\\w+',key)):\n",
      "    del img_most_common[key]\n",
      "img_most_common = img_most_common.most_common(100)\n",
      "87/55:\n",
      "img_text = img_text.values().map(lambda x: [i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0])\n",
      "\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(df[\"body_lemmatized\"].sum())\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "\n",
      "for key, cnts in list(img_most_common.items()):\n",
      "  if not key in emoji.UNICODE_EMOJI and (key in stopwords.words('english') or not re.match(r'\\w+',key)):\n",
      "    del img_most_common[key]\n",
      "img_most_common = img_most_common.most_common(100)\n",
      "87/56:\n",
      "img_text = [[i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(df[\"body_lemmatized\"].sum())\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "\n",
      "for key, cnts in list(img_most_common.items()):\n",
      "  if not key in emoji.UNICODE_EMOJI and (key in stopwords.words('english') or not re.match(r'\\w+',key)):\n",
      "    del img_most_common[key]\n",
      "img_most_common = img_most_common.most_common(100)\n",
      "87/57:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "img_text = [[i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(df[\"body_lemmatized\"].sum())\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "\n",
      "for key, cnts in list(img_most_common.items()):\n",
      "  if not key in emoji.UNICODE_EMOJI and (key in stopwords.words('english') or not re.match(r'\\w+',key)):\n",
      "    del img_most_common[key]\n",
      "img_most_common = img_most_common.most_common(100)\n",
      "87/58:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "img_text = [[i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(df[\"body_lemmatized\"].sum())\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "\n",
      "for key, cnts in list(img_most_common.items()):\n",
      "  if not key in emoji.UNICODE_EMOJI and (key in stopwords.words('english') or not re.match(r'\\w+',key)):\n",
      "    del img_most_common[key]\n",
      "img_most_common = img_most_common.most_common(100)\n",
      "87/59:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "!python3 -m spacy download en_core_web_sm\n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "img_text = [[i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(df[\"body_lemmatized\"].sum())\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "\n",
      "for key, cnts in list(img_most_common.items()):\n",
      "  if not key in emoji.UNICODE_EMOJI and (key in stopwords.words('english') or not re.match(r'\\w+',key)):\n",
      "    del img_most_common[key]\n",
      "img_most_common = img_most_common.most_common(100)\n",
      "87/60:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "!python -m spacy download en_core_web_sm\n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "img_text = [[i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(df[\"body_lemmatized\"].sum())\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "\n",
      "for key, cnts in list(img_most_common.items()):\n",
      "  if not key in emoji.UNICODE_EMOJI and (key in stopwords.words('english') or not re.match(r'\\w+',key)):\n",
      "    del img_most_common[key]\n",
      "img_most_common = img_most_common.most_common(100)\n",
      "87/61:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "img_text = [[i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(df[\"body_lemmatized\"].sum())\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "\n",
      "for key, cnts in list(img_most_common.items()):\n",
      "  if not key in emoji.UNICODE_EMOJI and (key in stopwords.words('english') or not re.match(r'\\w+',key)):\n",
      "    del img_most_common[key]\n",
      "img_most_common = img_most_common.most_common(100)\n",
      "88/1:\n",
      "print('PyDev console: using IPython 7.19.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['C:\\\\Users\\\\luker\\\\Documents\\\\Github\\\\PED', 'C:/Users/luker/Documents/Github/PED'])\n",
      "88/2: python -m spacy download en\n",
      "88/3: spacy download en_core_web_sm\n",
      "88/4: spacy\n",
      "88/5: import os\n",
      "88/6: import sys\n",
      "88/7: os.path.dirname(sys.executable)\n",
      "87/62:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "!python -m spacy download en_core_web_sm\n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "img_text = [[i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(df[\"body_lemmatized\"].sum())\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "\n",
      "for key, cnts in list(img_most_common.items()):\n",
      "  if not key in emoji.UNICODE_EMOJI and (key in stopwords.words('english') or not re.match(r'\\w+',key)):\n",
      "    del img_most_common[key]\n",
      "img_most_common = img_most_common.most_common(100)\n",
      "89/1:\n",
      "# from sklearn.decomposition import PCA\n",
      "# img_class = pickle.load(open(\"gdrive/MyDrive/PED/image_classes.pkl\", \"rb\"))\n",
      "\n",
      "# class_table = {}\n",
      "# # class_table['id'] = []\n",
      "\n",
      "# for i in range(1000):\n",
      "#   class_table[str(i)] = []\n",
      "# for key in img_class.keys():\n",
      "#   # class_table['id'].append(key)\n",
      "#   for i in range(1000):\n",
      "#     class_table[str(i)].append(img_class[key]['all_prob'][0][i])\n",
      "\n",
      "# class_table = pandas.DataFrame(class_table)\n",
      "# pca = PCA(n_components=10)\n",
      "# pca.fit(class_table)\n",
      "# print(pca.components_ )\n",
      "import pandas\n",
      "import pickle\n",
      "import numpy as np\n",
      "from collections import Counter\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "import nltk\n",
      "nltk.download('punkt')\n",
      "from nltk import word_tokenize\n",
      "from collections import Counter\n",
      "from nltk import word_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
      "89/2:\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "df[\"body\"] = df[\"body\"].fillna('')\n",
      "df[\"title\"] = df[\"title\"].fillna('')\n",
      "img_clusters = pickle.load(open(\"../../pickle/image_color_clusters.pkl\", \"rb\"))\n",
      "img_means = pickle.load(open(\"../../pickle/image_hsv_means.pkl\", \"rb\"))\n",
      "img_text = pickle.load(open(\"../../pickle/texts.pkl\", \"rb\"))\n",
      "img_classes = pickle.load(open(\"../../pickle/image_classes.pkl\", \"rb\"))\n",
      "89/3:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "!python -m spacy download en_core_web_sm\n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "img_text = [[i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(df[\"body_lemmatized\"].sum())\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "\n",
      "for key, cnts in list(img_most_common.items()):\n",
      "  if not key in emoji.UNICODE_EMOJI and (key in stopwords.words('english') or not re.match(r'\\w+',key)):\n",
      "    del img_most_common[key]\n",
      "img_most_common = img_most_common.most_common(100)\n",
      "89/4:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "img_text = [[i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(df[\"body_lemmatized\"].sum())\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "\n",
      "for key, cnts in list(img_most_common.items()):\n",
      "  if not key in emoji.UNICODE_EMOJI and (key in stopwords.words('english') or not re.match(r'\\w+',key)):\n",
      "    del img_most_common[key]\n",
      "img_most_common = img_most_common.most_common(100)\n",
      "90/1:\n",
      "\n",
      "from tensorflow.keras.applications import EfficientNetB4\n",
      "import tensorflow as tf\n",
      "model = EfficientNetB4(weights='imagenet')\n",
      "\n",
      "def center_crop(img, new_width=None, new_height=None):        \n",
      "\n",
      "    width = img.shape[1]\n",
      "    height = img.shape[0]\n",
      "\n",
      "    if new_width is None:\n",
      "        new_width = min(width, height)\n",
      "\n",
      "    if new_height is None:\n",
      "        new_height = min(width, height)\n",
      "\n",
      "    left = int(np.ceil((width - new_width) / 2))\n",
      "    right = width - int(np.floor((width - new_width) / 2))\n",
      "\n",
      "    top = int(np.ceil((height - new_height) / 2))\n",
      "    bottom = height - int(np.floor((height - new_height) / 2))\n",
      "\n",
      "    if len(img.shape) == 2:\n",
      "        center_cropped_img = img[top:bottom, left:right]\n",
      "    else:\n",
      "        center_cropped_img = img[top:bottom, left:right, ...]\n",
      "\n",
      "    return center_cropped_img\n",
      "\n",
      "class_dict = {0: 'tench, Tinca tinca',\n",
      " 1: 'goldfish, Carassius auratus',\n",
      " 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n",
      " 3: 'tiger shark, Galeocerdo cuvieri',\n",
      " 4: 'hammerhead, hammerhead shark',\n",
      " 5: 'electric ray, crampfish, numbfish, torpedo',\n",
      " 6: 'stingray',\n",
      " 7: 'cock',\n",
      " 8: 'hen',\n",
      " 9: 'ostrich, Struthio camelus',\n",
      " 10: 'brambling, Fringilla montifringilla',\n",
      " 11: 'goldfinch, Carduelis carduelis',\n",
      " 12: 'house finch, linnet, Carpodacus mexicanus',\n",
      " 13: 'junco, snowbird',\n",
      " 14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea',\n",
      " 15: 'robin, American robin, Turdus migratorius',\n",
      " 16: 'bulbul',\n",
      " 17: 'jay',\n",
      " 18: 'magpie',\n",
      " 19: 'chickadee',\n",
      " 20: 'water ouzel, dipper',\n",
      " 21: 'kite',\n",
      " 22: 'bald eagle, American eagle, Haliaeetus leucocephalus',\n",
      " 23: 'vulture',\n",
      " 24: 'great grey owl, great gray owl, Strix nebulosa',\n",
      " 25: 'European fire salamander, Salamandra salamandra',\n",
      " 26: 'common newt, Triturus vulgaris',\n",
      " 27: 'eft',\n",
      " 28: 'spotted salamander, Ambystoma maculatum',\n",
      " 29: 'axolotl, mud puppy, Ambystoma mexicanum',\n",
      " 30: 'bullfrog, Rana catesbeiana',\n",
      " 31: 'tree frog, tree-frog',\n",
      " 32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui',\n",
      " 33: 'loggerhead, loggerhead turtle, Caretta caretta',\n",
      " 34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea',\n",
      " 35: 'mud turtle',\n",
      " 36: 'terrapin',\n",
      " 37: 'box turtle, box tortoise',\n",
      " 38: 'banded gecko',\n",
      " 39: 'common iguana, iguana, Iguana iguana',\n",
      " 40: 'American chameleon, anole, Anolis carolinensis',\n",
      " 41: 'whiptail, whiptail lizard',\n",
      " 42: 'agama',\n",
      " 43: 'frilled lizard, Chlamydosaurus kingi',\n",
      " 44: 'alligator lizard',\n",
      " 45: 'Gila monster, Heloderma suspectum',\n",
      " 46: 'green lizard, Lacerta viridis',\n",
      " 47: 'African chameleon, Chamaeleo chamaeleon',\n",
      " 48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis',\n",
      " 49: 'African crocodile, Nile crocodile, Crocodylus niloticus',\n",
      " 50: 'American alligator, Alligator mississipiensis',\n",
      " 51: 'triceratops',\n",
      " 52: 'thunder snake, worm snake, Carphophis amoenus',\n",
      " 53: 'ringneck snake, ring-necked snake, ring snake',\n",
      " 54: 'hognose snake, puff adder, sand viper',\n",
      " 55: 'green snake, grass snake',\n",
      " 56: 'king snake, kingsnake',\n",
      " 57: 'garter snake, grass snake',\n",
      " 58: 'water snake',\n",
      " 59: 'vine snake',\n",
      " 60: 'night snake, Hypsiglena torquata',\n",
      " 61: 'boa constrictor, Constrictor constrictor',\n",
      " 62: 'rock python, rock snake, Python sebae',\n",
      " 63: 'Indian cobra, Naja naja',\n",
      " 64: 'green mamba',\n",
      " 65: 'sea snake',\n",
      " 66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus',\n",
      " 67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus',\n",
      " 68: 'sidewinder, horned rattlesnake, Crotalus cerastes',\n",
      " 69: 'trilobite',\n",
      " 70: 'harvestman, daddy longlegs, Phalangium opilio',\n",
      " 71: 'scorpion',\n",
      " 72: 'black and gold garden spider, Argiope aurantia',\n",
      " 73: 'barn spider, Araneus cavaticus',\n",
      " 74: 'garden spider, Aranea diademata',\n",
      " 75: 'black widow, Latrodectus mactans',\n",
      " 76: 'tarantula',\n",
      " 77: 'wolf spider, hunting spider',\n",
      " 78: 'tick',\n",
      " 79: 'centipede',\n",
      " 80: 'black grouse',\n",
      " 81: 'ptarmigan',\n",
      " 82: 'ruffed grouse, partridge, Bonasa umbellus',\n",
      " 83: 'prairie chicken, prairie grouse, prairie fowl',\n",
      " 84: 'peacock',\n",
      " 85: 'quail',\n",
      " 86: 'partridge',\n",
      " 87: 'African grey, African gray, Psittacus erithacus',\n",
      " 88: 'macaw',\n",
      " 89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n",
      " 90: 'lorikeet',\n",
      " 91: 'coucal',\n",
      " 92: 'bee eater',\n",
      " 93: 'hornbill',\n",
      " 94: 'hummingbird',\n",
      " 95: 'jacamar',\n",
      " 96: 'toucan',\n",
      " 97: 'drake',\n",
      " 98: 'red-breasted merganser, Mergus serrator',\n",
      " 99: 'goose',\n",
      " 100: 'black swan, Cygnus atratus',\n",
      " 101: 'tusker',\n",
      " 102: 'echidna, spiny anteater, anteater',\n",
      " 103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n",
      " 104: 'wallaby, brush kangaroo',\n",
      " 105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n",
      " 106: 'wombat',\n",
      " 107: 'jellyfish',\n",
      " 108: 'sea anemone, anemone',\n",
      " 109: 'brain coral',\n",
      " 110: 'flatworm, platyhelminth',\n",
      " 111: 'nematode, nematode worm, roundworm',\n",
      " 112: 'conch',\n",
      " 113: 'snail',\n",
      " 114: 'slug',\n",
      " 115: 'sea slug, nudibranch',\n",
      " 116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore',\n",
      " 117: 'chambered nautilus, pearly nautilus, nautilus',\n",
      " 118: 'Dungeness crab, Cancer magister',\n",
      " 119: 'rock crab, Cancer irroratus',\n",
      " 120: 'fiddler crab',\n",
      " 121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica',\n",
      " 122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus',\n",
      " 123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish',\n",
      " 124: 'crayfish, crawfish, crawdad, crawdaddy',\n",
      " 125: 'hermit crab',\n",
      " 126: 'isopod',\n",
      " 127: 'white stork, Ciconia ciconia',\n",
      " 128: 'black stork, Ciconia nigra',\n",
      " 129: 'spoonbill',\n",
      " 130: 'flamingo',\n",
      " 131: 'little blue heron, Egretta caerulea',\n",
      " 132: 'American egret, great white heron, Egretta albus',\n",
      " 133: 'bittern',\n",
      " 134: 'crane',\n",
      " 135: 'limpkin, Aramus pictus',\n",
      " 136: 'European gallinule, Porphyrio porphyrio',\n",
      " 137: 'American coot, marsh hen, mud hen, water hen, Fulica americana',\n",
      " 138: 'bustard',\n",
      " 139: 'ruddy turnstone, Arenaria interpres',\n",
      " 140: 'red-backed sandpiper, dunlin, Erolia alpina',\n",
      " 141: 'redshank, Tringa totanus',\n",
      " 142: 'dowitcher',\n",
      " 143: 'oystercatcher, oyster catcher',\n",
      " 144: 'pelican',\n",
      " 145: 'king penguin, Aptenodytes patagonica',\n",
      " 146: 'albatross, mollymawk',\n",
      " 147: 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus',\n",
      " 148: 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca',\n",
      " 149: 'dugong, Dugong dugon',\n",
      " 150: 'sea lion',\n",
      " 151: 'Chihuahua',\n",
      " 152: 'Japanese spaniel',\n",
      " 153: 'Maltese dog, Maltese terrier, Maltese',\n",
      " 154: 'Pekinese, Pekingese, Peke',\n",
      " 155: 'Shih-Tzu',\n",
      " 156: 'Blenheim spaniel',\n",
      " 157: 'papillon',\n",
      " 158: 'toy terrier',\n",
      " 159: 'Rhodesian ridgeback',\n",
      " 160: 'Afghan hound, Afghan',\n",
      " 161: 'basset, basset hound',\n",
      " 162: 'beagle',\n",
      " 163: 'bloodhound, sleuthhound',\n",
      " 164: 'bluetick',\n",
      " 165: 'black-and-tan coonhound',\n",
      " 166: 'Walker hound, Walker foxhound',\n",
      " 167: 'English foxhound',\n",
      " 168: 'redbone',\n",
      " 169: 'borzoi, Russian wolfhound',\n",
      " 170: 'Irish wolfhound',\n",
      " 171: 'Italian greyhound',\n",
      " 172: 'whippet',\n",
      " 173: 'Ibizan hound, Ibizan Podenco',\n",
      " 174: 'Norwegian elkhound, elkhound',\n",
      " 175: 'otterhound, otter hound',\n",
      " 176: 'Saluki, gazelle hound',\n",
      " 177: 'Scottish deerhound, deerhound',\n",
      " 178: 'Weimaraner',\n",
      " 179: 'Staffordshire bullterrier, Staffordshire bull terrier',\n",
      " 180: 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier',\n",
      " 181: 'Bedlington terrier',\n",
      " 182: 'Border terrier',\n",
      " 183: 'Kerry blue terrier',\n",
      " 184: 'Irish terrier',\n",
      " 185: 'Norfolk terrier',\n",
      " 186: 'Norwich terrier',\n",
      " 187: 'Yorkshire terrier',\n",
      " 188: 'wire-haired fox terrier',\n",
      " 189: 'Lakeland terrier',\n",
      " 190: 'Sealyham terrier, Sealyham',\n",
      " 191: 'Airedale, Airedale terrier',\n",
      " 192: 'cairn, cairn terrier',\n",
      " 193: 'Australian terrier',\n",
      " 194: 'Dandie Dinmont, Dandie Dinmont terrier',\n",
      " 195: 'Boston bull, Boston terrier',\n",
      " 196: 'miniature schnauzer',\n",
      " 197: 'giant schnauzer',\n",
      " 198: 'standard schnauzer',\n",
      " 199: 'Scotch terrier, Scottish terrier, Scottie',\n",
      " 200: 'Tibetan terrier, chrysanthemum dog',\n",
      " 201: 'silky terrier, Sydney silky',\n",
      " 202: 'soft-coated wheaten terrier',\n",
      " 203: 'West Highland white terrier',\n",
      " 204: 'Lhasa, Lhasa apso',\n",
      " 205: 'flat-coated retriever',\n",
      " 206: 'curly-coated retriever',\n",
      " 207: 'golden retriever',\n",
      " 208: 'Labrador retriever',\n",
      " 209: 'Chesapeake Bay retriever',\n",
      " 210: 'German short-haired pointer',\n",
      " 211: 'vizsla, Hungarian pointer',\n",
      " 212: 'English setter',\n",
      " 213: 'Irish setter, red setter',\n",
      " 214: 'Gordon setter',\n",
      " 215: 'Brittany spaniel',\n",
      " 216: 'clumber, clumber spaniel',\n",
      " 217: 'English springer, English springer spaniel',\n",
      " 218: 'Welsh springer spaniel',\n",
      " 219: 'cocker spaniel, English cocker spaniel, cocker',\n",
      " 220: 'Sussex spaniel',\n",
      " 221: 'Irish water spaniel',\n",
      " 222: 'kuvasz',\n",
      " 223: 'schipperke',\n",
      " 224: 'groenendael',\n",
      " 225: 'malinois',\n",
      " 226: 'briard',\n",
      " 227: 'kelpie',\n",
      " 228: 'komondor',\n",
      " 229: 'Old English sheepdog, bobtail',\n",
      " 230: 'Shetland sheepdog, Shetland sheep dog, Shetland',\n",
      " 231: 'collie',\n",
      " 232: 'Border collie',\n",
      " 233: 'Bouvier des Flandres, Bouviers des Flandres',\n",
      " 234: 'Rottweiler',\n",
      " 235: 'German shepherd, German shepherd dog, German police dog, alsatian',\n",
      " 236: 'Doberman, Doberman pinscher',\n",
      " 237: 'miniature pinscher',\n",
      " 238: 'Greater Swiss Mountain dog',\n",
      " 239: 'Bernese mountain dog',\n",
      " 240: 'Appenzeller',\n",
      " 241: 'EntleBucher',\n",
      " 242: 'boxer',\n",
      " 243: 'bull mastiff',\n",
      " 244: 'Tibetan mastiff',\n",
      " 245: 'French bulldog',\n",
      " 246: 'Great Dane',\n",
      " 247: 'Saint Bernard, St Bernard',\n",
      " 248: 'Eskimo dog, husky',\n",
      " 249: 'malamute, malemute, Alaskan malamute',\n",
      " 250: 'Siberian husky',\n",
      " 251: 'dalmatian, coach dog, carriage dog',\n",
      " 252: 'affenpinscher, monkey pinscher, monkey dog',\n",
      " 253: 'basenji',\n",
      " 254: 'pug, pug-dog',\n",
      " 255: 'Leonberg',\n",
      " 256: 'Newfoundland, Newfoundland dog',\n",
      " 257: 'Great Pyrenees',\n",
      " 258: 'Samoyed, Samoyede',\n",
      " 259: 'Pomeranian',\n",
      " 260: 'chow, chow chow',\n",
      " 261: 'keeshond',\n",
      " 262: 'Brabancon griffon',\n",
      " 263: 'Pembroke, Pembroke Welsh corgi',\n",
      " 264: 'Cardigan, Cardigan Welsh corgi',\n",
      " 265: 'toy poodle',\n",
      " 266: 'miniature poodle',\n",
      " 267: 'standard poodle',\n",
      " 268: 'Mexican hairless',\n",
      " 269: 'timber wolf, grey wolf, gray wolf, Canis lupus',\n",
      " 270: 'white wolf, Arctic wolf, Canis lupus tundrarum',\n",
      " 271: 'red wolf, maned wolf, Canis rufus, Canis niger',\n",
      " 272: 'coyote, prairie wolf, brush wolf, Canis latrans',\n",
      " 273: 'dingo, warrigal, warragal, Canis dingo',\n",
      " 274: 'dhole, Cuon alpinus',\n",
      " 275: 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus',\n",
      " 276: 'hyena, hyaena',\n",
      " 277: 'red fox, Vulpes vulpes',\n",
      " 278: 'kit fox, Vulpes macrotis',\n",
      " 279: 'Arctic fox, white fox, Alopex lagopus',\n",
      " 280: 'grey fox, gray fox, Urocyon cinereoargenteus',\n",
      " 281: 'tabby, tabby cat',\n",
      " 282: 'tiger cat',\n",
      " 283: 'Persian cat',\n",
      " 284: 'Siamese cat, Siamese',\n",
      " 285: 'Egyptian cat',\n",
      " 286: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n",
      " 287: 'lynx, catamount',\n",
      " 288: 'leopard, Panthera pardus',\n",
      " 289: 'snow leopard, ounce, Panthera uncia',\n",
      " 290: 'jaguar, panther, Panthera onca, Felis onca',\n",
      " 291: 'lion, king of beasts, Panthera leo',\n",
      " 292: 'tiger, Panthera tigris',\n",
      " 293: 'cheetah, chetah, Acinonyx jubatus',\n",
      " 294: 'brown bear, bruin, Ursus arctos',\n",
      " 295: 'American black bear, black bear, Ursus americanus, Euarctos americanus',\n",
      " 296: 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus',\n",
      " 297: 'sloth bear, Melursus ursinus, Ursus ursinus',\n",
      " 298: 'mongoose',\n",
      " 299: 'meerkat, mierkat',\n",
      " 300: 'tiger beetle',\n",
      " 301: 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle',\n",
      " 302: 'ground beetle, carabid beetle',\n",
      " 303: 'long-horned beetle, longicorn, longicorn beetle',\n",
      " 304: 'leaf beetle, chrysomelid',\n",
      " 305: 'dung beetle',\n",
      " 306: 'rhinoceros beetle',\n",
      " 307: 'weevil',\n",
      " 308: 'fly',\n",
      " 309: 'bee',\n",
      " 310: 'ant, emmet, pismire',\n",
      " 311: 'grasshopper, hopper',\n",
      " 312: 'cricket',\n",
      " 313: 'walking stick, walkingstick, stick insect',\n",
      " 314: 'cockroach, roach',\n",
      " 315: 'mantis, mantid',\n",
      " 316: 'cicada, cicala',\n",
      " 317: 'leafhopper',\n",
      " 318: 'lacewing, lacewing fly',\n",
      " 319: \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\",\n",
      " 320: 'damselfly',\n",
      " 321: 'admiral',\n",
      " 322: 'ringlet, ringlet butterfly',\n",
      " 323: 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus',\n",
      " 324: 'cabbage butterfly',\n",
      " 325: 'sulphur butterfly, sulfur butterfly',\n",
      " 326: 'lycaenid, lycaenid butterfly',\n",
      " 327: 'starfish, sea star',\n",
      " 328: 'sea urchin',\n",
      " 329: 'sea cucumber, holothurian',\n",
      " 330: 'wood rabbit, cottontail, cottontail rabbit',\n",
      " 331: 'hare',\n",
      " 332: 'Angora, Angora rabbit',\n",
      " 333: 'hamster',\n",
      " 334: 'porcupine, hedgehog',\n",
      " 335: 'fox squirrel, eastern fox squirrel, Sciurus niger',\n",
      " 336: 'marmot',\n",
      " 337: 'beaver',\n",
      " 338: 'guinea pig, Cavia cobaya',\n",
      " 339: 'sorrel',\n",
      " 340: 'zebra',\n",
      " 341: 'hog, pig, grunter, squealer, Sus scrofa',\n",
      " 342: 'wild boar, boar, Sus scrofa',\n",
      " 343: 'warthog',\n",
      " 344: 'hippopotamus, hippo, river horse, Hippopotamus amphibius',\n",
      " 345: 'ox',\n",
      " 346: 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis',\n",
      " 347: 'bison',\n",
      " 348: 'ram, tup',\n",
      " 349: 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis',\n",
      " 350: 'ibex, Capra ibex',\n",
      " 351: 'hartebeest',\n",
      " 352: 'impala, Aepyceros melampus',\n",
      " 353: 'gazelle',\n",
      " 354: 'Arabian camel, dromedary, Camelus dromedarius',\n",
      " 355: 'llama',\n",
      " 356: 'weasel',\n",
      " 357: 'mink',\n",
      " 358: 'polecat, fitch, foulmart, foumart, Mustela putorius',\n",
      " 359: 'black-footed ferret, ferret, Mustela nigripes',\n",
      " 360: 'otter',\n",
      " 361: 'skunk, polecat, wood pussy',\n",
      " 362: 'badger',\n",
      " 363: 'armadillo',\n",
      " 364: 'three-toed sloth, ai, Bradypus tridactylus',\n",
      " 365: 'orangutan, orang, orangutang, Pongo pygmaeus',\n",
      " 366: 'gorilla, Gorilla gorilla',\n",
      " 367: 'chimpanzee, chimp, Pan troglodytes',\n",
      " 368: 'gibbon, Hylobates lar',\n",
      " 369: 'siamang, Hylobates syndactylus, Symphalangus syndactylus',\n",
      " 370: 'guenon, guenon monkey',\n",
      " 371: 'patas, hussar monkey, Erythrocebus patas',\n",
      " 372: 'baboon',\n",
      " 373: 'macaque',\n",
      " 374: 'langur',\n",
      " 375: 'colobus, colobus monkey',\n",
      " 376: 'proboscis monkey, Nasalis larvatus',\n",
      " 377: 'marmoset',\n",
      " 378: 'capuchin, ringtail, Cebus capucinus',\n",
      " 379: 'howler monkey, howler',\n",
      " 380: 'titi, titi monkey',\n",
      " 381: 'spider monkey, Ateles geoffroyi',\n",
      " 382: 'squirrel monkey, Saimiri sciureus',\n",
      " 383: 'Madagascar cat, ring-tailed lemur, Lemur catta',\n",
      " 384: 'indri, indris, Indri indri, Indri brevicaudatus',\n",
      " 385: 'Indian elephant, Elephas maximus',\n",
      " 386: 'African elephant, Loxodonta africana',\n",
      " 387: 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens',\n",
      " 388: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca',\n",
      " 389: 'barracouta, snoek',\n",
      " 390: 'eel',\n",
      " 391: 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch',\n",
      " 392: 'rock beauty, Holocanthus tricolor',\n",
      " 393: 'anemone fish',\n",
      " 394: 'sturgeon',\n",
      " 395: 'gar, garfish, garpike, billfish, Lepisosteus osseus',\n",
      " 396: 'lionfish',\n",
      " 397: 'puffer, pufferfish, blowfish, globefish',\n",
      " 398: 'abacus',\n",
      " 399: 'abaya',\n",
      " 400: \"academic gown, academic robe, judge's robe\",\n",
      " 401: 'accordion, piano accordion, squeeze box',\n",
      " 402: 'acoustic guitar',\n",
      " 403: 'aircraft carrier, carrier, flattop, attack aircraft carrier',\n",
      " 404: 'airliner',\n",
      " 405: 'airship, dirigible',\n",
      " 406: 'altar',\n",
      " 407: 'ambulance',\n",
      " 408: 'amphibian, amphibious vehicle',\n",
      " 409: 'analog clock',\n",
      " 410: 'apiary, bee house',\n",
      " 411: 'apron',\n",
      " 412: 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin',\n",
      " 413: 'assault rifle, assault gun',\n",
      " 414: 'backpack, back pack, knapsack, packsack, rucksack, haversack',\n",
      " 415: 'bakery, bakeshop, bakehouse',\n",
      " 416: 'balance beam, beam',\n",
      " 417: 'balloon',\n",
      " 418: 'ballpoint, ballpoint pen, ballpen, Biro',\n",
      " 419: 'Band Aid',\n",
      " 420: 'banjo',\n",
      " 421: 'bannister, banister, balustrade, balusters, handrail',\n",
      " 422: 'barbell',\n",
      " 423: 'barber chair',\n",
      " 424: 'barbershop',\n",
      " 425: 'barn',\n",
      " 426: 'barometer',\n",
      " 427: 'barrel, cask',\n",
      " 428: 'barrow, garden cart, lawn cart, wheelbarrow',\n",
      " 429: 'baseball',\n",
      " 430: 'basketball',\n",
      " 431: 'bassinet',\n",
      " 432: 'bassoon',\n",
      " 433: 'bathing cap, swimming cap',\n",
      " 434: 'bath towel',\n",
      " 435: 'bathtub, bathing tub, bath, tub',\n",
      " 436: 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon',\n",
      " 437: 'beacon, lighthouse, beacon light, pharos',\n",
      " 438: 'beaker',\n",
      " 439: 'bearskin, busby, shako',\n",
      " 440: 'beer bottle',\n",
      " 441: 'beer glass',\n",
      " 442: 'bell cote, bell cot',\n",
      " 443: 'bib',\n",
      " 444: 'bicycle-built-for-two, tandem bicycle, tandem',\n",
      " 445: 'bikini, two-piece',\n",
      " 446: 'binder, ring-binder',\n",
      " 447: 'binoculars, field glasses, opera glasses',\n",
      " 448: 'birdhouse',\n",
      " 449: 'boathouse',\n",
      " 450: 'bobsled, bobsleigh, bob',\n",
      " 451: 'bolo tie, bolo, bola tie, bola',\n",
      " 452: 'bonnet, poke bonnet',\n",
      " 453: 'bookcase',\n",
      " 454: 'bookshop, bookstore, bookstall',\n",
      " 455: 'bottlecap',\n",
      " 456: 'bow',\n",
      " 457: 'bow tie, bow-tie, bowtie',\n",
      " 458: 'brass, memorial tablet, plaque',\n",
      " 459: 'brassiere, bra, bandeau',\n",
      " 460: 'breakwater, groin, groyne, mole, bulwark, seawall, jetty',\n",
      " 461: 'breastplate, aegis, egis',\n",
      " 462: 'broom',\n",
      " 463: 'bucket, pail',\n",
      " 464: 'buckle',\n",
      " 465: 'bulletproof vest',\n",
      " 466: 'bullet train, bullet',\n",
      " 467: 'butcher shop, meat market',\n",
      " 468: 'cab, hack, taxi, taxicab',\n",
      " 469: 'caldron, cauldron',\n",
      " 470: 'candle, taper, wax light',\n",
      " 471: 'cannon',\n",
      " 472: 'canoe',\n",
      " 473: 'can opener, tin opener',\n",
      " 474: 'cardigan',\n",
      " 475: 'car mirror',\n",
      " 476: 'carousel, carrousel, merry-go-round, roundabout, whirligig',\n",
      " 477: \"carpenter's kit, tool kit\",\n",
      " 478: 'carton',\n",
      " 479: 'car wheel',\n",
      " 480: 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM',\n",
      " 481: 'cassette',\n",
      " 482: 'cassette player',\n",
      " 483: 'castle',\n",
      " 484: 'catamaran',\n",
      " 485: 'CD player',\n",
      " 486: 'cello, violoncello',\n",
      " 487: 'cellular telephone, cellular phone, cellphone, cell, mobile phone',\n",
      " 488: 'chain',\n",
      " 489: 'chainlink fence',\n",
      " 490: 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour',\n",
      " 491: 'chain saw, chainsaw',\n",
      " 492: 'chest',\n",
      " 493: 'chiffonier, commode',\n",
      " 494: 'chime, bell, gong',\n",
      " 495: 'china cabinet, china closet',\n",
      " 496: 'Christmas stocking',\n",
      " 497: 'church, church building',\n",
      " 498: 'cinema, movie theater, movie theatre, movie house, picture palace',\n",
      " 499: 'cleaver, meat cleaver, chopper',\n",
      " 500: 'cliff dwelling',\n",
      " 501: 'cloak',\n",
      " 502: 'clog, geta, patten, sabot',\n",
      " 503: 'cocktail shaker',\n",
      " 504: 'coffee mug',\n",
      " 505: 'coffeepot',\n",
      " 506: 'coil, spiral, volute, whorl, helix',\n",
      " 507: 'combination lock',\n",
      " 508: 'computer keyboard, keypad',\n",
      " 509: 'confectionery, confectionary, candy store',\n",
      " 510: 'container ship, containership, container vessel',\n",
      " 511: 'convertible',\n",
      " 512: 'corkscrew, bottle screw',\n",
      " 513: 'cornet, horn, trumpet, trump',\n",
      " 514: 'cowboy boot',\n",
      " 515: 'cowboy hat, ten-gallon hat',\n",
      " 516: 'cradle',\n",
      " 517: 'crane',\n",
      " 518: 'crash helmet',\n",
      " 519: 'crate',\n",
      " 520: 'crib, cot',\n",
      " 521: 'Crock Pot',\n",
      " 522: 'croquet ball',\n",
      " 523: 'crutch',\n",
      " 524: 'cuirass',\n",
      " 525: 'dam, dike, dyke',\n",
      " 526: 'desk',\n",
      " 527: 'desktop computer',\n",
      " 528: 'dial telephone, dial phone',\n",
      " 529: 'diaper, nappy, napkin',\n",
      " 530: 'digital clock',\n",
      " 531: 'digital watch',\n",
      " 532: 'dining table, board',\n",
      " 533: 'dishrag, dishcloth',\n",
      " 534: 'dishwasher, dish washer, dishwashing machine',\n",
      " 535: 'disk brake, disc brake',\n",
      " 536: 'dock, dockage, docking facility',\n",
      " 537: 'dogsled, dog sled, dog sleigh',\n",
      " 538: 'dome',\n",
      " 539: 'doormat, welcome mat',\n",
      " 540: 'drilling platform, offshore rig',\n",
      " 541: 'drum, membranophone, tympan',\n",
      " 542: 'drumstick',\n",
      " 543: 'dumbbell',\n",
      " 544: 'Dutch oven',\n",
      " 545: 'electric fan, blower',\n",
      " 546: 'electric guitar',\n",
      " 547: 'electric locomotive',\n",
      " 548: 'entertainment center',\n",
      " 549: 'envelope',\n",
      " 550: 'espresso maker',\n",
      " 551: 'face powder',\n",
      " 552: 'feather boa, boa',\n",
      " 553: 'file, file cabinet, filing cabinet',\n",
      " 554: 'fireboat',\n",
      " 555: 'fire engine, fire truck',\n",
      " 556: 'fire screen, fireguard',\n",
      " 557: 'flagpole, flagstaff',\n",
      " 558: 'flute, transverse flute',\n",
      " 559: 'folding chair',\n",
      " 560: 'football helmet',\n",
      " 561: 'forklift',\n",
      " 562: 'fountain',\n",
      " 563: 'fountain pen',\n",
      " 564: 'four-poster',\n",
      " 565: 'freight car',\n",
      " 566: 'French horn, horn',\n",
      " 567: 'frying pan, frypan, skillet',\n",
      " 568: 'fur coat',\n",
      " 569: 'garbage truck, dustcart',\n",
      " 570: 'gasmask, respirator, gas helmet',\n",
      " 571: 'gas pump, gasoline pump, petrol pump, island dispenser',\n",
      " 572: 'goblet',\n",
      " 573: 'go-kart',\n",
      " 574: 'golf ball',\n",
      " 575: 'golfcart, golf cart',\n",
      " 576: 'gondola',\n",
      " 577: 'gong, tam-tam',\n",
      " 578: 'gown',\n",
      " 579: 'grand piano, grand',\n",
      " 580: 'greenhouse, nursery, glasshouse',\n",
      " 581: 'grille, radiator grille',\n",
      " 582: 'grocery store, grocery, food market, market',\n",
      " 583: 'guillotine',\n",
      " 584: 'hair slide',\n",
      " 585: 'hair spray',\n",
      " 586: 'half track',\n",
      " 587: 'hammer',\n",
      " 588: 'hamper',\n",
      " 589: 'hand blower, blow dryer, blow drier, hair dryer, hair drier',\n",
      " 590: 'hand-held computer, hand-held microcomputer',\n",
      " 591: 'handkerchief, hankie, hanky, hankey',\n",
      " 592: 'hard disc, hard disk, fixed disk',\n",
      " 593: 'harmonica, mouth organ, harp, mouth harp',\n",
      " 594: 'harp',\n",
      " 595: 'harvester, reaper',\n",
      " 596: 'hatchet',\n",
      " 597: 'holster',\n",
      " 598: 'home theater, home theatre',\n",
      " 599: 'honeycomb',\n",
      " 600: 'hook, claw',\n",
      " 601: 'hoopskirt, crinoline',\n",
      " 602: 'horizontal bar, high bar',\n",
      " 603: 'horse cart, horse-cart',\n",
      " 604: 'hourglass',\n",
      " 605: 'iPod',\n",
      " 606: 'iron, smoothing iron',\n",
      " 607: \"jack-o'-lantern\",\n",
      " 608: 'jean, blue jean, denim',\n",
      " 609: 'jeep, landrover',\n",
      " 610: 'jersey, T-shirt, tee shirt',\n",
      " 611: 'jigsaw puzzle',\n",
      " 612: 'jinrikisha, ricksha, rickshaw',\n",
      " 613: 'joystick',\n",
      " 614: 'kimono',\n",
      " 615: 'knee pad',\n",
      " 616: 'knot',\n",
      " 617: 'lab coat, laboratory coat',\n",
      " 618: 'ladle',\n",
      " 619: 'lampshade, lamp shade',\n",
      " 620: 'laptop, laptop computer',\n",
      " 621: 'lawn mower, mower',\n",
      " 622: 'lens cap, lens cover',\n",
      " 623: 'letter opener, paper knife, paperknife',\n",
      " 624: 'library',\n",
      " 625: 'lifeboat',\n",
      " 626: 'lighter, light, igniter, ignitor',\n",
      " 627: 'limousine, limo',\n",
      " 628: 'liner, ocean liner',\n",
      " 629: 'lipstick, lip rouge',\n",
      " 630: 'Loafer',\n",
      " 631: 'lotion',\n",
      " 632: 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system',\n",
      " 633: \"loupe, jeweler's loupe\",\n",
      " 634: 'lumbermill, sawmill',\n",
      " 635: 'magnetic compass',\n",
      " 636: 'mailbag, postbag',\n",
      " 637: 'mailbox, letter box',\n",
      " 638: 'maillot',\n",
      " 639: 'maillot, tank suit',\n",
      " 640: 'manhole cover',\n",
      " 641: 'maraca',\n",
      " 642: 'marimba, xylophone',\n",
      " 643: 'mask',\n",
      " 644: 'matchstick',\n",
      " 645: 'maypole',\n",
      " 646: 'maze, labyrinth',\n",
      " 647: 'measuring cup',\n",
      " 648: 'medicine chest, medicine cabinet',\n",
      " 649: 'megalith, megalithic structure',\n",
      " 650: 'microphone, mike',\n",
      " 651: 'microwave, microwave oven',\n",
      " 652: 'military uniform',\n",
      " 653: 'milk can',\n",
      " 654: 'minibus',\n",
      " 655: 'miniskirt, mini',\n",
      " 656: 'minivan',\n",
      " 657: 'missile',\n",
      " 658: 'mitten',\n",
      " 659: 'mixing bowl',\n",
      " 660: 'mobile home, manufactured home',\n",
      " 661: 'Model T',\n",
      " 662: 'modem',\n",
      " 663: 'monastery',\n",
      " 664: 'monitor',\n",
      " 665: 'moped',\n",
      " 666: 'mortar',\n",
      " 667: 'mortarboard',\n",
      " 668: 'mosque',\n",
      " 669: 'mosquito net',\n",
      " 670: 'motor scooter, scooter',\n",
      " 671: 'mountain bike, all-terrain bike, off-roader',\n",
      " 672: 'mountain tent',\n",
      " 673: 'mouse, computer mouse',\n",
      " 674: 'mousetrap',\n",
      " 675: 'moving van',\n",
      " 676: 'muzzle',\n",
      " 677: 'nail',\n",
      " 678: 'neck brace',\n",
      " 679: 'necklace',\n",
      " 680: 'nipple',\n",
      " 681: 'notebook, notebook computer',\n",
      " 682: 'obelisk',\n",
      " 683: 'oboe, hautboy, hautbois',\n",
      " 684: 'ocarina, sweet potato',\n",
      " 685: 'odometer, hodometer, mileometer, milometer',\n",
      " 686: 'oil filter',\n",
      " 687: 'organ, pipe organ',\n",
      " 688: 'oscilloscope, scope, cathode-ray oscilloscope, CRO',\n",
      " 689: 'overskirt',\n",
      " 690: 'oxcart',\n",
      " 691: 'oxygen mask',\n",
      " 692: 'packet',\n",
      " 693: 'paddle, boat paddle',\n",
      " 694: 'paddlewheel, paddle wheel',\n",
      " 695: 'padlock',\n",
      " 696: 'paintbrush',\n",
      " 697: \"pajama, pyjama, pj's, jammies\",\n",
      " 698: 'palace',\n",
      " 699: 'panpipe, pandean pipe, syrinx',\n",
      " 700: 'paper towel',\n",
      " 701: 'parachute, chute',\n",
      " 702: 'parallel bars, bars',\n",
      " 703: 'park bench',\n",
      " 704: 'parking meter',\n",
      " 705: 'passenger car, coach, carriage',\n",
      " 706: 'patio, terrace',\n",
      " 707: 'pay-phone, pay-station',\n",
      " 708: 'pedestal, plinth, footstall',\n",
      " 709: 'pencil box, pencil case',\n",
      " 710: 'pencil sharpener',\n",
      " 711: 'perfume, essence',\n",
      " 712: 'Petri dish',\n",
      " 713: 'photocopier',\n",
      " 714: 'pick, plectrum, plectron',\n",
      " 715: 'pickelhaube',\n",
      " 716: 'picket fence, paling',\n",
      " 717: 'pickup, pickup truck',\n",
      " 718: 'pier',\n",
      " 719: 'piggy bank, penny bank',\n",
      " 720: 'pill bottle',\n",
      " 721: 'pillow',\n",
      " 722: 'ping-pong ball',\n",
      " 723: 'pinwheel',\n",
      " 724: 'pirate, pirate ship',\n",
      " 725: 'pitcher, ewer',\n",
      " 726: \"plane, carpenter's plane, woodworking plane\",\n",
      " 727: 'planetarium',\n",
      " 728: 'plastic bag',\n",
      " 729: 'plate rack',\n",
      " 730: 'plow, plough',\n",
      " 731: \"plunger, plumber's helper\",\n",
      " 732: 'Polaroid camera, Polaroid Land camera',\n",
      " 733: 'pole',\n",
      " 734: 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria',\n",
      " 735: 'poncho',\n",
      " 736: 'pool table, billiard table, snooker table',\n",
      " 737: 'pop bottle, soda bottle',\n",
      " 738: 'pot, flowerpot',\n",
      " 739: \"potter's wheel\",\n",
      " 740: 'power drill',\n",
      " 741: 'prayer rug, prayer mat',\n",
      " 742: 'printer',\n",
      " 743: 'prison, prison house',\n",
      " 744: 'projectile, missile',\n",
      " 745: 'projector',\n",
      " 746: 'puck, hockey puck',\n",
      " 747: 'punching bag, punch bag, punching ball, punchball',\n",
      " 748: 'purse',\n",
      " 749: 'quill, quill pen',\n",
      " 750: 'quilt, comforter, comfort, puff',\n",
      " 751: 'racer, race car, racing car',\n",
      " 752: 'racket, racquet',\n",
      " 753: 'radiator',\n",
      " 754: 'radio, wireless',\n",
      " 755: 'radio telescope, radio reflector',\n",
      " 756: 'rain barrel',\n",
      " 757: 'recreational vehicle, RV, R.V.',\n",
      " 758: 'reel',\n",
      " 759: 'reflex camera',\n",
      " 760: 'refrigerator, icebox',\n",
      " 761: 'remote control, remote',\n",
      " 762: 'restaurant, eating house, eating place, eatery',\n",
      " 763: 'revolver, six-gun, six-shooter',\n",
      " 764: 'rifle',\n",
      " 765: 'rocking chair, rocker',\n",
      " 766: 'rotisserie',\n",
      " 767: 'rubber eraser, rubber, pencil eraser',\n",
      " 768: 'rugby ball',\n",
      " 769: 'rule, ruler',\n",
      " 770: 'running shoe',\n",
      " 771: 'safe',\n",
      " 772: 'safety pin',\n",
      " 773: 'saltshaker, salt shaker',\n",
      " 774: 'sandal',\n",
      " 775: 'sarong',\n",
      " 776: 'sax, saxophone',\n",
      " 777: 'scabbard',\n",
      " 778: 'scale, weighing machine',\n",
      " 779: 'school bus',\n",
      " 780: 'schooner',\n",
      " 781: 'scoreboard',\n",
      " 782: 'screen, CRT screen',\n",
      " 783: 'screw',\n",
      " 784: 'screwdriver',\n",
      " 785: 'seat belt, seatbelt',\n",
      " 786: 'sewing machine',\n",
      " 787: 'shield, buckler',\n",
      " 788: 'shoe shop, shoe-shop, shoe store',\n",
      " 789: 'shoji',\n",
      " 790: 'shopping basket',\n",
      " 791: 'shopping cart',\n",
      " 792: 'shovel',\n",
      " 793: 'shower cap',\n",
      " 794: 'shower curtain',\n",
      " 795: 'ski',\n",
      " 796: 'ski mask',\n",
      " 797: 'sleeping bag',\n",
      " 798: 'slide rule, slipstick',\n",
      " 799: 'sliding door',\n",
      " 800: 'slot, one-armed bandit',\n",
      " 801: 'snorkel',\n",
      " 802: 'snowmobile',\n",
      " 803: 'snowplow, snowplough',\n",
      " 804: 'soap dispenser',\n",
      " 805: 'soccer ball',\n",
      " 806: 'sock',\n",
      " 807: 'solar dish, solar collector, solar furnace',\n",
      " 808: 'sombrero',\n",
      " 809: 'soup bowl',\n",
      " 810: 'space bar',\n",
      " 811: 'space heater',\n",
      " 812: 'space shuttle',\n",
      " 813: 'spatula',\n",
      " 814: 'speedboat',\n",
      " 815: \"spider web, spider's web\",\n",
      " 816: 'spindle',\n",
      " 817: 'sports car, sport car',\n",
      " 818: 'spotlight, spot',\n",
      " 819: 'stage',\n",
      " 820: 'steam locomotive',\n",
      " 821: 'steel arch bridge',\n",
      " 822: 'steel drum',\n",
      " 823: 'stethoscope',\n",
      " 824: 'stole',\n",
      " 825: 'stone wall',\n",
      " 826: 'stopwatch, stop watch',\n",
      " 827: 'stove',\n",
      " 828: 'strainer',\n",
      " 829: 'streetcar, tram, tramcar, trolley, trolley car',\n",
      " 830: 'stretcher',\n",
      " 831: 'studio couch, day bed',\n",
      " 832: 'stupa, tope',\n",
      " 833: 'submarine, pigboat, sub, U-boat',\n",
      " 834: 'suit, suit of clothes',\n",
      " 835: 'sundial',\n",
      " 836: 'sunglass',\n",
      " 837: 'sunglasses, dark glasses, shades',\n",
      " 838: 'sunscreen, sunblock, sun blocker',\n",
      " 839: 'suspension bridge',\n",
      " 840: 'swab, swob, mop',\n",
      " 841: 'sweatshirt',\n",
      " 842: 'swimming trunks, bathing trunks',\n",
      " 843: 'swing',\n",
      " 844: 'switch, electric switch, electrical switch',\n",
      " 845: 'syringe',\n",
      " 846: 'table lamp',\n",
      " 847: 'tank, army tank, armored combat vehicle, armoured combat vehicle',\n",
      " 848: 'tape player',\n",
      " 849: 'teapot',\n",
      " 850: 'teddy, teddy bear',\n",
      " 851: 'television, television system',\n",
      " 852: 'tennis ball',\n",
      " 853: 'thatch, thatched roof',\n",
      " 854: 'theater curtain, theatre curtain',\n",
      " 855: 'thimble',\n",
      " 856: 'thresher, thrasher, threshing machine',\n",
      " 857: 'throne',\n",
      " 858: 'tile roof',\n",
      " 859: 'toaster',\n",
      " 860: 'tobacco shop, tobacconist shop, tobacconist',\n",
      " 861: 'toilet seat',\n",
      " 862: 'torch',\n",
      " 863: 'totem pole',\n",
      " 864: 'tow truck, tow car, wrecker',\n",
      " 865: 'toyshop',\n",
      " 866: 'tractor',\n",
      " 867: 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi',\n",
      " 868: 'tray',\n",
      " 869: 'trench coat',\n",
      " 870: 'tricycle, trike, velocipede',\n",
      " 871: 'trimaran',\n",
      " 872: 'tripod',\n",
      " 873: 'triumphal arch',\n",
      " 874: 'trolleybus, trolley coach, trackless trolley',\n",
      " 875: 'trombone',\n",
      " 876: 'tub, vat',\n",
      " 877: 'turnstile',\n",
      " 878: 'typewriter keyboard',\n",
      " 879: 'umbrella',\n",
      " 880: 'unicycle, monocycle',\n",
      " 881: 'upright, upright piano',\n",
      " 882: 'vacuum, vacuum cleaner',\n",
      " 883: 'vase',\n",
      " 884: 'vault',\n",
      " 885: 'velvet',\n",
      " 886: 'vending machine',\n",
      " 887: 'vestment',\n",
      " 888: 'viaduct',\n",
      " 889: 'violin, fiddle',\n",
      " 890: 'volleyball',\n",
      " 891: 'waffle iron',\n",
      " 892: 'wall clock',\n",
      " 893: 'wallet, billfold, notecase, pocketbook',\n",
      " 894: 'wardrobe, closet, press',\n",
      " 895: 'warplane, military plane',\n",
      " 896: 'washbasin, handbasin, washbowl, lavabo, wash-hand basin',\n",
      " 897: 'washer, automatic washer, washing machine',\n",
      " 898: 'water bottle',\n",
      " 899: 'water jug',\n",
      " 900: 'water tower',\n",
      " 901: 'whiskey jug',\n",
      " 902: 'whistle',\n",
      " 903: 'wig',\n",
      " 904: 'window screen',\n",
      " 905: 'window shade',\n",
      " 906: 'Windsor tie',\n",
      " 907: 'wine bottle',\n",
      " 908: 'wing',\n",
      " 909: 'wok',\n",
      " 910: 'wooden spoon',\n",
      " 911: 'wool, woolen, woollen',\n",
      " 912: 'worm fence, snake fence, snake-rail fence, Virginia fence',\n",
      " 913: 'wreck',\n",
      " 914: 'yawl',\n",
      " 915: 'yurt',\n",
      " 916: 'web site, website, internet site, site',\n",
      " 917: 'comic book',\n",
      " 918: 'crossword puzzle, crossword',\n",
      " 919: 'street sign',\n",
      " 920: 'traffic light, traffic signal, stoplight',\n",
      " 921: 'book jacket, dust cover, dust jacket, dust wrapper',\n",
      " 922: 'menu',\n",
      " 923: 'plate',\n",
      " 924: 'guacamole',\n",
      " 925: 'consomme',\n",
      " 926: 'hot pot, hotpot',\n",
      " 927: 'trifle',\n",
      " 928: 'ice cream, icecream',\n",
      " 929: 'ice lolly, lolly, lollipop, popsicle',\n",
      " 930: 'French loaf',\n",
      " 931: 'bagel, beigel',\n",
      " 932: 'pretzel',\n",
      " 933: 'cheeseburger',\n",
      " 934: 'hotdog, hot dog, red hot',\n",
      " 935: 'mashed potato',\n",
      " 936: 'head cabbage',\n",
      " 937: 'broccoli',\n",
      " 938: 'cauliflower',\n",
      " 939: 'zucchini, courgette',\n",
      " 940: 'spaghetti squash',\n",
      " 941: 'acorn squash',\n",
      " 942: 'butternut squash',\n",
      " 943: 'cucumber, cuke',\n",
      " 944: 'artichoke, globe artichoke',\n",
      " 945: 'bell pepper',\n",
      " 946: 'cardoon',\n",
      " 947: 'mushroom',\n",
      " 948: 'Granny Smith',\n",
      " 949: 'strawberry',\n",
      " 950: 'orange',\n",
      " 951: 'lemon',\n",
      " 952: 'fig',\n",
      " 953: 'pineapple, ananas',\n",
      " 954: 'banana',\n",
      " 955: 'jackfruit, jak, jack',\n",
      " 956: 'custard apple',\n",
      " 957: 'pomegranate',\n",
      " 958: 'hay',\n",
      " 959: 'carbonara',\n",
      " 960: 'chocolate sauce, chocolate syrup',\n",
      " 961: 'dough',\n",
      " 962: 'meat loaf, meatloaf',\n",
      " 963: 'pizza, pizza pie',\n",
      " 964: 'potpie',\n",
      " 965: 'burrito',\n",
      " 966: 'red wine',\n",
      " 967: 'espresso',\n",
      " 968: 'cup',\n",
      " 969: 'eggnog',\n",
      " 970: 'alp',\n",
      " 971: 'bubble',\n",
      " 972: 'cliff, drop, drop-off',\n",
      " 973: 'coral reef',\n",
      " 974: 'geyser',\n",
      " 975: 'lakeside, lakeshore',\n",
      " 976: 'promontory, headland, head, foreland',\n",
      " 977: 'sandbar, sand bar',\n",
      " 978: 'seashore, coast, seacoast, sea-coast',\n",
      " 979: 'valley, vale',\n",
      " 980: 'volcano',\n",
      " 981: 'ballplayer, baseball player',\n",
      " 982: 'groom, bridegroom',\n",
      " 983: 'scuba diver',\n",
      " 984: 'rapeseed',\n",
      " 985: 'daisy',\n",
      " 986: \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\",\n",
      " 987: 'corn',\n",
      " 988: 'acorn',\n",
      " 989: 'hip, rose hip, rosehip',\n",
      " 990: 'buckeye, horse chestnut, conker',\n",
      " 991: 'coral fungus',\n",
      " 992: 'agaric',\n",
      " 993: 'gyromitra',\n",
      " 994: 'stinkhorn, carrion fungus',\n",
      " 995: 'earthstar',\n",
      " 996: 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa',\n",
      " 997: 'bolete',\n",
      " 998: 'ear, spike, capitulum',\n",
      " 999: 'toilet tissue, toilet paper, bathroom tissue'}\n",
      "91/1:\n",
      "# from sklearn.decomposition import PCA\n",
      "# img_class = pickle.load(open(\"gdrive/MyDrive/PED/image_classes.pkl\", \"rb\"))\n",
      "\n",
      "# class_table = {}\n",
      "# # class_table['id'] = []\n",
      "\n",
      "# for i in range(1000):\n",
      "#   class_table[str(i)] = []\n",
      "# for key in img_class.keys():\n",
      "#   # class_table['id'].append(key)\n",
      "#   for i in range(1000):\n",
      "#     class_table[str(i)].append(img_class[key]['all_prob'][0][i])\n",
      "\n",
      "# class_table = pandas.DataFrame(class_table)\n",
      "# pca = PCA(n_components=10)\n",
      "# pca.fit(class_table)\n",
      "# print(pca.components_ )\n",
      "import pandas\n",
      "import pickle\n",
      "import numpy as np\n",
      "from collections import Counter\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "import nltk\n",
      "nltk.download('punkt')\n",
      "from nltk import word_tokenize\n",
      "from collections import Counter\n",
      "from nltk import word_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
      "91/2:\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "df[\"body\"] = df[\"body\"].fillna('')\n",
      "df[\"title\"] = df[\"title\"].fillna('')\n",
      "img_clusters = pickle.load(open(\"../../pickle/image_color_clusters.pkl\", \"rb\"))\n",
      "img_means = pickle.load(open(\"../../pickle/image_hsv_means.pkl\", \"rb\"))\n",
      "img_text = pickle.load(open(\"../../pickle/texts.pkl\", \"rb\"))\n",
      "img_classes = pickle.load(open(\"../../pickle/image_classes.pkl\", \"rb\"))\n",
      "91/3:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "img_text = [[i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(df[\"body_lemmatized\"].sum())\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "\n",
      "for key, cnts in list(img_most_common.items()):\n",
      "  if not key in emoji.UNICODE_EMOJI and (key in stopwords.words('english') or not re.match(r'\\w+',key)):\n",
      "    del img_most_common[key]\n",
      "img_most_common = img_most_common.most_common(100)\n",
      "91/4:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "img_text = [[i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in \"\".join(img_text.values()).split()]\n",
      "\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(df[\"body_lemmatized\"].sum())\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "\n",
      "for key, cnts in list(img_most_common.items()):\n",
      "  if not key in emoji.UNICODE_EMOJI and (key in stopwords.words('english') or not re.match(r'\\w+',key)):\n",
      "    del img_most_common[key]\n",
      "img_most_common = img_most_common.most_common(100)\n",
      "91/5:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "img_text = [[i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in \"\".join(img_text.values())]\n",
      "\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(df[\"body_lemmatized\"].sum())\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "\n",
      "for key, cnts in list(img_most_common.items()):\n",
      "  if not key in emoji.UNICODE_EMOJI and (key in stopwords.words('english') or not re.match(r'\\w+',key)):\n",
      "    del img_most_common[key]\n",
      "img_most_common = img_most_common.most_common(100)\n",
      "91/6:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "img_text = [[i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(df[\"body_lemmatized\"].sum())\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "\n",
      "for key, cnts in list(img_most_common.items()):\n",
      "  if not key in emoji.UNICODE_EMOJI and (key in stopwords.words('english') or not re.match(r'\\w+',key)):\n",
      "    del img_most_common[key]\n",
      "img_most_common = img_most_common.most_common(100)\n",
      "91/7:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "print(img_text.values())\n",
      "img_text = [[i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(df[\"body_lemmatized\"].sum())\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "\n",
      "for key, cnts in list(img_most_common.items()):\n",
      "  if not key in emoji.UNICODE_EMOJI and (key in stopwords.words('english') or not re.match(r'\\w+',key)):\n",
      "    del img_most_common[key]\n",
      "img_most_common = img_most_common.most_common(100)\n",
      "91/8:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "print(img_text.values()[0])\n",
      "img_text = [[i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(df[\"body_lemmatized\"].sum())\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "\n",
      "for key, cnts in list(img_most_common.items()):\n",
      "  if not key in emoji.UNICODE_EMOJI and (key in stopwords.words('english') or not re.match(r'\\w+',key)):\n",
      "    del img_most_common[key]\n",
      "img_most_common = img_most_common.most_common(100)\n",
      "91/9:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "print(list(img_text.values())[0])\n",
      "img_text = [[i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(df[\"body_lemmatized\"].sum())\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "\n",
      "for key, cnts in list(img_most_common.items()):\n",
      "  if not key in emoji.UNICODE_EMOJI and (key in stopwords.words('english') or not re.match(r'\\w+',key)):\n",
      "    del img_most_common[key]\n",
      "img_most_common = img_most_common.most_common(100)\n",
      "91/10:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "print(list(img_text.values())[0])\n",
      "img_text = [[i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in list(img_text.values())]\n",
      "\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(df[\"body_lemmatized\"].sum())\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "\n",
      "for key, cnts in list(img_most_common.items()):\n",
      "  if not key in emoji.UNICODE_EMOJI and (key in stopwords.words('english') or not re.match(r'\\w+',key)):\n",
      "    del img_most_common[key]\n",
      "img_most_common = img_most_common.most_common(100)\n",
      "91/11:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "print(list(img_text.values())[0].lower())\n",
      "img_text = [[i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in list(img_text.values())]\n",
      "\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(df[\"body_lemmatized\"].sum())\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "\n",
      "for key, cnts in list(img_most_common.items()):\n",
      "  if not key in emoji.UNICODE_EMOJI and (key in stopwords.words('english') or not re.match(r'\\w+',key)):\n",
      "    del img_most_common[key]\n",
      "img_most_common = img_most_common.most_common(100)\n",
      "91/12:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "print(str(list(img_text.values())[0]).lower())\n",
      "img_text = [[i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in list(img_text.values())]\n",
      "\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(df[\"body_lemmatized\"].sum())\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "\n",
      "for key, cnts in list(img_most_common.items()):\n",
      "  if not key in emoji.UNICODE_EMOJI and (key in stopwords.words('english') or not re.match(r'\\w+',key)):\n",
      "    del img_most_common[key]\n",
      "img_most_common = img_most_common.most_common(100)\n",
      "91/13:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "img_text = [[i.lemma_ for i in nlp(str(x).lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(df[\"body_lemmatized\"].sum())\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "\n",
      "for key, cnts in list(img_most_common.items()):\n",
      "  if not key in emoji.UNICODE_EMOJI and (key in stopwords.words('english') or not re.match(r'\\w+',key)):\n",
      "    del img_most_common[key]\n",
      "img_most_common = img_most_common.most_common(100)\n",
      "91/14:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "img_text = [[i.lemma_ for i in nlp(str(x).lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "91/15:\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(\"\".join(img_text).split(\" \")\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "91/16:\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(\"\".join(img_text).split(\" \"))\n",
      "print(img_most_common)\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "91/17:\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(\"\".join(str(img_text)).split(\" \"))\n",
      "print(img_most_common)\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "91/18:\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(\"\".join(str(img_text)).split(\" \"))\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "91/19:\n",
      "print(img_text)\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(\"\".join(str(img_text)).split(\" \"))\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "91/20:\n",
      "print(img_text[0])\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(\"\".join(str(img_text)).split(\" \"))\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "91/21:\n",
      "print(img_text[0][0])\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(\"\".join(str(img_text)).split(\" \"))\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "91/22:\n",
      "print(img_text[0][0])\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(\"\".join(str(img_text)).split(\" \"))\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "91/23:\n",
      "print(img_text[0])\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(\"\".join(str(img_text)).split(\" \"))\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "91/24:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "print(img_text.values()[0])\n",
      "#img_text = [[i.lemma_ for i in nlp(str(x).lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "91/25:\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "df[\"body\"] = df[\"body\"].fillna('')\n",
      "df[\"title\"] = df[\"title\"].fillna('')\n",
      "img_clusters = pickle.load(open(\"../../pickle/image_color_clusters.pkl\", \"rb\"))\n",
      "img_means = pickle.load(open(\"../../pickle/image_hsv_means.pkl\", \"rb\"))\n",
      "img_text = pickle.load(open(\"../../pickle/texts.pkl\", \"rb\"))\n",
      "img_classes = pickle.load(open(\"../../pickle/image_classes.pkl\", \"rb\"))\n",
      "91/26:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "print(img_text.values()[0])\n",
      "#img_text = [[i.lemma_ for i in nlp(str(x).lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "91/27:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "print(list(img_text.values())[0])\n",
      "#img_text = [[i.lemma_ for i in nlp(str(x).lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "91/28:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "for key, value in img_text.items():\n",
      "    img_text[key] = [[i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text[key]]\n",
      "print(img_text)\n",
      "#img_text = [[i.lemma_ for i in nlp(str(x).lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "91/29:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "i=0\n",
      "for key, value in img_text.items():\n",
      "    print(str(i)+'/'+str(len(img_text.items())))\n",
      "    img_text[key] = [[i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text[key]]\n",
      "    i++\n",
      "print(img_text)\n",
      "#img_text = [[i.lemma_ for i in nlp(str(x).lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "91/30:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "i=0\n",
      "for key, value in img_text.items():\n",
      "    print(str(i)+'/'+str(len(img_text.items())))\n",
      "    img_text[key] = [[i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text[key]]\n",
      "    i+=1\n",
      "print(img_text)\n",
      "#img_text = [[i.lemma_ for i in nlp(str(x).lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "91/31:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "i=0\n",
      "for key, value in img_text.items():\n",
      "    print(str(i)+'/'+str(len(img_text.items())))\n",
      "    print(img_text[key])\n",
      "    img_text[key] = [[i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text[key]]\n",
      "    i+=1\n",
      "print(img_text)\n",
      "#img_text = [[i.lemma_ for i in nlp(str(x).lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "91/32:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "i=0\n",
      "for key, value in img_text.items():\n",
      "    print(str(i)+'/'+str(len(img_text.items())))\n",
      "    print(value)\n",
      "    img_text[key] = [[i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text[key]]\n",
      "    i+=1\n",
      "print(img_text)\n",
      "#img_text = [[i.lemma_ for i in nlp(str(x).lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "91/33:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "i=0\n",
      "for key, value in img_text.items():\n",
      "    print(str(i)+'/'+str(len(img_text.items())))\n",
      "    print(np.array(value).flatten())\n",
      "    img_text[key] = [[i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in value]\n",
      "    i+=1\n",
      "print(img_text)\n",
      "#img_text = [[i.lemma_ for i in nlp(str(x).lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "91/34:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "i=0\n",
      "for key, value in img_text.items():\n",
      "    print(str(i)+'/'+str(len(img_text.items())))\n",
      "    print(np.array(value).ravel())\n",
      "    img_text[key] = [[i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in value]\n",
      "    i+=1\n",
      "print(img_text)\n",
      "#img_text = [[i.lemma_ for i in nlp(str(x).lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "91/35:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "i=0\n",
      "for key, value in img_text.items():\n",
      "    print(str(i)+'/'+str(len(img_text.items())))\n",
      "    print(np.array(value[0]).ravel())\n",
      "    img_text[key] = [[i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in value]\n",
      "    i+=1\n",
      "print(img_text)\n",
      "#img_text = [[i.lemma_ for i in nlp(str(x).lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "91/36:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "i=0\n",
      "for key, value in img_text.items():\n",
      "    print(str(i)+'/'+str(len(img_text.items())))\n",
      "    print(np.array(value).ravel())\n",
      "    img_text[key] = [[i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in value]\n",
      "    i+=1\n",
      "print(img_text)\n",
      "#img_text = [[i.lemma_ for i in nlp(str(x).lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "91/37:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "i=0\n",
      "print(img_text)\n",
      "for key, value in img_text.items():\n",
      "    print(str(i)+'/'+str(len(img_text.items())))\n",
      "    print(np.array(value).ravel())\n",
      "    img_text[key] = [[i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in value]\n",
      "    i+=1\n",
      "print(img_text)\n",
      "#img_text = [[i.lemma_ for i in nlp(str(x).lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "91/38:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "i=0\n",
      "print(img_text.items[0])\n",
      "for key, value in img_text.items():\n",
      "    print(str(i)+'/'+str(len(img_text.items())))\n",
      "    print(np.array(value).ravel())\n",
      "    img_text[key] = [[i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in value]\n",
      "    i+=1\n",
      "print(img_text)\n",
      "#img_text = [[i.lemma_ for i in nlp(str(x).lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "91/39:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "i=0\n",
      "print(img_text.values[0])\n",
      "for key, value in img_text.items():\n",
      "    print(str(i)+'/'+str(len(img_text.items())))\n",
      "    print(np.array(value).ravel())\n",
      "    img_text[key] = [[i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in value]\n",
      "    i+=1\n",
      "print(img_text)\n",
      "#img_text = [[i.lemma_ for i in nlp(str(x).lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "91/40:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "i=0\n",
      "print(list(img_text.items)[0])\n",
      "for key, value in img_text.items():\n",
      "    print(str(i)+'/'+str(len(img_text.items())))\n",
      "    print(np.array(value).ravel())\n",
      "    img_text[key] = [[i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in value]\n",
      "    i+=1\n",
      "print(img_text)\n",
      "#img_text = [[i.lemma_ for i in nlp(str(x).lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "91/41:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "i=0\n",
      "print(list(img_text.items())[0])\n",
      "for key, value in img_text.items():\n",
      "    print(str(i)+'/'+str(len(img_text.items())))\n",
      "    print(np.array(value).ravel())\n",
      "    img_text[key] = [[i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in value]\n",
      "    i+=1\n",
      "print(img_text)\n",
      "#img_text = [[i.lemma_ for i in nlp(str(x).lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "91/42:\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "df[\"body\"] = df[\"body\"].fillna('')\n",
      "df[\"title\"] = df[\"title\"].fillna('')\n",
      "img_clusters = pickle.load(open(\"../../pickle/image_color_clusters.pkl\", \"rb\"))\n",
      "img_means = pickle.load(open(\"../../pickle/image_hsv_means.pkl\", \"rb\"))\n",
      "img_text = pickle.load(open(\"../../pickle/texts.pkl\", \"rb\"))\n",
      "img_classes = pickle.load(open(\"../../pickle/image_classes.pkl\", \"rb\"))\n",
      "91/43:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "i=0\n",
      "print(list(img_text.items())[0])\n",
      "for key, value in img_text.items():\n",
      "    print(str(i)+'/'+str(len(img_text.items())))\n",
      "    print(np.array(value).ravel())\n",
      "    img_text[key] = [[i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in value]\n",
      "    i+=1\n",
      "print(img_text)\n",
      "#img_text = [[i.lemma_ for i in nlp(str(x).lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "91/44:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "i=0\n",
      "print(list(img_text.items())[0])\n",
      "for key, value in img_text.items():\n",
      "    print(str(i)+'/'+str(len(img_text.items())))\n",
      "    print(np.array(value).ravel())\n",
      "    img_text[key] = [[i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in value]\n",
      "    i+=1\n",
      "print(img_text)\n",
      "#img_text = [[i.lemma_ for i in nlp(str(x).lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "91/45:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "i=0\n",
      "for key, value in img_text.items():\n",
      "    print(value)\n",
      "    print(str(i)+'/'+str(len(img_text.items())))\n",
      "    img_text[key] = [[i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in value]\n",
      "    i+=1\n",
      "print(img_text)\n",
      "#img_text = [[i.lemma_ for i in nlp(str(x).lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "91/46:\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "df[\"body\"] = df[\"body\"].fillna('')\n",
      "df[\"title\"] = df[\"title\"].fillna('')\n",
      "img_clusters = pickle.load(open(\"../../pickle/image_color_clusters.pkl\", \"rb\"))\n",
      "img_means = pickle.load(open(\"../../pickle/image_hsv_means.pkl\", \"rb\"))\n",
      "img_text = pickle.load(open(\"../../pickle/texts.pkl\", \"rb\"))\n",
      "img_classes = pickle.load(open(\"../../pickle/image_classes.pkl\", \"rb\"))\n",
      "91/47:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "i=0\n",
      "for key, value in img_text.items():\n",
      "    print(value)\n",
      "    print(str(i)+'/'+str(len(img_text.items())))\n",
      "    img_text[key] = [[i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in value]\n",
      "    i+=1\n",
      "print(img_text)\n",
      "#img_text = [[i.lemma_ for i in nlp(str(x).lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "91/48:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "i=0\n",
      "for key, value in img_text.items():\n",
      "    print(value)\n",
      "    print(str(i)+'/'+str(len(img_text.items())))\n",
      "    img_text[key] = [[i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in \"\".join(value)]\n",
      "    i+=1\n",
      "print(img_text)\n",
      "#img_text = [[i.lemma_ for i in nlp(str(x).lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "91/49:\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "df[\"body\"] = df[\"body\"].fillna('')\n",
      "df[\"title\"] = df[\"title\"].fillna('')\n",
      "img_clusters = pickle.load(open(\"../../pickle/image_color_clusters.pkl\", \"rb\"))\n",
      "img_means = pickle.load(open(\"../../pickle/image_hsv_means.pkl\", \"rb\"))\n",
      "img_text = pickle.load(open(\"../../pickle/texts.pkl\", \"rb\"))\n",
      "img_classes = pickle.load(open(\"../../pickle/image_classes.pkl\", \"rb\"))\n",
      "91/50:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "i=0\n",
      "for key, value in img_text.items():\n",
      "    print(value)\n",
      "    print(str(i)+'/'+str(len(img_text.items())))\n",
      "    img_text[key] = [[i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in \"\".join(value)]\n",
      "    i+=1\n",
      "print(img_text)\n",
      "#img_text = [[i.lemma_ for i in nlp(str(x).lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "91/51:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "i=0\n",
      "for key, value in img_text.items():\n",
      "    print(value)\n",
      "    print(str(i)+'/'+str(len(img_text.items())))\n",
      "    img_text[key] = [[i.lemma_ for i in nlp(x.lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in \"\".join(value)]\n",
      "    i+=1\n",
      "print(img_text)\n",
      "#img_text = [[i.lemma_ for i in nlp(str(x).lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "91/52:\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "df[\"body\"] = df[\"body\"].fillna('')\n",
      "df[\"title\"] = df[\"title\"].fillna('')\n",
      "img_clusters = pickle.load(open(\"../../pickle/image_color_clusters.pkl\", \"rb\"))\n",
      "img_means = pickle.load(open(\"../../pickle/image_hsv_means.pkl\", \"rb\"))\n",
      "img_text = pickle.load(open(\"../../pickle/texts.pkl\", \"rb\"))\n",
      "img_classes = pickle.load(open(\"../../pickle/image_classes.pkl\", \"rb\"))\n",
      "91/53:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "i=0\n",
      "for key, value in img_text.items():\n",
      "    print(value)\n",
      "    print(str(i)+'/'+str(len(img_text.items())))\n",
      "    img_text[key] = [i.lemma_ for i in nlp(\"\".join(value).lower()) if (not i.is_stop) and len(i.lemma_) > 0]\n",
      "    i+=1\n",
      "print(img_text)\n",
      "#img_text = [[i.lemma_ for i in nlp(str(x).lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "91/54:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "i=0\n",
      "for key, value in img_text.items():\n",
      "    print(value)\n",
      "    print(str(i)+'/'+str(len(img_text.items())))\n",
      "    img_text[key] = [i.lemma_ for i in nlp(\"\".join(value).lower()) if (not i.is_stop) and len(i.lemma_) > 0]\n",
      "    i+=1\n",
      "print(img_text)\n",
      "#img_text = [[i.lemma_ for i in nlp(str(x).lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "91/55:\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "df[\"body\"] = df[\"body\"].fillna('')\n",
      "df[\"title\"] = df[\"title\"].fillna('')\n",
      "img_clusters = pickle.load(open(\"../../pickle/image_color_clusters.pkl\", \"rb\"))\n",
      "img_means = pickle.load(open(\"../../pickle/image_hsv_means.pkl\", \"rb\"))\n",
      "img_text = pickle.load(open(\"../../pickle/texts.pkl\", \"rb\"))\n",
      "img_classes = pickle.load(open(\"../../pickle/image_classes.pkl\", \"rb\"))\n",
      "91/56:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "i=0\n",
      "for key, value in img_text.items():\n",
      "    print(value)\n",
      "    print(str(i)+'/'+str(len(img_text.items())))\n",
      "    img_text[key] = [i.lemma_ for i in nlp(\"\".join(value).lower()) if (not i.is_stop) and len(i.lemma_) > 0]\n",
      "    print(img_text[key])\n",
      "    i+=1\n",
      "print(img_text)\n",
      "#img_text = [[i.lemma_ for i in nlp(str(x).lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "91/57:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "i=0\n",
      "for key, value in img_text.items():\n",
      "    print(value)\n",
      "    print(str(i)+'/'+str(len(img_text.items())))\n",
      "    img_text[key] = [i.lemma_ for i in nlp(value) if (not i.is_stop) and len(i.lemma_) > 0]\n",
      "    print(img_text[key])\n",
      "    i+=1\n",
      "print(img_text)\n",
      "#img_text = [[i.lemma_ for i in nlp(str(x).lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "91/58:\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "df[\"body\"] = df[\"body\"].fillna('')\n",
      "df[\"title\"] = df[\"title\"].fillna('')\n",
      "img_clusters = pickle.load(open(\"../../pickle/image_color_clusters.pkl\", \"rb\"))\n",
      "img_means = pickle.load(open(\"../../pickle/image_hsv_means.pkl\", \"rb\"))\n",
      "img_text = pickle.load(open(\"../../pickle/texts.pkl\", \"rb\"))\n",
      "img_classes = pickle.load(open(\"../../pickle/image_classes.pkl\", \"rb\"))\n",
      "91/59:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "i=0\n",
      "for key, value in img_text.items():\n",
      "    print(value)\n",
      "    print(str(i)+'/'+str(len(img_text.items())))\n",
      "    img_text[key] = [i.lemma_ for i in nlp(value) if (not i.is_stop) and len(i.lemma_) > 0]\n",
      "    print(img_text[key])\n",
      "    i+=1\n",
      "print(img_text)\n",
      "#img_text = [[i.lemma_ for i in nlp(str(x).lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "91/60:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "i=0\n",
      "for key, value in img_text.items():\n",
      "    print(value)\n",
      "    print(str(i)+'/'+str(len(img_text.items())))\n",
      "    img_text[key] = [i.lemma_ for i in nlp(\" \".join(value)) if (not i.is_stop) and len(i.lemma_) > 0]\n",
      "    print(img_text[key])\n",
      "    i+=1\n",
      "print(img_text)\n",
      "#img_text = [[i.lemma_ for i in nlp(str(x).lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "91/61:\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "df[\"body\"] = df[\"body\"].fillna('')\n",
      "df[\"title\"] = df[\"title\"].fillna('')\n",
      "img_clusters = pickle.load(open(\"../../pickle/image_color_clusters.pkl\", \"rb\"))\n",
      "img_means = pickle.load(open(\"../../pickle/image_hsv_means.pkl\", \"rb\"))\n",
      "img_text = pickle.load(open(\"../../pickle/texts.pkl\", \"rb\"))\n",
      "img_classes = pickle.load(open(\"../../pickle/image_classes.pkl\", \"rb\"))\n",
      "91/62:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "i=0\n",
      "for key, value in img_text.items():\n",
      "    print(value)\n",
      "    print(str(i)+'/'+str(len(img_text.items())))\n",
      "    img_text[key] = [i.lemma_ for i in nlp(\" \".join(value).lower()) if (not i.is_stop) and len(i.lemma_) > 0]\n",
      "    print(img_text[key])\n",
      "    i+=1\n",
      "print(img_text)\n",
      "#img_text = [[i.lemma_ for i in nlp(str(x).lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "91/63:\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "df[\"body\"] = df[\"body\"].fillna('')\n",
      "df[\"title\"] = df[\"title\"].fillna('')\n",
      "img_clusters = pickle.load(open(\"../../pickle/image_color_clusters.pkl\", \"rb\"))\n",
      "img_means = pickle.load(open(\"../../pickle/image_hsv_means.pkl\", \"rb\"))\n",
      "img_text = pickle.load(open(\"../../pickle/texts.pkl\", \"rb\"))\n",
      "img_classes = pickle.load(open(\"../../pickle/image_classes.pkl\", \"rb\"))\n",
      "91/64:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "i=0\n",
      "for key, value in img_text.items():\n",
      "    print(str(i)+'/'+str(len(img_text.items())))\n",
      "    img_text[key] = [i.lemma_ for i in nlp(\" \".join(value).lower()) if (not i.is_stop) and len(i.lemma_) > 0]\n",
      "    i+=1\n",
      "print(img_text)\n",
      "#img_text = [[i.lemma_ for i in nlp(str(x).lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "91/65:\n",
      "print(img_text[0])\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(\"\".join(str(img_text)).split(\" \"))\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "91/66:\n",
      "print(img_text)\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(\"\".join(str(img_text)).split(\" \"))\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "91/67:\n",
      "print(img_text)\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(\" \".join(str(img_text)).split(\" \"))\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "91/68:\n",
      "print(img_text)\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(\" \".join(str(img_text)).split(\" \"))\n",
      "print(img_most_common)\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "91/69:\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "df[\"body\"] = df[\"body\"].fillna('')\n",
      "df[\"title\"] = df[\"title\"].fillna('')\n",
      "img_clusters = pickle.load(open(\"../../pickle/image_color_clusters.pkl\", \"rb\"))\n",
      "img_means = pickle.load(open(\"../../pickle/image_hsv_means.pkl\", \"rb\"))\n",
      "img_text = pickle.load(open(\"../../pickle/texts.pkl\", \"rb\"))\n",
      "img_classes = pickle.load(open(\"../../pickle/image_classes.pkl\", \"rb\"))\n",
      "91/70:\n",
      "from collections import Counter\n",
      "import string\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "from spacy.lang.en.examples import sentences \n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "i=0\n",
      "for key, value in img_text.items():\n",
      "    print(str(i)+'/'+str(len(img_text.items())))\n",
      "    img_text[key] = [i.lemma_ for i in nlp(\" \".join(value).lower()) if (not i.is_stop) and len(i.lemma_) > 0]\n",
      "    print(img_text[key])\n",
      "    i+=1\n",
      "print(img_text)\n",
      "#img_text = [[i.lemma_ for i in nlp(str(x).lower()) if (not i.is_stop) and len(i.lemma_) > 0] for x in img_text.values()]\n",
      "91/71:\n",
      "print(img_text)\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(\" \".join(\" \".join(img_text)).split(\" \"))\n",
      "print(img_most_common)\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "91/72:\n",
      "print(img_text)\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(\" \".join(\" \".join(img_text)).split(\" \"))\n",
      "print(img_most_common[0])\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "91/73:\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(\" \".join(\" \".join(img_text)).split(\" \"))\n",
      "print(img_most_common[0])\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "91/74:\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(\" \".join(\" \".join(img_text)).split(\" \"))\n",
      "print(img_most_common)\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "91/75:\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "print(\" \".join(\" \".join(img_text)))\n",
      "img_most_common = Counter(\" \".join(\" \".join(img_text)).split(\" \"))\n",
      "print(img_most_common)\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "91/76:\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "print(\" \".join(img_text))\n",
      "img_most_common = Counter(\" \".join(\" \".join(img_text)).split(\" \"))\n",
      "print(img_most_common)\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "91/77:\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "print(\" \".join(img_text.values()[0]))\n",
      "img_most_common = Counter(\" \".join(\" \".join(img_text)).split(\" \"))\n",
      "print(img_most_common)\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "91/78:\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "print(\" \".join(list(img_text.values())[0]))\n",
      "img_most_common = Counter(\" \".join(\" \".join(img_text)).split(\" \"))\n",
      "print(img_most_common)\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91/79:\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "print(\" \".join(list(img_text.values())))\n",
      "img_most_common = Counter(\" \".join(\" \".join(img_text)).split(\" \"))\n",
      "print(img_most_common)\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "91/80:\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "print(\" \".join(list(img_text.values()).map(lambda x: \" \".join(x))))\n",
      "img_most_common = Counter(\" \".join(\" \".join(img_text)).split(\" \"))\n",
      "print(img_most_common)\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "91/81:\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "print(\" \".join(img_text.values().map(lambda x: \" \".join(x))))\n",
      "img_most_common = Counter(\" \".join(\" \".join(img_text)).split(\" \"))\n",
      "print(img_most_common)\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "91/82:\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "print(\" \".join(list(img_text.values()).map(lambda x: \" \".join(x))))\n",
      "img_most_common = Counter(\" \".join(\" \".join(img_text)).split(\" \"))\n",
      "print(img_most_common)\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "91/83:\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "print(\" \".join(.map(lambda x: \" \".join(x), list(img_text.values()))))\n",
      "img_most_common = Counter(\" \".join(\" \".join(img_text)).split(\" \"))\n",
      "print(img_most_common)\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "91/84:\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "print(\" \".join(map(lambda x: \" \".join(x), list(img_text.values()))))\n",
      "img_most_common = Counter(\" \".join(\" \".join(img_text)).split(\" \"))\n",
      "print(img_most_common)\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "91/85:\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "print(map(lambda x: \" \".join(x), list(img_text.values()))[0])\n",
      "img_most_common = Counter(\" \".join(\" \".join(img_text)).split(\" \"))\n",
      "print(img_most_common)\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "91/86:\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "print(\" \".join(list(map(lambda x: \" \".join(x), list(img_text.values())))))\n",
      "img_most_common = Counter(\" \".join(\" \".join(img_text)).split(\" \"))\n",
      "print(img_most_common)\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "91/87:\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "print(list(map(lambda x: \" \".join(x), list(img_text.values())))[0])\n",
      "img_most_common = Counter(\" \".join(list(map(lambda x: \" \".join(x), list(img_text.values())))))\n",
      "print(img_most_common)\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "91/88:\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "print(list(map(lambda x: \" \".join(x), list(img_text.values())))[0])\n",
      "# img_most_common = Counter(\" \".join(list(map(lambda x: \" \".join(x), list(img_text.values())))))\n",
      "# print(img_most_common)\n",
      "# prune(img_most_common)\n",
      "# img_most_common.most_common(100)\n",
      "91/89:\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "print(\" \".join(list(map(lambda x: \" \".join(x), list(img_text.values())))))\n",
      "# img_most_common = Counter(\" \".join(list(map(lambda x: \" \".join(x), list(img_text.values())))))\n",
      "# print(img_most_common)\n",
      "# prune(img_most_common)\n",
      "# img_most_common.most_common(100)\n",
      "91/90:\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(\" \".join(list(map(lambda x: \" \".join(x), list(img_text.values())))))\n",
      "# print(img_most_common)\n",
      "# prune(img_most_common)\n",
      "# img_most_common.most_common(100)\n",
      "91/91:\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(\" \".join(list(map(lambda x: \" \".join(x), list(img_text.values())))))\n",
      "print(img_most_common)\n",
      "# prune(img_most_common)\n",
      "# img_most_common.most_common(100)\n",
      "91/92:\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(\" \".join(list(map(lambda x: \" \".join(x), list(img_text.values())))).split(\" \"))\n",
      "print(img_most_common)\n",
      "# prune(img_most_common)\n",
      "# img_most_common.most_common(100)\n",
      "91/93:\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(\" \".join(list(map(lambda x: \" \".join(x), list(img_text.values())))).split(\" \"))\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "91/94:\n",
      "pic_data = {}\n",
      "pic_data['id'] = []\n",
      "pic_data['text'] = []\n",
      "pic_data['clusters'] = []\n",
      "pic_data['means'] = []\n",
      "pic_data['score'] = []\n",
      "pic_data['classses'] = []\n",
      "\n",
      "for key in img_means.keys():\n",
      "  pic_data['id'].append(key)\n",
      "  pic_data['text'].append(\" \".join(img_text[key[:-4]]))\n",
      "  pic_data['clusters'].append(img_clusters[key])\n",
      "  pic_data['means'].append(img_means[key])\n",
      "  pic_data['score'].append(df.loc[df['id'] == key[:-4]]['score'].values[0])\n",
      "  pic_data['classses'].append(img_classes[key]['all_prob'][0])\n",
      "\n",
      "pic_data = pandas.DataFrame(pic_data)\n",
      "91/95:\n",
      "temp = {}\n",
      "for i in range(1000):\n",
      "    temp['class_'+str(i)] = pic_data['classses'].apply(lambda s: s[i])\n",
      "    \n",
      "temp = pandas.DataFrame(temp)\n",
      "print(len(temp.columns))\n",
      "for column in temp:\n",
      "    if np.max(temp[column])<0.1:\n",
      "        del temp[column]\n",
      "\n",
      "print(len(temp.columns))\n",
      "91/96:\n",
      "pic_attributes = {'id': pic_data['id']}\n",
      "for key, cnts in img_most_common: \n",
      "  pic_attributes['text_' + key] =  pic_data['text'].str.lower().str.count(key.lower())\n",
      "\n",
      "pic_attributes['h_sin'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['h_cos'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['s'] = pic_data['means'].apply(lambda s: s[1]/255)\n",
      "pic_attributes['v'] = pic_data['means'].apply(lambda s: s[2]/255)\n",
      "pic_attributes['score'] = pic_data['score']\n",
      "for j in range(5):\n",
      "  for i in range(3):\n",
      "    pic_attributes['color_'+str(j)+'_'+'rgb'[i]] = pic_data['clusters'].apply(lambda s: s['color'][j][i])\n",
      "  pic_attributes['color_'+str(j)+'_%'] = pic_data['clusters'].apply(lambda s: s['percentage'][j])\n",
      "\n",
      "for i in temp.columns:\n",
      "    pic_attributes[i] = temp[i]\n",
      "\n",
      "pic_attributes = pandas.DataFrame(pic_attributes)\n",
      "print(pic_attributes)\n",
      "91/97:\n",
      "pic_attributes = {'id': pic_data['id']}\n",
      "for key, cnts in img_most_common.most_common(100): \n",
      "  pic_attributes['text_' + key] =  pic_data['text'].str.lower().str.count(key.lower())\n",
      "\n",
      "pic_attributes['h_sin'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['h_cos'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['s'] = pic_data['means'].apply(lambda s: s[1]/255)\n",
      "pic_attributes['v'] = pic_data['means'].apply(lambda s: s[2]/255)\n",
      "pic_attributes['score'] = pic_data['score']\n",
      "for j in range(5):\n",
      "  for i in range(3):\n",
      "    pic_attributes['color_'+str(j)+'_'+'rgb'[i]] = pic_data['clusters'].apply(lambda s: s['color'][j][i])\n",
      "  pic_attributes['color_'+str(j)+'_%'] = pic_data['clusters'].apply(lambda s: s['percentage'][j])\n",
      "\n",
      "for i in temp.columns:\n",
      "    pic_attributes[i] = temp[i]\n",
      "\n",
      "pic_attributes = pandas.DataFrame(pic_attributes)\n",
      "print(pic_attributes)\n",
      "91/98: corr_image_matrix = pic_attributes.corr(method='spearman')\n",
      "91/99:\n",
      "correlations = []\n",
      "for i,column in enumerate(corr_image_matrix):\n",
      "  x = corr_image_matrix[column][i+1:]\n",
      "  for row,v in x.items():\n",
      "    correlations.append([v,column,row])\n",
      "correlations.sort(key = lambda x: -abs(x[0]))\n",
      "91/100:\n",
      "score_corr = corr_image_matrix['score'][corr_image_matrix['score']<1]\n",
      "for corr in correlations:\n",
      "  if len(pic_attributes.columns) <= 200: break\n",
      "  c,a1,a2 = corr\n",
      "  if 'score' in [a1,a2]: continue\n",
      "  if a1 in pic_attributes and a2 in pic_attributes:\n",
      "    to_be_removed = a1 if score_corr[a1] < score_corr[a2] else a2\n",
      "    del pic_attributes[to_be_removed]\n",
      "91/101:\n",
      "rf = RandomForestRegressor()\n",
      "rf.fit(pic_attributes[pic_attributes.columns.difference(['id', 'score'])], pic_attributes['score'])\n",
      "print(sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), pic_attributes.head()), \n",
      "             reverse=True))\n",
      "91/102:\n",
      "result = sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), pic_attributes.head()), \n",
      "             reverse=True)\n",
      "91/103: pickle.dump(result, open( \"../../pickle/image_attributes_processed.pkl\", \"wb\" ))\n",
      "91/104:\n",
      "selected = []\n",
      "for x in result:\n",
      "    selected.append(x[0])\n",
      "selected_attributes = pic_attributes[selected]\n",
      "91/105:\n",
      "selected = []\n",
      "for x in result:\n",
      "    selected.append(x[1])\n",
      "selected_attributes = pic_attributes[selected]\n",
      "print(selected_attributes)\n",
      "91/106:\n",
      "selected = []\n",
      "for x in result[:30]:\n",
      "    selected.append(x[1])\n",
      "selected_attributes = pic_attributes[selected]\n",
      "print(selected_attributes)\n",
      "91/107: pickle.dump(selected_attributes, open( \"../../pickle/image_attributes_processed.pkl\", \"wb\" ))\n",
      "91/108:\n",
      "selected = []\n",
      "for x in result[:30]:\n",
      "    selected.append(x[1])\n",
      "selected_attributes = pic_attributes[['id'] + selected + ['score']]\n",
      "print(selected_attributes)\n",
      "91/109: pickle.dump(selected_attributes, open( \"../../pickle/image_attributes_processed.pkl\", \"wb\" ))\n",
      "94/1:\n",
      "import pandas\n",
      "import pickle\n",
      "import numpy as np\n",
      "\n",
      "attributes = pickle.load(open( \"../../pickle/image_attributes_processed.pkl\", \"rb\" ))\n",
      "94/2:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import r2_score,mean_squared_error\n",
      "\n",
      "train, test = train_test_split(df, test_size=0.2)\n",
      "\n",
      "rf = RandomForestRegressor()\n",
      "rf.fit(pic_attributes[train.columns.difference(['id', 'score'])], train['score'])\n",
      "94/3:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import r2_score,mean_squared_error\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "\n",
      "rf = RandomForestRegressor()\n",
      "rf.fit(pic_attributes[train.columns.difference(['id', 'score'])], train['score'])\n",
      "94/4:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import r2_score,mean_squared_error\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "\n",
      "rf = RandomForestRegressor()\n",
      "rf.fit(train[train.columns.difference(['id', 'score'])], train['score'])\n",
      "94/5:\n",
      "y_pred = rf.predict(test[test.columns.difference(['id', 'score'])])\n",
      "\n",
      "mse = mean_squared_error(y_pred, test['score'])\n",
      "rmse = np.sqrt(mse)\n",
      "rmse\n",
      "96/1:\n",
      "import pandas\n",
      "import pickle\n",
      "import numpy as np\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "96/2: df = pickle.load(open(\"../../pickle/text_attr.pkl\", \"rb\"))\n",
      "96/3:\n",
      "reduced_attr_count = 200\n",
      "corr_matrix = df.corr(method='spearman')\n",
      "96/4:\n",
      "correlations = []\n",
      "for i,column in enumerate(corr_matrix):\n",
      "  x = corr_matrix[column][i+1:]\n",
      "  for row,v in x.items():\n",
      "    correlations.append([v,column,row])\n",
      "correlations.sort(key = lambda x: -abs(x[0]))\n",
      "96/5:\n",
      "score_corr = corr_matrix['score'][corr_matrix['score']<1]\n",
      "for corr in correlations:\n",
      "  if len(df.columns) <= reduced_attr_count: break\n",
      "  c,a1,a2 = corr\n",
      "  if 'score' in [a1,a2]: continue\n",
      "  if a1 in df and a2 in df:\n",
      "    to_be_removed = a1 if score_corr[a1] < score_corr[a2] else a2\n",
      "    del df[to_be_removed]\n",
      "96/6:\n",
      "final_attr_count = 30\n",
      "rf = RandomForestRegressor()\n",
      "rf.fit(df[df.columns.difference(['id', 'score'])], df['score'])\n",
      "\n",
      "result = sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), df.head()), \n",
      "             reverse=True)\n",
      "result\n",
      "96/7:\n",
      "df = pickle.load(open(\"../../pickle/text_attr.pkl\", \"rb\"))\n",
      "print(df.head())\n",
      "96/8:\n",
      "reduced_attr_count = 100\n",
      "corr_matrix = df.corr(method='spearman')\n",
      "96/9:\n",
      "correlations = []\n",
      "for i,column in enumerate(corr_matrix):\n",
      "  x = corr_matrix[column][i+1:]\n",
      "  for row,v in x.items():\n",
      "    correlations.append([v,column,row])\n",
      "correlations.sort(key = lambda x: -abs(x[0]))\n",
      "96/10:\n",
      "score_corr = corr_matrix['score'][corr_matrix['score']<1]\n",
      "for corr in correlations:\n",
      "  if len(df.columns) <= reduced_attr_count: break\n",
      "  c,a1,a2 = corr\n",
      "  if 'score' in [a1,a2]: continue\n",
      "  if a1 in df and a2 in df:\n",
      "    to_be_removed = a1 if score_corr[a1] < score_corr[a2] else a2\n",
      "    del df[to_be_removed]\n",
      "96/11:\n",
      "final_attr_count = 30\n",
      "rf = RandomForestRegressor()\n",
      "rf.fit(df[df.columns.difference(['id', 'score'])], df['score'])\n",
      "\n",
      "result = sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), df.head()), \n",
      "             reverse=True)\n",
      "result\n",
      "96/12:\n",
      "selected = []\n",
      "for x in result[:final_attr_count]:\n",
      "    selected.append(x[1])\n",
      "selected_attributes = df[['id'] + selected + ['score']]\n",
      "print(selected_attributes)\n",
      "96/13:\n",
      "selected = []\n",
      "for x in result[:final_attr_count]:\n",
      "    selected.append(x[1])\n",
      "selected_attributes = df[selected + ['score']]\n",
      "print(selected_attributes)\n",
      "96/14: pickle.dump(selected_attributes, open( \"../../pickle/text_attributes_processed.pkl\", \"wb\" ))\n",
      "97/1:\n",
      "import pandas\n",
      "import pickle\n",
      "import numpy as np\n",
      "\n",
      "attributes = pickle.load(open( \"../../pickle/text_attributes_processed.pkl\", \"rb\" ))\n",
      "97/2:\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import r2_score,mean_squared_error\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "\n",
      "rf = RandomForestRegressor()\n",
      "rf.fit(train[train.columns.difference(['score'])], train['score'])\n",
      "97/3:\n",
      "y_pred = rf.predict(test[test.columns.difference(['score'])])\n",
      "\n",
      "mse = mean_squared_error(y_pred, test['score'])\n",
      "rmse = np.sqrt(mse)\n",
      "rmse\n",
      "99/1:\n",
      "import pandas\n",
      "import pickle\n",
      "import numpy as np\n",
      "\n",
      "attributes = pickle.load(open( \"../../pickle/image_attributes_processed.pkl\", \"rb\" ))\n",
      "99/2:\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import r2_score,mean_squared_error\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "\n",
      "nn = MLPRegressor(random_state=1, max_iter=500)\n",
      "nn.fit(train[train.columns.difference(['id', 'score'])], train['score'])\n",
      "99/3:\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import r2_score,mean_squared_error\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "\n",
      "nn = MLPRegressor(random_state=1)\n",
      "nn.fit(train[train.columns.difference(['id', 'score'])], train['score'])\n",
      "99/4:\n",
      "y_pred = nn.predict(test[test.columns.difference(['id', 'score'])])\n",
      "\n",
      "mse = mean_squared_error(y_pred, test['score'])\n",
      "rmse = np.sqrt(mse)\n",
      "rmse\n",
      "99/5:\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import r2_score,mean_squared_error\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "\n",
      "nn = MLPRegressor(hidden_layer_sizes=(30,60,30,))\n",
      "nn.fit(train[train.columns.difference(['id', 'score'])], train['score'])\n",
      "99/6:\n",
      "y_pred = nn.predict(test[test.columns.difference(['id', 'score'])])\n",
      "\n",
      "mse = mean_squared_error(y_pred, test['score'])\n",
      "rmse = np.sqrt(mse)\n",
      "rmse\n",
      "100/1:\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import r2_score,mean_squared_error\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "\n",
      "for i in [(30,), (60,), (30,60), (30,60,15)]:\n",
      "    mlp = MLPRegressor(hidden_layer_sizes=(100,))\n",
      "    mlp.fit(train[train.columns.difference(['id', 'score'])], train['score'])\n",
      "    y_pred = mlp.predict(test[test.columns.difference(['id', 'score'])])\n",
      "\n",
      "    mse = mean_squared_error(y_pred, test['score'])\n",
      "    rmse = np.sqrt(mse)\n",
      "    rmse\n",
      "100/2:\n",
      "import pandas\n",
      "import pickle\n",
      "import numpy as np\n",
      "\n",
      "attributes = pickle.load(open( \"../../pickle/image_attributes_processed.pkl\", \"rb\" ))\n",
      "100/3:\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import r2_score,mean_squared_error\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "\n",
      "for i in [(30,), (60,), (30,60), (30,60,15)]:\n",
      "    mlp = MLPRegressor(hidden_layer_sizes=i)\n",
      "    mlp.fit(train[train.columns.difference(['id', 'score'])], train['score'])\n",
      "    y_pred = mlp.predict(test[test.columns.difference(['id', 'score'])])\n",
      "\n",
      "    mse = mean_squared_error(y_pred, test['score'])\n",
      "    rmse = np.sqrt(mse)\n",
      "    rmse\n",
      "100/4:\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import r2_score,mean_squared_error\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "\n",
      "for i in [(30,), (60,), (30,60), (30,60,15)]:\n",
      "    mlp = MLPRegressor(hidden_layer_sizes=i, max_iter=1000, learning_rate='adaptive')\n",
      "    mlp.fit(train[train.columns.difference(['id', 'score'])], train['score'])\n",
      "    y_pred = mlp.predict(test[test.columns.difference(['id', 'score'])])\n",
      "\n",
      "    mse = mean_squared_error(y_pred, test['score'])\n",
      "    rmse = np.sqrt(mse)\n",
      "    print(rmse)\n",
      "100/5:\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import r2_score,mean_squared_error\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "\n",
      "for i in [(15,), (30,), (60,), (30,60), (30,60,15)]:\n",
      "    mlp = MLPRegressor(hidden_layer_sizes=i, max_iter=1000, learning_rate='adaptive')\n",
      "    mlp.fit(train[train.columns.difference(['id', 'score'])], train['score'])\n",
      "    y_pred = mlp.predict(test[test.columns.difference(['id', 'score'])])\n",
      "\n",
      "    mse = mean_squared_error(y_pred, test['score'])\n",
      "    rmse = np.sqrt(mse)\n",
      "    print(rmse)\n",
      "100/6:\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import r2_score,mean_squared_error\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "\n",
      "for i in [(1,),(15,), (30,), (60,), (30,60), (30,60,15)]:\n",
      "    mlp = MLPRegressor(hidden_layer_sizes=i, max_iter=1000, learning_rate='adaptive')\n",
      "    mlp.fit(train[train.columns.difference(['id', 'score'])], train['score'])\n",
      "    y_pred = mlp.predict(test[test.columns.difference(['id', 'score'])])\n",
      "\n",
      "    mse = mean_squared_error(y_pred, test['score'])\n",
      "    rmse = np.sqrt(mse)\n",
      "    print(rmse)\n",
      "100/7:\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import r2_score,mean_squared_error\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "\n",
      "for i in [(1,),(15,), (30,), (60,), (30,60), (30,60,15)]:\n",
      "    mlp = MLPRegressor(hidden_layer_sizes=i, max_iter=300, learning_rate='adaptive')\n",
      "    mlp.fit(train[train.columns.difference(['id', 'score'])], train['score'])\n",
      "    y_pred = mlp.predict(test[test.columns.difference(['id', 'score'])])\n",
      "\n",
      "    mse = mean_squared_error(y_pred, test['score'])\n",
      "    rmse = np.sqrt(mse)\n",
      "    print(rmse)\n",
      "100/8:\n",
      "fig, axes = plt.subplots(4, 4)\n",
      "# use global min / max to ensure all weights are shown on the same scale\n",
      "vmin, vmax = mlp.coefs_[0].min(), mlp.coefs_[0].max()\n",
      "for coef, ax in zip(mlp.coefs_[0].T, axes.ravel()):\n",
      "    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=.5 * vmin,\n",
      "               vmax=.5 * vmax)\n",
      "    ax.set_xticks(())\n",
      "    ax.set_yticks(())\n",
      "\n",
      "plt.show()\n",
      "100/9:\n",
      "import matplotlib.pyplot as plt\n",
      "fig, axes = plt.subplots(4, 4)\n",
      "# use global min / max to ensure all weights are shown on the same scale\n",
      "vmin, vmax = mlp.coefs_[0].min(), mlp.coefs_[0].max()\n",
      "for coef, ax in zip(mlp.coefs_[0].T, axes.ravel()):\n",
      "    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=.5 * vmin,\n",
      "               vmax=.5 * vmax)\n",
      "    ax.set_xticks(())\n",
      "    ax.set_yticks(())\n",
      "\n",
      "plt.show()\n",
      "100/10:\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components=2)\n",
      "pca.fit(attributes)\n",
      "print(pca.components_ )\n",
      "100/11:\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components=2)\n",
      "pca.fit(attributes[attributes.columns.difference(['id', 'score'])])\n",
      "print(pca.components_ )\n",
      "100/12:\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components=2)\n",
      "pca.fit(attributes[attributes.columns.difference(['id', 'score'])])\n",
      "print(pca.transform(attributes[attributes.columns.difference(['id', 'score'])) )\n",
      "100/13:\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components=2)\n",
      "pca.fit(attributes[attributes.columns.difference(['id', 'score'])])\n",
      "print(pca.transform(attributes[attributes.columns.difference(['id', 'score'])]))\n",
      "100/14:\n",
      "fig = px.scatter_matrix(\n",
      "    components,\n",
      "    color=attributes['score']\n",
      ")\n",
      "fig.update_traces(diagonal_visible=False)\n",
      "fig.show()\n",
      "100/15:\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components=2)\n",
      "pca.fit(attributes[attributes.columns.difference(['id', 'score'])])\n",
      "components = pca.transform(attributes[attributes.columns.difference(['id', 'score'])])\n",
      "100/16:\n",
      "import plotly.express as px\n",
      "fig = px.scatter_matrix(\n",
      "    components,\n",
      "    color=attributes['score']\n",
      ")\n",
      "fig.update_traces(diagonal_visible=False)\n",
      "fig.show()\n",
      "100/17:\n",
      "import matplotlib.pyplot as plt\n",
      "ax = ax.scatter(components[0], components[1], attributes['score'], marker=m)\n",
      "plt.show()\n",
      "100/18:\n",
      "import matplotlib.pyplot as plt\n",
      "ax = ax.scatter(components[0], components[1], attributes['score'])\n",
      "plt.show()\n",
      "100/19:\n",
      "import matplotlib.pyplot as plt\n",
      "print(components)\n",
      "ax = ax.scatter(components[0], components[1], attributes['score'])\n",
      "plt.show()\n",
      "100/20:\n",
      "import matplotlib.pyplot as plt\n",
      "ax = ax.scatter(components.T[0], components.T[1], attributes['score'])\n",
      "plt.show()\n",
      "100/21:\n",
      "import matplotlib.pyplot as plt\n",
      "ax.scatter(components.T[0], components.T[1], attributes['score'])\n",
      "plt.show()\n",
      "100/22:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(components.T[0], components.T[1], attributes['score'])\n",
      "plt.show()\n",
      "100/23:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "print(components.T)\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(components.T[0], components.T[1], attributes['score'])\n",
      "plt.show()\n",
      "104/1:\n",
      "import pandas\n",
      "import pickle\n",
      "import numpy as np\n",
      "\n",
      "attributes = pickle.load(open( \"../../pickle/text_attributes_processed.pkl\", \"rb\" ))\n",
      "104/2:\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import r2_score,mean_squared_error\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "\n",
      "for i in [(1,), (15,), (30,), (60,), (30,60), (30,60,15)]:\n",
      "    mlp = MLPRegressor(hidden_layer_sizes=i, max_iter=300, learning_rate='adaptive')\n",
      "    mlp.fit(train[train.columns.difference(['score'])], train['score'])\n",
      "    y_pred = mlp.predict(test[test.columns.difference(['score'])])\n",
      "\n",
      "    mse = mean_squared_error(y_pred, test['score'])\n",
      "    rmse = np.sqrt(mse)\n",
      "    print(rmse)\n",
      "104/3:\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components=2)\n",
      "pca.fit(attributes[attributes.columns.difference(['score'])])\n",
      "print(pca.transform(attributes[attributes.columns.difference(['score'])) )\n",
      "104/4:\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components=2)\n",
      "pca.fit(attributes[attributes.columns.difference(['score'])])\n",
      "print(pca.transform(attributes[attributes.columns.difference(['score']))\n",
      "104/5:\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components=2)\n",
      "pca.fit(attributes[attributes.columns.difference(['score'])])\n",
      "print(pca.transform(attributes[attributes.columns.difference(['score'])]))\n",
      "104/6:\n",
      "from sklearn.decomposition import PCA\n",
      "\n",
      "pca = PCA(n_components=2)\n",
      "pca.fit(attributes[attributes.columns.difference(['id', 'score'])])\n",
      "components = pca.transform(attributes[attributes.columns.difference(['id', 'score'])])\n",
      "104/7:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "print(components.T)\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(projection='3d')\n",
      "ax.scatter(components.T[0], components.T[1], attributes['score'])\n",
      "plt.show()\n",
      "110/1:\n",
      "print('PyDev console: using IPython 7.19.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['C:\\\\Users\\\\luker\\\\Documents\\\\Github\\\\BicycleGAN-Tensorflow', 'C:/Users/luker/Documents/Github/BicycleGAN-Tensorflow'])\n",
      "112/1: from ../../test import nasza_pienkna_funkcja\n",
      "112/2: from test import nasza_pienkna_funkcja\n",
      "112/3: from test import nasza_pienkna_funkcja\n",
      "112/4: from test import nasza_pienkna_funkcja\n",
      "112/5:\n",
      "rmse, exp = nasza_pienkna_funkcja(image_or_text='text', all_or_pruned='pruned', mlp_or_rf='mpl', score_or_comms_num='score',\n",
      "                          unchanged_or_bracketed='unchanged')\n",
      "112/6:\n",
      "rmse, exp = nasza_pienkna_funkcja(image_or_text='text', all_or_pruned='pruned', mlp_or_rf='mlp', score_or_comms_num='score',\n",
      "                          unchanged_or_bracketed='unchanged')\n",
      "112/7:\n",
      "rmse, exp = nasza_pienkna_funkcja(image_or_text='text', all_or_pruned='pruned', mlp_or_rf='mlp', score_or_comms_num='score',\n",
      "                          unchanged_or_bracketed='unchanged')\n",
      "112/8: from test import nasza_pienkna_funkcja\n",
      "112/9:\n",
      "rmse, exp = nasza_pienkna_funkcja(image_or_text='text', all_or_pruned='pruned', mlp_or_rf='mlp', score_or_comms_num='score',\n",
      "                          unchanged_or_bracketed='unchanged')\n",
      "112/10: from test import nasza_pienkna_funkcja\n",
      "112/11:\n",
      "rmse, exp = nasza_pienkna_funkcja(image_or_text='text', all_or_pruned='pruned', mlp_or_rf='mlp', score_or_comms_num='score',\n",
      "                          unchanged_or_bracketed='unchanged')\n",
      "112/12: from test import analyze\n",
      "113/1: from test import analyze\n",
      "113/2:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "113/3:\n",
      "attributes = pickle.load(open( \"../../pickle/text_attributes_processed.pkl\", \"rb\" ))\n",
      "rf = RandomForestRegressor()\n",
      "rmse, exp = analyze(attributes, rf, score_or_comms_num='score', unchanged_or_bracketed='unchanged')\n",
      "113/4:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "import pickle\n",
      "113/5:\n",
      "attributes = pickle.load(open( \"../../pickle/text_attributes_processed.pkl\", \"rb\" ))\n",
      "rf = RandomForestRegressor()\n",
      "rmse, exp = analyze(attributes, rf, score_or_comms_num='score', unchanged_or_bracketed='unchanged')\n",
      "113/6:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "import pickle\n",
      "113/7:\n",
      "attributes = pickle.load(open( \"../../pickle/text_attributes_processed.pkl\", \"rb\" ))\n",
      "rf = RandomForestRegressor()\n",
      "rmse, exp = analyze(attributes, rf, score_or_comms_num='score')\n",
      "114/1:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "import pickle\n",
      "114/2:\n",
      "attributes = pickle.load(open( \"../../pickle/text_attributes_processed.pkl\", \"rb\" ))\n",
      "rf = RandomForestRegressor()\n",
      "rmse, exp = analyze(attributes, rf, score_or_comms_num='score')\n",
      "114/3:\n",
      "attributes = pickle.load(open( \"../../pickle/text_attributes_processed.pkl\", \"rb\" ))\n",
      "rf = RandomForestRegressor()\n",
      "rmse, exp = analyze(attributes, rf, score_or_comms_num='score')\n",
      "114/4:\n",
      "attributes = pickle.load(open( \"../../pickle/text_attributes_processed.pkl\", \"rb\" ))\n",
      "rf = RandomForestRegressor()\n",
      "rmse, exp = analyze(attributes, rf, score_or_comms_num='score')\n",
      "114/5:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "import pickle\n",
      "114/6:\n",
      "attributes = pickle.load(open( \"../../pickle/text_attributes_processed.pkl\", \"rb\" ))\n",
      "rf = RandomForestRegressor()\n",
      "rmse, exp = analyze(attributes, rf, score_or_comms_num='score')\n",
      "115/1:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "import pickle\n",
      "115/2:\n",
      "attributes = pickle.load(open( \"../../pickle/text_attributes_processed.pkl\", \"rb\" ))\n",
      "rf = RandomForestRegressor()\n",
      "rmse, exp = analyze(attributes, rf, score_or_comms_num='score')\n",
      "117/1:\n",
      "import pandas\n",
      "import pickle\n",
      "import numpy as np\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "117/2:\n",
      "df = pickle.load(open(\"../../pickle/text_attr.pkl\", \"rb\"))\n",
      "print(df.head())\n",
      "117/3:\n",
      "reduced_attr_count = 120\n",
      "corr_matrix = df.corr(method='spearman')\n",
      "117/4:\n",
      "correlations = []\n",
      "for i,column in enumerate(corr_matrix):\n",
      "  x = corr_matrix[column][i+1:]\n",
      "  for row,v in x.items():\n",
      "    correlations.append([v,column,row])\n",
      "correlations.sort(key = lambda x: -abs(x[0]))\n",
      "117/5:\n",
      "score_corr = corr_matrix['score'][corr_matrix['score']<1]\n",
      "for corr in correlations:\n",
      "  if len(df.columns) <= reduced_attr_count: break\n",
      "  c,a1,a2 = corr\n",
      "  if 'score' in [a1,a2]: continue\n",
      "  if a1 in df and a2 in df:\n",
      "    to_be_removed = a1 if score_corr[a1] < score_corr[a2] else a2\n",
      "    del df[to_be_removed]\n",
      "117/6:\n",
      "final_attr_count = 80\n",
      "rf = RandomForestRegressor()\n",
      "rf.fit(df[df.columns.difference(['score'])], df['score'])\n",
      "\n",
      "result = sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), df.head()), \n",
      "             reverse=True)\n",
      "result\n",
      "117/7:\n",
      "selected = []\n",
      "for x in result[:final_attr_count]:\n",
      "    selected.append(x[1])\n",
      "selected_attributes = df[selected + ['score']]\n",
      "print(selected_attributes)\n",
      "117/8: pickle.dump(selected_attributes, open( \"../../pickle/text_attributes_processed.pkl\", \"wb\" ))\n",
      "117/9:\n",
      "import pandas\n",
      "import pickle\n",
      "import numpy as np\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "import pandas\n",
      "\n",
      "df0 = pandas.read_csv('../../reddit_wsb.csv')\n",
      "117/10:\n",
      "import pandas\n",
      "import pickle\n",
      "import numpy as np\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "import pandas\n",
      "\n",
      "df0 = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "117/11:\n",
      "import pandas\n",
      "import pickle\n",
      "import numpy as np\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "import pandas\n",
      "\n",
      "df0 = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "117/12:\n",
      "df = pickle.load(open(\"../../pickle/text_attr.pkl\", \"rb\"))\n",
      "df['comms_num'] = df0['comms_num']\n",
      "print(df.head())\n",
      "117/13:\n",
      "reduced_attr_count = 120\n",
      "corr_matrix = df.corr(method='spearman')\n",
      "117/14:\n",
      "correlations = []\n",
      "for i,column in enumerate(corr_matrix):\n",
      "  x = corr_matrix[column][i+1:]\n",
      "  for row,v in x.items():\n",
      "    correlations.append([v,column,row])\n",
      "correlations.sort(key = lambda x: -abs(x[0]))\n",
      "117/15:\n",
      "score_corr = corr_matrix['score'][corr_matrix['score']<1]\n",
      "for corr in correlations:\n",
      "  if len(df.columns) <= reduced_attr_count: break\n",
      "  c,a1,a2 = corr\n",
      "  if 'score' in [a1,a2]: continue\n",
      "  if a1 in df and a2 in df:\n",
      "    to_be_removed = a1 if score_corr[a1] < score_corr[a2] else a2\n",
      "    del df[to_be_removed]\n",
      "117/16:\n",
      "final_attr_count = 80\n",
      "rf = RandomForestRegressor()\n",
      "rf.fit(df[df.columns.difference(['score'])], df['score'])\n",
      "\n",
      "result = sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), df.head()), \n",
      "             reverse=True)\n",
      "result\n",
      "117/17:\n",
      "selected = []\n",
      "for x in result[:final_attr_count]:\n",
      "    selected.append(x[1])\n",
      "selected_attributes = df[selected + ['score']]\n",
      "print(selected_attributes)\n",
      "117/18: pickle.dump(selected_attributes, open( \"../../pickle/text_attributes_processed.pkl\", \"wb\" ))\n",
      "115/3:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "import pickle\n",
      "attributes = pickle.load(open( \"../../pickle/text_attributes_processed.pkl\", \"rb\" ))\n",
      "115/4:\n",
      "rf = RandomForestRegressor()\n",
      "rmse, exp = analyze(attributes, rf, score_or_comms_num='score')\n",
      "115/5:\n",
      "mlp = MLPRegressor(hidden_layer_sizes=(15,), max_iter=500, learning_rate='adaptive')\n",
      "rmse, exp = analyze(attributes, mlp, score_or_comms_num='score')\n",
      "115/6:\n",
      "rf_rmse_score, rf_exp_score, rf_score = analyze(attributes, RandomForestRegressor(), score_or_comms_num='score')\n",
      "rf_rmse_comms_num, rf_exp_comms_num, rf_comms_num = analyze(attributes, RandomForestRegressor(), score_or_comms_num='comms_num')\n",
      "115/7:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "import pickle\n",
      "attributes = pickle.load(open( \"../../pickle/text_attributes_processed.pkl\", \"rb\" ))\n",
      "115/8:\n",
      "rf = RandomForestRegressor()\n",
      "rf_rmse_score, rf_exp_score, rf_score = analyze(attributes, rf, score_or_comms_num='score')\n",
      "rf = RandomForestRegressor()\n",
      "rf_rmse_comms_num, rf_exp_comms_num, rf_comms_num = analyze(attributes, rf, score_or_comms_num='comms_num')\n",
      "120/1:\n",
      "rf = RandomForestRegressor()\n",
      "rf_rmse_score, rf_exp_score, rf_score = analyze(attributes, rf, score_or_comms_num='score')\n",
      "rf = RandomForestRegressor()\n",
      "rf_rmse_comms_num, rf_exp_comms_num, rf_comms_num = analyze(attributes, rf, score_or_comms_num='comms_num')\n",
      "120/2:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "import pickle\n",
      "attributes = pickle.load(open( \"../../pickle/text_attributes_processed.pkl\", \"rb\" ))\n",
      "124/1:\n",
      "\n",
      "# from google.colab import drive\n",
      "\n",
      "# drive.mount('/content/gdrive')\n",
      "124/2:\n",
      "\n",
      "# !ls gdrive/MyDrive/PED\n",
      "124/3:\n",
      "\n",
      "import pandas\n",
      "from collections import Counter\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import nltk\n",
      "nltk.download('punkt')\n",
      "%matplotlib inline\n",
      "\n",
      "df = pandas.read_csv('../../reddit_wsb.csv')\n",
      "df[\"body\"] = df[\"body\"].fillna('')\n",
      "df[\"title\"] = df[\"title\"].fillna('')\n",
      "\n",
      "df\n",
      "125/1:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "import pickle\n",
      "attributes = pickle.load(open( \"../../pickle/text_attributes_processed.pkl\", \"rb\" ))\n",
      "125/2:\n",
      "rf = RandomForestRegressor()\n",
      "rf_rmse_score, rf_exp_score, rf_score = analyze(attributes, rf, score_or_comms_num='score')\n",
      "125/3:\n",
      "rf = RandomForestRegressor()\n",
      "rf_rmse_comms_num, rf_exp_comms_num, rf_comms_num = analyze(attributes, rf, score_or_comms_num='comms_num')\n",
      "125/4:\n",
      "# rf = RandomForestRegressor()\n",
      "# rf_rmse_comms_num, rf_exp_comms_num, rf_comms_num = analyze(attributes, rf, score_or_comms_num='comms_num')\n",
      "125/5:\n",
      "mlp = MLPRegressor(hidden_layer_sizes=(15,), max_iter=500, learning_rate='adaptive')\n",
      "mlp_rmse_score, mlp_exp_score, mlp_score = analyze(attributes, mlp, score_or_comms_num='score')\n",
      "126/1:\n",
      "import pandas\n",
      "import pickle\n",
      "import numpy as np\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "import pandas\n",
      "\n",
      "df0 = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "126/2:\n",
      "df = pickle.load(open(\"../../pickle/text_attr.pkl\", \"rb\"))\n",
      "df['comms_num'] = df0['comms_num']\n",
      "print(df.head())\n",
      "126/3:\n",
      "reduced_attr_count = 120\n",
      "corr_matrix = df.corr(method='spearman')\n",
      "126/4:\n",
      "correlations = []\n",
      "for i,column in enumerate(corr_matrix):\n",
      "  x = corr_matrix[column][i+1:]\n",
      "  for row,v in x.items():\n",
      "    correlations.append([v,column,row])\n",
      "correlations.sort(key = lambda x: -abs(x[0]))\n",
      "126/5:\n",
      "score_corr = corr_matrix['score'][corr_matrix['score']<1]\n",
      "for corr in correlations:\n",
      "  if len(df.columns) <= reduced_attr_count: break\n",
      "  c,a1,a2 = corr\n",
      "  if 'score' in [a1,a2]: continue\n",
      "  if a1 in df and a2 in df:\n",
      "    to_be_removed = a1 if score_corr[a1] < score_corr[a2] else a2\n",
      "    del df[to_be_removed]\n",
      "126/6:\n",
      "final_attr_count = 50\n",
      "rf = RandomForestRegressor()\n",
      "rf.fit(df[df.columns.difference(['score'])], df['score'])\n",
      "\n",
      "result = sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), df.head()), \n",
      "             reverse=True)\n",
      "result\n",
      "126/7:\n",
      "selected = []\n",
      "for x in result[:final_attr_count]:\n",
      "    selected.append(x[1])\n",
      "selected_attributes = df[selected + ['score']]\n",
      "print(selected_attributes)\n",
      "126/8: pickle.dump(selected_attributes, open( \"../../pickle/text_attributes_processed.pkl\", \"wb\" ))\n",
      "125/6:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "import pickle\n",
      "attributes = pickle.load(open( \"../../pickle/text_attributes_processed.pkl\", \"rb\" ))\n",
      "125/7:\n",
      "rf = RandomForestRegressor()\n",
      "rf_rmse_score, rf_exp_score, rf_score = analyze(attributes, rf, score_or_comms_num='score')\n",
      "125/8:\n",
      "# rf = RandomForestRegressor()\n",
      "# rf_rmse_comms_num, rf_exp_comms_num, rf_comms_num = analyze(attributes, rf, score_or_comms_num='comms_num')\n",
      "125/9:\n",
      "mlp = MLPRegressor(hidden_layer_sizes=(15,), max_iter=500, learning_rate='adaptive')\n",
      "mlp_rmse_score, mlp_exp_score, mlp_score = analyze(attributes, mlp, score_or_comms_num='score')\n",
      "125/10:\n",
      "# mlp = MLPRegressor(hidden_layer_sizes=(15,), max_iter=500, learning_rate='adaptive')\n",
      "# mlp_rmse_comms_num, mlp_exp_comms_num, mlp_comms_num = analyze(attributes, mlp, score_or_comms_num='comms_num')\n",
      "127/1:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "import pickle\n",
      "attributes = pickle.load(open( \"../../pickle/image_attributes_processed.pkl\", \"rb\" ))\n",
      "127/2:\n",
      "rf = RandomForestRegressor()\n",
      "rf_rmse_score, rf_exp_score, rf_score = analyze(attributes, rf, score_or_comms_num='score')\n",
      "127/3:\n",
      "# rf = RandomForestRegressor()\n",
      "# rf_rmse_comms_num, rf_exp_comms_num, rf_comms_num = analyze(attributes, rf, score_or_comms_num='comms_num')\n",
      "127/4:\n",
      "mlp = MLPRegressor(hidden_layer_sizes=(30,60), max_iter=500, learning_rate='adaptive')\n",
      "mlp_rmse_score, mlp_exp_score, mlp_score = analyze(attributes, mlp, score_or_comms_num='score')\n",
      "127/5:\n",
      "# mlp = MLPRegressor(hidden_layer_sizes=(30,60), max_iter=500, learning_rate='adaptive')\n",
      "# mlp_rmse_comms_num, mlp_exp_comms_num, mlp_comms_num = analyze(attributes, mlp, score_or_comms_num='comms_num')\n",
      "126/9:\n",
      "import pandas\n",
      "import pickle\n",
      "import numpy as np\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "import pandas\n",
      "\n",
      "df0 = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "126/10:\n",
      "df = pickle.load(open(\"../../pickle/text_attr.pkl\", \"rb\"))\n",
      "df['comms_num'] = df0['comms_num']\n",
      "print(df.head())\n",
      "126/11:\n",
      "reduced_attr_count = 120\n",
      "corr_matrix = df.corr(method='spearman')\n",
      "126/12:\n",
      "correlations = []\n",
      "for i,column in enumerate(corr_matrix):\n",
      "  x = corr_matrix[column][i+1:]\n",
      "  for row,v in x.items():\n",
      "    correlations.append([v,column,row])\n",
      "correlations.sort(key = lambda x: -abs(x[0]))\n",
      "126/13:\n",
      "df_score = df.copy()\n",
      "score_corr = corr_matrix['score'][corr_matrix['score']<1]\n",
      "for corr in correlations:\n",
      "  if len(df_score.columns) <= reduced_attr_count: break\n",
      "  c,a1,a2 = corr\n",
      "  if 'score' in [a1,a2]: continue\n",
      "  if a1 in df_score and a2 in df_score:\n",
      "    to_be_removed = a1 if score_corr[a1] < score_corr[a2] else a2\n",
      "    del df_score[to_be_removed]\n",
      "126/14:\n",
      "df_comms_num = df.copy()\n",
      "comms_num_corr = corr_matrix['comms_num'][corr_matrix['comms_num']<1]\n",
      "for corr in correlations:\n",
      "  if len(df_comms_num.columns) <= reduced_attr_count: break\n",
      "  c,a1,a2 = corr\n",
      "  if 'comms_num' in [a1,a2]: continue\n",
      "  if a1 in df_comms_num and a2 in df_comms_num:\n",
      "    to_be_removed = a1 if comms_num_corr[a1] < comms_num_corr[a2] else a2\n",
      "    del df_comms_num[to_be_removed]\n",
      "126/15:\n",
      "final_attr_count = 50\n",
      "rf = RandomForestRegressor()\n",
      "rf.fit(df_score[df_score.columns.difference(['score', 'comms_num'])], df_score['score'])\n",
      "\n",
      "result = sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), df_score.head()), \n",
      "             reverse=True)\n",
      "selected = []\n",
      "for x in result[:final_attr_count]:\n",
      "    selected.append(x[1])\n",
      "selected_attributes = df_score[selected + ['score']]\n",
      "print(selected_attributes)\n",
      "\n",
      "pickle.dump(selected_attributes, open( \"../../pickle/text_attributes_processed_score.pkl\", \"wb\" ))\n",
      "126/16:\n",
      "final_attr_count = 50\n",
      "rf = RandomForestRegressor()\n",
      "rf.fit(df_comms_num[df_comms_num.columns.difference(['score', 'comms_num'])], df_comms_num['comms_num'])\n",
      "\n",
      "result = sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), df_comms_num.head()), \n",
      "             reverse=True)\n",
      "selected = []\n",
      "for x in result[:final_attr_count]:\n",
      "    selected.append(x[1])\n",
      "selected_attributes = df_comms_num[selected + ['comms_num']]\n",
      "print(selected_attributes)\n",
      "\n",
      "pickle.dump(selected_attributes, open( \"../../pickle/text_attributes_processed_comms_num.pkl\", \"wb\" ))\n",
      "125/11:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "import pickle\n",
      "\n",
      "attributes_score = pickle.load(open( \"../../pickle/text_attributes_processed_score.pkl\", \"rb\" ))\n",
      "attributes_comms_num = pickle.load(open( \"../../pickle/text_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "125/12:\n",
      "rf = RandomForestRegressor()\n",
      "rf_rmse_score, rf_exp_score, rf_score = analyze(attributes_score, rf, score_or_comms_num='score')\n",
      "125/13:\n",
      "rf = RandomForestRegressor()\n",
      "rf_rmse_comms_num, rf_exp_comms_num, rf_comms_num = analyze(attributes_comms_num, rf, score_or_comms_num='comms_num')\n",
      "129/1:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "import pickle\n",
      "\n",
      "attributes_score = pickle.load(open( \"../../pickle/text_attributes_processed_score.pkl\", \"rb\" ))\n",
      "attributes_comms_num = pickle.load(open( \"../../pickle/text_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "129/2:\n",
      "rf = RandomForestRegressor()\n",
      "rf_rmse_comms_num, rf_exp_comms_num, rf_comms_num = analyze(attributes_comms_num, rf, score_or_comms_num='comms_num')\n",
      "129/3:\n",
      "mlp = MLPRegressor(hidden_layer_sizes=(15,), max_iter=500, learning_rate='adaptive')\n",
      "mlp_rmse_score, mlp_exp_score, mlp_score = analyze(attributes_score, mlp, score_or_comms_num='score')\n",
      "129/4:\n",
      "mlp = MLPRegressor(hidden_layer_sizes=(15,), max_iter=500, learning_rate='adaptive')\n",
      "mlp_rmse_comms_num, mlp_exp_comms_num, mlp_comms_num = analyze(attributes_comms_num, mlp, score_or_comms_num='comms_num')\n",
      "130/1:\n",
      "# from sklearn.decomposition import PCA\n",
      "# img_class = pickle.load(open(\"gdrive/MyDrive/PED/image_classes.pkl\", \"rb\"))\n",
      "\n",
      "# class_table = {}\n",
      "# # class_table['id'] = []\n",
      "\n",
      "# for i in range(1000):\n",
      "#   class_table[str(i)] = []\n",
      "# for key in img_class.keys():\n",
      "#   # class_table['id'].append(key)\n",
      "#   for i in range(1000):\n",
      "#     class_table[str(i)].append(img_class[key]['all_prob'][0][i])\n",
      "\n",
      "# class_table = pandas.DataFrame(class_table)\n",
      "# pca = PCA(n_components=10)\n",
      "# pca.fit(class_table)\n",
      "# print(pca.components_ )\n",
      "import pandas\n",
      "import pickle\n",
      "import numpy as np\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "130/2:\n",
      "df = pandas.read_csv('../../reddit_wsb.csv')\n",
      "df[\"body\"] = df[\"body\"].fillna('')\n",
      "df[\"title\"] = df[\"title\"].fillna('')\n",
      "img_clusters = pickle.load(open(\"../../pickle/image_color_clusters.pkl\", \"rb\"))\n",
      "img_means = pickle.load(open(\"../../pickle/image_hsv_means.pkl\", \"rb\"))\n",
      "img_text = pickle.load(open(\"../../pickle/texts.pkl\", \"rb\"))\n",
      "img_classes = pickle.load(open(\"../../pickle/image_classes.pkl\", \"rb\"))\n",
      "130/3:\n",
      "# from sklearn.decomposition import PCA\n",
      "# img_class = pickle.load(open(\"gdrive/MyDrive/PED/image_classes.pkl\", \"rb\"))\n",
      "\n",
      "# class_table = {}\n",
      "# # class_table['id'] = []\n",
      "\n",
      "# for i in range(1000):\n",
      "#   class_table[str(i)] = []\n",
      "# for key in img_class.keys():\n",
      "#   # class_table['id'].append(key)\n",
      "#   for i in range(1000):\n",
      "#     class_table[str(i)].append(img_class[key]['all_prob'][0][i])\n",
      "\n",
      "# class_table = pandas.DataFrame(class_table)\n",
      "# pca = PCA(n_components=10)\n",
      "# pca.fit(class_table)\n",
      "# print(pca.components_ )\n",
      "import pandas\n",
      "import pickle\n",
      "import numpy as np\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "130/4:\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "df[\"body\"] = df[\"body\"].fillna('')\n",
      "df[\"title\"] = df[\"title\"].fillna('')\n",
      "img_clusters = pickle.load(open(\"../../pickle/image_color_clusters.pkl\", \"rb\"))\n",
      "img_means = pickle.load(open(\"../../pickle/image_hsv_means.pkl\", \"rb\"))\n",
      "img_text = pickle.load(open(\"../../pickle/texts.pkl\", \"rb\"))\n",
      "img_classes = pickle.load(open(\"../../pickle/image_classes.pkl\", \"rb\"))\n",
      "130/5:\n",
      "from collections import Counter\n",
      "import emoji\n",
      "import re\n",
      "import spacy\n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "i=0\n",
      "for key, value in img_text.items():\n",
      "    print(str(i)+'/'+str(len(img_text.items())))\n",
      "    img_text[key] = [i.lemma_ for i in nlp(\" \".join(value).lower()) if (not i.is_stop) and len(i.lemma_) > 0]\n",
      "    print(img_text[key])\n",
      "    i+=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/6:\n",
      "def prune(most_common_words):\n",
      "  for key, cnts in list(most_common_words.items()):\n",
      "    if not key in emoji.UNICODE_EMOJI and (not re.match(r'\\w+',key) or len(key) < 2):\n",
      "      del most_common_words[key]\n",
      "  for i in ['don\\\\u2019', 'i\\\\u2019', 'it\\\\u2019s']:\n",
      "    del most_common_words[i]\n",
      "\n",
      "img_most_common = Counter(\" \".join(list(map(lambda x: \" \".join(x), list(img_text.values())))).split(\" \"))\n",
      "prune(img_most_common)\n",
      "img_most_common.most_common(100)\n",
      "130/7:\n",
      "pic_data = {}\n",
      "pic_data['id'] = []\n",
      "pic_data['text'] = []\n",
      "pic_data['clusters'] = []\n",
      "pic_data['means'] = []\n",
      "pic_data['score'] = []\n",
      "pic_data['classses'] = []\n",
      "\n",
      "for key in img_means.keys():\n",
      "  pic_data['id'].append(key)\n",
      "  pic_data['text'].append(\" \".join(img_text[key[:-4]]))\n",
      "  pic_data['clusters'].append(img_clusters[key])\n",
      "  pic_data['means'].append(img_means[key])\n",
      "  pic_data['score'].append(df.loc[df['id'] == key[:-4]]['score'].values[0])\n",
      "  pic_data['classses'].append(img_classes[key]['all_prob'][0])\n",
      "\n",
      "pic_data = pandas.DataFrame(pic_data)\n",
      "130/8:\n",
      "temp = {}\n",
      "for i in range(1000):\n",
      "    temp['class_'+str(i)] = pic_data['classses'].apply(lambda s: s[i])\n",
      "    \n",
      "temp = pandas.DataFrame(temp)\n",
      "print(len(temp.columns))\n",
      "for column in temp:\n",
      "    if np.max(temp[column])<0.1:\n",
      "        del temp[column]\n",
      "\n",
      "print(len(temp.columns))\n",
      "130/9:\n",
      "pic_attributes = {'id': pic_data['id']}\n",
      "for key, cnts in img_most_common.most_common(100): \n",
      "  pic_attributes['text_' + key] =  pic_data['text'].str.lower().str.count(key.lower())\n",
      "\n",
      "pic_attributes['h_sin'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['h_cos'] = pic_data['means'].apply(lambda s:  np.sin(2*np.pi*(s[0]/255)))\n",
      "pic_attributes['s'] = pic_data['means'].apply(lambda s: s[1]/255)\n",
      "pic_attributes['v'] = pic_data['means'].apply(lambda s: s[2]/255)\n",
      "pic_attributes['score'] = pic_data['score']\n",
      "for j in range(5):\n",
      "  for i in range(3):\n",
      "    pic_attributes['color_'+str(j)+'_'+'rgb'[i]] = pic_data['clusters'].apply(lambda s: s['color'][j][i])\n",
      "  pic_attributes['color_'+str(j)+'_%'] = pic_data['clusters'].apply(lambda s: s['percentage'][j])\n",
      "\n",
      "for i in temp.columns:\n",
      "    pic_attributes[i] = temp[i]\n",
      "\n",
      "pic_attributes = pandas.DataFrame(pic_attributes)\n",
      "print(pic_attributes)\n",
      "130/10:\n",
      "pic_attributes['id'] = pic_attributes['id'].apply(lambda x: x[:-4])\n",
      "pic_attributes\n",
      "pic_attributes['comms_num'] = pandas.merge(pic_attributes, df, how='left', left_on=['id'], right_on=['id'])['comms_num']\n",
      "pickle.dump(pic_attributes, open( \"../../pickle/image_attributes_all.pkl\", \"wb\"))\n",
      "130/11:\n",
      "reduced_attr_count = 150\n",
      "corr_image_matrix = pic_attributes.corr(method='spearman')\n",
      "130/12:\n",
      "correlations = []\n",
      "for i,column in enumerate(corr_image_matrix):\n",
      "  x = corr_image_matrix[column][i+1:]\n",
      "  for row,v in x.items():\n",
      "    correlations.append([v,column,row])\n",
      "correlations.sort(key = lambda x: -abs(x[0]))\n",
      "130/13:\n",
      "df_score = pic_attributes.copy()\n",
      "score_corr = corr_matrix['score'][corr_matrix['score']<1]\n",
      "for corr in correlations:\n",
      "  if len(df_score.columns) <= reduced_attr_count: break\n",
      "  c,a1,a2 = corr\n",
      "  if 'score' in [a1,a2]: continue\n",
      "  if a1 in df_score and a2 in df_score:\n",
      "    to_be_removed = a1 if score_corr[a1] < score_corr[a2] else a2\n",
      "    del df_score[to_be_removed]\n",
      "130/14:\n",
      "df_score = pic_attributes.copy()\n",
      "score_corr = corr_image_matrix['score'][corr_image_matrix['score']<1]\n",
      "for corr in correlations:\n",
      "  if len(df_score.columns) <= reduced_attr_count: break\n",
      "  c,a1,a2 = corr\n",
      "  if 'score' in [a1,a2]: continue\n",
      "  if a1 in df_score and a2 in df_score:\n",
      "    to_be_removed = a1 if score_corr[a1] < score_corr[a2] else a2\n",
      "    del df_score[to_be_removed]\n",
      "130/15:\n",
      "df_comms_num = pic_attributes.copy()\n",
      "comms_num_corr = corr_image_matrix['comms_num'][corr_image_matrix['comms_num']<1]\n",
      "for corr in correlations:\n",
      "  if len(df_comms_num.columns) <= reduced_attr_count: break\n",
      "  c,a1,a2 = corr\n",
      "  if 'comms_num' in [a1,a2]: continue\n",
      "  if a1 in df_comms_num and a2 in df_comms_num:\n",
      "    to_be_removed = a1 if comms_num_corr[a1] < comms_num_corr[a2] else a2\n",
      "    del df_comms_num[to_be_removed]\n",
      "130/16:\n",
      "final_attr_count = 50\n",
      "rf = RandomForestRegressor()\n",
      "rf.fit(df_score[df_score.columns.difference(['score', 'comms_num', 'id'])], df_score['score'])\n",
      "\n",
      "result = sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), df_score.head()), \n",
      "             reverse=True)\n",
      "selected = []\n",
      "for x in result[:final_attr_count]:\n",
      "    selected.append(x[1])\n",
      "selected_attributes = df_score[selected + ['score']]\n",
      "print(selected_attributes)\n",
      "\n",
      "pickle.dump(selected_attributes, open( \"../../pickle/image_attributes_processed_score.pkl\", \"wb\" ))\n",
      "130/17:\n",
      "rf = RandomForestRegressor()\n",
      "rf.fit(df_comms_num[df_comms_num.columns.difference(['score', 'comms_num', 'id'])], df_comms_num['comms_num'])\n",
      "\n",
      "result = sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), df_comms_num.head()), \n",
      "             reverse=True)\n",
      "selected = []\n",
      "for x in result[:final_attr_count]:\n",
      "    selected.append(x[1])\n",
      "selected_attributes = df_comms_num[selected + ['comms_num']]\n",
      "print(selected_attributes)\n",
      "\n",
      "pickle.dump(selected_attributes, open( \"../../pickle/image_attributes_processed_comms_num.pkl\", \"wb\" ))\n",
      "131/1:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "import pickle\n",
      "\n",
      "attributes_score = pickle.load(open( \"../../pickle/text_attributes_processed_score.pkl\", \"rb\" ))\n",
      "attributes_comms_num = pickle.load(open( \"../../pickle/text_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "131/2:\n",
      "rf = RandomForestRegressor()\n",
      "rf_rmse_score, rf_exp_score, rf_score = analyze(attributes, rf, score_or_comms_num='score')\n",
      "131/3:\n",
      "mlp = MLPRegressor(hidden_layer_sizes=(30,60), max_iter=500, learning_rate='adaptive')\n",
      "mlp_rmse_comms_num, mlp_exp_comms_num, mlp_comms_num = analyze(attributes_comms_num, mlp, score_or_comms_num='comms_num')\n",
      "131/4:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "import pickle\n",
      "\n",
      "attributes_score = pickle.load(open( \"../../pickle/text_attributes_processed_score.pkl\", \"rb\" ))\n",
      "attributes_comms_num = pickle.load(open( \"../../pickle/text_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "131/5:\n",
      "rf = RandomForestRegressor()\n",
      "rf_rmse_score, rf_exp_score, rf_score = analyze(attributes_score, rf, score_or_comms_num='score')\n",
      "131/6:\n",
      "rf = RandomForestRegressor()\n",
      "rf_rmse_comms_num, rf_exp_comms_num, rf_comms_num = analyze(attributes_comms_num, rf, score_or_comms_num='comms_num')\n",
      "131/7:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "import pickle\n",
      "\n",
      "attributes_score = pickle.load(open( \"../../pickle/text_attributes_processed_score.pkl\", \"rb\" ))\n",
      "attributes_comms_num = pickle.load(open( \"../../pickle/text_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "131/8:\n",
      "rf = RandomForestRegressor()\n",
      "rf_rmse_score, rf_exp_score, rf_score = analyze(attributes_score, rf, score_or_comms_num='score')\n",
      "131/9:\n",
      "rf = RandomForestRegressor()\n",
      "rf_rmse_comms_num, rf_exp_comms_num, rf_comms_num = analyze(attributes_comms_num, rf, score_or_comms_num='comms_num')\n",
      "131/10:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "import pickle\n",
      "\n",
      "attributes_score = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "attributes_comms_num = pickle.load(open( \"../../pickle/image_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "131/11:\n",
      "rf = RandomForestRegressor()\n",
      "rf_rmse_score, rf_exp_score, rf_score = analyze(attributes_score, rf, score_or_comms_num='score')\n",
      "131/12:\n",
      "rf = RandomForestRegressor()\n",
      "rf_rmse_comms_num, rf_exp_comms_num, rf_comms_num = analyze(attributes_comms_num, rf, score_or_comms_num='comms_num')\n",
      "131/13:\n",
      "mlp = MLPRegressor(hidden_layer_sizes=(30,60), max_iter=500, learning_rate='adaptive')\n",
      "mlp_rmse_score, mlp_exp_score, mlp_score = analyze(attributes_score, mlp, score_or_comms_num='score')\n",
      "131/14:\n",
      "mlp = MLPRegressor(hidden_layer_sizes=(30,60), max_iter=500, learning_rate='adaptive')\n",
      "mlp_rmse_comms_num, mlp_exp_comms_num, mlp_comms_num = analyze(attributes_comms_num, mlp, score_or_comms_num='comms_num')\n",
      "131/15:\n",
      "rf = RandomForestRegressor()\n",
      "rf_rmse_score, rf_exp_score, rf_score = analyze(attributes_score, rf, score_or_comms_num='score')\n",
      "rf_exp_score.show_in_notebook(show_table=True)\n",
      "131/16:\n",
      "rf = RandomForestRegressor()\n",
      "rf_rmse_comms_num, rf_exp_comms_num, rf_comms_num = analyze(attributes_comms_num, rf, score_or_comms_num='comms_num')\n",
      "rf_exp_comms_num.show_in_notebook(show_table=True)\n",
      "131/17:\n",
      "mlp = MLPRegressor(hidden_layer_sizes=(30,60), max_iter=500, learning_rate='adaptive')\n",
      "mlp_rmse_score, mlp_exp_score, mlp_score = analyze(attributes_score, mlp, score_or_comms_num='score')\n",
      "mlp_exp_score.show_in_notebook(show_table=True)\n",
      "131/18:\n",
      "mlp = MLPRegressor(hidden_layer_sizes=(30,60), max_iter=500, learning_rate='adaptive')\n",
      "mlp_rmse_comms_num, mlp_exp_comms_num, mlp_comms_num = analyze(attributes_comms_num, mlp, score_or_comms_num='comms_num')\n",
      "mlp_exp_comms_num.show_in_notebook(show_table=True)\n",
      "129/5:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "import pickle\n",
      "\n",
      "attributes_score = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "attributes_comms_num = pickle.load(open( \"../../pickle/image_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "129/6:\n",
      "rf = RandomForestRegressor()\n",
      "rf_rmse_score, rf_exp_score, rf_score = analyze(attributes_score, rf, score_or_comms_num='score')\n",
      "rf_exp_score.show_in_notebook(show_table=True)\n",
      "129/7:\n",
      "rf = RandomForestRegressor()\n",
      "rf_rmse_comms_num, rf_exp_comms_num, rf_comms_num = analyze(attributes_comms_num, rf, score_or_comms_num='comms_num')\n",
      "rf_exp_comms_num.show_in_notebook(show_table=True)\n",
      "129/8:\n",
      "mlp = MLPRegressor(hidden_layer_sizes=(15,), max_iter=500, learning_rate='adaptive')\n",
      "mlp_rmse_score, mlp_exp_score, mlp_score = analyze(attributes_score, mlp, score_or_comms_num='score')\n",
      "mlp_exp_score.show_in_notebook(show_table=True)\n",
      "129/9:\n",
      "mlp = MLPRegressor(hidden_layer_sizes=(15,), max_iter=500, learning_rate='adaptive')\n",
      "mlp_rmse_comms_num, mlp_exp_comms_num, mlp_comms_num = analyze(attributes_comms_num, mlp, score_or_comms_num='comms_num')\n",
      "mlp_exp_comms_num.show_in_notebook(show_table=True)\n",
      "134/1:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "import pickle\n",
      "\n",
      "attributes_score = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "attributes_comms_num = pickle.load(open( \"../../pickle/image_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "\n",
      "attributes_score_class = attributes_score.copy()\n",
      "attributes_comms_num_class = attributes_score.copy()\n",
      "\n",
      "attributes_score_class[\"score\"] = attributes_score_class[\"score\"]>2000\n",
      "attributes_comms_num_class[\"comms_num\"] = attributes_comms_num_class[\"comms_num\"]>200\n",
      "135/1:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "import pickle\n",
      "\n",
      "attributes_score = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "attributes_comms_num = pickle.load(open( \"../../pickle/image_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "\n",
      "attributes_score[\"score\"] = attributes_score[\"score\"]>2000\n",
      "attributes_comms_num[\"comms_num\"] = attributes_comms_num[\"comms_num\"]>200\n",
      "134/2:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "import pickle\n",
      "\n",
      "attributes_score = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "attributes_comms_num = pickle.load(open( \"../../pickle/image_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "135/2:\n",
      "rf = RandomForestClassifier()\n",
      "rf_rmse_score, rf_exp_score, rf_score = analyze(attributes_score, rf, score_or_comms_num='score')\n",
      "rf_exp_score.show_in_notebook(show_table=True)\n",
      "135/3:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "import pickle\n",
      "\n",
      "attributes_score = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "attributes_comms_num = pickle.load(open( \"../../pickle/image_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "\n",
      "attributes_score[\"score\"] = attributes_score[\"score\"]>2000\n",
      "attributes_comms_num[\"comms_num\"] = attributes_comms_num[\"comms_num\"]>200\n",
      "135/4:\n",
      "rf = RandomForestClassifier()\n",
      "rf_rmse_score, rf_exp_score, rf_score = analyze(attributes_score, rf, score_or_comms_num='score')\n",
      "rf_exp_score.show_in_notebook(show_table=True)\n",
      "136/1:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "import pickle\n",
      "\n",
      "attributes_score = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "attributes_comms_num = pickle.load(open( \"../../pickle/image_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "\n",
      "attributes_score[\"score\"] = attributes_score[\"score\"]>2000\n",
      "attributes_comms_num[\"comms_num\"] = attributes_comms_num[\"comms_num\"]>200\n",
      "136/2:\n",
      "rf = RandomForestClassifier()\n",
      "rf_rmse_score, rf_exp_score, rf_score = analyze(attributes_score, rf, score_or_comms_num='score', reg_or_class='class')\n",
      "rf_exp_score.show_in_notebook(show_table=True)\n",
      "137/1:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "import pickle\n",
      "\n",
      "attributes_score = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "attributes_comms_num = pickle.load(open( \"../../pickle/image_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "\n",
      "attributes_score[\"score\"] = attributes_score[\"score\"]>2000\n",
      "attributes_comms_num[\"comms_num\"] = attributes_comms_num[\"comms_num\"]>200\n",
      "137/2:\n",
      "rf = RandomForestClassifier()\n",
      "rf_rmse_score, rf_exp_score, rf_score = analyze(attributes_score, rf, score_or_comms_num='score', reg_or_class='class')\n",
      "rf_exp_score.show_in_notebook(show_table=True)\n",
      "138/1:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "import pickle\n",
      "\n",
      "attributes_score = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "attributes_comms_num = pickle.load(open( \"../../pickle/image_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "\n",
      "attributes_score[\"score\"] = attributes_score[\"score\"]>2000\n",
      "attributes_comms_num[\"comms_num\"] = attributes_comms_num[\"comms_num\"]>200\n",
      "138/2:\n",
      "rf = RandomForestClassifier()\n",
      "rf_rmse_score, rf_exp_score, rf_score = analyze(attributes_score, rf, score_or_comms_num='score', reg_or_class='class')\n",
      "rf_exp_score.show_in_notebook(show_table=True)\n",
      "138/3:\n",
      "\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "import pickle\n",
      "\n",
      "attributes_score = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "attributes_comms_num = pickle.load(open( \"../../pickle/image_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "\n",
      "attributes_score[\"score\"] = attributes_score[\"score\"]>2000\n",
      "attributes_comms_num[\"comms_num\"] = attributes_comms_num[\"comms_num\"]>200\n",
      "138/4:\n",
      "\n",
      "rf = RandomForestClassifier()\n",
      "rf_rmse_score, rf_exp_score, rf_score = analyze(attributes_score, rf, score_or_comms_num='score', reg_or_class='class')\n",
      "rf_exp_score.show_in_notebook(show_table=True)\n",
      "139/1:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "import pickle\n",
      "\n",
      "attributes_score = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "attributes_comms_num = pickle.load(open( \"../../pickle/image_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "\n",
      "attributes_score[\"score\"] = attributes_score[\"score\"]>2000\n",
      "attributes_comms_num[\"comms_num\"] = attributes_comms_num[\"comms_num\"]>200\n",
      "139/2:\n",
      "rf = RandomForestClassifier()\n",
      "rf_rmse_score, rf_exp_score, rf_score = analyze(attributes_score, rf, score_or_comms_num='score', reg_or_class='class')\n",
      "rf_exp_score.show_in_notebook(show_table=True)\n",
      "140/1:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "import pickle\n",
      "\n",
      "attributes_score = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "attributes_comms_num = pickle.load(open( \"../../pickle/image_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "\n",
      "attributes_score[\"score\"] = attributes_score[\"score\"]>2000\n",
      "attributes_comms_num[\"comms_num\"] = attributes_comms_num[\"comms_num\"]>200\n",
      "140/2:\n",
      "rf = RandomForestClassifier()\n",
      "rf_rmse_score, rf_exp_score, rf_score = analyze(attributes_score, rf, score_or_comms_num='score', reg_or_class='class')\n",
      "rf_exp_score.show_in_notebook(show_table=True)\n",
      "141/1:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "import pickle\n",
      "\n",
      "attributes_score = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "attributes_comms_num = pickle.load(open( \"../../pickle/image_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "\n",
      "attributes_score[\"score\"] = attributes_score[\"score\"]>2000\n",
      "attributes_comms_num[\"comms_num\"] = attributes_comms_num[\"comms_num\"]>200\n",
      "141/2:\n",
      "rf = RandomForestClassifier()\n",
      "rf_rmse_score, rf_exp_score, rf_score = analyze(attributes_score, rf, score_or_comms_num='score', reg_or_class='class')\n",
      "rf_exp_score.show_in_notebook(show_table=True)\n",
      "141/3:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "import pickle\n",
      "\n",
      "attributes_score = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "attributes_comms_num = pickle.load(open( \"../../pickle/image_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "\n",
      "attributes_score[\"score\"] = attributes_score[\"score\"]>2000\n",
      "attributes_comms_num[\"comms_num\"] = attributes_comms_num[\"comms_num\"]>200\n",
      "141/4:\n",
      "rf = RandomForestClassifier()\n",
      "rf_rmse_score, rf_exp_score, rf_score = analyze(attributes_score, rf, score_or_comms_num='score', reg_or_class='class')\n",
      "rf_exp_score.show_in_notebook(show_table=True)\n",
      "142/1:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "import pickle\n",
      "\n",
      "attributes_score = pickle.load(open( \"../../pickle/text_attributes_processed_score.pkl\", \"rb\" ))\n",
      "attributes_comms_num = pickle.load(open( \"../../pickle/text_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "142/2:\n",
      "rf = RandomForestRegressor()\n",
      "rf_rmse_score, rf_exp_score, rf_score = analyze(attributes_score, rf, score_or_comms_num='score')\n",
      "rf_exp_score.show_in_notebook(show_table=True)\n",
      "140/3:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "import pickle\n",
      "\n",
      "attributes_score = pickle.load(open( \"../../pickle/text_attributes_processed_score.pkl\", \"rb\" ))\n",
      "attributes_comms_num = pickle.load(open( \"../../pickle/text_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "\n",
      "attributes_score[\"score\"] = attributes_score[\"score\"]>2000\n",
      "attributes_comms_num[\"comms_num\"] = attributes_comms_num[\"comms_num\"]>200\n",
      "140/4:\n",
      "rf = RandomForestClassifier()\n",
      "rf_rmse_score, rf_exp_score, rf_score = analyze(attributes_score, rf, score_or_comms_num='score', reg_or_class='class')\n",
      "rf_exp_score.show_in_notebook(show_table=True)\n",
      "142/3:\n",
      "rf = RandomForestRegressor()\n",
      "rf_rmse_comms_num, rf_exp_comms_num, rf_comms_num = analyze(attributes_comms_num, rf, score_or_comms_num='comms_num')\n",
      "rf_exp_comms_num.show_in_notebook(show_table=True)\n",
      "142/4:\n",
      "mlp = MLPRegressor(hidden_layer_sizes=(15,), max_iter=500, learning_rate='adaptive')\n",
      "mlp_rmse_score, mlp_exp_score, mlp_score = analyze(attributes_score, mlp, score_or_comms_num='score')\n",
      "mlp_exp_score.show_in_notebook(show_table=True)\n",
      "142/5:\n",
      "mlp = MLPRegressor(hidden_layer_sizes=(15,), max_iter=500, learning_rate='adaptive')\n",
      "mlp_rmse_comms_num, mlp_exp_comms_num, mlp_comms_num = analyze(attributes_comms_num, mlp, score_or_comms_num='comms_num')\n",
      "mlp_exp_comms_num.show_in_notebook(show_table=True)\n",
      "145/1:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "import pickle\n",
      "\n",
      "attributes_score = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "attributes_comms_num = pickle.load(open( \"../../pickle/image_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "\n",
      "attributes_score[\"score\"] = attributes_score[\"score\"]>2000\n",
      "attributes_comms_num[\"comms_num\"] = attributes_comms_num[\"comms_num\"]>200\n",
      "145/2:\n",
      "rf = RandomForestClassifier()\n",
      "rf_rmse_score, rf_exp_score, rf_score = analyze(attributes_score, rf, score_or_comms_num='score', reg_or_class='class')\n",
      "rf_exp_score.show_in_notebook(show_table=True)\n",
      "145/3:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "import pickle\n",
      "\n",
      "attributes_score = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "attributes_comms_num = pickle.load(open( \"../../pickle/image_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "\n",
      "attributes_score[\"score\"] = attributes_score[\"score\"]>2000\n",
      "attributes_comms_num[\"comms_num\"] = attributes_comms_num[\"comms_num\"]>200\n",
      "145/4:\n",
      "rf = RandomForestClassifier()\n",
      "rf_rmse_score, rf_exp_score, rf_score = analyze(attributes_score, rf, score_or_comms_num='score', reg_or_class='class')\n",
      "rf_exp_score.show_in_notebook(show_table=True)\n",
      "145/5:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "import pickle\n",
      "\n",
      "attributes_score = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "attributes_comms_num = pickle.load(open( \"../../pickle/image_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "\n",
      "attributes_score[\"score\"] = attributes_score[\"score\"]>2000\n",
      "attributes_comms_num[\"comms_num\"] = attributes_comms_num[\"comms_num\"]>200\n",
      "145/6:\n",
      "rf = RandomForestClassifier()\n",
      "rf_rmse_score, rf_exp_score, rf_score = analyze(attributes_score, rf, score_or_comms_num='score', reg_or_class='class')\n",
      "rf_exp_score.show_in_notebook(show_table=True)\n",
      "146/1:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "import pickle\n",
      "\n",
      "attributes_score = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "attributes_comms_num = pickle.load(open( \"../../pickle/image_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "\n",
      "attributes_score[\"score\"] = attributes_score[\"score\"]>2000\n",
      "attributes_comms_num[\"comms_num\"] = attributes_comms_num[\"comms_num\"]>200\n",
      "146/2:\n",
      "rf = RandomForestClassifier()\n",
      "rf_rmse_score, rf_exp_score, rf_score = analyze(attributes_score, rf, score_or_comms_num='score', reg_or_class='class')\n",
      "rf_exp_score.show_in_notebook(show_table=True)\n",
      "147/1:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "import pickle\n",
      "\n",
      "attributes_score = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "attributes_comms_num = pickle.load(open( \"../../pickle/image_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "\n",
      "attributes_score[\"score\"] = attributes_score[\"score\"]>2000\n",
      "attributes_comms_num[\"comms_num\"] = attributes_comms_num[\"comms_num\"]>200\n",
      "147/2:\n",
      "rf = RandomForestClassifier()\n",
      "rf_rmse_score, rf_exp_score, rf_score = analyze(attributes_score, rf, score_or_comms_num='score', reg_or_class='class')\n",
      "rf_exp_score.show_in_notebook(show_table=True)\n",
      "148/1:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "import pickle\n",
      "\n",
      "attributes_score = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "attributes_comms_num = pickle.load(open( \"../../pickle/image_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "148/2:\n",
      "rf = RandomForestRegressor()\n",
      "rf_rmse_score, rf_exp_score, rf_score = analyze(attributes_score, rf, score_or_comms_num='score')\n",
      "rf_exp_score.show_in_notebook(show_table=True)\n",
      "150/1:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "import pickle\n",
      "\n",
      "attributes_score = pickle.load(open( \"../../pickle/text_attributes_processed_score.pkl\", \"rb\" ))\n",
      "attributes_comms_num = pickle.load(open( \"../../pickle/text_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "\n",
      "attributes_score[\"score\"] = attributes_score[\"score\"]>2000\n",
      "attributes_comms_num[\"comms_num\"] = attributes_comms_num[\"comms_num\"]>200\n",
      "148/3:\n",
      "rf = RandomForestRegressor()\n",
      "rf_rmse_comms_num, rf_exp_comms_num, rf_comms_num = analyze(attributes_comms_num, rf, score_or_comms_num='comms_num')\n",
      "rf_exp_comms_num.show_in_notebook(show_table=True)\n",
      "150/2:\n",
      "rf = RandomForestClassifier()\n",
      "rf_rmse_score, rf_exp_score, rf_score = analyze(attributes_score, rf, score_or_comms_num='score', reg_or_class='class')\n",
      "rf_exp_score.show_in_notebook(show_table=True)\n",
      "149/1:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "import pickle\n",
      "\n",
      "attributes_score = pickle.load(open( \"../../pickle/text_attributes_processed_score.pkl\", \"rb\" ))\n",
      "attributes_comms_num = pickle.load(open( \"../../pickle/text_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "149/2:\n",
      "rf = RandomForestRegressor()\n",
      "rf_rmse_score, rf_exp_score, rf_score = analyze(attributes_score, rf, score_or_comms_num='score')\n",
      "rf_exp_score.show_in_notebook(show_table=True)\n",
      "148/4:\n",
      "mlp = MLPRegressor(hidden_layer_sizes=(30,60), max_iter=500, learning_rate='adaptive')\n",
      "mlp_rmse_score, mlp_exp_score, mlp_score = analyze(attributes_score, mlp, score_or_comms_num='score')\n",
      "mlp_exp_score.show_in_notebook(show_table=True)\n",
      "149/3:\n",
      "rf = RandomForestRegressor()\n",
      "rf_rmse_comms_num, rf_exp_comms_num, rf_comms_num = analyze(attributes_comms_num, rf, score_or_comms_num='comms_num')\n",
      "rf_exp_comms_num.show_in_notebook(show_table=True)\n",
      "149/4:\n",
      "mlp = MLPRegressor(hidden_layer_sizes=(15,), max_iter=500, learning_rate='adaptive')\n",
      "mlp_rmse_score, mlp_exp_score, mlp_score = analyze(attributes_score, mlp, score_or_comms_num='score')\n",
      "mlp_exp_score.show_in_notebook(show_table=True)\n",
      "148/5:\n",
      "mlp = MLPRegressor(hidden_layer_sizes=(30,60), max_iter=500, learning_rate='adaptive')\n",
      "mlp_rmse_comms_num, mlp_exp_comms_num, mlp_comms_num = analyze(attributes_comms_num, mlp, score_or_comms_num='comms_num')\n",
      "mlp_exp_comms_num.show_in_notebook(show_table=True)\n",
      "149/5:\n",
      "mlp = MLPRegressor(hidden_layer_sizes=(15,), max_iter=500, learning_rate='adaptive')\n",
      "mlp_rmse_comms_num, mlp_exp_comms_num, mlp_comms_num = analyze(attributes_comms_num, mlp, score_or_comms_num='comms_num')\n",
      "mlp_exp_comms_num.show_in_notebook(show_table=True)\n",
      "151/1:\n",
      "print('PyDev console: using IPython 7.19.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['C:\\\\Users\\\\luker\\\\Documents\\\\Github\\\\PED', 'C:/Users/luker/Documents/Github/PED'])\n",
      "148/6:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "import pickle\n",
      "\n",
      "attributes_score = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "attributes_comms_num = pickle.load(open( \"../../pickle/image_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "148/7:\n",
      "rf = RandomForestRegressor()\n",
      "rf_rmse_score, rf_exp_score, rf_score = analyze(attributes_score, rf, score_or_comms_num='score')\n",
      "print(\"RMSE:\", rf_rmse_score)\n",
      "rf_exp_score.show_in_notebook(show_table=True)\n",
      "148/8:\n",
      "rf = RandomForestRegressor()\n",
      "rf_rmse_comms_num, rf_exp_comms_num, rf_comms_num = analyze(attributes_comms_num, rf, score_or_comms_num='comms_num')\n",
      "print(\"RMSE:\", rf_rmse_comms_num)\n",
      "rf_exp_comms_num.show_in_notebook(show_table=True)\n",
      "147/3:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "import pickle\n",
      "\n",
      "attributes_score = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "attributes_comms_num = pickle.load(open( \"../../pickle/image_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "\n",
      "attributes_score[\"score\"] = attributes_score[\"score\"]>2000\n",
      "attributes_comms_num[\"comms_num\"] = attributes_comms_num[\"comms_num\"]>200\n",
      "147/4:\n",
      "rf = RandomForestClassifier()\n",
      "rf_acc_score, rf_exp_score, rf_score = analyze(attributes_score, rf, score_or_comms_num='score', reg_or_class='class')\n",
      "print(\"Accuracy:\", rf_acc_score)\n",
      "rf_exp_score.show_in_notebook(show_table=True)\n",
      "148/9:\n",
      "mlp = MLPRegressor(hidden_layer_sizes=(30,60), max_iter=500, learning_rate='adaptive')\n",
      "mlp_rmse_score, mlp_exp_score, mlp_score = analyze(attributes_score, mlp, score_or_comms_num='score')\n",
      "print(\"RMSE:\", mlp_rmse_score)\n",
      "mlp_exp_score.show_in_notebook(show_table=True)\n",
      "148/10:\n",
      "mlp = MLPRegressor(hidden_layer_sizes=(30,60), max_iter=500, learning_rate='adaptive')\n",
      "mlp_rmse_comms_num, mlp_exp_comms_num, mlp_comms_num = analyze(attributes_comms_num, mlp, score_or_comms_num='comms_num')\n",
      "print(\"RMSE:\", mlp_rmse_comms_num)\n",
      "mlp_exp_comms_num.show_in_notebook(show_table=True)\n",
      "149/6:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "import pickle\n",
      "\n",
      "attributes_score = pickle.load(open( \"../../pickle/text_attributes_processed_score.pkl\", \"rb\" ))\n",
      "attributes_comms_num = pickle.load(open( \"../../pickle/text_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "149/7:\n",
      "rf = RandomForestRegressor()\n",
      "rf_rmse_score, rf_exp_score, rf_score = analyze(attributes_score, rf, score_or_comms_num='score')\n",
      "print(\"RMSE:\", rf_rmse_score)\n",
      "rf_exp_score.show_in_notebook(show_table=True)\n",
      "149/8:\n",
      "rf = RandomForestRegressor()\n",
      "rf_rmse_comms_num, rf_exp_comms_num, rf_comms_num = analyze(attributes_comms_num, rf, score_or_comms_num='comms_num')\n",
      "print(\"RMSE:\", rf_rmse_comms_num)\n",
      "rf_exp_comms_num.show_in_notebook(show_table=True)\n",
      "150/3:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "import pickle\n",
      "\n",
      "attributes_score = pickle.load(open( \"../../pickle/text_attributes_processed_score.pkl\", \"rb\" ))\n",
      "attributes_comms_num = pickle.load(open( \"../../pickle/text_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "\n",
      "attributes_score[\"score\"] = attributes_score[\"score\"]>2000\n",
      "attributes_comms_num[\"comms_num\"] = attributes_comms_num[\"comms_num\"]>200\n",
      "150/4:\n",
      "rf = RandomForestClassifier()\n",
      "rf_acc_score, rf_exp_score, rf_score = analyze(attributes_score, rf, score_or_comms_num='score', reg_or_class='class')\n",
      "print(\"Accuracy:\", rf_acc_score)\n",
      "rf_exp_score.show_in_notebook(show_table=True)\n",
      "149/9:\n",
      "mlp = MLPRegressor(hidden_layer_sizes=(15,), max_iter=500, learning_rate='adaptive')\n",
      "mlp_rmse_score, mlp_exp_score, mlp_score = analyze(attributes_score, mlp, score_or_comms_num='score')\n",
      "print(\"RMSE:\", mlp_rmse_score)\n",
      "mlp_exp_score.show_in_notebook(show_table=True)\n",
      "149/10:\n",
      "mlp = MLPRegressor(hidden_layer_sizes=(15,), max_iter=500, learning_rate='adaptive')\n",
      "mlp_rmse_comms_num, mlp_exp_comms_num, mlp_comms_num = analyze(attributes_comms_num, mlp, score_or_comms_num='comms_num')\n",
      "print(\"RMSE:\", mlp_rmse_comms_num)\n",
      "mlp_exp_comms_num.show_in_notebook(show_table=True)\n",
      "152/1:\n",
      "print('PyDev console: using IPython 7.19.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['C:\\\\Users\\\\luker\\\\Documents\\\\Github\\\\PED', 'C:/Users/luker/Documents/Github/PED'])\n",
      "153/1:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "import pickle\n",
      "\n",
      "comments = pickle.load(open( \"../../pickle/comments.pkl\", \"rb\" ))\n",
      "153/2:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "import pickle\n",
      "\n",
      "comments = pickle.load(open( \"../../pickle/comments.pkl\", \"rb\" ))\n",
      "comments\n",
      "153/3:\n",
      "times = []\n",
      "for comment in comments:\n",
      "    times.append(comment.created_utc)\n",
      "\n",
      "times\n",
      "153/4:\n",
      "times = []\n",
      "for comment in comments:\n",
      "    print(comment)\n",
      "    times.append(comment.created_utc)\n",
      "\n",
      "times\n",
      "153/5:\n",
      "times = []\n",
      "for post in comments:\n",
      "    for comment in post:\n",
      "        print(comment)\n",
      "        times.append(comment.created_utc)\n",
      "\n",
      "times\n",
      "153/6:\n",
      "from test import analyze\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.neural_network import MLPRegressor\n",
      "import pickle\n",
      "\n",
      "comments = pickle.load(open( \"../../pickle/comments.pkl\", \"rb\" ))\n",
      "comments\n",
      "153/7:\n",
      "times = []\n",
      "for post in comments:\n",
      "    for comment in post:\n",
      "        print(comment)\n",
      "        times.append(comment.created_utc)\n",
      "\n",
      "times\n",
      "153/8:\n",
      "times = []\n",
      "for id in comments:\n",
      "    for comment in comments[id]:\n",
      "        print(comment)\n",
      "        times.append(comment.created_utc)\n",
      "\n",
      "times\n",
      "153/9:\n",
      "times = []\n",
      "for id in comments:\n",
      "    for comment in comments[id]:\n",
      "        times.append(comment.created_utc)\n",
      "\n",
      "times\n",
      "153/10:\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "\n",
      "comments = pickle.load(open( \"../../pickle/comments.pkl\", \"rb\" ))\n",
      "comments\n",
      "153/11:\n",
      "plt.hist(times], bins=20)\n",
      "plt.ylabel('Count')\n",
      "plt.yscale('log')\n",
      "plt.xlabel('Data')\n",
      "\n",
      "plt.show()\n",
      "153/12:\n",
      "plt.hist(times, bins=20)\n",
      "plt.ylabel('Count')\n",
      "plt.yscale('log')\n",
      "plt.xlabel('Data')\n",
      "\n",
      "plt.show()\n",
      "153/13:\n",
      "plt.hist(times, bins=2)\n",
      "plt.ylabel('Count')\n",
      "plt.yscale('log')\n",
      "plt.xlabel('Data')\n",
      "\n",
      "plt.show()\n",
      "153/14:\n",
      "plt.hist(times, bins=200)\n",
      "plt.ylabel('Count')\n",
      "plt.yscale('log')\n",
      "plt.xlabel('Data')\n",
      "\n",
      "plt.show()\n",
      "153/15:\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "\n",
      "comments = pickle.load(open( \"../../pickle/comments.pkl\", \"rb\" ))\n",
      "comments\n",
      "153/16:\n",
      "times = []\n",
      "for id in comments:\n",
      "    for comment in comments[id]:\n",
      "        times.append(comment.created_utc)\n",
      "\n",
      "times\n",
      "153/17:\n",
      "plt.hist(times, bins=200)\n",
      "plt.ylabel('Count')\n",
      "plt.yscale('log')\n",
      "plt.xlabel('Data')\n",
      "\n",
      "plt.show()\n",
      "153/18:\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "\n",
      "comments = pickle.load(open( \"../../pickle/comments.pkl\", \"rb\" ))\n",
      "comments\n",
      "153/19:\n",
      "times = []\n",
      "for id in comments:\n",
      "    for comment in comments[id]:\n",
      "        times.append(comment.created_utc)\n",
      "\n",
      "times\n",
      "153/20:\n",
      "plt.hist(times, bins=200)\n",
      "plt.ylabel('Count')\n",
      "plt.yscale('log')\n",
      "plt.xlabel('Data')\n",
      "\n",
      "plt.show()\n",
      "153/21:\n",
      "plt.hist(times[:20], bins=200)\n",
      "plt.ylabel('Count')\n",
      "plt.yscale('log')\n",
      "plt.xlabel('Data')\n",
      "\n",
      "plt.show()\n",
      "153/22:\n",
      "plt.hist(times[1:], bins=200)\n",
      "plt.ylabel('Count')\n",
      "plt.yscale('log')\n",
      "plt.xlabel('Data')\n",
      "\n",
      "plt.show()\n",
      "153/23:\n",
      "plt.hist(times[1:], bins=20)\n",
      "plt.ylabel('Count')\n",
      "plt.yscale('log')\n",
      "plt.xlabel('Data')\n",
      "\n",
      "plt.show()\n",
      "153/24:\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "\n",
      "comments = pickle.load(open( \"../../pickle/comments.pkl\", \"rb\" ))\n",
      "comments\n",
      "153/25:\n",
      "times = []\n",
      "for id in comments:\n",
      "    for comment in comments[id]:\n",
      "        times.append(comment.created_utc)\n",
      "\n",
      "times\n",
      "153/26:\n",
      "times = {}\n",
      "for id in comments:\n",
      "    times[id] = []\n",
      "    for comment in comments[id]:\n",
      "        times.append(comment.created_utc)\n",
      "\n",
      "times\n",
      "153/27:\n",
      "times = {}\n",
      "for id in comments:\n",
      "    times[id] = []\n",
      "    for comment in comments[id]:\n",
      "        times[id].append(comment.created_utc)\n",
      "\n",
      "times\n",
      "153/28:\n",
      "for id in comments:\n",
      "    plt.hist(times[1:], bins=20)\n",
      "    plt.ylabel('Count')\n",
      "    plt.yscale('log')\n",
      "    plt.xlabel('Data')\n",
      "\n",
      "    plt.show()\n",
      "153/29:\n",
      "for id in comments:\n",
      "    plt.hist(times[id][1:], bins=20)\n",
      "    plt.ylabel('Count')\n",
      "    plt.yscale('log')\n",
      "    plt.xlabel('Data')\n",
      "\n",
      "    plt.show()\n",
      "153/30:\n",
      "for id in comments:\n",
      "    plt.hist(times[id][1:], bins=50)\n",
      "    plt.ylabel('Count')\n",
      "    plt.yscale('log')\n",
      "    plt.xlabel('Data')\n",
      "\n",
      "    plt.show()\n",
      "153/31:\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "\n",
      "comments_circa_397 = pickle.load(open( \"../../pickle/comments_circa_397.pkl\", \"rb\" ))\n",
      "comments_circa_1044 = pickle.load(open( \"../../pickle/comments_circa_1044.pkl\", \"rb\" ))\n",
      "comments_circa_97998 = pickle.load(open( \"../../pickle/comments_circa_97998.pkl\", \"rb\" ))\n",
      "comments_circa_97998\n",
      "153/32:\n",
      "times_circa_397 = {}\n",
      "for id in comments_circa_397:\n",
      "    times_circa_397[id] = []\n",
      "    for comment in comments_circa_397[id]:\n",
      "        times_circa_397[id].append(comment.created_utc)\n",
      "        \n",
      "time_circa_1044 = {}\n",
      "for id in comments_circa_1044:\n",
      "    time_circa_1044[id] = []\n",
      "    for comment in comments_circa_1044[id]:\n",
      "        time_circa_1044[id].append(comment.created_utc)\n",
      "        \n",
      "time_circa_97998 = {}\n",
      "for id in comments_circa_97998:\n",
      "    time_circa_97998[id] = []\n",
      "    for comment in comments_circa_97998[id]:\n",
      "        time_circa_97998[id].append(comment.created_utc)\n",
      "153/33:\n",
      "for id in times_circa_397:\n",
      "    plt.hist(times_circa_397[id][1:], bins=50)\n",
      "    plt.ylabel('Count')\n",
      "    plt.yscale('log')\n",
      "    plt.xlabel('Data')\n",
      "\n",
      "    plt.show()\n",
      "153/34:\n",
      "for id in time_circa_1044:\n",
      "    plt.hist(time_circa_1044[id][1:], bins=50)\n",
      "    plt.ylabel('Count')\n",
      "    plt.yscale('log')\n",
      "    plt.xlabel('Data')\n",
      "\n",
      "    plt.show()\n",
      "153/35:\n",
      "for id in comments_circa_97998:\n",
      "    plt.hist(comments_circa_97998[id][1:], bins=50)\n",
      "    plt.ylabel('Count')\n",
      "    plt.yscale('log')\n",
      "    plt.xlabel('Data')\n",
      "\n",
      "    plt.show()\n",
      "153/36:\n",
      "for id in time_circa_97998:\n",
      "    plt.hist(time_circa_97998[id][1:], bins=50)\n",
      "    plt.ylabel('Count')\n",
      "    plt.yscale('log')\n",
      "    plt.xlabel('Data')\n",
      "\n",
      "    plt.show()\n",
      "153/37:\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "\n",
      "comments_circa_397 = pickle.load(open( \"../../pickle/comments_circa_397.pkl\", \"rb\" ))\n",
      "comments_circa_1044 = pickle.load(open( \"../../pickle/comments_circa_1044.pkl\", \"rb\" ))\n",
      "comments_circa_97998 = pickle.load(open( \"../../pickle/comments_circa_97998.pkl\", \"rb\" ))\n",
      "comments_circa_97998\n",
      "153/38:\n",
      "times_circa_397 = {}\n",
      "for id in comments_circa_397:\n",
      "    times_circa_397[id] = []\n",
      "    for comment in comments_circa_397[id]:\n",
      "        times_circa_397[id].append(comment.created_utc)\n",
      "        \n",
      "time_circa_1044 = {}\n",
      "for id in comments_circa_1044:\n",
      "    time_circa_1044[id] = []\n",
      "    for comment in comments_circa_1044[id]:\n",
      "        time_circa_1044[id].append(comment.created_utc)\n",
      "        \n",
      "time_circa_97998 = {}\n",
      "for id in comments_circa_97998:\n",
      "    time_circa_97998[id] = []\n",
      "    for comment in comments_circa_97998[id]:\n",
      "        time_circa_97998[id].append(comment.created_utc)\n",
      "153/39:\n",
      "for id in times_circa_397:\n",
      "    plt.hist(times_circa_397[id][5:], bins=50)\n",
      "    plt.ylabel('Count')\n",
      "    plt.yscale('log')\n",
      "    plt.xlabel('Data')\n",
      "\n",
      "    plt.show()\n",
      "153/40:\n",
      "for id in time_circa_1044:\n",
      "    plt.hist(time_circa_1044[id][5:], bins=50)\n",
      "    plt.ylabel('Count')\n",
      "    plt.yscale('log')\n",
      "    plt.xlabel('Data')\n",
      "\n",
      "    plt.show()\n",
      "153/41:\n",
      "for id in time_circa_97998:\n",
      "    plt.hist(time_circa_97998[id][5:], bins=50)\n",
      "    plt.ylabel('Count')\n",
      "    plt.yscale('log')\n",
      "    plt.xlabel('Data')\n",
      "\n",
      "    plt.show()\n",
      "153/42:\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "\n",
      "comments_circa_397 = pickle.load(open( \"../../pickle/comments_circa_397.pkl\", \"rb\" ))\n",
      "comments_circa_1044 = pickle.load(open( \"../../pickle/comments_circa_1044.pkl\", \"rb\" ))\n",
      "comments_circa_97998 = pickle.load(open( \"../../pickle/comments_circa_97998.pkl\", \"rb\" ))\n",
      "comments_circa_97998\n",
      "153/43:\n",
      "times_circa_397 = {}\n",
      "for id in comments_circa_397:\n",
      "    times_circa_397[id] = []\n",
      "    for comment in comments_circa_397[id]:\n",
      "        times_circa_397[id].append(comment.created_utc)\n",
      "        \n",
      "time_circa_1044 = {}\n",
      "for id in comments_circa_1044:\n",
      "    time_circa_1044[id] = []\n",
      "    for comment in comments_circa_1044[id]:\n",
      "        time_circa_1044[id].append(comment.created_utc)\n",
      "        \n",
      "time_circa_97998 = {}\n",
      "for id in comments_circa_97998:\n",
      "    time_circa_97998[id] = []\n",
      "    for comment in comments_circa_97998[id]:\n",
      "        time_circa_97998[id].append(comment.created_utc)\n",
      "153/44:\n",
      "for id in times_circa_397:\n",
      "    plt.hist(times_circa_397[id][1:], bins=50)\n",
      "    plt.ylabel('Count')\n",
      "    plt.yscale('log')\n",
      "    plt.xlabel('Data')\n",
      "\n",
      "    plt.show()\n",
      "153/45:\n",
      "for id in time_circa_1044:\n",
      "    plt.hist(time_circa_1044[id][1:], bins=50)\n",
      "    plt.ylabel('Count')\n",
      "    plt.yscale('log')\n",
      "    plt.xlabel('Data')\n",
      "\n",
      "    plt.show()\n",
      "153/46:\n",
      "for id in time_circa_97998:\n",
      "    plt.hist(time_circa_97998[id][1:], bins=50)\n",
      "    plt.ylabel('Count')\n",
      "    plt.yscale('log')\n",
      "    plt.xlabel('Data')\n",
      "\n",
      "    plt.show()\n",
      "156/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "\n",
      "comments_circa_397 = pickle.load(open( \"../../pickle/comments_circa_397.pkl\", \"rb\" ))\n",
      "comments_circa_1044 = pickle.load(open( \"../../pickle/comments_circa_1044.pkl\", \"rb\" ))\n",
      "comments_circa_97998 = pickle.load(open( \"../../pickle/comments_circa_97998.pkl\", \"rb\" ))\n",
      "comments_circa_97998\n",
      "156/2:\n",
      "# for i in comments_circa_397.values():\n",
      "#     print(i[:10])\n",
      "#     i.sort(key=lambda x: x.created)\n",
      "#     print(i[:10])\n",
      "#     for c in i:\n",
      "#         print(c.created, len(c.body), c.body)\n",
      "156/3:\n",
      "import pandas\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "156/4: df.loc[df['id'] == 'l6ea1b'].iloc[0].created\n",
      "156/5:\n",
      "times_circa_397 = {}\n",
      "times_circa_1044 = {}\n",
      "times_circa_97998 = {}\n",
      "for times, comments in zip([times_circa_397, times_circa_1044, times_circa_97998],[comments_circa_397, comments_circa_1044,comments_circa_97998]):\n",
      "    for id in comments:\n",
      "        times[id] = []\n",
      "        created = df.loc[df['id'] == id].iloc[0].created\n",
      "        for comment in comments[id]:\n",
      "                times[id].append(comment.created_utc - created)\n",
      "    for id in times.keys():\n",
      "        times[id].sort()\n",
      "        \n",
      "print(times_circa_97998)\n",
      "156/6:\n",
      "bins_num = 50\n",
      "\n",
      "max_delta_tolerance = 3600\n",
      "\n",
      "for id,v in times_circa_397.items():\n",
      "    filtered = []\n",
      "    for i in range(len(v)):\n",
      "        if i == len(v) - 1 or v[i+1]-v[i] < max_delta_tolerance:\n",
      "            filtered.append(v[i])\n",
      "        else:\n",
      "            break\n",
      "    n, bins, patches = plt.hist(filtered, bins=bins_num)\n",
      "    plt.ylabel('Count')\n",
      "    plt.yscale('log')\n",
      "    plt.xlabel('Time')\n",
      "    plt.title('filtered')\n",
      "    plt.show()\n",
      "#     n, bins, patches = plt.hist(v, bins=bins_num)\n",
      "#     plt.ylabel('Count')\n",
      "#     plt.yscale('log')\n",
      "#     plt.xlabel('Time')\n",
      "#     plt.title('original')\n",
      "#     plt.show()\n",
      "156/7:\n",
      "for id,v in times_circa_1044.items():\n",
      "    filtered = []\n",
      "    for i in range(len(v)):\n",
      "        if i == len(v) - 1 or v[i+1]-v[i] < max_delta_tolerance:\n",
      "            filtered.append(v[i])\n",
      "        else:\n",
      "            break\n",
      "    n, bins, patches = plt.hist(filtered, bins=bins_num)\n",
      "    plt.ylabel('Count')\n",
      "    plt.yscale('log')\n",
      "    plt.xlabel('Time')\n",
      "    plt.title('filtered')\n",
      "    plt.show()\n",
      "156/8:\n",
      "for id,v in times_circa_97998.items():\n",
      "    filtered = []\n",
      "    for i in range(len(v)):\n",
      "        if i == len(v) - 1 or v[i+1]-v[i] < max_delta_tolerance:\n",
      "            filtered.append(v[i])\n",
      "        else:\n",
      "            break\n",
      "    n, bins, patches = plt.hist(filtered, bins=bins_num)\n",
      "    plt.ylabel('Count')\n",
      "    plt.yscale('log')\n",
      "    plt.xlabel('Time')\n",
      "    plt.title('filtered')\n",
      "    plt.show()\n",
      "156/9:\n",
      "bins_num = 100\n",
      "\n",
      "max_delta_tolerance = 3600\n",
      "\n",
      "for id,v in times_circa_397.items():\n",
      "    filtered = []\n",
      "    for i in range(len(v)):\n",
      "        if i == len(v) - 1 or v[i+1]-v[i] < max_delta_tolerance:\n",
      "            filtered.append(v[i])\n",
      "        else:\n",
      "            break\n",
      "    n, bins, patches = plt.hist(filtered, bins=bins_num)\n",
      "    plt.ylabel('Count')\n",
      "    plt.yscale('log')\n",
      "    plt.xlabel('Time')\n",
      "    plt.title('filtered')\n",
      "    plt.show()\n",
      "#     n, bins, patches = plt.hist(v, bins=bins_num)\n",
      "#     plt.ylabel('Count')\n",
      "#     plt.yscale('log')\n",
      "#     plt.xlabel('Time')\n",
      "#     plt.title('original')\n",
      "#     plt.show()\n",
      "156/10:\n",
      "for id,v in times_circa_1044.items():\n",
      "    filtered = []\n",
      "    for i in range(len(v)):\n",
      "        if i == len(v) - 1 or v[i+1]-v[i] < max_delta_tolerance:\n",
      "            filtered.append(v[i])\n",
      "        else:\n",
      "            break\n",
      "    n, bins, patches = plt.hist(filtered, bins=bins_num)\n",
      "    plt.ylabel('Count')\n",
      "    plt.yscale('log')\n",
      "    plt.xlabel('Time')\n",
      "    plt.title('filtered')\n",
      "    plt.show()\n",
      "156/11:\n",
      "for id,v in times_circa_97998.items():\n",
      "    filtered = []\n",
      "    for i in range(len(v)):\n",
      "        if i == len(v) - 1 or v[i+1]-v[i] < max_delta_tolerance:\n",
      "            filtered.append(v[i])\n",
      "        else:\n",
      "            break\n",
      "    n, bins, patches = plt.hist(filtered, bins=bins_num)\n",
      "    plt.ylabel('Count')\n",
      "    plt.yscale('log')\n",
      "    plt.xlabel('Time')\n",
      "    plt.title('filtered')\n",
      "    plt.show()\n",
      "156/12:\n",
      "bins_num = 40\n",
      "\n",
      "max_delta_tolerance = 3600\n",
      "\n",
      "for id,v in times_circa_397.items():\n",
      "    filtered = []\n",
      "    for i in range(len(v)):\n",
      "        if i == len(v) - 1 or v[i+1]-v[i] < max_delta_tolerance:\n",
      "            filtered.append(v[i])\n",
      "        else:\n",
      "            break\n",
      "    n, bins, patches = plt.hist(filtered, bins=bins_num)\n",
      "    plt.ylabel('Count')\n",
      "    plt.yscale('log')\n",
      "    plt.xlabel('Time')\n",
      "    plt.title('filtered')\n",
      "    plt.show()\n",
      "#     n, bins, patches = plt.hist(v, bins=bins_num)\n",
      "#     plt.ylabel('Count')\n",
      "#     plt.yscale('log')\n",
      "#     plt.xlabel('Time')\n",
      "#     plt.title('original')\n",
      "#     plt.show()\n",
      "156/13:\n",
      "for id,v in times_circa_1044.items():\n",
      "    filtered = []\n",
      "    for i in range(len(v)):\n",
      "        if i == len(v) - 1 or v[i+1]-v[i] < max_delta_tolerance:\n",
      "            filtered.append(v[i])\n",
      "        else:\n",
      "            break\n",
      "    n, bins, patches = plt.hist(filtered, bins=bins_num)\n",
      "    plt.ylabel('Count')\n",
      "    plt.yscale('log')\n",
      "    plt.xlabel('Time')\n",
      "    plt.title('filtered')\n",
      "    plt.show()\n",
      "156/14:\n",
      "for id,v in times_circa_97998.items():\n",
      "    filtered = []\n",
      "    for i in range(len(v)):\n",
      "        if i == len(v) - 1 or v[i+1]-v[i] < max_delta_tolerance:\n",
      "            filtered.append(v[i])\n",
      "        else:\n",
      "            break\n",
      "    n, bins, patches = plt.hist(filtered, bins=bins_num)\n",
      "    plt.ylabel('Count')\n",
      "    plt.yscale('log')\n",
      "    plt.xlabel('Time')\n",
      "    plt.title('filtered')\n",
      "    plt.show()\n",
      "156/15:\n",
      "bins_num = 40\n",
      "\n",
      "max_delta_tolerance = 3600\n",
      "\n",
      "for id,v in times_circa_397.items():\n",
      "    filtered = []\n",
      "    for i in range(len(v)):\n",
      "        if i == len(v) - 1 or v[i+1]-v[i] < max_delta_tolerance:\n",
      "            filtered.append(v[i])\n",
      "        else:\n",
      "            break\n",
      "    n, bins, patches = plt.hist(filtered, bins=bins_num)\n",
      "    print(filtered)\n",
      "    plt.ylabel('Count')\n",
      "    plt.yscale('log')\n",
      "    plt.xlabel('Time')\n",
      "    plt.title('filtered')\n",
      "    plt.show()\n",
      "#     n, bins, patches = plt.hist(v, bins=bins_num)\n",
      "#     plt.ylabel('Count')\n",
      "#     plt.yscale('log')\n",
      "#     plt.xlabel('Time')\n",
      "#     plt.title('original')\n",
      "#     plt.show()\n",
      "156/16:\n",
      "bins_num = 40\n",
      "\n",
      "max_delta_tolerance = 3600\n",
      "\n",
      "for id,v in times_circa_397.items():\n",
      "    filtered = []\n",
      "    for i in range(len(v)):\n",
      "        if i == len(v) - 1 or v[i+1]-v[i] < max_delta_tolerance:\n",
      "            filtered.append(v[i])\n",
      "        else:\n",
      "            break\n",
      "    n, bins, patches = plt.hist(filtered, bins=bins_num)\n",
      "    plt.ylabel('Count')\n",
      "    plt.yscale('log')\n",
      "    plt.xlabel('Time')\n",
      "    plt.title('filtered')\n",
      "    plt.show()\n",
      "#     n, bins, patches = plt.hist(v, bins=bins_num)\n",
      "#     plt.ylabel('Count')\n",
      "#     plt.yscale('log')\n",
      "#     plt.xlabel('Time')\n",
      "#     plt.title('original')\n",
      "#     plt.show()\n",
      "157/1:\n",
      "from sklearn.manifold import MDS\n",
      "import numpy as np\n",
      "158/1:\n",
      "print('PyDev console: using IPython 7.19.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['C:\\\\Users\\\\luker\\\\Documents\\\\Github\\\\SkaiWiD', 'C:/Users/luker/Documents/Github/SkaiWiD'])\n",
      "159/1:\n",
      "from sklearn.manifold import MDS\n",
      "import numpy as np\n",
      "\n",
      "sklearn.datasets.make swiss roll\n",
      "159/2:\n",
      "from sklearn.manifold import MDS\n",
      "import numpy as np\n",
      "159/3: X, color = datasets.make_swiss_roll()\n",
      "159/4: X, color = sklearn.datasets.make_swiss_roll()\n",
      "159/5:\n",
      "from sklearn.manifold import MDS\n",
      "from sklearn import datasets\n",
      "import numpy as np\n",
      "159/6: X, color = sklearn.datasets.make_swiss_roll()\n",
      "159/7: X, color = datasets.make_swiss_roll()\n",
      "159/8:\n",
      "X, color = datasets.make_swiss_roll()\n",
      "mds = MDS(n_components=2)\n",
      "X_transformed = mds.fit_transform(X)\n",
      "159/9:\n",
      "from collections import OrderedDict\n",
      "from functools import partial\n",
      "from time import time\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "from matplotlib.ticker import NullFormatter\n",
      "\n",
      "from sklearn import manifold, datasets\n",
      "\n",
      "# Next line to silence pyflakes. This import is needed.\n",
      "Axes3D\n",
      "\n",
      "n_points = 1000\n",
      "X, color = datasets.make_s_curve(n_points, random_state=0)\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "# Create figure\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "fig.suptitle(\"Manifold Learning with %i points, %i neighbors\"\n",
      "             % (1000, n_neighbors), fontsize=14)\n",
      "\n",
      "# Add 3d scatter plot\n",
      "ax = fig.add_subplot(251, projection='3d')\n",
      "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)\n",
      "ax.view_init(4, -72)\n",
      "\n",
      "# Set-up manifold methods\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "# Plot results\n",
      "for i, (label, method) in enumerate(methods.items()):\n",
      "    t0 = time()\n",
      "    Y = method.fit_transform(X)\n",
      "    t1 = time()\n",
      "    print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "    ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "    ax.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\n",
      "    ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "    ax.xaxis.set_major_formatter(NullFormatter())\n",
      "    ax.yaxis.set_major_formatter(NullFormatter())\n",
      "    ax.axis('tight')\n",
      "\n",
      "plt.show()\n",
      "159/10:\n",
      "from collections import OrderedDict\n",
      "from functools import partial\n",
      "from time import time\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "from matplotlib.ticker import NullFormatter\n",
      "\n",
      "from sklearn import manifold, datasets\n",
      "\n",
      "# Next line to silence pyflakes. This import is needed.\n",
      "Axes3D\n",
      "\n",
      "X, color = datasets.make_s_curve()\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "# Create figure\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "fig.suptitle(\"Manifold Learning with %i points, %i neighbors\"\n",
      "             % (1000, n_neighbors), fontsize=14)\n",
      "\n",
      "# Add 3d scatter plot\n",
      "ax = fig.add_subplot(251, projection='3d')\n",
      "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)\n",
      "ax.view_init(4, -72)\n",
      "\n",
      "# Set-up manifold methods\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "# Plot results\n",
      "for i, (label, method) in enumerate(methods.items()):\n",
      "    t0 = time()\n",
      "    Y = method.fit_transform(X)\n",
      "    t1 = time()\n",
      "    print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "    ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "    ax.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\n",
      "    ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "    ax.xaxis.set_major_formatter(NullFormatter())\n",
      "    ax.yaxis.set_major_formatter(NullFormatter())\n",
      "    ax.axis('tight')\n",
      "\n",
      "plt.show()\n",
      "159/11:\n",
      "from collections import OrderedDict\n",
      "from functools import partial\n",
      "from time import time\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "from matplotlib.ticker import NullFormatter\n",
      "\n",
      "from sklearn import manifold, datasets\n",
      "\n",
      "# Next line to silence pyflakes. This import is needed.\n",
      "Axes3D\n",
      "\n",
      "n_points = 1000\n",
      "X, color = datasets.make_s_curve()\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "# Create figure\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "fig.suptitle(\"Manifold Learning with %i points, %i neighbors\"\n",
      "             % (1000, n_neighbors), fontsize=14)\n",
      "\n",
      "# Add 3d scatter plot\n",
      "ax = fig.add_subplot(251, projection='3d')\n",
      "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)\n",
      "ax.view_init(4, -72)\n",
      "\n",
      "# Set-up manifold methods\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "# Plot results\n",
      "for i, (label, method) in enumerate(methods.items()):\n",
      "    t0 = time()\n",
      "    Y = method.fit_transform(X)\n",
      "    t1 = time()\n",
      "    print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "    ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "    ax.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\n",
      "    ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "    ax.xaxis.set_major_formatter(NullFormatter())\n",
      "    ax.yaxis.set_major_formatter(NullFormatter())\n",
      "    ax.axis('tight')\n",
      "\n",
      "plt.show()\n",
      "159/12:\n",
      "Axes3D\n",
      "\n",
      "n_points = 1000\n",
      "X, color = datasets.make_s_curve()\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "# Create figure\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "fig.suptitle(\"Manifold Learning with %i points, %i neighbors\"\n",
      "             % (1000, n_neighbors), fontsize=14)\n",
      "\n",
      "# Add 3d scatter plot\n",
      "ax = fig.add_subplot(251, projection='3d')\n",
      "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)\n",
      "ax.view_init(4, -72)\n",
      "\n",
      "# Set-up manifold methods\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "# Plot results\n",
      "for i, (label, method) in enumerate(methods.items()):\n",
      "    t0 = time()\n",
      "    Y = method.fit_transform(X)\n",
      "    t1 = time()\n",
      "    print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "    ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "    ax.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\n",
      "    ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "    ax.xaxis.set_major_formatter(NullFormatter())\n",
      "    ax.yaxis.set_major_formatter(NullFormatter())\n",
      "    ax.axis('tight')\n",
      "\n",
      "plt.show()\n",
      "159/13:\n",
      "cars = pd.read_csv('data/cars.csv')  \n",
      "\n",
      "n_points = 1000\n",
      "swiss_X, swiss_Y = datasets.make_swiss_roll(n_points)\n",
      "159/14:\n",
      "from collections import OrderedDict\n",
      "from functools import partial\n",
      "from time import time\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "from matplotlib.ticker import NullFormatter\n",
      "\n",
      "from sklearn import manifold, datasets\n",
      "import pandas as pd\n",
      "159/15:\n",
      "cars = pd.read_csv('data/cars.csv')  \n",
      "\n",
      "n_points = 1000\n",
      "swiss_X, swiss_Y = datasets.make_swiss_roll(n_points)\n",
      "159/16:\n",
      "cars = pd.read_csv('./data/cars.csv')  \n",
      "\n",
      "n_points = 1000\n",
      "swiss_X, swiss_Y = datasets.make_swiss_roll(n_points)\n",
      "159/17:\n",
      "from collections import OrderedDict\n",
      "from functools import partial\n",
      "from time import time\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "from matplotlib.ticker import NullFormatter\n",
      "\n",
      "from sklearn import manifold, datasets\n",
      "import pandas as pd\n",
      "159/18:\n",
      "cars = pd.read_csv('./data/cars.csv')  \n",
      "\n",
      "n_points = 1000\n",
      "swiss_X, swiss_Y = datasets.make_swiss_roll(n_points)\n",
      "159/19:\n",
      "Axes3D\n",
      "\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "# Create figure\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "fig.suptitle(\"Manifold Learning with %i points, %i neighbors\"\n",
      "             % (1000, n_neighbors), fontsize=14)\n",
      "\n",
      "# Add 3d scatter plot\n",
      "ax = fig.add_subplot(251, projection='3d')\n",
      "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)\n",
      "ax.view_init(4, -72)\n",
      "\n",
      "# Set-up manifold methods\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "# Plot results\n",
      "for i, (label, method) in enumerate(methods.items()):\n",
      "    t0 = time()\n",
      "    Y = method.fit_transform(X)\n",
      "    t1 = time()\n",
      "    print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "    ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "    ax.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\n",
      "    ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "    ax.xaxis.set_major_formatter(NullFormatter())\n",
      "    ax.yaxis.set_major_formatter(NullFormatter())\n",
      "    ax.axis('tight')\n",
      "\n",
      "plt.show()\n",
      "159/20:\n",
      "Axes3D\n",
      "\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "# Create figure\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "fig.suptitle(\"Manifold Learning with %i points, %i neighbors\"\n",
      "             % (1000, n_neighbors), fontsize=14)\n",
      "\n",
      "# Add 3d scatter plot\n",
      "ax = fig.add_subplot(251, projection='3d')\n",
      "ax.scatter(swiss_X[:, 0], swiss_X[:, 1], swiss_X[:, 2], c=color, cmap=plt.cm.Spectral)\n",
      "ax.view_init(4, -72)\n",
      "\n",
      "# Set-up manifold methods\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "# Plot results\n",
      "for i, (label, method) in enumerate(methods.items()):\n",
      "    t0 = time()\n",
      "    Y = method.fit_transform(X)\n",
      "    t1 = time()\n",
      "    print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "    ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "    ax.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\n",
      "    ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "    ax.xaxis.set_major_formatter(NullFormatter())\n",
      "    ax.yaxis.set_major_formatter(NullFormatter())\n",
      "    ax.axis('tight')\n",
      "\n",
      "plt.show()\n",
      "159/21:\n",
      "cars = pd.read_csv('./data/cars.csv')  \n",
      "\n",
      "n_points = 1000\n",
      "swiss_X, swiss_Y = datasets.make_swiss_roll()\n",
      "159/22:\n",
      "Axes3D\n",
      "\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "# Create figure\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "fig.suptitle(\"Manifold Learning with %i points, %i neighbors\"\n",
      "             % (1000, n_neighbors), fontsize=14)\n",
      "\n",
      "# Add 3d scatter plot\n",
      "ax = fig.add_subplot(251, projection='3d')\n",
      "ax.scatter(swiss_X[:, 0], swiss_X[:, 1], swiss_X[:, 2], c=swiss_Y, cmap=plt.cm.Spectral)\n",
      "ax.view_init(4, -72)\n",
      "\n",
      "# Set-up manifold methods\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "# Plot results\n",
      "for i, (label, method) in enumerate(methods.items()):\n",
      "    t0 = time()\n",
      "    Y = method.fit_transform(X)\n",
      "    t1 = time()\n",
      "    print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "    ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "    ax.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\n",
      "    ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "    ax.xaxis.set_major_formatter(NullFormatter())\n",
      "    ax.yaxis.set_major_formatter(NullFormatter())\n",
      "    ax.axis('tight')\n",
      "\n",
      "plt.show()\n",
      "159/23:\n",
      "cars = pd.read_csv('./data/cars.csv')  \n",
      "\n",
      "n_points = 1000\n",
      "swiss_X, swiss_Y = datasets.make_swiss_roll(n_points)\n",
      "159/24:\n",
      "Axes3D\n",
      "\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "# Create figure\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "fig.suptitle(\"Manifold Learning with %i points, %i neighbors\"\n",
      "             % (1000, n_neighbors), fontsize=14)\n",
      "\n",
      "# Add 3d scatter plot\n",
      "ax = fig.add_subplot(251, projection='3d')\n",
      "ax.scatter(swiss_X[:, 0], swiss_X[:, 1], swiss_X[:, 2], c=swiss_Y, cmap=plt.cm.Spectral)\n",
      "ax.view_init(4, -72)\n",
      "\n",
      "# Set-up manifold methods\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "# Plot results\n",
      "for i, (label, method) in enumerate(methods.items()):\n",
      "    t0 = time()\n",
      "    Y = method.fit_transform(X)\n",
      "    t1 = time()\n",
      "    print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "    ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "    ax.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\n",
      "    ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "    ax.xaxis.set_major_formatter(NullFormatter())\n",
      "    ax.yaxis.set_major_formatter(NullFormatter())\n",
      "    ax.axis('tight')\n",
      "\n",
      "plt.show()\n",
      "159/25:\n",
      "Axes3D\n",
      "\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "# Create figure\n",
      "fig = plt.figure(figsize=(15, 8))\n",
      "fig.suptitle(\"Manifold Learning with %i points, %i neighbors\"\n",
      "             % (1000, n_neighbors), fontsize=14)\n",
      "\n",
      "# Add 3d scatter plot\n",
      "ax = fig.add_subplot(251, projection='3d')\n",
      "ax.scatter(swiss_X[:, 0], swiss_X[:, 1], swiss_X[:, 2], c=swiss_Y, cmap=plt.cm.Spectral)\n",
      "ax.view_init(4, -72)\n",
      "\n",
      "# Set-up manifold methods\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "# Plot results\n",
      "for i, (label, method) in enumerate(methods.items()):\n",
      "    t0 = time()\n",
      "    Y = method.fit_transform(swiss_X)\n",
      "    t1 = time()\n",
      "    print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "    ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "    ax.scatter(Y[:, 0], Y[:, 1], c=swiss_Y, cmap=plt.cm.Spectral)\n",
      "    ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "    ax.xaxis.set_major_formatter(NullFormatter())\n",
      "    ax.yaxis.set_major_formatter(NullFormatter())\n",
      "    ax.axis('tight')\n",
      "\n",
      "plt.show()\n",
      "159/26:\n",
      "cars = pd.read_csv('./data/cars.csv')  \n",
      "cars_X = cars[0]\n",
      "\n",
      "n_points = 1000\n",
      "swiss_X, swiss_Y = datasets.make_swiss_roll(n_points)\n",
      "159/27:\n",
      "cars = pd.read_csv('./data/cars.csv', header=None)  \n",
      "cars_X = cars[0]\n",
      "\n",
      "n_points = 1000\n",
      "swiss_X, swiss_Y = datasets.make_swiss_roll(n_points)\n",
      "159/28:\n",
      "cars = pd.read_csv('./data/cars.csv', header=None)  \n",
      "cars_X = cars[0]\n",
      "\n",
      "n_points = 1000\n",
      "swiss_X, swiss_Y = datasets.make_swiss_roll(n_points)\n",
      "159/29:\n",
      "cars = pd.read_csv('./data/cars.csv', header=None)  \n",
      "cars_X = cars[0]\n",
      "cars_Y = cars[1:]\n",
      "\n",
      "n_points = 1000\n",
      "swiss_X, swiss_Y = datasets.make_swiss_roll(n_points)\n",
      "159/30:\n",
      "data_X = []\n",
      "data_Y = []\n",
      "\n",
      "cars = pd.read_csv('./data/cars.csv', header=None)  \n",
      "data_X.append(cars[0])\n",
      "data_Y.append(cars[1:])\n",
      "\n",
      "n_points = 1000\n",
      "swiss_X, swiss_Y = datasets.make_swiss_roll(n_points)\n",
      "data_X.append(swiss_X)\n",
      "data_Y.append(swiss_Y)\n",
      "159/31:\n",
      "Axes3D\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "for i in range(len(data_Y)):\n",
      "    fig = plt.figure(figsize=(15, 8))\n",
      "    fig.suptitle(\"Manifold Learning with %i points, %i neighbors\"\n",
      "                 % (1000, n_neighbors), fontsize=14)\n",
      "\n",
      "    ax = fig.add_subplot(251, projection='3d')\n",
      "    ax.scatter(swiss_X[i, :, 0], swiss_X[i, :, 1], swiss_X[i, :, 2], c=swiss_Y[i], cmap=plt.cm.Spectral)\n",
      "    ax.view_init(4, -72)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        t0 = time()\n",
      "        Y = method.fit_transform(swiss_X)\n",
      "        t1 = time()\n",
      "        print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "        ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "        ax.scatter(Y[:, 0], Y[:, 1], c=swiss_Y, cmap=plt.cm.Spectral)\n",
      "        ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "        ax.xaxis.set_major_formatter(NullFormatter())\n",
      "        ax.yaxis.set_major_formatter(NullFormatter())\n",
      "        ax.axis('tight')\n",
      "\n",
      "    plt.show()\n",
      "159/32:\n",
      "Axes3D\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "for i in range(len(data_Y)):\n",
      "    fig = plt.figure(figsize=(15, 8))\n",
      "    fig.suptitle(\"Manifold Learning with %i points, %i neighbors\"\n",
      "                 % (1000, n_neighbors), fontsize=14)\n",
      "\n",
      "    ax = fig.add_subplot(251, projection='3d')\n",
      "    ax.scatter(data_X[i, :, 0], data_X[i, :, 1], data_X[i, :, 2], c=data_Y[i], cmap=plt.cm.Spectral)\n",
      "    ax.view_init(4, -72)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        t0 = time()\n",
      "        Y = method.fit_transform(swiss_X)\n",
      "        t1 = time()\n",
      "        print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "        ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "        ax.scatter(Y[:, 0], Y[:, 1], c=swiss_Y, cmap=plt.cm.Spectral)\n",
      "        ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "        ax.xaxis.set_major_formatter(NullFormatter())\n",
      "        ax.yaxis.set_major_formatter(NullFormatter())\n",
      "        ax.axis('tight')\n",
      "\n",
      "    plt.show()\n",
      "159/33:\n",
      "data_X = []\n",
      "data_Y = []\n",
      "\n",
      "n_points = 1000\n",
      "swiss_X, swiss_Y = datasets.make_swiss_roll(n_points)\n",
      "data_X.append(swiss_X)\n",
      "data_Y.append(swiss_Y)\n",
      "\n",
      "cars = pd.read_csv('./data/cars.csv', header=None)  \n",
      "data_X.append(cars[0])\n",
      "data_Y.append(cars[1:])\n",
      "159/34:\n",
      "Axes3D\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "for i in range(len(data_Y)):\n",
      "    fig = plt.figure(figsize=(15, 8))\n",
      "    fig.suptitle(\"Manifold Learning with %i points, %i neighbors\"\n",
      "                 % (1000, n_neighbors), fontsize=14)\n",
      "\n",
      "    ax = fig.add_subplot(251, projection='3d')\n",
      "    ax.scatter(data_X[i, :, 0], data_X[i, :, 1], data_X[i, :, 2], c=data_Y[i], cmap=plt.cm.Spectral)\n",
      "    ax.view_init(4, -72)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        t0 = time()\n",
      "        Y = method.fit_transform(swiss_X)\n",
      "        t1 = time()\n",
      "        print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "        ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "        ax.scatter(Y[:, 0], Y[:, 1], c=swiss_Y, cmap=plt.cm.Spectral)\n",
      "        ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "        ax.xaxis.set_major_formatter(NullFormatter())\n",
      "        ax.yaxis.set_major_formatter(NullFormatter())\n",
      "        ax.axis('tight')\n",
      "\n",
      "    plt.show()\n",
      "159/35:\n",
      "Axes3D\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "for i in range(len(data_Y)):\n",
      "    fig = plt.figure(figsize=(15, 8))\n",
      "    fig.suptitle(\"Manifold Learning with %i points, %i neighbors\"\n",
      "                 % (1000, n_neighbors), fontsize=14)\n",
      "\n",
      "    ax = fig.add_subplot(251, projection='3d')\n",
      "    ax.scatter(data_X[i][:, 0], data_X[i][:, 1], data_X[i][:, 2], c=data_Y[i], cmap=plt.cm.Spectral)\n",
      "    ax.view_init(4, -72)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        t0 = time()\n",
      "        Y = method.fit_transform(swiss_X)\n",
      "        t1 = time()\n",
      "        print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "        ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "        ax.scatter(Y[:, 0], Y[:, 1], c=swiss_Y, cmap=plt.cm.Spectral)\n",
      "        ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "        ax.xaxis.set_major_formatter(NullFormatter())\n",
      "        ax.yaxis.set_major_formatter(NullFormatter())\n",
      "        ax.axis('tight')\n",
      "\n",
      "    plt.show()\n",
      "159/36:\n",
      "data_X = []\n",
      "data_Y = []\n",
      "\n",
      "n_points = 1000\n",
      "swiss_X, swiss_Y = datasets.make_swiss_roll(n_points)\n",
      "data_X.append(swiss_X)\n",
      "data_Y.append(swiss_Y)\n",
      "\n",
      "cars = pd.read_csv('./data/cars.csv', header=None)  \n",
      "data_X.append(cars[0])\n",
      "data_Y.append(cars[1:])\n",
      "\n",
      "print(data_X[1])\n",
      "159/37:\n",
      "data_X = []\n",
      "data_Y = []\n",
      "\n",
      "n_points = 1000\n",
      "swiss_X, swiss_Y = datasets.make_swiss_roll(n_points)\n",
      "data_X.append(swiss_X)\n",
      "data_Y.append(swiss_Y)\n",
      "\n",
      "cars = pd.read_csv('./data/cars.csv', header=None)  \n",
      "data_X.append(cars[1:])\n",
      "data_Y.append(cars[0])\n",
      "159/38:\n",
      "Axes3D\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "for i in range(len(data_Y)):\n",
      "    fig = plt.figure(figsize=(15, 8))\n",
      "    fig.suptitle(\"Manifold Learning with %i points, %i neighbors\"\n",
      "                 % (1000, n_neighbors), fontsize=14)\n",
      "\n",
      "    ax = fig.add_subplot(251, projection='3d')\n",
      "    ax.scatter(data_X[i][:, 0], data_X[i][:, 1], data_X[i][:, 2], c=data_Y[i], cmap=plt.cm.Spectral)\n",
      "    ax.view_init(4, -72)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        t0 = time()\n",
      "        Y = method.fit_transform(swiss_X)\n",
      "        t1 = time()\n",
      "        print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "        ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "        ax.scatter(Y[:, 0], Y[:, 1], c=swiss_Y, cmap=plt.cm.Spectral)\n",
      "        ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "        ax.xaxis.set_major_formatter(NullFormatter())\n",
      "        ax.yaxis.set_major_formatter(NullFormatter())\n",
      "        ax.axis('tight')\n",
      "\n",
      "    plt.show()\n",
      "159/39:\n",
      "data_X = []\n",
      "data_Y = []\n",
      "\n",
      "n_points = 1000\n",
      "swiss_X, swiss_Y = datasets.make_swiss_roll(n_points)\n",
      "data_X.append(swiss_X)\n",
      "data_Y.append(swiss_Y)\n",
      "\n",
      "cars = pd.read_csv('./data/cars.csv', header=None)  \n",
      "data_X.append(cars[1:])\n",
      "data_Y.append(cars[0])\n",
      "\n",
      "print(data_X)\n",
      "159/40:\n",
      "data_X = []\n",
      "data_Y = []\n",
      "\n",
      "n_points = 1000\n",
      "swiss_X, swiss_Y = datasets.make_swiss_roll(n_points)\n",
      "data_X.append(swiss_X)\n",
      "data_Y.append(swiss_Y)\n",
      "\n",
      "cars = pd.read_csv('./data/cars.csv', header=None)  \n",
      "data_X.append(cars[1:])\n",
      "data_Y.append(cars[0])\n",
      "\n",
      "print(data_X[1])\n",
      "159/41:\n",
      "data_X = []\n",
      "data_Y = []\n",
      "\n",
      "n_points = 1000\n",
      "swiss_X, swiss_Y = datasets.make_swiss_roll(n_points)\n",
      "data_X.append(swiss_X)\n",
      "data_Y.append(swiss_Y)\n",
      "\n",
      "cars = pd.read_csv('./data/cars.csv', header=None)  \n",
      "data_X.append(cars[2:])\n",
      "data_Y.append(cars[0])\n",
      "\n",
      "print(data_X[1])\n",
      "159/42:\n",
      "data_X = []\n",
      "data_Y = []\n",
      "\n",
      "n_points = 1000\n",
      "swiss_X, swiss_Y = datasets.make_swiss_roll(n_points)\n",
      "data_X.append(swiss_X)\n",
      "data_Y.append(swiss_Y)\n",
      "\n",
      "cars = pd.read_csv('./data/cars.csv', header=None)  \n",
      "data_X.append(cars[1])\n",
      "data_Y.append(cars[0])\n",
      "\n",
      "print(data_X[1])\n",
      "159/43:\n",
      "data_X = []\n",
      "data_Y = []\n",
      "\n",
      "n_points = 1000\n",
      "swiss_X, swiss_Y = datasets.make_swiss_roll(n_points)\n",
      "data_X.append(swiss_X)\n",
      "data_Y.append(swiss_Y)\n",
      "\n",
      "cars = pd.read_csv('./data/cars.csv', header=None)  \n",
      "data_X.append(cars[:, df.columns != 0])\n",
      "data_Y.append(cars[0])\n",
      "\n",
      "print(data_X[1])\n",
      "159/44:\n",
      "data_X = []\n",
      "data_Y = []\n",
      "\n",
      "n_points = 1000\n",
      "swiss_X, swiss_Y = datasets.make_swiss_roll(n_points)\n",
      "data_X.append(swiss_X)\n",
      "data_Y.append(swiss_Y)\n",
      "\n",
      "cars = pd.read_csv('./data/cars.csv', header=None)  \n",
      "data_X.append(cars[:, cars.columns != 0])\n",
      "data_Y.append(cars[0])\n",
      "\n",
      "print(data_X[1])\n",
      "159/45:\n",
      "data_X = []\n",
      "data_Y = []\n",
      "\n",
      "n_points = 1000\n",
      "swiss_X, swiss_Y = datasets.make_swiss_roll(n_points)\n",
      "data_X.append(swiss_X)\n",
      "data_Y.append(swiss_Y)\n",
      "\n",
      "cars = pd.read_csv('./data/cars.csv', header=None)  \n",
      "data_X.append(cars[:, cars.columns != '0'])\n",
      "data_Y.append(cars[0])\n",
      "\n",
      "print(data_X[1])\n",
      "159/46:\n",
      "data_X = []\n",
      "data_Y = []\n",
      "\n",
      "n_points = 1000\n",
      "swiss_X, swiss_Y = datasets.make_swiss_roll(n_points)\n",
      "data_X.append(swiss_X)\n",
      "data_Y.append(swiss_Y)\n",
      "\n",
      "collist = cars.columns.tolist()\n",
      "cars = pd.read_csv('./data/cars.csv', header=None)  \n",
      "data_X.append(cars[cars.columns[1:]])\n",
      "data_Y.append(cars[0])\n",
      "\n",
      "print(data_X[1])\n",
      "159/47:\n",
      "data_X = []\n",
      "data_Y = []\n",
      "\n",
      "n_points = 1000\n",
      "swiss_X, swiss_Y = datasets.make_swiss_roll(n_points)\n",
      "data_X.append(swiss_X)\n",
      "data_Y.append(swiss_Y)\n",
      "\n",
      "collist = cars.columns.tolist()\n",
      "cars = pd.read_csv('./data/cars.csv', header=None)  \n",
      "data_X.append(cars[cars.columns[1:]])\n",
      "data_Y.append(cars[0])\n",
      "159/48:\n",
      "Axes3D\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "for i in range(len(data_Y)):\n",
      "    fig = plt.figure(figsize=(15, 8))\n",
      "    fig.suptitle(\"Manifold Learning with %i points, %i neighbors\"\n",
      "                 % (1000, n_neighbors), fontsize=14)\n",
      "\n",
      "    ax = fig.add_subplot(251, projection='3d')\n",
      "    ax.scatter(data_X[i][:, 0], data_X[i][:, 1], data_X[i][:, 2], c=data_Y[i], cmap=plt.cm.Spectral)\n",
      "    ax.view_init(4, -72)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        t0 = time()\n",
      "        Y = method.fit_transform(swiss_X)\n",
      "        t1 = time()\n",
      "        print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "        ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "        ax.scatter(Y[:, 0], Y[:, 1], c=swiss_Y, cmap=plt.cm.Spectral)\n",
      "        ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "        ax.xaxis.set_major_formatter(NullFormatter())\n",
      "        ax.yaxis.set_major_formatter(NullFormatter())\n",
      "        ax.axis('tight')\n",
      "\n",
      "    plt.show()\n",
      "159/49:\n",
      "data_X = []\n",
      "data_Y = []\n",
      "\n",
      "collist = cars.columns.tolist()\n",
      "cars = pd.read_csv('./data/cars.csv', header=None)  \n",
      "data_X.append(cars[cars.columns[1:]])\n",
      "data_Y.append(cars[0])\n",
      "\n",
      "n_points = 1000\n",
      "swiss_X, swiss_Y = datasets.make_swiss_roll(n_points)\n",
      "data_X.append(swiss_X)\n",
      "data_Y.append(swiss_Y)\n",
      "159/50:\n",
      "Axes3D\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "for i in range(len(data_Y)):\n",
      "    fig = plt.figure(figsize=(15, 8))\n",
      "    fig.suptitle(\"Manifold Learning with %i points, %i neighbors\"\n",
      "                 % (1000, n_neighbors), fontsize=14)\n",
      "\n",
      "    ax = fig.add_subplot(251, projection='3d')\n",
      "    ax.scatter(data_X[i][:, 0], data_X[i][:, 1], data_X[i][:, 2], c=data_Y[i], cmap=plt.cm.Spectral)\n",
      "    ax.view_init(4, -72)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        t0 = time()\n",
      "        Y = method.fit_transform(swiss_X)\n",
      "        t1 = time()\n",
      "        print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "        ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "        ax.scatter(Y[:, 0], Y[:, 1], c=swiss_Y, cmap=plt.cm.Spectral)\n",
      "        ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "        ax.xaxis.set_major_formatter(NullFormatter())\n",
      "        ax.yaxis.set_major_formatter(NullFormatter())\n",
      "        ax.axis('tight')\n",
      "\n",
      "    plt.show()\n",
      "160/1:\n",
      "from collections import OrderedDict\n",
      "from functools import partial\n",
      "from time import time\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "from matplotlib.ticker import NullFormatter\n",
      "\n",
      "from sklearn import manifold, datasets\n",
      "import pandas as pd\n",
      "160/2:\n",
      "data_X = []\n",
      "data_Y = []\n",
      "\n",
      "collist = cars.columns.tolist()\n",
      "cars = pd.read_csv('./data/cars.csv', header=None)  \n",
      "data_X.append(cars[cars.columns[1:]])\n",
      "data_Y.append(cars[0])\n",
      "\n",
      "n_points = 1000\n",
      "swiss_X, swiss_Y = datasets.make_swiss_roll(n_points)\n",
      "data_X.append(swiss_X)\n",
      "data_Y.append(swiss_Y)\n",
      "160/3:\n",
      "data_X = []\n",
      "data_Y = []\n",
      "\n",
      "cars = pd.read_csv('./data/cars.csv', header=None)  \n",
      "collist = cars.columns.tolist()\n",
      "data_X.append(cars[cars.columns[1:]])\n",
      "data_Y.append(cars[0])\n",
      "\n",
      "n_points = 1000\n",
      "swiss_X, swiss_Y = datasets.make_swiss_roll(n_points)\n",
      "data_X.append(swiss_X)\n",
      "data_Y.append(swiss_Y)\n",
      "160/4:\n",
      "Axes3D\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "for i in range(len(data_Y)):\n",
      "    fig = plt.figure(figsize=(15, 8))\n",
      "    fig.suptitle(\"Manifold Learning with %i points, %i neighbors\"\n",
      "                 % (1000, n_neighbors), fontsize=14)\n",
      "\n",
      "    ax = fig.add_subplot(251, projection='3d')\n",
      "    ax.scatter(data_X[i][:, 0], data_X[i][:, 1], data_X[i][:, 2], c=data_Y[i], cmap=plt.cm.Spectral)\n",
      "    ax.view_init(4, -72)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        t0 = time()\n",
      "        Y = method.fit_transform(swiss_X)\n",
      "        t1 = time()\n",
      "        print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "        ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "        ax.scatter(Y[:, 0], Y[:, 1], c=swiss_Y, cmap=plt.cm.Spectral)\n",
      "        ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "        ax.xaxis.set_major_formatter(NullFormatter())\n",
      "        ax.yaxis.set_major_formatter(NullFormatter())\n",
      "        ax.axis('tight')\n",
      "\n",
      "    plt.show()\n",
      "160/5:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "for i in range(len(data_Y)):\n",
      "    fig = plt.figure(figsize=(15, 8))\n",
      "    fig.suptitle(\"Manifold Learning with %i points, %i neighbors\"\n",
      "                 % (1000, n_neighbors), fontsize=14)\n",
      "\n",
      "    ax = fig.add_subplot(251, projection='3d')\n",
      "    ax.scatter(data_X[i][:, 0], data_X[i][:, 1], data_X[i][:, 2], c=data_Y[i], cmap=plt.cm.Spectral)\n",
      "    ax.view_init(4, -72)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        t0 = time()\n",
      "        Y = method.fit_transform(swiss_X)\n",
      "        t1 = time()\n",
      "        print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "        ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "        ax.scatter(Y[:, 0], Y[:, 1], c=swiss_Y, cmap=plt.cm.Spectral)\n",
      "        ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "        ax.xaxis.set_major_formatter(NullFormatter())\n",
      "        ax.yaxis.set_major_formatter(NullFormatter())\n",
      "        ax.axis('tight')\n",
      "\n",
      "    plt.show()\n",
      "160/6:\n",
      "data_X = []\n",
      "data_Y = []\n",
      "\n",
      "cars = pd.read_csv('./data/cars.csv', header=None)  \n",
      "collist = cars.columns.tolist()\n",
      "data_X.append(cars[cars.columns[1:]])\n",
      "data_Y.append(cars[0])\n",
      "print(cars[cars.columns[1:]])\n",
      "print(cars[0])\n",
      "\n",
      "n_points = 1000\n",
      "swiss_X, swiss_Y = datasets.make_swiss_roll(n_points)\n",
      "data_X.append(swiss_X)\n",
      "data_Y.append(swiss_Y)\n",
      "160/7:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "for i in range(len(data_Y)):\n",
      "    fig = plt.figure(figsize=(15, 8))\n",
      "    fig.suptitle(\"Manifold Learning with %i points, %i neighbors\"\n",
      "                 % (1000, n_neighbors), fontsize=14)\n",
      "\n",
      "#     ax = fig.add_subplot(251, projection='3d')\n",
      "#     ax.scatter(data_X[i][:, 0], data_X[i][:, 1], data_X[i][:, 2], c=data_Y[i], cmap=plt.cm.Spectral)\n",
      "#     ax.view_init(4, -72)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        t0 = time()\n",
      "        Y = method.fit_transform(swiss_X)\n",
      "        t1 = time()\n",
      "        print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "        ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "        ax.scatter(Y[:, 0], Y[:, 1], c=swiss_Y, cmap=plt.cm.Spectral)\n",
      "        ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "        ax.xaxis.set_major_formatter(NullFormatter())\n",
      "        ax.yaxis.set_major_formatter(NullFormatter())\n",
      "        ax.axis('tight')\n",
      "\n",
      "    plt.show()\n",
      "160/8:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "for i in range(len(data_Y)):\n",
      "    fig = plt.figure(figsize=(15, 8))\n",
      "    fig.suptitle(\"Manifold Learning with %i points, %i neighbors\"\n",
      "                 % (1000, n_neighbors), fontsize=14)\n",
      "\n",
      "#     ax = fig.add_subplot(251, projection='3d')\n",
      "#     ax.scatter(data_X[i][:, 0], data_X[i][:, 1], data_X[i][:, 2], c=data_Y[i], cmap=plt.cm.Spectral)\n",
      "#     ax.view_init(4, -72)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        t0 = time()\n",
      "        Y = method.fit_transform(data_X[i])\n",
      "        t1 = time()\n",
      "        print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "        ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "        ax.scatter(Y[:, 0], Y[:, 1], c=data_Y[i], cmap=plt.cm.Spectral)\n",
      "        ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "        ax.xaxis.set_major_formatter(NullFormatter())\n",
      "        ax.yaxis.set_major_formatter(NullFormatter())\n",
      "        ax.axis('tight')\n",
      "\n",
      "    plt.show()\n",
      "160/9:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "for i in range(len(data_Y)):\n",
      "    fig = plt.figure(figsize=(15, 8))\n",
      "    fig.suptitle(\"Manifold Learning with %i points, %i neighbors\"\n",
      "                 % (1000, n_neighbors), fontsize=14)\n",
      "\n",
      "#     ax = fig.add_subplot(251, projection='3d')\n",
      "#     ax.scatter(data_X[i][:, 0], data_X[i][:, 1], data_X[i][:, 2], c=data_Y[i], cmap=plt.cm.Spectral)\n",
      "#     ax.view_init(4, -72)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        t0 = time()\n",
      "        Y = method.fit_transform(data_X[i])\n",
      "        t1 = time()\n",
      "        print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "        ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "        ax.scatter(Y[:, 0], Y[:, 1], cmap=plt.cm.Spectral)\n",
      "        ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "        ax.xaxis.set_major_formatter(NullFormatter())\n",
      "        ax.yaxis.set_major_formatter(NullFormatter())\n",
      "        ax.axis('tight')\n",
      "\n",
      "    plt.show()\n",
      "160/10:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "for j in range(len(data_Y)):\n",
      "    fig = plt.figure(figsize=(15, 8))\n",
      "    fig.suptitle(\"Manifold Learning with %i points, %i neighbors\"\n",
      "                 % (1000, n_neighbors), fontsize=14)\n",
      "\n",
      "#     ax = fig.add_subplot(251, projection='3d')\n",
      "#     ax.scatter(data_X[i][:, 0], data_X[i][:, 1], data_X[i][:, 2], c=data_Y[i], cmap=plt.cm.Spectral)\n",
      "#     ax.view_init(4, -72)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        t0 = time()\n",
      "        Y = method.fit_transform(data_X[j])\n",
      "        t1 = time()\n",
      "        print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "        ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "        ax.scatter(Y[:, 0], Y[:, 1], cmap=plt.cm.Spectral)\n",
      "        ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "        ax.xaxis.set_major_formatter(NullFormatter())\n",
      "        ax.yaxis.set_major_formatter(NullFormatter())\n",
      "        ax.axis('tight')\n",
      "\n",
      "    plt.show()\n",
      "160/11:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "for j in range(len(data_Y)):\n",
      "    fig = plt.figure(figsize=(15, 8))\n",
      "    fig.suptitle(\"Manifold Learning with %i points, %i neighbors\"\n",
      "                 % (1000, n_neighbors), fontsize=14)\n",
      "\n",
      "#     ax = fig.add_subplot(251, projection='3d')\n",
      "#     ax.scatter(data_X[i][:, 0], data_X[i][:, 1], data_X[i][:, 2], c=data_Y[i], cmap=plt.cm.Spectral)\n",
      "#     ax.view_init(4, -72)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        t0 = time()\n",
      "        Y = method.fit_transform(data_X[j])\n",
      "        t1 = time()\n",
      "        print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "        ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "        if j==1:\n",
      "            ax.scatter(Y[:, 0], Y[:, 1], c=data_Y[j], cmap=plt.cm.Spectral)\n",
      "        else:\n",
      "            ax.scatter(Y[:, 0], Y[:, 1])\n",
      "            for h, txt in enumerate(data_Y[j]):\n",
      "                ax.annotate(txt, (Y[h], Y[h]))\n",
      "        ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "        ax.xaxis.set_major_formatter(NullFormatter())\n",
      "        ax.yaxis.set_major_formatter(NullFormatter())\n",
      "        ax.axis('tight')\n",
      "\n",
      "    plt.show()\n",
      "160/12:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "for j in range(len(data_Y)):\n",
      "    fig = plt.figure(figsize=(15, 8))\n",
      "    fig.suptitle(\"Manifold Learning with %i points, %i neighbors\"\n",
      "                 % (1000, n_neighbors), fontsize=14)\n",
      "\n",
      "#     ax = fig.add_subplot(251, projection='3d')\n",
      "#     ax.scatter(data_X[i][:, 0], data_X[i][:, 1], data_X[i][:, 2], c=data_Y[i], cmap=plt.cm.Spectral)\n",
      "#     ax.view_init(4, -72)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        t0 = time()\n",
      "        Y = method.fit_transform(data_X[j])\n",
      "        t1 = time()\n",
      "        print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "        ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "        if j==1:\n",
      "            ax.scatter(Y[:, 0], Y[:, 1], c=data_Y[j], cmap=plt.cm.Spectral)\n",
      "        else:\n",
      "            ax.scatter(Y[:, 0], Y[:, 1])\n",
      "            for h, txt in enumerate(data_Y[j]):\n",
      "                print(txt)\n",
      "                ax.annotate(txt, (Y[h], Y[h]))\n",
      "        ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "        ax.xaxis.set_major_formatter(NullFormatter())\n",
      "        ax.yaxis.set_major_formatter(NullFormatter())\n",
      "        ax.axis('tight')\n",
      "\n",
      "    plt.show()\n",
      "160/13:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "for j in range(len(data_Y)):\n",
      "    fig = plt.figure(figsize=(15, 8))\n",
      "    fig.suptitle(\"Manifold Learning with %i points, %i neighbors\"\n",
      "                 % (1000, n_neighbors), fontsize=14)\n",
      "\n",
      "#     ax = fig.add_subplot(251, projection='3d')\n",
      "#     ax.scatter(data_X[i][:, 0], data_X[i][:, 1], data_X[i][:, 2], c=data_Y[i], cmap=plt.cm.Spectral)\n",
      "#     ax.view_init(4, -72)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        t0 = time()\n",
      "        Y = method.fit_transform(data_X[j])\n",
      "        t1 = time()\n",
      "        print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "        ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "        if j==1:\n",
      "            ax.scatter(Y[:, 0], Y[:, 1], c=data_Y[j], cmap=plt.cm.Spectral)\n",
      "        else:\n",
      "            ax.scatter(Y[:, 0], Y[:, 1])\n",
      "            for h, txt in enumerate(data_Y[j]):\n",
      "                ax.annotate(txt, (Y[h,0], Y[h,1]))\n",
      "        ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "        ax.xaxis.set_major_formatter(NullFormatter())\n",
      "        ax.yaxis.set_major_formatter(NullFormatter())\n",
      "        ax.axis('tight')\n",
      "\n",
      "    plt.show()\n",
      "160/14:\n",
      "from collections import OrderedDict\n",
      "from functools import partial\n",
      "from time import time\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "from matplotlib.ticker import NullFormatter\n",
      "\n",
      "from sklearn import manifold, datasets\n",
      "import pandas as pd\n",
      "160/15:\n",
      "data_X = []\n",
      "data_Y = []\n",
      "\n",
      "cars = pd.read_csv('./data/cars.csv', header=None)  \n",
      "collist = cars.columns.tolist()\n",
      "data_X.append(cars[cars.columns[1:]])\n",
      "data_Y.append(cars[0])\n",
      "\n",
      "n_points = 1000\n",
      "swiss_X, swiss_Y = datasets.make_swiss_roll(n_points)\n",
      "data_X.append(swiss_X)\n",
      "data_Y.append(swiss_Y)\n",
      "160/16:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "for j in range(len(data_Y)):\n",
      "    fig = plt.figure(figsize=(15, 8))\n",
      "    fig.suptitle(\"Manifold Learning with %i points, %i neighbors\"\n",
      "                 % (1000, n_neighbors), fontsize=14)\n",
      "\n",
      "#     ax = fig.add_subplot(251, projection='3d')\n",
      "#     ax.scatter(data_X[i][:, 0], data_X[i][:, 1], data_X[i][:, 2], c=data_Y[i], cmap=plt.cm.Spectral)\n",
      "#     ax.view_init(4, -72)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        t0 = time()\n",
      "        Y = method.fit_transform(data_X[j])\n",
      "        t1 = time()\n",
      "        print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "        ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "        if j==1:\n",
      "            ax.scatter(Y[:, 0], Y[:, 1], c=data_Y[j], cmap=plt.cm.Spectral)\n",
      "        else:\n",
      "            ax.scatter(Y[:, 0], Y[:, 1])\n",
      "            for h, txt in enumerate(data_Y[j]):\n",
      "                ax.annotate(txt, (Y[h,0], Y[h,1]))\n",
      "        ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "        ax.xaxis.set_major_formatter(NullFormatter())\n",
      "        ax.yaxis.set_major_formatter(NullFormatter())\n",
      "        ax.axis('tight')\n",
      "\n",
      "    plt.show()\n",
      "160/17:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "def visualize_data(data_X, data_Y, color):\n",
      "    fig = plt.figure(figsize=(15, 8))\n",
      "    fig.suptitle(\"Manifold Learning with %i points, %i neighbors\"\n",
      "                 % (1000, n_neighbors), fontsize=14)\n",
      "\n",
      "#     ax = fig.add_subplot(251, projection='3d')\n",
      "#     ax.scatter(data_X[i][:, 0], data_X[i][:, 1], data_X[i][:, 2], c=data_Y[i], cmap=plt.cm.Spectral)\n",
      "#     ax.view_init(4, -72)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        t0 = time()\n",
      "        Y = method.fit_transform(data_X)\n",
      "        t1 = time()\n",
      "        print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "        ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "        if color==True:\n",
      "            ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        else:\n",
      "            ax.scatter(Y[:, 0], Y[:, 1])\n",
      "            for h, txt in enumerate(data_Y[j]):\n",
      "                ax.annotate(txt, (Y[h,0], Y[h,1]))\n",
      "        ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "        ax.xaxis.set_major_formatter(NullFormatter())\n",
      "        ax.yaxis.set_major_formatter(NullFormatter())\n",
      "        ax.axis('tight')\n",
      "\n",
      "    plt.show()\n",
      "\n",
      "for i in range(len(data_Y)):\n",
      "    if i == 1:\n",
      "        visualize_data(data_X[i], data_Y[i], True)\n",
      "    else:\n",
      "        visualize_data(data_X[i], data_Y[i])\n",
      "160/18:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "def visualize_data(data_X, data_Y, color):\n",
      "    fig = plt.figure(figsize=(15, 8))\n",
      "    fig.suptitle(\"Manifold Learning with %i points, %i neighbors\"\n",
      "                 % (1000, n_neighbors), fontsize=14)\n",
      "\n",
      "#     ax = fig.add_subplot(251, projection='3d')\n",
      "#     ax.scatter(data_X[i][:, 0], data_X[i][:, 1], data_X[i][:, 2], c=data_Y[i], cmap=plt.cm.Spectral)\n",
      "#     ax.view_init(4, -72)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        t0 = time()\n",
      "        Y = method.fit_transform(data_X)\n",
      "        t1 = time()\n",
      "        print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "        ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "        if color==True:\n",
      "            ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        else:\n",
      "            ax.scatter(Y[:, 0], Y[:, 1])\n",
      "            for h, txt in enumerate(data_Y[j]):\n",
      "                ax.annotate(txt, (Y[h,0], Y[h,1]))\n",
      "        ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "        ax.xaxis.set_major_formatter(NullFormatter())\n",
      "        ax.yaxis.set_major_formatter(NullFormatter())\n",
      "        ax.axis('tight')\n",
      "\n",
      "    plt.show()\n",
      "\n",
      "for i in range(len(data_Y)):\n",
      "    if i == 1:\n",
      "        visualize_data(data_X[i], data_Y[i], True)\n",
      "    else:\n",
      "        visualize_data(data_X[i], data_Y[i], False)\n",
      "160/19:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "def visualize_data(data_X, data_Y, color):\n",
      "    fig = plt.figure(figsize=(15, 8))\n",
      "    fig.suptitle(\"Manifold Learning with %i points, %i neighbors\"\n",
      "                 % (1000, n_neighbors), fontsize=14)\n",
      "\n",
      "#     ax = fig.add_subplot(251, projection='3d')\n",
      "#     ax.scatter(data_X[i][:, 0], data_X[i][:, 1], data_X[i][:, 2], c=data_Y[i], cmap=plt.cm.Spectral)\n",
      "#     ax.view_init(4, -72)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        t0 = time()\n",
      "        Y = method.fit_transform(data_X)\n",
      "        t1 = time()\n",
      "        print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "        ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "        if color==True:\n",
      "            ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        else:\n",
      "            ax.scatter(Y[:, 0], Y[:, 1])\n",
      "            for h, txt in enumerate(data_Y):\n",
      "                ax.annotate(txt, (Y[h,0], Y[h,1]))\n",
      "        ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "        ax.xaxis.set_major_formatter(NullFormatter())\n",
      "        ax.yaxis.set_major_formatter(NullFormatter())\n",
      "        ax.axis('tight')\n",
      "\n",
      "    plt.show()\n",
      "\n",
      "for i in range(len(data_Y)):\n",
      "    if i == 1:\n",
      "        visualize_data(data_X[i], data_Y[i], True)\n",
      "    else:\n",
      "        visualize_data(data_X[i], data_Y[i], False)\n",
      "160/20:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "def visualize_data(data_X, data_Y, color):\n",
      "    fig = plt.figure(figsize=(15, 8))\n",
      "    fig.suptitle(\"Manifold Learning with %i points, %i neighbors\"\n",
      "                 % (1000, n_neighbors), fontsize=14)\n",
      "\n",
      "    if color==True:\n",
      "        ax = fig.add_subplot(251, projection='3d')\n",
      "        ax.scatter(data_X[i][:, 0], data_X[i][:, 1], data_X[i][:, 2], c=data_Y[i], cmap=plt.cm.Spectral)\n",
      "        ax.view_init(4, -72)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        t0 = time()\n",
      "        Y = method.fit_transform(data_X)\n",
      "        t1 = time()\n",
      "        print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "        ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "        if color==True:\n",
      "            ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        else:\n",
      "            ax.scatter(Y[:, 0], Y[:, 1])\n",
      "            for h, txt in enumerate(data_Y):\n",
      "                ax.annotate(txt, (Y[h,0], Y[h,1]))\n",
      "        ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "        ax.xaxis.set_major_formatter(NullFormatter())\n",
      "        ax.yaxis.set_major_formatter(NullFormatter())\n",
      "        ax.axis('tight')\n",
      "\n",
      "    plt.show()\n",
      "\n",
      "for i in range(len(data_Y)):\n",
      "    if i == 1:\n",
      "        visualize_data(data_X[i], data_Y[i], True)\n",
      "    else:\n",
      "        visualize_data(data_X[i], data_Y[i], False)\n",
      "160/21:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "def visualize_data(data_X, data_Y, color):\n",
      "    fig = plt.figure(figsize=(15, 8))\n",
      "    fig.suptitle(\"Manifold Learning with %i points, %i neighbors\"\n",
      "                 % (1000, n_neighbors), fontsize=14)\n",
      "\n",
      "    if color==True:\n",
      "        ax = fig.add_subplot(251, projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        ax.view_init(4, -72)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        t0 = time()\n",
      "        Y = method.fit_transform(data_X)\n",
      "        t1 = time()\n",
      "        print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "        ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "        if color==True:\n",
      "            ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        else:\n",
      "            ax.scatter(Y[:, 0], Y[:, 1])\n",
      "            for h, txt in enumerate(data_Y):\n",
      "                ax.annotate(txt, (Y[h,0], Y[h,1]))\n",
      "        ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "        ax.xaxis.set_major_formatter(NullFormatter())\n",
      "        ax.yaxis.set_major_formatter(NullFormatter())\n",
      "        ax.axis('tight')\n",
      "\n",
      "    plt.show()\n",
      "\n",
      "for i in range(len(data_Y)):\n",
      "    if i == 1:\n",
      "        visualize_data(data_X[i], data_Y[i], True)\n",
      "    else:\n",
      "        visualize_data(data_X[i], data_Y[i], False)\n",
      "160/22:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "def visualize_data(data_X, data_Y, color, name):\n",
      "    fig = plt.figure(figsize=(15, 8))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    if color==True:\n",
      "        ax = fig.add_subplot(251, projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        ax.view_init(4, -72)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        t0 = time()\n",
      "        Y = method.fit_transform(data_X)\n",
      "        t1 = time()\n",
      "        print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "        ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "        if color==True:\n",
      "            ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        else:\n",
      "            ax.scatter(Y[:, 0], Y[:, 1])\n",
      "            for h, txt in enumerate(data_Y):\n",
      "                ax.annotate(txt, (Y[h,0], Y[h,1]))\n",
      "        ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "        ax.xaxis.set_major_formatter(NullFormatter())\n",
      "        ax.yaxis.set_major_formatter(NullFormatter())\n",
      "        ax.axis('tight')\n",
      "\n",
      "    plt.show()\n",
      "\n",
      "for i in range(len(data_Y)):\n",
      "    if i == 1:\n",
      "        visualize_data(data_X[i], data_Y[i], True, \"Swiss roll dataset\")\n",
      "    else:\n",
      "        visualize_data(data_X[i], data_Y[i], False, \"Cars dataset\")\n",
      "160/23:\n",
      "from collections import OrderedDict\n",
      "from functools import partial\n",
      "from time import time\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "from matplotlib.ticker import NullFormatter\n",
      "\n",
      "from sklearn import manifold, datasets\n",
      "import pandas as pd\n",
      "import pickle\n",
      "160/24:\n",
      "data_X = []\n",
      "data_Y = []\n",
      "\n",
      "cars = pd.read_csv('./data/cars.csv', header=None)  \n",
      "collist = cars.columns.tolist()\n",
      "data_X.append(cars[cars.columns[1:]])\n",
      "data_Y.append(cars[0])\n",
      "\n",
      "n_points = 1000\n",
      "swiss_X, swiss_Y = datasets.make_swiss_roll(n_points)\n",
      "data_X.append(swiss_X)\n",
      "data_Y.append(swiss_Y)\n",
      "\n",
      "attributes_comms_num = pickle.load(open( \"./data/text_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "160/25:\n",
      "data_X = []\n",
      "data_Y = []\n",
      "\n",
      "cars = pd.read_csv('./data/cars.csv', header=None)  \n",
      "collist = cars.columns.tolist()\n",
      "data_X.append(cars[cars.columns[1:]])\n",
      "data_Y.append(cars[0])\n",
      "\n",
      "n_points = 1000\n",
      "swiss_X, swiss_Y = datasets.make_swiss_roll(n_points)\n",
      "data_X.append(swiss_X)\n",
      "data_Y.append(swiss_Y)\n",
      "\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num']\n",
      "attributes_comms_num = pickle.load(open( \"./data/text_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "attributes_without_excluded = exclude(attributes_comms_num)\n",
      "data_X.append(attributes_without_excluded)\n",
      "data_Y.append(attributes_comms_num['comms_num'])\n",
      "160/26:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "def visualize_data(data_X, data_Y, color, name):\n",
      "    fig = plt.figure(figsize=(15, 8))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    if color==True:\n",
      "        ax = fig.add_subplot(251, projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        ax.view_init(4, -72)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        t0 = time()\n",
      "        Y = method.fit_transform(data_X)\n",
      "        t1 = time()\n",
      "        print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "        ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "        if color==True:\n",
      "            ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        else:\n",
      "            ax.scatter(Y[:, 0], Y[:, 1])\n",
      "            for h, txt in enumerate(data_Y):\n",
      "                ax.annotate(txt, (Y[h,0], Y[h,1]))\n",
      "        ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "        ax.xaxis.set_major_formatter(NullFormatter())\n",
      "        ax.yaxis.set_major_formatter(NullFormatter())\n",
      "        ax.axis('tight')\n",
      "\n",
      "    plt.show()\n",
      "\n",
      "\n",
      "visualize_data(data_X[0], data_Y[0], True, \"Swiss roll dataset\")\n",
      "visualize_data(data_X[1], data_Y[1], False, \"Cars dataset\")\n",
      "visualize_data(data_X[1], data_Y[1], False, \"WSB dataset\")\n",
      "160/27:\n",
      "data_X = []\n",
      "data_Y = []\n",
      "\n",
      "cars = pd.read_csv('./data/cars.csv', header=None)  \n",
      "collist = cars.columns.tolist()\n",
      "data_X.append(cars[cars.columns[1:]])\n",
      "data_Y.append(cars[0])\n",
      "\n",
      "n_points = 1000\n",
      "swiss_X, swiss_Y = datasets.make_swiss_roll(n_points)\n",
      "data_X.append(swiss_X)\n",
      "data_Y.append(swiss_Y)\n",
      "\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num']\n",
      "attributes_comms_num = pickle.load(open( \"./data/text_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "attributes_without_excluded = exclude(attributes_comms_num)\n",
      "data_X.append(attributes_without_excluded)\n",
      "data_Y.append(attributes_comms_num['comms_num'])\n",
      "160/28:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "def visualize_data(data_X, data_Y, color, name):\n",
      "    fig = plt.figure(figsize=(15, 8))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    if color==True:\n",
      "        ax = fig.add_subplot(251, projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        ax.view_init(4, -72)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        t0 = time()\n",
      "        Y = method.fit_transform(data_X)\n",
      "        t1 = time()\n",
      "        print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "        ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "        if color==True:\n",
      "            ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        else:\n",
      "            ax.scatter(Y[:, 0], Y[:, 1])\n",
      "            for h, txt in enumerate(data_Y):\n",
      "                ax.annotate(txt, (Y[h,0], Y[h,1]))\n",
      "        ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "        ax.xaxis.set_major_formatter(NullFormatter())\n",
      "        ax.yaxis.set_major_formatter(NullFormatter())\n",
      "        ax.axis('tight')\n",
      "\n",
      "    plt.show()\n",
      "\n",
      "\n",
      "visualize_data(data_X[0], data_Y[0], True, \"Swiss roll dataset\")\n",
      "visualize_data(data_X[1], data_Y[1], False, \"Cars dataset\")\n",
      "visualize_data(data_X[1], data_Y[1], False, \"WSB dataset\")\n",
      "160/29:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "def visualize_data(data_X, data_Y, color, name):\n",
      "    fig = plt.figure(figsize=(15, 8))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    if color==True:\n",
      "        ax = fig.add_subplot(251, projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        ax.view_init(4, -72)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        t0 = time()\n",
      "        Y = method.fit_transform(data_X)\n",
      "        t1 = time()\n",
      "        print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "        ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "        if color==True:\n",
      "            ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        else:\n",
      "            ax.scatter(Y[:, 0], Y[:, 1])\n",
      "            for h, txt in enumerate(data_Y):\n",
      "                ax.annotate(txt, (Y[h,0], Y[h,1]))\n",
      "        ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "        ax.xaxis.set_major_formatter(NullFormatter())\n",
      "        ax.yaxis.set_major_formatter(NullFormatter())\n",
      "        ax.axis('tight')\n",
      "\n",
      "    plt.show()\n",
      "\n",
      "\n",
      "visualize_data(data_X[0], data_Y[0], True, \"Swiss roll dataset\")\n",
      "visualize_data(data_X[1], data_Y[1], False, \"Cars dataset\")\n",
      "visualize_data(data_X[2], data_Y[2], False, \"WSB dataset\")\n",
      "160/30:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "def visualize_data(data_X, data_Y, color, name):\n",
      "    fig = plt.figure(figsize=(15, 8))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    if color==True:\n",
      "        ax = fig.add_subplot(251, projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        ax.view_init(4, -72)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        t0 = time()\n",
      "        Y = method.fit_transform(data_X)\n",
      "        t1 = time()\n",
      "        print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "        ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "        if color==True:\n",
      "            ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        else:\n",
      "            ax.scatter(Y[:, 0], Y[:, 1])\n",
      "            for h, txt in enumerate(data_Y):\n",
      "                ax.annotate(txt, (Y[h,0], Y[h,1]))\n",
      "        ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "        ax.xaxis.set_major_formatter(NullFormatter())\n",
      "        ax.yaxis.set_major_formatter(NullFormatter())\n",
      "        ax.axis('tight')\n",
      "\n",
      "    plt.show()\n",
      "\n",
      "for i in range(len(data_Y)):\n",
      "    if i == 1:\n",
      "        visualize_data(data_X[i], data_Y[i], True, \"Swiss roll dataset\")\n",
      "    else:\n",
      "        visualize_data(data_X[i], data_Y[i], False, \"Cars dataset\")\n",
      "161/1:\n",
      "data_X = []\n",
      "data_Y = []\n",
      "\n",
      "cars = pd.read_csv('./data/cars.csv', header=None)  \n",
      "collist = cars.columns.tolist()\n",
      "data_X.append(cars[cars.columns[1:]])\n",
      "data_Y.append(cars[0])\n",
      "\n",
      "n_points = 1000\n",
      "swiss_X, swiss_Y = datasets.make_swiss_roll(n_points)\n",
      "data_X.append(swiss_X)\n",
      "data_Y.append(swiss_Y)\n",
      "\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num']\n",
      "attributes_comms_num = pickle.load(open( \"./data/text_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "attributes_comms_num = attributes_comms_num.head(1000)\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "attributes_without_excluded = exclude(attributes_comms_num)\n",
      "data_X.append(attributes_without_excluded)\n",
      "data_Y.append(attributes_comms_num['comms_num'])\n",
      "print(attributes_comms_num)\n",
      "161/2:\n",
      "from collections import OrderedDict\n",
      "from functools import partial\n",
      "from time import time\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "from matplotlib.ticker import NullFormatter\n",
      "\n",
      "from sklearn import manifold, datasets\n",
      "import pandas as pd\n",
      "import pickle\n",
      "161/3:\n",
      "data_X = []\n",
      "data_Y = []\n",
      "\n",
      "cars = pd.read_csv('./data/cars.csv', header=None)  \n",
      "collist = cars.columns.tolist()\n",
      "data_X.append(cars[cars.columns[1:]])\n",
      "data_Y.append(cars[0])\n",
      "\n",
      "n_points = 1000\n",
      "swiss_X, swiss_Y = datasets.make_swiss_roll(n_points)\n",
      "data_X.append(swiss_X)\n",
      "data_Y.append(swiss_Y)\n",
      "\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num']\n",
      "attributes_comms_num = pickle.load(open( \"./data/text_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "attributes_comms_num = attributes_comms_num.head(1000)\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "attributes_without_excluded = exclude(attributes_comms_num)\n",
      "data_X.append(attributes_without_excluded)\n",
      "data_Y.append(attributes_comms_num['comms_num'])\n",
      "print(attributes_comms_num)\n",
      "161/4:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "def visualize_data(data_X, data_Y, color, name):\n",
      "    fig = plt.figure(figsize=(15, 8))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    if color==True:\n",
      "        ax = fig.add_subplot(251, projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        ax.view_init(4, -72)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        t0 = time()\n",
      "        Y = method.fit_transform(data_X)\n",
      "        t1 = time()\n",
      "        print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "        ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "        if color==True:\n",
      "            ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        else:\n",
      "            ax.scatter(Y[:, 0], Y[:, 1])\n",
      "            for h, txt in enumerate(data_Y):\n",
      "                ax.annotate(txt, (Y[h,0], Y[h,1]))\n",
      "        ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "        ax.xaxis.set_major_formatter(NullFormatter())\n",
      "        ax.yaxis.set_major_formatter(NullFormatter())\n",
      "        ax.axis('tight')\n",
      "\n",
      "    plt.show()\n",
      "\n",
      "for i in range(len(data_Y)):\n",
      "    if i == 1:\n",
      "        visualize_data(data_X[i], data_Y[i], True, \"Swiss roll dataset\")\n",
      "    else:\n",
      "        visualize_data(data_X[i], data_Y[i], False, \"Cars dataset\")\n",
      "161/5:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "def visualize_data(data_X, data_Y, color, name, _3d):\n",
      "    fig = plt.figure(figsize=(15, 8))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    if color==True:\n",
      "        ax = fig.add_subplot(251, projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        ax.view_init(4, -72)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        t0 = time()\n",
      "        Y = method.fit_transform(data_X)\n",
      "        t1 = time()\n",
      "        print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "        ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "        if color==True:\n",
      "            ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        else:\n",
      "            ax.scatter(Y[:, 0], Y[:, 1])\n",
      "            for h, txt in enumerate(data_Y):\n",
      "                ax.annotate(txt, (Y[h,0], Y[h,1]))\n",
      "        ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "        ax.xaxis.set_major_formatter(NullFormatter())\n",
      "        ax.yaxis.set_major_formatter(NullFormatter())\n",
      "        ax.axis('tight')\n",
      "\n",
      "    plt.show()\n",
      "\n",
      "visualize_data(data_X[0], data_Y[0], True, \"Swiss roll dataset\")\n",
      "visualize_data(data_X[1], data_Y[1], False, \"Cars dataset\")\n",
      "161/6:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "def visualize_data(data_X, data_Y, color, name):\n",
      "    fig = plt.figure(figsize=(15, 8))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    if color==True:\n",
      "        ax = fig.add_subplot(251, projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        ax.view_init(4, -72)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        t0 = time()\n",
      "        Y = method.fit_transform(data_X)\n",
      "        t1 = time()\n",
      "        print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "        ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "        if color==True:\n",
      "            ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        else:\n",
      "            ax.scatter(Y[:, 0], Y[:, 1])\n",
      "            for h, txt in enumerate(data_Y):\n",
      "                ax.annotate(txt, (Y[h,0], Y[h,1]))\n",
      "        ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "        ax.xaxis.set_major_formatter(NullFormatter())\n",
      "        ax.yaxis.set_major_formatter(NullFormatter())\n",
      "        ax.axis('tight')\n",
      "\n",
      "    plt.show()\n",
      "\n",
      "visualize_data(data_X[0], data_Y[0], True, \"Swiss roll dataset\")\n",
      "visualize_data(data_X[1], data_Y[1], False, \"Cars dataset\")\n",
      "161/7:\n",
      "data_X = []\n",
      "data_Y = []\n",
      "\n",
      "cars = pd.read_csv('./data/cars.csv', header=None)  \n",
      "collist = cars.columns.tolist()\n",
      "data_X.append(cars[cars.columns[1:]])\n",
      "data_Y.append(cars[0])\n",
      "\n",
      "n_points = 1000\n",
      "swiss_X, swiss_Y = datasets.make_swiss_roll(n_points)\n",
      "data_X.append(swiss_X)\n",
      "data_Y.append(swiss_Y)\n",
      "\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num']\n",
      "attributes_comms_num = pickle.load(open( \"./data/image_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "attributes_comms_num = attributes_comms_num.head(1000)\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "attributes_without_excluded = exclude(attributes_comms_num)\n",
      "data_X.append(attributes_without_excluded)\n",
      "data_Y.append(attributes_comms_num['comms_num'])\n",
      "161/8:\n",
      "data_X = []\n",
      "data_Y = []\n",
      "\n",
      "cars = pd.read_csv('./data/cars.csv', header=None)  \n",
      "collist = cars.columns.tolist()\n",
      "data_X.append(cars[cars.columns[1:]])\n",
      "data_Y.append(cars[0])\n",
      "\n",
      "n_points = 1000\n",
      "swiss_X, swiss_Y = datasets.make_swiss_roll(n_points)\n",
      "data_X.append(swiss_X)\n",
      "data_Y.append(swiss_Y)\n",
      "\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num']\n",
      "attributes_score = pickle.load(open( \"./data/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "attributes_score[\"score\"] = attributes_score[\"score\"]>2000\n",
      "attributes_score = attributes_score.head(1000)\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "attributes_without_excluded = exclude(attributes_score)\n",
      "data_X.append(attributes_without_excluded)\n",
      "data_Y.append(attributes_score['score'])\n",
      "161/9:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "def visualize_data(data_X, data_Y, mode, name):\n",
      "    fig = plt.figure(figsize=(15, 8))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    if mode=='colors':\n",
      "        ax = fig.add_subplot(251, projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        ax.view_init(4, -72)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        t0 = time()\n",
      "        Y = method.fit_transform(data_X)\n",
      "        t1 = time()\n",
      "        print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "        ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "        if mode=='colors':\n",
      "            ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        elif mode=='3d':\n",
      "            for color, i, target_name in zip(['Red', 'Green'], [False, True], ['Unpopular', 'Popular']):\n",
      "                ax.scatter(Y[attributes['score'] == i, 0], Y[attributes['score'] == i, 1], color=color, alpha=.3,\n",
      "                        label=target_name)\n",
      "        else:\n",
      "            ax.scatter(Y[:, 0], Y[:, 1])\n",
      "            for h, txt in enumerate(data_Y):\n",
      "                ax.annotate(txt, (Y[h,0], Y[h,1]))\n",
      "        ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "        ax.xaxis.set_major_formatter(NullFormatter())\n",
      "        ax.yaxis.set_major_formatter(NullFormatter())\n",
      "        ax.axis('tight')\n",
      "\n",
      "    plt.show()\n",
      "\n",
      "for i in range(len(data_Y)):\n",
      "    if i == 0:\n",
      "        visualize_data(data_X[i], data_Y[i], 'labels', \"Cars dataset\")\n",
      "    if i == 1:\n",
      "        visualize_data(data_X[i], data_Y[i], 'colors', \"Swiss roll dataset\")\n",
      "    if i == 2:\n",
      "        visualize_data(data_X[i], data_Y[i], '3d', \"WSB dataset\")\n",
      "161/10:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "def visualize_data(data_X, data_Y, mode, name):\n",
      "    fig = plt.figure(figsize=(15, 8))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    if mode=='colors':\n",
      "        ax = fig.add_subplot(251, projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        ax.view_init(4, -72)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        t0 = time()\n",
      "        Y = method.fit_transform(data_X)\n",
      "        t1 = time()\n",
      "        print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "        ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "        if mode=='colors':\n",
      "            ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        elif mode=='wsb':\n",
      "            for color, i, target_name in zip(['Red', 'Green'], [False, True], ['Unpopular', 'Popular']):\n",
      "                ax.scatter(Y[data_Y == i, 0], Y[data_Y == i, 1], color=color, alpha=.3,\n",
      "                        label=target_name)\n",
      "        else:\n",
      "            ax.scatter(Y[:, 0], Y[:, 1])\n",
      "            for h, txt in enumerate(data_Y):\n",
      "                ax.annotate(txt, (Y[h,0], Y[h,1]))\n",
      "        ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "        ax.xaxis.set_major_formatter(NullFormatter())\n",
      "        ax.yaxis.set_major_formatter(NullFormatter())\n",
      "        ax.axis('tight')\n",
      "\n",
      "    plt.show()\n",
      "\n",
      "for i in range(len(data_Y)):\n",
      "    if i == 0:\n",
      "        visualize_data(data_X[i], data_Y[i], 'labels', \"Cars dataset\")\n",
      "    if i == 1:\n",
      "        visualize_data(data_X[i], data_Y[i], 'colors', \"Swiss roll dataset\")\n",
      "    if i == 2:\n",
      "        visualize_data(data_X[i], data_Y[i], 'wsb', \"WSB dataset\")\n",
      "161/11:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "def visualize_data(data_X, data_Y, mode, name):\n",
      "    fig = plt.figure(figsize=(15, 8))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    if mode=='colors':\n",
      "        ax = fig.add_subplot(251, projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        ax.view_init(4, -72)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        try:\n",
      "            t0 = time()\n",
      "            Y = method.fit_transform(data_X)\n",
      "            t1 = time()\n",
      "            print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "            ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "            if mode=='colors':\n",
      "                ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "            elif mode=='wsb':\n",
      "                for color, i, target_name in zip(['Red', 'Green'], [False, True], ['Unpopular', 'Popular']):\n",
      "                    ax.scatter(Y[data_Y == i, 0], Y[data_Y == i, 1], color=color, alpha=.3,\n",
      "                            label=target_name)\n",
      "            else:\n",
      "                ax.scatter(Y[:, 0], Y[:, 1])\n",
      "                for h, txt in enumerate(data_Y):\n",
      "                    ax.annotate(txt, (Y[h,0], Y[h,1]))\n",
      "            ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "            ax.xaxis.set_major_formatter(NullFormatter())\n",
      "            ax.yaxis.set_major_formatter(NullFormatter())\n",
      "            ax.axis('tight')\n",
      "        except:\n",
      "            print('Metoda nie ogarna')\n",
      "\n",
      "    plt.show()\n",
      "\n",
      "for i in range(len(data_Y)):\n",
      "    if i == 0:\n",
      "        visualize_data(data_X[i], data_Y[i], 'labels', \"Cars dataset\")\n",
      "    if i == 1:\n",
      "        visualize_data(data_X[i], data_Y[i], 'colors', \"Swiss roll dataset\")\n",
      "    if i == 2:\n",
      "        visualize_data(data_X[i], data_Y[i], 'wsb', \"WSB dataset\")\n",
      "161/12:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "def visualize_data(data_X, data_Y, mode, name):\n",
      "    fig = plt.figure(figsize=(15, 8))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    if mode=='colors':\n",
      "        ax = fig.add_subplot(251, projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        ax.view_init(4, -72)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        try:\n",
      "            t0 = time()\n",
      "            Y = method.fit_transform(data_X)\n",
      "            t1 = time()\n",
      "            print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "            ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "            if mode=='colors':\n",
      "                ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "            elif mode=='wsb':\n",
      "                for color, i, target_name in zip(['Red', 'Green'], [False, True], ['Unpopular', 'Popular']):\n",
      "                    ax.scatter(Y[data_Y == i, 0], Y[data_Y == i, 1], color=color, alpha=.3,\n",
      "                            label=target_name)\n",
      "            else:\n",
      "                ax.scatter(Y[:, 0], Y[:, 1])\n",
      "                for h, txt in enumerate(data_Y):\n",
      "                    ax.annotate(txt, (Y[h,0], Y[h,1]))\n",
      "            ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "            ax.xaxis.set_major_formatter(NullFormatter())\n",
      "            ax.yaxis.set_major_formatter(NullFormatter())\n",
      "            ax.axis('tight')\n",
      "        except:\n",
      "            print('Metoda nie ogarna')\n",
      "\n",
      "    plt.show()\n",
      "\n",
      "for i in range(len(data_Y)):\n",
      "    print(i)\n",
      "    if i == 0:\n",
      "        visualize_data(data_X[i], data_Y[i], 'labels', \"Cars dataset\")\n",
      "    if i == 1:\n",
      "        visualize_data(data_X[i], data_Y[i], 'colors', \"Swiss roll dataset\")\n",
      "    if i == 2:\n",
      "        visualize_data(data_X[i], data_Y[i], 'wsb', \"WSB dataset\")\n",
      "161/13:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "def visualize_data(data_X, data_Y, mode, name):\n",
      "    fig = plt.figure(figsize=(15, 8))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    if mode=='colors':\n",
      "        ax = fig.add_subplot(251, projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        ax.view_init(4, -72)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        try:\n",
      "            t0 = time()\n",
      "            Y = method.fit_transform(data_X)\n",
      "            t1 = time()\n",
      "            print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "            ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "            if mode=='colors':\n",
      "                ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "            elif mode=='wsb':\n",
      "                for color, i, target_name in zip(['Red', 'Green'], [False, True], ['Unpopular', 'Popular']):\n",
      "                    ax.scatter(Y[data_Y == i, 0], Y[data_Y == i, 1], color=color, alpha=.3,\n",
      "                            label=target_name)\n",
      "            else:\n",
      "                ax.scatter(Y[:, 0], Y[:, 1])\n",
      "                for h, txt in enumerate(data_Y):\n",
      "                    ax.annotate(txt, (Y[h,0], Y[h,1]))\n",
      "            ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "            ax.xaxis.set_major_formatter(NullFormatter())\n",
      "            ax.yaxis.set_major_formatter(NullFormatter())\n",
      "            ax.axis('tight')\n",
      "        except:\n",
      "            print('Metoda nie ogarna')\n",
      "\n",
      "    plt.show()\n",
      "\n",
      "for i in range(len(data_Y)):\n",
      "    if i == 0:\n",
      "        visualize_data(data_X[0], data_Y[0], 'labels', \"Cars dataset\")\n",
      "    if i == 1:\n",
      "        visualize_data(data_X[i], data_Y[i], 'colors', \"Swiss roll dataset\")\n",
      "    if i == 2:\n",
      "        visualize_data(data_X[i], data_Y[i], 'wsb', \"WSB dataset\")\n",
      "161/14:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "def visualize_data(data_X, data_Y, mode, name):\n",
      "    fig = plt.figure(figsize=(15, 8))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    if mode=='colors':\n",
      "        ax = fig.add_subplot(251, projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        ax.view_init(4, -72)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        try:\n",
      "            t0 = time()\n",
      "            Y = method.fit_transform(data_X)\n",
      "            t1 = time()\n",
      "            print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "            ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "            if mode=='colors':\n",
      "                ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "            elif mode=='wsb':\n",
      "                for color, i, target_name in zip(['Red', 'Green'], [False, True], ['Unpopular', 'Popular']):\n",
      "                    ax.scatter(Y[data_Y == i, 0], Y[data_Y == i, 1], color=color, alpha=.3,\n",
      "                            label=target_name)\n",
      "            else:\n",
      "                ax.scatter(Y[:, 0], Y[:, 1])\n",
      "                for h, txt in enumerate(data_Y):\n",
      "                    ax.annotate(txt, (Y[h,0], Y[h,1]))\n",
      "            ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "            ax.xaxis.set_major_formatter(NullFormatter())\n",
      "            ax.yaxis.set_major_formatter(NullFormatter())\n",
      "            ax.axis('tight')\n",
      "        except:\n",
      "            print('Metoda nie ogarna')\n",
      "\n",
      "    plt.show()\n",
      "\n",
      "for i in range(len(data_Y)):\n",
      "    if i == 0:\n",
      "        visualize_data(data_X[0], data_Y[0], 'labels', \"Cars dataset\")\n",
      "    if i == 1:\n",
      "        visualize_data(data_X[1], data_Y[1], 'colors', \"Swiss roll dataset\")\n",
      "    if i == 2:\n",
      "        visualize_data(data_X[2], data_Y[2], 'wsb', \"WSB dataset\")\n",
      "161/15: visualize_data(data_X[0], data_Y[0], 'labels', \"Cars dataset\")\n",
      "161/16: visualize_data(data_X[1], data_Y[1], 'colors', \"Swiss roll dataset\")\n",
      "161/17: visualize_data(data_X[2], data_Y[2], 'wsb', \"WSB dataset\")\n",
      "161/18:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "def visualize_data(data_X, data_Y, mode, name):\n",
      "    fig = plt.figure(figsize=(15, 8))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    if mode=='colors':\n",
      "        ax = fig.add_subplot(251, projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        ax.view_init(4, -100)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        try:\n",
      "            t0 = time()\n",
      "            Y = method.fit_transform(data_X)\n",
      "            t1 = time()\n",
      "            print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "            ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "            if mode=='colors':\n",
      "                ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "            elif mode=='wsb':\n",
      "                for color, i, target_name in zip(['Red', 'Green'], [False, True], ['Unpopular', 'Popular']):\n",
      "                    ax.scatter(Y[data_Y == i, 0], Y[data_Y == i, 1], color=color, alpha=.3,\n",
      "                            label=target_name)\n",
      "            else:\n",
      "                ax.scatter(Y[:, 0], Y[:, 1])\n",
      "                for h, txt in enumerate(data_Y):\n",
      "                    ax.annotate(txt, (Y[h,0], Y[h,1]))\n",
      "            ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "            ax.xaxis.set_major_formatter(NullFormatter())\n",
      "            ax.yaxis.set_major_formatter(NullFormatter())\n",
      "            ax.axis('tight')\n",
      "        except:\n",
      "            print('Metoda nie ogarna')\n",
      "\n",
      "    plt.show()\n",
      "161/19: visualize_data(data_X[0], data_Y[0], 'labels', \"Cars dataset\")\n",
      "161/20:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "def visualize_data(data_X, data_Y, mode, name):\n",
      "    fig = plt.figure(figsize=(16, 16))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    if mode=='colors':\n",
      "        ax = fig.add_subplot(251, projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        ax.view_init(4, -100)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        try:\n",
      "            t0 = time()\n",
      "            Y = method.fit_transform(data_X)\n",
      "            t1 = time()\n",
      "            print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "            ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "            if mode=='colors':\n",
      "                ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "            elif mode=='wsb':\n",
      "                for color, i, target_name in zip(['Red', 'Green'], [False, True], ['Unpopular', 'Popular']):\n",
      "                    ax.scatter(Y[data_Y == i, 0], Y[data_Y == i, 1], color=color, alpha=.3,\n",
      "                            label=target_name)\n",
      "            else:\n",
      "                ax.scatter(Y[:, 0], Y[:, 1])\n",
      "                for h, txt in enumerate(data_Y):\n",
      "                    ax.annotate(txt, (Y[h,0], Y[h,1]))\n",
      "            ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "            ax.xaxis.set_major_formatter(NullFormatter())\n",
      "            ax.yaxis.set_major_formatter(NullFormatter())\n",
      "            ax.axis('tight')\n",
      "        except:\n",
      "            print('Metoda nie ogarna')\n",
      "\n",
      "    plt.show()\n",
      "161/21: visualize_data(data_X[0], data_Y[0], 'labels', \"Cars dataset\")\n",
      "161/22:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "def visualize_data(data_X, data_Y, mode, name):\n",
      "    fig = plt.figure(figsize=(16, 8))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    if mode=='colors':\n",
      "        ax = fig.add_subplot(251, projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        ax.view_init(4, -100)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        try:\n",
      "            t0 = time()\n",
      "            Y = method.fit_transform(data_X)\n",
      "            t1 = time()\n",
      "            print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "            ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "            if mode=='colors':\n",
      "                ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "            elif mode=='wsb':\n",
      "                for color, i, target_name in zip(['Red', 'Green'], [False, True], ['Unpopular', 'Popular']):\n",
      "                    ax.scatter(Y[data_Y == i, 0], Y[data_Y == i, 1], color=color, alpha=.3,\n",
      "                            label=target_name)\n",
      "            else:\n",
      "                ax.scatter(Y[:, 0], Y[:, 1])\n",
      "                for h, txt in enumerate(data_Y):\n",
      "                    ax.annotate(txt, (Y[h,0], Y[h,1]))\n",
      "            ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "            ax.xaxis.set_major_formatter(NullFormatter())\n",
      "            ax.yaxis.set_major_formatter(NullFormatter())\n",
      "            ax.axis('tight')\n",
      "        except:\n",
      "            print('Metoda nie ogarna')\n",
      "\n",
      "    plt.show()\n",
      "161/23: visualize_data(data_X[0], data_Y[0], 'labels', \"Cars dataset\")\n",
      "161/24:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "def visualize_data(data_X, data_Y, mode, name):\n",
      "    fig = plt.figure(figsize=(20, 8))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    if mode=='colors':\n",
      "        ax = fig.add_subplot(251, projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        ax.view_init(4, -100)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        try:\n",
      "            t0 = time()\n",
      "            Y = method.fit_transform(data_X)\n",
      "            t1 = time()\n",
      "            print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "            ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "            if mode=='colors':\n",
      "                ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "            elif mode=='wsb':\n",
      "                for color, i, target_name in zip(['Red', 'Green'], [False, True], ['Unpopular', 'Popular']):\n",
      "                    ax.scatter(Y[data_Y == i, 0], Y[data_Y == i, 1], color=color, alpha=.3,\n",
      "                            label=target_name)\n",
      "            else:\n",
      "                ax.scatter(Y[:, 0], Y[:, 1])\n",
      "                for h, txt in enumerate(data_Y):\n",
      "                    ax.annotate(txt, (Y[h,0], Y[h,1]))\n",
      "            ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "            ax.xaxis.set_major_formatter(NullFormatter())\n",
      "            ax.yaxis.set_major_formatter(NullFormatter())\n",
      "            ax.axis('tight')\n",
      "        except:\n",
      "            print('Metoda nie ogarna')\n",
      "\n",
      "    plt.show()\n",
      "161/25: visualize_data(data_X[0], data_Y[0], 'labels', \"Cars dataset\")\n",
      "161/26:\n",
      "Axes3D\n",
      "n_neighbors = 2\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "def visualize_data(data_X, data_Y, mode, name):\n",
      "    fig = plt.figure(figsize=(20, 8))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    if mode=='colors':\n",
      "        ax = fig.add_subplot(251, projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        ax.view_init(4, -100)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        try:\n",
      "            t0 = time()\n",
      "            Y = method.fit_transform(data_X)\n",
      "            t1 = time()\n",
      "            print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "            ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "            if mode=='colors':\n",
      "                ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "            elif mode=='wsb':\n",
      "                for color, i, target_name in zip(['Red', 'Green'], [False, True], ['Unpopular', 'Popular']):\n",
      "                    ax.scatter(Y[data_Y == i, 0], Y[data_Y == i, 1], color=color, alpha=.3,\n",
      "                            label=target_name)\n",
      "            else:\n",
      "                ax.scatter(Y[:, 0], Y[:, 1])\n",
      "                for h, txt in enumerate(data_Y):\n",
      "                    ax.annotate(txt, (Y[h,0], Y[h,1]))\n",
      "            ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "            ax.xaxis.set_major_formatter(NullFormatter())\n",
      "            ax.yaxis.set_major_formatter(NullFormatter())\n",
      "            ax.axis('tight')\n",
      "        except:\n",
      "            print('Metoda nie ogarna')\n",
      "\n",
      "    plt.show()\n",
      "161/27:\n",
      "Axes3D\n",
      "n_neighbors = 2\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "def visualize_data(data_X, data_Y, mode, name):\n",
      "    fig = plt.figure(figsize=(20, 8))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    if mode=='colors':\n",
      "        ax = fig.add_subplot(251, projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        ax.view_init(4, -100)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        try:\n",
      "            t0 = time()\n",
      "            Y = method.fit_transform(data_X)\n",
      "            t1 = time()\n",
      "            print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "            ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "            if mode=='colors':\n",
      "                ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "            elif mode=='wsb':\n",
      "                for color, i, target_name in zip(['Red', 'Green'], [False, True], ['Unpopular', 'Popular']):\n",
      "                    ax.scatter(Y[data_Y == i, 0], Y[data_Y == i, 1], color=color, alpha=.3,\n",
      "                            label=target_name)\n",
      "            else:\n",
      "                ax.scatter(Y[:, 0], Y[:, 1])\n",
      "                for h, txt in enumerate(data_Y):\n",
      "                    ax.annotate(txt, (Y[h,0], Y[h,1]))\n",
      "            ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "            ax.xaxis.set_major_formatter(NullFormatter())\n",
      "            ax.yaxis.set_major_formatter(NullFormatter())\n",
      "            ax.axis('tight')\n",
      "        except:\n",
      "            print('Metoda nie ogarna')\n",
      "\n",
      "    plt.show()\n",
      "161/28: visualize_data(data_X[0], data_Y[0], 'labels', \"Cars dataset\")\n",
      "161/29:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "def visualize_data(data_X, data_Y, mode, name):\n",
      "    fig = plt.figure(figsize=(20, 8))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    if mode=='colors':\n",
      "        ax = fig.add_subplot(251, projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        ax.view_init(4, -100)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        try:\n",
      "            t0 = time()\n",
      "            Y = method.fit_transform(data_X)\n",
      "            t1 = time()\n",
      "            print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "            ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "            if mode=='colors':\n",
      "                ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "            elif mode=='wsb':\n",
      "                for color, i, target_name in zip(['Red', 'Green'], [False, True], ['Unpopular', 'Popular']):\n",
      "                    ax.scatter(Y[data_Y == i, 0], Y[data_Y == i, 1], color=color, alpha=.3,\n",
      "                            label=target_name)\n",
      "            else:\n",
      "                ax.scatter(Y[:, 0], Y[:, 1])\n",
      "                for h, txt in enumerate(data_Y):\n",
      "                    ax.annotate(txt, (Y[h,0], Y[h,1]))\n",
      "            ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "            ax.xaxis.set_major_formatter(NullFormatter())\n",
      "            ax.yaxis.set_major_formatter(NullFormatter())\n",
      "            ax.axis('tight')\n",
      "        except:\n",
      "            print('Metoda nie ogarna')\n",
      "\n",
      "    plt.show()\n",
      "161/30: visualize_data(data_X[0], data_Y[0], 'labels', \"Cars dataset\")\n",
      "161/31:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['LTSA'] = LLE(method='ltsa')\n",
      "methods['Hessian LLE'] = LLE(method='hessian')\n",
      "methods['Modified LLE'] = LLE(method='modified')\n",
      "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components)\n",
      "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
      "                                           n_neighbors=n_neighbors)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
      "                                 random_state=0)\n",
      "\n",
      "def visualize_data(data_X, data_Y, mode, name):\n",
      "    fig = plt.figure(figsize=(20, 8))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    if mode=='colors':\n",
      "        ax = fig.add_subplot(251, projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        ax.view_init(4, -100)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        try:\n",
      "            t0 = time()\n",
      "            Y = method.fit_transform(data_X)\n",
      "            t1 = time()\n",
      "            print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "            ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "            if mode=='colors':\n",
      "                ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "            elif mode=='wsb':\n",
      "                for color, i, target_name in zip(['Red', 'Green'], [False, True], ['Unpopular', 'Popular']):\n",
      "                    ax.scatter(Y[data_Y == i, 0], Y[data_Y == i, 1], color=color, alpha=.3,\n",
      "                            label=target_name)\n",
      "            else:\n",
      "                ax.scatter(Y[:, 0], Y[:, 1])\n",
      "                for h, txt in enumerate(data_Y):\n",
      "                    ax.annotate(txt[:], (Y[h,0], Y[h,1]))\n",
      "            ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "            ax.xaxis.set_major_formatter(NullFormatter())\n",
      "            ax.yaxis.set_major_formatter(NullFormatter())\n",
      "            ax.axis('tight')\n",
      "        except:\n",
      "            print('Metoda nie ogarna')\n",
      "\n",
      "    plt.show()\n",
      "161/32: visualize_data(data_X[0], data_Y[0], 'labels', \"Cars dataset\")\n",
      "161/33: visualize_data(data_X[2], data_Y[2], 'wsb', \"WSB dataset\")\n",
      "161/34: visualize_data(data_X[1], data_Y[1], 'colors', \"Swiss roll dataset\")\n",
      "161/35:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['Isomap n=10'] = manifold.Isomap(10, n_components)\n",
      "methods['Isomap n=5'] = manifold.Isomap(5, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, perplexity=10)\n",
      "methods['t-SNE'] = manifold.TSNE(n_components=n_components, perplexity=5)\n",
      "\n",
      "def visualize_data(data_X, data_Y, mode, name):\n",
      "    fig = plt.figure(figsize=(20, 8))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    if mode=='colors':\n",
      "        ax = fig.add_subplot(251, projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        ax.view_init(4, -100)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        try:\n",
      "            t0 = time()\n",
      "            Y = method.fit_transform(data_X)\n",
      "            t1 = time()\n",
      "            print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "            ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "            if mode=='colors':\n",
      "                ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "            elif mode=='wsb':\n",
      "                for color, i, target_name in zip(['Red', 'Green'], [False, True], ['Unpopular', 'Popular']):\n",
      "                    ax.scatter(Y[data_Y == i, 0], Y[data_Y == i, 1], color=color, alpha=.3,\n",
      "                            label=target_name)\n",
      "            else:\n",
      "                ax.scatter(Y[:, 0], Y[:, 1])\n",
      "                for h, txt in enumerate(data_Y):\n",
      "                    ax.annotate(txt[:], (Y[h,0], Y[h,1]))\n",
      "            ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "            ax.xaxis.set_major_formatter(NullFormatter())\n",
      "            ax.yaxis.set_major_formatter(NullFormatter())\n",
      "            ax.axis('tight')\n",
      "        except:\n",
      "            print('Metoda nie ogarna')\n",
      "\n",
      "    plt.show()\n",
      "161/36: visualize_data(data_X[0], data_Y[0], 'labels', \"Cars dataset\")\n",
      "161/37:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['Isomap n=10'] = manifold.Isomap(10, n_components)\n",
      "methods['Isomap n=5'] = manifold.Isomap(5, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components)\n",
      "methods['t-SNE p=10'] = manifold.TSNE(n_components=n_components, perplexity=10)\n",
      "methods['t-SNE p=5'] = manifold.TSNE(n_components=n_components, perplexity=5)\n",
      "\n",
      "def visualize_data(data_X, data_Y, mode, name):\n",
      "    fig = plt.figure(figsize=(20, 8))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    if mode=='colors':\n",
      "        ax = fig.add_subplot(251, projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        ax.view_init(4, -100)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        try:\n",
      "            t0 = time()\n",
      "            Y = method.fit_transform(data_X)\n",
      "            t1 = time()\n",
      "            print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "            ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "            if mode=='colors':\n",
      "                ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "            elif mode=='wsb':\n",
      "                for color, i, target_name in zip(['Red', 'Green'], [False, True], ['Unpopular', 'Popular']):\n",
      "                    ax.scatter(Y[data_Y == i, 0], Y[data_Y == i, 1], color=color, alpha=.3,\n",
      "                            label=target_name)\n",
      "            else:\n",
      "                ax.scatter(Y[:, 0], Y[:, 1])\n",
      "                for h, txt in enumerate(data_Y):\n",
      "                    ax.annotate(txt[:], (Y[h,0], Y[h,1]))\n",
      "            ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "            ax.xaxis.set_major_formatter(NullFormatter())\n",
      "            ax.yaxis.set_major_formatter(NullFormatter())\n",
      "            ax.axis('tight')\n",
      "        except:\n",
      "            print('Metoda nie ogarna')\n",
      "\n",
      "    plt.show()\n",
      "161/38: visualize_data(data_X[0], data_Y[0], 'labels', \"Cars dataset\")\n",
      "161/39: visualize_data(data_X[1], data_Y[1], 'colors', \"Swiss roll dataset\")\n",
      "161/40:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['Isomap k=10'] = manifold.Isomap(10, n_components)\n",
      "methods['Isomap k=3'] = manifold.Isomap(3, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components)\n",
      "methods['t-SNE perplexity=5'] = manifold.TSNE(n_components=n_components, perplexity=5)\n",
      "methods['t-SNE perplexity=50'] = manifold.TSNE(n_components=n_components, perplexity=50)\n",
      "\n",
      "def visualize_data(data_X, data_Y, mode, name):\n",
      "    fig = plt.figure(figsize=(20, 8))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    if mode=='colors':\n",
      "        ax = fig.add_subplot(251, projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        ax.view_init(4, -100)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        try:\n",
      "            t0 = time()\n",
      "            Y = method.fit_transform(data_X)\n",
      "            t1 = time()\n",
      "            print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "            ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
      "            if mode=='colors':\n",
      "                ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "            elif mode=='wsb':\n",
      "                for color, i, target_name in zip(['Red', 'Green'], [False, True], ['Unpopular', 'Popular']):\n",
      "                    ax.scatter(Y[data_Y == i, 0], Y[data_Y == i, 1], color=color, alpha=.3,\n",
      "                            label=target_name)\n",
      "            else:\n",
      "                ax.scatter(Y[:, 0], Y[:, 1])\n",
      "                for h, txt in enumerate(data_Y):\n",
      "                    ax.annotate(txt[:], (Y[h,0], Y[h,1]))\n",
      "            ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "            ax.xaxis.set_major_formatter(NullFormatter())\n",
      "            ax.yaxis.set_major_formatter(NullFormatter())\n",
      "            ax.axis('tight')\n",
      "        except:\n",
      "            print('Metoda nie ogarna')\n",
      "\n",
      "    plt.show()\n",
      "161/41:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['Isomap k=10'] = manifold.Isomap(10, n_components)\n",
      "methods['Isomap k=3'] = manifold.Isomap(3, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components)\n",
      "methods['t-SNE perplexity=5'] = manifold.TSNE(n_components=n_components, perplexity=5)\n",
      "methods['t-SNE perplexity=50'] = manifold.TSNE(n_components=n_components, perplexity=50)\n",
      "\n",
      "def visualize_data(data_X, data_Y, mode, name):\n",
      "    fig = plt.figure(figsize=(20, 8))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    if mode=='colors':\n",
      "        ax = fig.add_subplot(251, projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        ax.view_init(4, -100)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        try:\n",
      "            t0 = time()\n",
      "            Y = method.fit_transform(data_X)\n",
      "            t1 = time()\n",
      "            print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "            ax = fig.add_subplot(2, 5, 2 + i + (i > 2))\n",
      "            if mode=='colors':\n",
      "                ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "            elif mode=='wsb':\n",
      "                for color, i, target_name in zip(['Red', 'Green'], [False, True], ['Unpopular', 'Popular']):\n",
      "                    ax.scatter(Y[data_Y == i, 0], Y[data_Y == i, 1], color=color, alpha=.3,\n",
      "                            label=target_name)\n",
      "            else:\n",
      "                ax.scatter(Y[:, 0], Y[:, 1])\n",
      "                for h, txt in enumerate(data_Y):\n",
      "                    ax.annotate(txt[:], (Y[h,0], Y[h,1]))\n",
      "            ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "            ax.xaxis.set_major_formatter(NullFormatter())\n",
      "            ax.yaxis.set_major_formatter(NullFormatter())\n",
      "            ax.axis('tight')\n",
      "        except:\n",
      "            print('Metoda nie ogarna')\n",
      "\n",
      "    plt.show()\n",
      "161/42: visualize_data(data_X[0], data_Y[0], 'labels', \"Cars dataset\")\n",
      "161/43:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['Isomap k=10'] = manifold.Isomap(10, n_components)\n",
      "methods['Isomap k=3'] = manifold.Isomap(3, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components)\n",
      "methods['t-SNE perplexity=5'] = manifold.TSNE(n_components=n_components, perplexity=5)\n",
      "methods['t-SNE perplexity=50'] = manifold.TSNE(n_components=n_components, perplexity=50)\n",
      "\n",
      "def visualize_data(data_X, data_Y, mode, name):\n",
      "    fig = plt.figure(figsize=(20, 8))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    if mode=='colors':\n",
      "        ax = fig.add_subplot(251, projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        ax.view_init(4, -100)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        try:\n",
      "            t0 = time()\n",
      "            Y = method.fit_transform(data_X)\n",
      "            t1 = time()\n",
      "            print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "            ax = fig.add_subplot(2, 5, 2 + i)\n",
      "            if mode=='colors':\n",
      "                ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "            elif mode=='wsb':\n",
      "                for color, i, target_name in zip(['Red', 'Green'], [False, True], ['Unpopular', 'Popular']):\n",
      "                    ax.scatter(Y[data_Y == i, 0], Y[data_Y == i, 1], color=color, alpha=.3,\n",
      "                            label=target_name)\n",
      "            else:\n",
      "                ax.scatter(Y[:, 0], Y[:, 1])\n",
      "                for h, txt in enumerate(data_Y):\n",
      "                    ax.annotate(txt[:], (Y[h,0], Y[h,1]))\n",
      "            ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "            ax.xaxis.set_major_formatter(NullFormatter())\n",
      "            ax.yaxis.set_major_formatter(NullFormatter())\n",
      "            ax.axis('tight')\n",
      "        except:\n",
      "            print('Metoda nie ogarna')\n",
      "\n",
      "    plt.show()\n",
      "161/44: visualize_data(data_X[0], data_Y[0], 'labels', \"Cars dataset\")\n",
      "161/45: visualize_data(data_X[1], data_Y[1], 'colors', \"Swiss roll dataset\")\n",
      "161/46:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['Isomap k=10'] = manifold.Isomap(10, n_components)\n",
      "methods['Isomap k=3'] = manifold.Isomap(3, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components)\n",
      "methods['t-SNE perplexity=5'] = manifold.TSNE(n_components=n_components, perplexity=5)\n",
      "methods['t-SNE perplexity=50'] = manifold.TSNE(n_components=n_components, perplexity=50)\n",
      "\n",
      "def visualize_data(data_X, data_Y, mode, name):\n",
      "    fig = plt.figure(figsize=(20, 8))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    if mode=='colors':\n",
      "        ax = fig.add_subplot(251, projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        ax.view_init(4, -100)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        try:\n",
      "            t0 = time()\n",
      "            Y = method.fit_transform(data_X)\n",
      "            t1 = time()\n",
      "            print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "            ax = fig.add_subplot(2, 5, 1 + i + (i > 2))\n",
      "            if mode=='colors':\n",
      "                ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "            elif mode=='wsb':\n",
      "                for color, i, target_name in zip(['Red', 'Green'], [False, True], ['Unpopular', 'Popular']):\n",
      "                    ax.scatter(Y[data_Y == i, 0], Y[data_Y == i, 1], color=color, alpha=.3,\n",
      "                            label=target_name)\n",
      "            else:\n",
      "                ax.scatter(Y[:, 0], Y[:, 1])\n",
      "                for h, txt in enumerate(data_Y):\n",
      "                    ax.annotate(txt[:], (Y[h,0], Y[h,1]))\n",
      "            ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "            ax.xaxis.set_major_formatter(NullFormatter())\n",
      "            ax.yaxis.set_major_formatter(NullFormatter())\n",
      "            ax.axis('tight')\n",
      "        except:\n",
      "            print('Metoda nie ogarna')\n",
      "\n",
      "    plt.show()\n",
      "161/47: visualize_data(data_X[0], data_Y[0], 'labels', \"Cars dataset\")\n",
      "161/48:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['Isomap k=10'] = manifold.Isomap(10, n_components)\n",
      "methods['Isomap k=3'] = manifold.Isomap(3, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components)\n",
      "methods['t-SNE perplexity=5'] = manifold.TSNE(n_components=n_components, perplexity=5)\n",
      "methods['t-SNE perplexity=50'] = manifold.TSNE(n_components=n_components, perplexity=50)\n",
      "\n",
      "def visualize_data(data_X, data_Y, mode, name):\n",
      "    fig = plt.figure(figsize=(20, 8))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    if mode=='colors':\n",
      "        ax = fig.add_subplot(251, projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        ax.view_init(4, -100)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        try:\n",
      "            t0 = time()\n",
      "            Y = method.fit_transform(data_X)\n",
      "            t1 = time()\n",
      "            print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "            ax = fig.add_subplot(2, i//3)\n",
      "            if mode=='colors':\n",
      "                ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "            elif mode=='wsb':\n",
      "                for color, i, target_name in zip(['Red', 'Green'], [False, True], ['Unpopular', 'Popular']):\n",
      "                    ax.scatter(Y[data_Y == i, 0], Y[data_Y == i, 1], color=color, alpha=.3,\n",
      "                            label=target_name)\n",
      "            else:\n",
      "                ax.scatter(Y[:, 0], Y[:, 1])\n",
      "                for h, txt in enumerate(data_Y):\n",
      "                    ax.annotate(txt[:], (Y[h,0], Y[h,1]))\n",
      "            ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "            ax.xaxis.set_major_formatter(NullFormatter())\n",
      "            ax.yaxis.set_major_formatter(NullFormatter())\n",
      "            ax.axis('tight')\n",
      "        except:\n",
      "            print('Metoda nie ogarna')\n",
      "\n",
      "    plt.show()\n",
      "161/49: visualize_data(data_X[0], data_Y[0], 'labels', \"Cars dataset\")\n",
      "161/50:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['Isomap k=10'] = manifold.Isomap(10, n_components)\n",
      "methods['Isomap k=3'] = manifold.Isomap(3, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components)\n",
      "methods['t-SNE perplexity=5'] = manifold.TSNE(n_components=n_components, perplexity=5)\n",
      "methods['t-SNE perplexity=50'] = manifold.TSNE(n_components=n_components, perplexity=50)\n",
      "\n",
      "def visualize_data(data_X, data_Y, mode, name):\n",
      "    fig = plt.figure(figsize=(20, 8))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    if mode=='colors':\n",
      "        ax = fig.add_subplot(251, projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        ax.view_init(4, -100)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        try:\n",
      "            t0 = time()\n",
      "            Y = method.fit_transform(data_X)\n",
      "            t1 = time()\n",
      "            print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "            ax = fig.add_subplot(2, i/3)\n",
      "            if mode=='colors':\n",
      "                ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "            elif mode=='wsb':\n",
      "                for color, i, target_name in zip(['Red', 'Green'], [False, True], ['Unpopular', 'Popular']):\n",
      "                    ax.scatter(Y[data_Y == i, 0], Y[data_Y == i, 1], color=color, alpha=.3,\n",
      "                            label=target_name)\n",
      "            else:\n",
      "                ax.scatter(Y[:, 0], Y[:, 1])\n",
      "                for h, txt in enumerate(data_Y):\n",
      "                    ax.annotate(txt[:], (Y[h,0], Y[h,1]))\n",
      "            ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "            ax.xaxis.set_major_formatter(NullFormatter())\n",
      "            ax.yaxis.set_major_formatter(NullFormatter())\n",
      "            ax.axis('tight')\n",
      "        except:\n",
      "            print('Metoda nie ogarna')\n",
      "\n",
      "    plt.show()\n",
      "161/51: visualize_data(data_X[0], data_Y[0], 'labels', \"Cars dataset\")\n",
      "161/52:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['Isomap k=10'] = manifold.Isomap(10, n_components)\n",
      "methods['Isomap k=3'] = manifold.Isomap(3, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components)\n",
      "methods['t-SNE perplexity=5'] = manifold.TSNE(n_components=n_components, perplexity=5)\n",
      "methods['t-SNE perplexity=50'] = manifold.TSNE(n_components=n_components, perplexity=50)\n",
      "\n",
      "def visualize_data(data_X, data_Y, mode, name):\n",
      "    fig = plt.figure(figsize=(20, 8))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    if mode=='colors':\n",
      "        ax = fig.add_subplot(251, projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        ax.view_init(4, -100)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        try:\n",
      "            t0 = time()\n",
      "            Y = method.fit_transform(data_X)\n",
      "            t1 = time()\n",
      "            print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "            ax = fig.add_subplot(2, i)\n",
      "            if mode=='colors':\n",
      "                ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "            elif mode=='wsb':\n",
      "                for color, i, target_name in zip(['Red', 'Green'], [False, True], ['Unpopular', 'Popular']):\n",
      "                    ax.scatter(Y[data_Y == i, 0], Y[data_Y == i, 1], color=color, alpha=.3,\n",
      "                            label=target_name)\n",
      "            else:\n",
      "                ax.scatter(Y[:, 0], Y[:, 1])\n",
      "                for h, txt in enumerate(data_Y):\n",
      "                    ax.annotate(txt[:], (Y[h,0], Y[h,1]))\n",
      "            ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "            ax.xaxis.set_major_formatter(NullFormatter())\n",
      "            ax.yaxis.set_major_formatter(NullFormatter())\n",
      "            ax.axis('tight')\n",
      "        except:\n",
      "            print('Metoda nie ogarna')\n",
      "\n",
      "    plt.show()\n",
      "161/53: visualize_data(data_X[0], data_Y[0], 'labels', \"Cars dataset\")\n",
      "161/54:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['Isomap k=10'] = manifold.Isomap(10, n_components)\n",
      "methods['Isomap k=3'] = manifold.Isomap(3, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components)\n",
      "methods['t-SNE perplexity=5'] = manifold.TSNE(n_components=n_components, perplexity=5)\n",
      "methods['t-SNE perplexity=50'] = manifold.TSNE(n_components=n_components, perplexity=50)\n",
      "\n",
      "def visualize_data(data_X, data_Y, mode, name):\n",
      "    fig = plt.figure(figsize=(20, 8))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    if mode=='colors':\n",
      "        ax = fig.add_subplot(251, projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        ax.view_init(4, -100)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        try:\n",
      "            t0 = time()\n",
      "            Y = method.fit_transform(data_X)\n",
      "            t1 = time()\n",
      "            print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "            ax = fig.add_subplot(2, 5, i)\n",
      "            if mode=='colors':\n",
      "                ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "            elif mode=='wsb':\n",
      "                for color, i, target_name in zip(['Red', 'Green'], [False, True], ['Unpopular', 'Popular']):\n",
      "                    ax.scatter(Y[data_Y == i, 0], Y[data_Y == i, 1], color=color, alpha=.3,\n",
      "                            label=target_name)\n",
      "            else:\n",
      "                ax.scatter(Y[:, 0], Y[:, 1])\n",
      "                for h, txt in enumerate(data_Y):\n",
      "                    ax.annotate(txt[:], (Y[h,0], Y[h,1]))\n",
      "            ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "            ax.xaxis.set_major_formatter(NullFormatter())\n",
      "            ax.yaxis.set_major_formatter(NullFormatter())\n",
      "            ax.axis('tight')\n",
      "        except:\n",
      "            print('Metoda nie ogarna')\n",
      "\n",
      "    plt.show()\n",
      "161/55: visualize_data(data_X[0], data_Y[0], 'labels', \"Cars dataset\")\n",
      "161/56:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['Isomap k=10'] = manifold.Isomap(10, n_components)\n",
      "methods['Isomap k=3'] = manifold.Isomap(3, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components)\n",
      "methods['t-SNE perplexity=5'] = manifold.TSNE(n_components=n_components, perplexity=5)\n",
      "methods['t-SNE perplexity=50'] = manifold.TSNE(n_components=n_components, perplexity=50)\n",
      "\n",
      "def visualize_data(data_X, data_Y, mode, name):\n",
      "    fig = plt.figure(figsize=(20, 8))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    if mode=='colors':\n",
      "        ax = fig.add_subplot(251, projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        ax.view_init(4, -100)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        try:\n",
      "            t0 = time()\n",
      "            Y = method.fit_transform(data_X)\n",
      "            t1 = time()\n",
      "            print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "            ax = fig.add_subplot(2, 3, i)\n",
      "            if mode=='colors':\n",
      "                ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "            elif mode=='wsb':\n",
      "                for color, i, target_name in zip(['Red', 'Green'], [False, True], ['Unpopular', 'Popular']):\n",
      "                    ax.scatter(Y[data_Y == i, 0], Y[data_Y == i, 1], color=color, alpha=.3,\n",
      "                            label=target_name)\n",
      "            else:\n",
      "                ax.scatter(Y[:, 0], Y[:, 1])\n",
      "                for h, txt in enumerate(data_Y):\n",
      "                    ax.annotate(txt[:], (Y[h,0], Y[h,1]))\n",
      "            ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "            ax.xaxis.set_major_formatter(NullFormatter())\n",
      "            ax.yaxis.set_major_formatter(NullFormatter())\n",
      "            ax.axis('tight')\n",
      "        except:\n",
      "            print('Metoda nie ogarna')\n",
      "\n",
      "    plt.show()\n",
      "161/57: visualize_data(data_X[0], data_Y[0], 'labels', \"Cars dataset\")\n",
      "161/58:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['Isomap k=10'] = manifold.Isomap(10, n_components)\n",
      "methods['Isomap k=3'] = manifold.Isomap(3, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components)\n",
      "methods['t-SNE perplexity=5'] = manifold.TSNE(n_components=n_components, perplexity=5)\n",
      "methods['t-SNE perplexity=50'] = manifold.TSNE(n_components=n_components, perplexity=50)\n",
      "\n",
      "def visualize_data(data_X, data_Y, mode, name):\n",
      "    fig = plt.figure(figsize=(20, 8))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    if mode=='colors':\n",
      "        ax = fig.add_subplot(251, projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        ax.view_init(4, -100)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        try:\n",
      "            t0 = time()\n",
      "            Y = method.fit_transform(data_X)\n",
      "            t1 = time()\n",
      "            print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "            ax = fig.add_subplot(2, 3, i+1)\n",
      "            if mode=='colors':\n",
      "                ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "            elif mode=='wsb':\n",
      "                for color, i, target_name in zip(['Red', 'Green'], [False, True], ['Unpopular', 'Popular']):\n",
      "                    ax.scatter(Y[data_Y == i, 0], Y[data_Y == i, 1], color=color, alpha=.3,\n",
      "                            label=target_name)\n",
      "            else:\n",
      "                ax.scatter(Y[:, 0], Y[:, 1])\n",
      "                for h, txt in enumerate(data_Y):\n",
      "                    ax.annotate(txt[:], (Y[h,0], Y[h,1]))\n",
      "            ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "            ax.xaxis.set_major_formatter(NullFormatter())\n",
      "            ax.yaxis.set_major_formatter(NullFormatter())\n",
      "            ax.axis('tight')\n",
      "        except:\n",
      "            print('Metoda nie ogarna')\n",
      "\n",
      "    plt.show()\n",
      "161/59: visualize_data(data_X[0], data_Y[0], 'labels', \"Cars dataset\")\n",
      "161/60: visualize_data(data_X[1], data_Y[1], 'colors', \"Swiss roll dataset\")\n",
      "161/61: visualize_data(data_X[2], data_Y[2], 'wsb', \"WSB dataset\")\n",
      "161/62:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['Isomap k=10'] = manifold.Isomap(10, n_components)\n",
      "methods['Isomap k=3'] = manifold.Isomap(3, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components)\n",
      "methods['t-SNE perplexity=5'] = manifold.TSNE(n_components=n_components, perplexity=5)\n",
      "methods['t-SNE perplexity=50'] = manifold.TSNE(n_components=n_components, perplexity=50)\n",
      "\n",
      "def visualize_data(data_X, data_Y, mode, name):\n",
      "    fig = plt.figure(figsize=(20, 8))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    if mode=='colors':\n",
      "        ax = fig.add_subplot(231, projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        ax.view_init(4, -100)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        try:\n",
      "            t0 = time()\n",
      "            Y = method.fit_transform(data_X)\n",
      "            t1 = time()\n",
      "            print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "            ax = fig.add_subplot(2, 3, i+1)\n",
      "            if mode=='colors':\n",
      "                ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "            elif mode=='wsb':\n",
      "                for color, i, target_name in zip(['Red', 'Green'], [False, True], ['Unpopular', 'Popular']):\n",
      "                    ax.scatter(Y[data_Y == i, 0], Y[data_Y == i, 1], color=color, alpha=.3,\n",
      "                            label=target_name)\n",
      "            else:\n",
      "                ax.scatter(Y[:, 0], Y[:, 1])\n",
      "                for h, txt in enumerate(data_Y):\n",
      "                    ax.annotate(txt[:], (Y[h,0], Y[h,1]))\n",
      "            ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "            ax.xaxis.set_major_formatter(NullFormatter())\n",
      "            ax.yaxis.set_major_formatter(NullFormatter())\n",
      "            ax.axis('tight')\n",
      "        except:\n",
      "            print('Metoda nie ogarna')\n",
      "\n",
      "    plt.show()\n",
      "161/63: visualize_data(data_X[1], data_Y[1], 'colors', \"Swiss roll dataset\")\n",
      "161/64:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['Isomap k=10'] = manifold.Isomap(10, n_components)\n",
      "methods['Isomap k=3'] = manifold.Isomap(3, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components)\n",
      "methods['t-SNE perplexity=5'] = manifold.TSNE(n_components=n_components, perplexity=5)\n",
      "methods['t-SNE perplexity=50'] = manifold.TSNE(n_components=n_components, perplexity=50)\n",
      "\n",
      "def visualize_data(data_X, data_Y, mode, name):\n",
      "    fig = plt.figure(figsize=(20, 8))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    if mode=='colors':\n",
      "        ax = fig.add_subplot(projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        try:\n",
      "            t0 = time()\n",
      "            Y = method.fit_transform(data_X)\n",
      "            t1 = time()\n",
      "            print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "            ax = fig.add_subplot(2, 3, i+1)\n",
      "            if mode=='colors':\n",
      "                ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "            elif mode=='wsb':\n",
      "                for color, i, target_name in zip(['Red', 'Green'], [False, True], ['Unpopular', 'Popular']):\n",
      "                    ax.scatter(Y[data_Y == i, 0], Y[data_Y == i, 1], color=color, alpha=.3,\n",
      "                            label=target_name)\n",
      "            else:\n",
      "                ax.scatter(Y[:, 0], Y[:, 1])\n",
      "                for h, txt in enumerate(data_Y):\n",
      "                    ax.annotate(txt[:], (Y[h,0], Y[h,1]))\n",
      "            ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "            ax.xaxis.set_major_formatter(NullFormatter())\n",
      "            ax.yaxis.set_major_formatter(NullFormatter())\n",
      "            ax.axis('tight')\n",
      "        except:\n",
      "            print('Metoda nie ogarna')\n",
      "\n",
      "    plt.show()\n",
      "161/65: visualize_data(data_X[1], data_Y[1], 'colors', \"Swiss roll dataset\")\n",
      "161/66:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['Isomap k=10'] = manifold.Isomap(10, n_components)\n",
      "methods['Isomap k=3'] = manifold.Isomap(3, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components)\n",
      "methods['t-SNE perplexity=5'] = manifold.TSNE(n_components=n_components, perplexity=5)\n",
      "methods['t-SNE perplexity=50'] = manifold.TSNE(n_components=n_components, perplexity=50)\n",
      "\n",
      "def visualize_data(data_X, data_Y, mode, name):\n",
      "    if mode=='colors':\n",
      "        fig = plt.figure(figsize=(20, 8))\n",
      "        fig.suptitle(name, fontsize=14)\n",
      "        ax = fig.add_subplot(projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        plt.show()\n",
      "        \n",
      "        fig = plt.figure(figsize=(20, 8))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        try:\n",
      "            t0 = time()\n",
      "            Y = method.fit_transform(data_X)\n",
      "            t1 = time()\n",
      "            print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "            ax = fig.add_subplot(2, 3, i+1)\n",
      "            if mode=='colors':\n",
      "                ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "            elif mode=='wsb':\n",
      "                for color, i, target_name in zip(['Red', 'Green'], [False, True], ['Unpopular', 'Popular']):\n",
      "                    ax.scatter(Y[data_Y == i, 0], Y[data_Y == i, 1], color=color, alpha=.3,\n",
      "                            label=target_name)\n",
      "            else:\n",
      "                ax.scatter(Y[:, 0], Y[:, 1])\n",
      "                for h, txt in enumerate(data_Y):\n",
      "                    ax.annotate(txt[:], (Y[h,0], Y[h,1]))\n",
      "            ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "            ax.xaxis.set_major_formatter(NullFormatter())\n",
      "            ax.yaxis.set_major_formatter(NullFormatter())\n",
      "            ax.axis('tight')\n",
      "        except:\n",
      "            print('Metoda nie ogarna')\n",
      "\n",
      "    plt.show()\n",
      "161/67: visualize_data(data_X[1], data_Y[1], 'colors', \"Swiss roll dataset\")\n",
      "161/68:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['Isomap k=10'] = manifold.Isomap(10, n_components)\n",
      "methods['Isomap k=3'] = manifold.Isomap(3, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components)\n",
      "methods['t-SNE perplexity=5'] = manifold.TSNE(n_components=n_components, perplexity=5)\n",
      "methods['t-SNE perplexity=50'] = manifold.TSNE(n_components=n_components, perplexity=50)\n",
      "\n",
      "def visualize_data(data_X, data_Y, mode, name):\n",
      "    if mode=='colors':\n",
      "        fig = plt.figure(figsize=(20, 8))\n",
      "        fig.suptitle(name, fontsize=14)\n",
      "        ax = fig.add_subplot(projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        ax.view_init(4, -50)\n",
      "        plt.show()\n",
      "        \n",
      "        fig = plt.figure(figsize=(20, 8))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        try:\n",
      "            t0 = time()\n",
      "            Y = method.fit_transform(data_X)\n",
      "            t1 = time()\n",
      "            print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "            ax = fig.add_subplot(2, 3, i+1)\n",
      "            if mode=='colors':\n",
      "                ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "            elif mode=='wsb':\n",
      "                for color, i, target_name in zip(['Red', 'Green'], [False, True], ['Unpopular', 'Popular']):\n",
      "                    ax.scatter(Y[data_Y == i, 0], Y[data_Y == i, 1], color=color, alpha=.3,\n",
      "                            label=target_name)\n",
      "            else:\n",
      "                ax.scatter(Y[:, 0], Y[:, 1])\n",
      "                for h, txt in enumerate(data_Y):\n",
      "                    ax.annotate(txt[:], (Y[h,0], Y[h,1]))\n",
      "            ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "            ax.xaxis.set_major_formatter(NullFormatter())\n",
      "            ax.yaxis.set_major_formatter(NullFormatter())\n",
      "            ax.axis('tight')\n",
      "        except:\n",
      "            print('Metoda nie ogarna')\n",
      "\n",
      "    plt.show()\n",
      "161/69: visualize_data(data_X[1], data_Y[1], 'colors', \"Swiss roll dataset\")\n",
      "161/70:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['Isomap k=10'] = manifold.Isomap(10, n_components)\n",
      "methods['Isomap k=3'] = manifold.Isomap(3, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components)\n",
      "methods['t-SNE perplexity=5'] = manifold.TSNE(n_components=n_components, perplexity=5)\n",
      "methods['t-SNE perplexity=50'] = manifold.TSNE(n_components=n_components, perplexity=50)\n",
      "\n",
      "def visualize_data(data_X, data_Y, mode, name):\n",
      "    if mode=='colors':\n",
      "        fig = plt.figure(figsize=(20, 8))\n",
      "        fig.suptitle(name, fontsize=14)\n",
      "        ax = fig.add_subplot(projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        ax.view_init(10, -80)\n",
      "        plt.show()\n",
      "        \n",
      "        fig = plt.figure(figsize=(20, 8))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        try:\n",
      "            t0 = time()\n",
      "            Y = method.fit_transform(data_X)\n",
      "            t1 = time()\n",
      "            print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "            ax = fig.add_subplot(2, 3, i+1)\n",
      "            if mode=='colors':\n",
      "                ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "            elif mode=='wsb':\n",
      "                for color, i, target_name in zip(['Red', 'Green'], [False, True], ['Unpopular', 'Popular']):\n",
      "                    ax.scatter(Y[data_Y == i, 0], Y[data_Y == i, 1], color=color, alpha=.3,\n",
      "                            label=target_name)\n",
      "            else:\n",
      "                ax.scatter(Y[:, 0], Y[:, 1])\n",
      "                for h, txt in enumerate(data_Y):\n",
      "                    ax.annotate(txt[:], (Y[h,0], Y[h,1]))\n",
      "            ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "            ax.xaxis.set_major_formatter(NullFormatter())\n",
      "            ax.yaxis.set_major_formatter(NullFormatter())\n",
      "            ax.axis('tight')\n",
      "        except:\n",
      "            print('Metoda nie ogarna')\n",
      "\n",
      "    plt.show()\n",
      "161/71: visualize_data(data_X[1], data_Y[1], 'colors', \"Swiss roll dataset\")\n",
      "161/72:\n",
      "from collections import OrderedDict\n",
      "from functools import partial\n",
      "from time import time\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "from matplotlib.ticker import NullFormatter\n",
      "\n",
      "from sklearn import manifold, datasets\n",
      "import pandas as pd\n",
      "import pickle\n",
      "161/73:\n",
      "data_X = []\n",
      "data_Y = []\n",
      "\n",
      "cars = pd.read_csv('./data/cars.csv', header=None)  \n",
      "collist = cars.columns.tolist()\n",
      "data_X.append(cars[cars.columns[1:]])\n",
      "data_Y.append(cars[0])\n",
      "\n",
      "n_points = 1000\n",
      "swiss_X, swiss_Y = datasets.make_swiss_roll(n_points)\n",
      "data_X.append(swiss_X)\n",
      "data_Y.append(swiss_Y)\n",
      "\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num']\n",
      "attributes_score = pickle.load(open( \"./data/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "attributes_score[\"score\"] = attributes_score[\"score\"]>2000\n",
      "attributes_score = attributes_score.head(1000)\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "attributes_without_excluded = exclude(attributes_score)\n",
      "data_X.append(attributes_without_excluded)\n",
      "data_Y.append(attributes_score['score'])\n",
      "161/74:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['Isomap k=10'] = manifold.Isomap(10, n_components)\n",
      "methods['Isomap k=3'] = manifold.Isomap(3, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components)\n",
      "methods['t-SNE perplexity=5'] = manifold.TSNE(n_components=n_components, perplexity=5)\n",
      "methods['t-SNE perplexity=50'] = manifold.TSNE(n_components=n_components, perplexity=50)\n",
      "\n",
      "def visualize_data(data_X, data_Y, mode, name):\n",
      "    if mode=='colors':\n",
      "        fig = plt.figure(figsize=(20, 8))\n",
      "        fig.suptitle(name, fontsize=14)\n",
      "        ax = fig.add_subplot(projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        ax.view_init(10, -80)\n",
      "        plt.show()\n",
      "        \n",
      "        fig = plt.figure(figsize=(20, 8))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        try:\n",
      "            t0 = time()\n",
      "            Y = method.fit_transform(data_X)\n",
      "            t1 = time()\n",
      "            print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "            ax = fig.add_subplot(2, 3, i+1)\n",
      "            if mode=='colors':\n",
      "                ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "            elif mode=='wsb':\n",
      "                for color, i, target_name in zip(['Red', 'Green'], [False, True], ['Unpopular', 'Popular']):\n",
      "                    ax.scatter(Y[data_Y == i, 0], Y[data_Y == i, 1], color=color, alpha=.3,\n",
      "                            label=target_name)\n",
      "            else:\n",
      "                ax.scatter(Y[:, 0], Y[:, 1])\n",
      "                for h, txt in enumerate(data_Y):\n",
      "                    ax.annotate(txt[:], (Y[h,0], Y[h,1]))\n",
      "            ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "            ax.xaxis.set_major_formatter(NullFormatter())\n",
      "            ax.yaxis.set_major_formatter(NullFormatter())\n",
      "            ax.axis('tight')\n",
      "        except:\n",
      "            print('Metoda nie ogarna')\n",
      "\n",
      "    plt.show()\n",
      "161/75: visualize_data(data_X[0], data_Y[0], 'labels', \"Cars dataset\")\n",
      "161/76:\n",
      "from collections import OrderedDict\n",
      "from functools import partial\n",
      "from time import time\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "from matplotlib.ticker import NullFormatter\n",
      "\n",
      "from sklearn import manifold, datasets\n",
      "import pandas as pd\n",
      "import pickle\n",
      "161/77:\n",
      "data_X = []\n",
      "data_Y = []\n",
      "\n",
      "cars = pd.read_csv('./data/cars.csv', header=None)  \n",
      "collist = cars.columns.tolist()\n",
      "data_X.append(cars[cars.columns[1:]])\n",
      "data_Y.append(cars[0])\n",
      "\n",
      "n_points = 1000\n",
      "swiss_X, swiss_Y = datasets.make_swiss_roll(n_points)\n",
      "data_X.append(swiss_X)\n",
      "data_Y.append(swiss_Y)\n",
      "\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num']\n",
      "attributes_score = pickle.load(open( \"./data/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "attributes_score[\"score\"] = attributes_score[\"score\"]>2000\n",
      "attributes_score = attributes_score.head(1000)\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "attributes_without_excluded = exclude(attributes_score)\n",
      "data_X.append(attributes_without_excluded)\n",
      "data_Y.append(attributes_score['score'])\n",
      "161/78:\n",
      "Axes3D\n",
      "n_neighbors = 10\n",
      "n_components = 2\n",
      "\n",
      "LLE = partial(manifold.LocallyLinearEmbedding,\n",
      "              n_neighbors, n_components, eigen_solver='auto')\n",
      "\n",
      "methods = OrderedDict()\n",
      "methods['LLE'] = LLE(method='standard')\n",
      "methods['Isomap k=10'] = manifold.Isomap(10, n_components)\n",
      "methods['Isomap k=3'] = manifold.Isomap(3, n_components)\n",
      "methods['MDS'] = manifold.MDS(n_components)\n",
      "methods['t-SNE perplexity=5'] = manifold.TSNE(n_components=n_components, perplexity=5)\n",
      "methods['t-SNE perplexity=50'] = manifold.TSNE(n_components=n_components, perplexity=50)\n",
      "\n",
      "def visualize_data(data_X, data_Y, mode, name):\n",
      "    if mode=='colors':\n",
      "        fig = plt.figure(figsize=(20, 8))\n",
      "        fig.suptitle(name, fontsize=14)\n",
      "        ax = fig.add_subplot(projection='3d')\n",
      "        ax.scatter(data_X[:, 0], data_X[:, 1], data_X[:, 2], c=data_Y, cmap=plt.cm.Spectral)\n",
      "        ax.view_init(10, -80)\n",
      "        plt.show()\n",
      "        \n",
      "    fig = plt.figure(figsize=(20, 8))\n",
      "    fig.suptitle(name, fontsize=14)\n",
      "\n",
      "    for i, (label, method) in enumerate(methods.items()):\n",
      "        try:\n",
      "            t0 = time()\n",
      "            Y = method.fit_transform(data_X)\n",
      "            t1 = time()\n",
      "            print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
      "            ax = fig.add_subplot(2, 3, i+1)\n",
      "            if mode=='colors':\n",
      "                ax.scatter(Y[:, 0], Y[:, 1], c=data_Y, cmap=plt.cm.Spectral)\n",
      "            elif mode=='wsb':\n",
      "                for color, i, target_name in zip(['Red', 'Green'], [False, True], ['Unpopular', 'Popular']):\n",
      "                    ax.scatter(Y[data_Y == i, 0], Y[data_Y == i, 1], color=color, alpha=.3,\n",
      "                            label=target_name)\n",
      "            else:\n",
      "                ax.scatter(Y[:, 0], Y[:, 1])\n",
      "                for h, txt in enumerate(data_Y):\n",
      "                    ax.annotate(txt[:], (Y[h,0], Y[h,1]))\n",
      "            ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
      "            ax.xaxis.set_major_formatter(NullFormatter())\n",
      "            ax.yaxis.set_major_formatter(NullFormatter())\n",
      "            ax.axis('tight')\n",
      "        except:\n",
      "            print('Metoda nie ogarna')\n",
      "\n",
      "    plt.show()\n",
      "161/79: visualize_data(data_X[0], data_Y[0], 'labels', \"Cars dataset\")\n",
      "161/80: visualize_data(data_X[1], data_Y[1], 'colors', \"Swiss roll dataset\")\n",
      "161/81: visualize_data(data_X[2], data_Y[2], 'wsb', \"WSB dataset\")\n",
      "163/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "\n",
      "comments_circa_all = pickle.load(open( \"../../pickle/comments_all.pkl\", \"rb\" ))\n",
      "163/2:\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "\n",
      "comments_circa_all = pickle.load(open( \"../../pickle/comments_all.pkl\", \"rb\" ))\n",
      "163/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "\n",
      "comments_circa_all = pickle.load(open( \"../../pickle/comments_all.pkl\", \"rb\" ))\n",
      "163/4:\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "\n",
      "comments_circa_all = pickle.load(open( \"../../pickle/comments_all.pkl\", \"rb\" ))\n",
      "163/5:\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "\n",
      "comments_circa_all = pickle.load(open( \"../../pickle/comments_all.pkl\", \"rb\" ))\n",
      "164/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "\n",
      "comments_circa_all = pickle.load(open( \"../../pickle/comments_all.pkl\", \"rb\" ))\n",
      "164/2:\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "\n",
      "comments_circa_all = pickle.load(open( \"../../pickle/test.pkl\", \"rb\" ))\n",
      "165/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "\n",
      "comments_circa_all = pickle.load(open( \"../../pickle/comments_all/comments_all_26000.pkl\", \"rb\" ))\n",
      "165/2:\n",
      "import pandas\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "165/3: df.loc[df['id'] == 'l6ea1b'].iloc[0].created\n",
      "165/4:\n",
      "times = {}\n",
      "    for id in comments_circa_all:\n",
      "        times[id] = []\n",
      "        created = df.loc[df['id'] == id].iloc[0].created\n",
      "        for comment in comments[id]:\n",
      "                times[id].append(comment.created_utc - created)\n",
      "    for id in times.keys():\n",
      "        times[id].sort()\n",
      "        \n",
      "print(times)\n",
      "165/5:\n",
      "times = {}\n",
      "  for id in comments_circa_all:\n",
      "    times[id] = []\n",
      "    created = df.loc[df['id'] == id].iloc[0].created\n",
      "    for comment in comments[id]:\n",
      "        times[id].append(comment.created_utc - created)\n",
      "  for id in times.keys():\n",
      "    times[id].sort()\n",
      "        \n",
      "print(times)\n",
      "165/6:\n",
      "times = {}\n",
      "for id in comments_circa_all:\n",
      "    times[id] = []\n",
      "    created = df.loc[df['id'] == id].iloc[0].created\n",
      "    for comment in comments[id]:\n",
      "        times[id].append(comment.created_utc - created)\n",
      "for id in times.keys():\n",
      "    times[id].sort()\n",
      "        \n",
      "print(times)\n",
      "165/7:\n",
      "times = {}\n",
      "for id in comments_circa_all:\n",
      "    times[id] = []\n",
      "    print(id)\n",
      "    created = df.loc[df['id'] == id].iloc[0].created\n",
      "    for comment in comments[id]:\n",
      "        times[id].append(comment.created_utc - created)\n",
      "for id in times.keys():\n",
      "    times[id].sort()\n",
      "        \n",
      "print(times)\n",
      "165/8:\n",
      "times = {}\n",
      "for id in comments_circa_all:\n",
      "    id !== \"post_id\":\n",
      "        times[id] = []\n",
      "        print(id)\n",
      "        created = df.loc[df['id'] == id].iloc[0].created\n",
      "        for comment in comments[id]:\n",
      "            times[id].append(comment.created_utc - created)\n",
      "for id in times.keys():\n",
      "    times[id].sort()\n",
      "        \n",
      "print(times)\n",
      "165/9:\n",
      "times = {}\n",
      "for id in comments_circa_all:\n",
      "    id != \"post_id\":\n",
      "        times[id] = []\n",
      "        print(id)\n",
      "        created = df.loc[df['id'] == id].iloc[0].created\n",
      "        for comment in comments[id]:\n",
      "            times[id].append(comment.created_utc - created)\n",
      "for id in times.keys():\n",
      "    times[id].sort()\n",
      "        \n",
      "print(times)\n",
      "165/10:\n",
      "times = {}\n",
      "for id in comments_circa_all:\n",
      "    if id != \"post_id\":\n",
      "        times[id] = []\n",
      "        print(id)\n",
      "        created = df.loc[df['id'] == id].iloc[0].created\n",
      "        for comment in comments[id]:\n",
      "            times[id].append(comment.created_utc - created)\n",
      "for id in times.keys():\n",
      "    times[id].sort()\n",
      "        \n",
      "print(times)\n",
      "165/11:\n",
      "times = {}\n",
      "for id in comments_circa_all:\n",
      "    if id != \"post_id\":\n",
      "        times[id] = []\n",
      "        print(id)\n",
      "        created = df.loc[df['id'] == id].iloc[0].created\n",
      "        for comment in comments_circa_all[id]:\n",
      "            times[id].append(comment.created_utc - created)\n",
      "for id in times.keys():\n",
      "    times[id].sort()\n",
      "        \n",
      "print(times)\n",
      "166/1:\n",
      "print('PyDev console: using IPython 7.19.0\\n')\n",
      "\n",
      "import sys; print('Python %s on %s' % (sys.version, sys.platform))\n",
      "sys.path.extend(['C:\\\\Users\\\\luker\\\\Documents\\\\Github\\\\PED', 'C:/Users/luker/Documents/Github/PED'])\n",
      "165/12:\n",
      "times = {}\n",
      "for id in comments_circa_all:\n",
      "    if id != \"post_id\":\n",
      "        times[id] = []\n",
      "        print(id)\n",
      "        created = df.loc[df['id'] == id].iloc[0].created\n",
      "        for comment in comments_circa_all[id]:\n",
      "            times[id].append(comment.created_utc - created)\n",
      "for id in times.keys():\n",
      "    times[id].sort(ascending=False)\n",
      "        \n",
      "print(times)\n",
      "165/13:\n",
      "times = {}\n",
      "for id in comments_circa_all:\n",
      "    if id != \"post_id\" and len(comments_circa_all[id])>0:\n",
      "        times[id] = []\n",
      "        print(id)\n",
      "        created = df.loc[df['id'] == id].iloc[0].created\n",
      "        for comment in comments_circa_all[id]:\n",
      "            times[id].append(comment.created_utc - created)\n",
      "for id in times.keys():\n",
      "    times[id].sort(ascending=False)\n",
      "        \n",
      "print(times)\n",
      "165/14:\n",
      "times = {}\n",
      "for id in comments_circa_all:\n",
      "    print(comments_circa_all[id])\n",
      "    if id != \"post_id\" and len(comments_circa_all[id])>0:\n",
      "        times[id] = []\n",
      "        print(id)\n",
      "        created = df.loc[df['id'] == id].iloc[0].created\n",
      "        for comment in comments_circa_all[id]:\n",
      "            times[id].append(comment.created_utc - created)\n",
      "for id in times.keys():\n",
      "    times[id].sort(ascending=False)\n",
      "        \n",
      "print(times)\n",
      "165/15:\n",
      "times = {}\n",
      "for id in comments_circa_all:\n",
      "    print(len(comments_circa_all[id]))\n",
      "    if id != \"post_id\" and len(comments_circa_all[id])>0:\n",
      "        times[id] = []\n",
      "        print(id)\n",
      "        created = df.loc[df['id'] == id].iloc[0].created\n",
      "        for comment in comments_circa_all[id]:\n",
      "            times[id].append(comment.created_utc - created)\n",
      "for id in times.keys():\n",
      "    times[id].sort(ascending=False)\n",
      "        \n",
      "print(times)\n",
      "165/16:\n",
      "times = {}\n",
      "for id in comments_circa_all:\n",
      "    if id != \"post_id\":\n",
      "        times[id] = []\n",
      "        created = df.loc[df['id'] == id].iloc[0].created\n",
      "        for comment in comments_circa_all[id]:\n",
      "            times[id].append(comment.created_utc - created)\n",
      "for id in times.keys():\n",
      "    times[id].sort(ascending=False)\n",
      "165/17:\n",
      "times = {}\n",
      "for id in comments_circa_all:\n",
      "    if id != \"post_id\":\n",
      "        times[id] = []\n",
      "        created = df.loc[df['id'] == id].iloc[0].created\n",
      "        for comment in comments_circa_all[id]:\n",
      "            times[id].append(comment.created_utc - created)\n",
      "for id in times.keys():\n",
      "    times[id].sort(ascending=False)\n",
      "165/18:\n",
      "times = {}\n",
      "for id in comments_circa_all:\n",
      "    if id != \"post_id\":\n",
      "        times[id] = []\n",
      "        created = df.loc[df['id'] == id].iloc[0].created\n",
      "        for comment in comments_circa_all[id]:\n",
      "            times[id].append(comment.created_utc - created)\n",
      "for id in times.keys():\n",
      "    times[id].sort()\n",
      "165/19:\n",
      "bins_num = 50\n",
      "\n",
      "max_delta_tolerance = 3600\n",
      "\n",
      "for id,v in times.items():\n",
      "    filtered = []\n",
      "    for i in range(len(v)):\n",
      "        if i == len(v) - 1 or v[i+1]-v[i] < max_delta_tolerance:\n",
      "            filtered.append(v[i])\n",
      "        else:\n",
      "            break\n",
      "    n, bins, patches = plt.hist(filtered, bins=bins_num)\n",
      "    plt.ylabel('Count')\n",
      "    plt.yscale('log')\n",
      "    plt.xlabel('Time')\n",
      "    plt.title('filtered')\n",
      "    plt.show()\n",
      "#     n, bins, patches = plt.hist(v, bins=bins_num)\n",
      "#     plt.ylabel('Count')\n",
      "#     plt.yscale('log')\n",
      "#     plt.xlabel('Time')\n",
      "#     plt.title('original')\n",
      "#     plt.show()\n",
      "165/20:\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "\n",
      "comments_circa_all = pickle.load(open( \"../../pickle/comments_all/comments_all_rev_38800.pkl\", \"rb\" ))\n",
      "165/21:\n",
      "times = {}\n",
      "for id in comments_circa_all:\n",
      "    if id != \"post_id\":\n",
      "        times[id] = []\n",
      "        created = df.loc[df['id'] == id].iloc[0].created\n",
      "        for comment in comments_circa_all[id]:\n",
      "            times[id].append(comment.created_utc - created)\n",
      "for id in times.keys():\n",
      "    times[id].sort()\n",
      "165/22:\n",
      "bins_num = 50\n",
      "\n",
      "max_delta_tolerance = 3600\n",
      "\n",
      "for id,v in times.items():\n",
      "    filtered = []\n",
      "    for i in range(len(v)):\n",
      "        if i == len(v) - 1 or v[i+1]-v[i] < max_delta_tolerance:\n",
      "            filtered.append(v[i])\n",
      "        else:\n",
      "            break\n",
      "    n, bins, patches = plt.hist(filtered, bins=bins_num)\n",
      "    plt.ylabel('Count')\n",
      "    plt.yscale('log')\n",
      "    plt.xlabel('Time')\n",
      "    plt.title('filtered')\n",
      "    plt.show()\n",
      "#     n, bins, patches = plt.hist(v, bins=bins_num)\n",
      "#     plt.ylabel('Count')\n",
      "#     plt.yscale('log')\n",
      "#     plt.xlabel('Time')\n",
      "#     plt.title('original')\n",
      "#     plt.show()\n",
      "165/23:\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "\n",
      "comments_circa_all = pickle.load(open( \"../../pickle/comments_all/comments_all_rev_38000.pkl\", \"rb\" ))\n",
      "165/24:\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "\n",
      "comments_circa_all = pickle.load(open( \"../../pickle/comments_all/comments_all_38000.pkl\", \"rb\" ))\n",
      "165/25:\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "\n",
      "comments_all = pickle.load(open( \"../../pickle/comments_all/comments_0_to_38000_all.pkl\", \"rb\" ))\n",
      "comments_10 = pickle.load(open( \"../../pickle/comments_all/comments_38000_to_38800_10.pkl\", \"rb\" ))\n",
      "\n",
      "comments_mix = comments_all.update(comments_10)\n",
      "del comments_mix['post_id']\n",
      "165/26:\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "\n",
      "comments_all = pickle.load(open( \"../../pickle/comments_all/comments_0_to_38000_all.pkl\", \"rb\" ))\n",
      "comments_10 = pickle.load(open( \"../../pickle/comments_all/comments_38000_to_38800_10.pkl\", \"rb\" ))\n",
      "\n",
      "comments_mix = comments_all.update(comments_10)\n",
      "del comments_mix[\"post_id\"]\n",
      "167/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "\n",
      "comments_all = pickle.load(open( \"../../pickle/comments_all/comments_0_to_38000_all.pkl\", \"rb\" ))\n",
      "comments_10 = pickle.load(open( \"../../pickle/comments_all/comments_38000_to_38800_10.pkl\", \"rb\" ))\n",
      "\n",
      "comments_mix = comments_all.update(comments_10)\n",
      "del comments_mix[\"post_id\"]\n",
      "167/2:\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "\n",
      "comments_all = pickle.load(open( \"../../pickle/comments_all/comments_0_to_38000_all.pkl\", \"rb\" ))\n",
      "print('loaded')\n",
      "comments_10 = pickle.load(open( \"../../pickle/comments_all/comments_38000_to_38800_10.pkl\", \"rb\" ))\n",
      "print('loaded')\n",
      "\n",
      "comments_mix = comments_all.update(comments_10)\n",
      "print('mixed')\n",
      "my_dict.pop('post_id', None)\n",
      "print('deleted')\n",
      "168/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "\n",
      "comments_all = pickle.load(open( \"../../pickle/comments_all/comments_0_to_38000_all.pkl\", \"rb\" ))\n",
      "print('loaded')\n",
      "comments_10 = pickle.load(open( \"../../pickle/comments_all/comments_38000_to_38800_10.pkl\", \"rb\" ))\n",
      "print('loaded')\n",
      "\n",
      "comments_mix = comments_all.update(comments_10)\n",
      "print('mixed')\n",
      "my_dict.pop('post_id', None)\n",
      "print('deleted')\n",
      "168/2:\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "\n",
      "comments_all = pickle.load(open( \"../../pickle/comments_all/comments_0_to_38000_all.pkl\", \"rb\" ))\n",
      "print('loaded')\n",
      "comments_10 = pickle.load(open( \"../../pickle/comments_all/comments_38000_to_38800_10.pkl\", \"rb\" ))\n",
      "print('loaded')\n",
      "\n",
      "comments_mix = comments_all.update(comments_10)\n",
      "print('mixed')\n",
      "comments_mix.pop('post_id', None)\n",
      "print('deleted')\n",
      "169/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "\n",
      "comments_all = pickle.load(open( \"../../pickle/comments_all/comments_0_to_38000_all.pkl\", \"rb\" ))\n",
      "print('loaded')\n",
      "comments_10 = pickle.load(open( \"../../pickle/comments_all/comments_38000_to_38800_10.pkl\", \"rb\" ))\n",
      "print('loaded')\n",
      "\n",
      "comments_mix = comments_all.update(comments_10)\n",
      "print('mixed')\n",
      "comments_mix.pop('post_id', None)\n",
      "print('deleted')\n",
      "170/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "\n",
      "comments_all = pickle.load(open( \"../../pickle/comments_all/comments_0_to_38000_all.pkl\", \"rb\" ))\n",
      "print('loaded')\n",
      "comments_10 = pickle.load(open( \"../../pickle/comments_all/comments_38000_to_38800_10.pkl\", \"rb\" ))\n",
      "print('loaded')\n",
      "\n",
      "comments_mix = {**comments_all, **comments_10}\n",
      "print('mixed')\n",
      "comments_mix.pop('post_id', None)\n",
      "print('deleted')\n",
      "170/2:\n",
      "import pandas\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "170/3: df.loc[df['id'] == 'l6ea1b'].iloc[0].created\n",
      "170/4:\n",
      "times = {}\n",
      "for id in comments_mix:\n",
      "    if id != \"post_id\":\n",
      "        times[id] = []\n",
      "        created = df.loc[df['id'] == id].iloc[0].created\n",
      "        for comment in comments_mix[id]:\n",
      "            times[id].append(comment.created_utc - created)\n",
      "for id in times.keys():\n",
      "    times[id].sort()\n",
      "170/5:\n",
      "times = {}\n",
      "for id in comments_mix:\n",
      "    times[id] = []\n",
      "    created = df.loc[df['id'] == id].iloc[0].created\n",
      "    for comment in comments_mix[id]:\n",
      "        times[id].append(comment.created_utc - created)\n",
      "for id in times.keys():\n",
      "    times[id].sort()\n",
      "170/6:\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "\n",
      "# comments_all = pickle.load(open( \"../../pickle/comments_all/comments_0_to_38000_all.pkl\", \"rb\" ))\n",
      "comments_10 = pickle.load(open( \"../../pickle/comments_all/comments_38000_to_38800_10.pkl\", \"rb\" ))\n",
      "\n",
      "comments_mix = {**comments_10}\n",
      "comments_mix.pop('post_id', None)\n",
      "170/7:\n",
      "times = {}\n",
      "for id in comments_mix:\n",
      "    times[id] = []\n",
      "    created = df.loc[df['id'] == id].iloc[0].created\n",
      "    for comment in comments_mix[id]:\n",
      "        times[id].append(comment.created_utc - created)\n",
      "for id in times.keys():\n",
      "    times[id].sort()\n",
      "170/8:\n",
      "bucket_size = 6000\n",
      "bucket_count = 12\n",
      "time_buckets = {}\n",
      "for id in times.keys():\n",
      "    time_buckets[id] = []\n",
      "    curr_index = 0\n",
      "    for i in range(bucket_count):\n",
      "        time_buckets[id].push(0)\n",
      "        for j in range(curr_index,len(times[id])):\n",
      "            if times[id]<bucket_size*i:\n",
      "                time_buckets[i]+=1\n",
      "                curr_index+=1\n",
      "                \n",
      "print(time_buckets)\n",
      "170/9:\n",
      "bucket_size = 6000\n",
      "bucket_count = 12\n",
      "time_buckets = {}\n",
      "for id in times.keys():\n",
      "    time_buckets[id] = []\n",
      "    curr_index = 0\n",
      "    for i in range(bucket_count):\n",
      "        time_buckets[id].append(0)\n",
      "        for j in range(curr_index,len(times[id])):\n",
      "            if times[id]<bucket_size*i:\n",
      "                time_buckets[i]+=1\n",
      "                curr_index+=1\n",
      "                \n",
      "print(time_buckets)\n",
      "170/10:\n",
      "bucket_size = 6000\n",
      "bucket_count = 12\n",
      "time_buckets = {}\n",
      "for id in times.keys():\n",
      "    time_buckets[id] = []\n",
      "    curr_index = 0\n",
      "    for i in range(bucket_count):\n",
      "        time_buckets[id].append(0)\n",
      "        for j in range(curr_index,len(times[id])):\n",
      "            if times[id][j]<bucket_size*i:\n",
      "                time_buckets[i]+=1\n",
      "                curr_index+=1\n",
      "                \n",
      "print(time_buckets)\n",
      "170/11:\n",
      "bucket_size = 6000\n",
      "bucket_count = 12\n",
      "time_buckets = {}\n",
      "for id in times.keys():\n",
      "    time_buckets[id] = []\n",
      "    curr_index = 0\n",
      "    for i in range(bucket_count):\n",
      "        time_buckets[id].append(0)\n",
      "        for j in range(curr_index,len(times[id])):\n",
      "            if times[id][j]<bucket_size*i:\n",
      "                time_buckets[id][i]+=1\n",
      "                curr_index+=1\n",
      "                \n",
      "print(time_buckets)\n",
      "170/12:\n",
      "bucket_size = 6000\n",
      "bucket_count = 12\n",
      "time_buckets = {}\n",
      "for id in times.keys():\n",
      "    time_buckets[id] = []\n",
      "    curr_index = 0\n",
      "    for i in range(bucket_count):\n",
      "        time_buckets[id].append(0)\n",
      "        for j in range(curr_index,len(times[id])):\n",
      "            if times[id][j]<bucket_size*(i+1):\n",
      "                time_buckets[id][i]+=1\n",
      "                curr_index+=1\n",
      "                \n",
      "print(time_buckets)\n",
      "170/13:\n",
      "bucket_size = 6000\n",
      "bucket_count = 24\n",
      "time_buckets = {}\n",
      "for id in times.keys():\n",
      "    time_buckets[id] = []\n",
      "    curr_index = 0\n",
      "    for i in range(bucket_count):\n",
      "        time_buckets[id].append(0)\n",
      "        for j in range(curr_index,len(times[id])):\n",
      "            if times[id][j]<bucket_size*(i+1):\n",
      "                time_buckets[id][i]+=1\n",
      "                curr_index+=1\n",
      "                \n",
      "print(time_buckets)\n",
      "170/14:\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "\n",
      "comments = pickle.load(open( \"../../pickle/comments_all/comments_all_38400.pkl\", \"rb\" ))\n",
      "\n",
      "comments.pop('post_id', None)\n",
      "171/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "\n",
      "comments = pickle.load(open( \"../../pickle/comments_all/comments_all_38400.pkl\", \"rb\" ))\n",
      "\n",
      "comments.pop('post_id', None)\n",
      "171/2:\n",
      "import pandas\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "171/3: df.loc[df['id'] == 'l6ea1b'].iloc[0].created\n",
      "171/4:\n",
      "times = {}\n",
      "for id in comments:\n",
      "    times[id] = []\n",
      "    created = df.loc[df['id'] == id].iloc[0].created\n",
      "    for comment in comments[id]:\n",
      "        times[id].append(comment.created_utc - created)\n",
      "for id in times.keys():\n",
      "    times[id].sort()\n",
      "171/5:\n",
      "bucket_size = 6000\n",
      "bucket_count = 24\n",
      "time_buckets = {}\n",
      "for id in times.keys():\n",
      "    time_buckets[id] = []\n",
      "    curr_index = 0\n",
      "    for i in range(bucket_count):\n",
      "        time_buckets[id].append(0)\n",
      "        for j in range(curr_index,len(times[id])):\n",
      "            if times[id][j]<bucket_size*(i+1):\n",
      "                time_buckets[id][i]+=1\n",
      "                curr_index+=1\n",
      "171/6: time_buckets[l69ulq]\n",
      "171/7: time_buckets[\"l69ulq\"]\n",
      "171/8:\n",
      "bucket_size = 6000\n",
      "bucket_count = 36\n",
      "time_buckets = {}\n",
      "for id in times.keys():\n",
      "    time_buckets[id] = []\n",
      "    curr_index = 0\n",
      "    for i in range(bucket_count):\n",
      "        time_buckets[id].append(0)\n",
      "        for j in range(curr_index,len(times[id])):\n",
      "            if times[id][j]<bucket_size*(i+1):\n",
      "                time_buckets[id][i]+=1\n",
      "                curr_index+=1\n",
      "171/9: time_buckets[\"l69ulq\"]\n",
      "171/10:\n",
      "bucket_size = 36000\n",
      "bucket_count = 24\n",
      "time_buckets = {}\n",
      "for id in times.keys():\n",
      "    time_buckets[id] = []\n",
      "    curr_index = 0\n",
      "    for i in range(bucket_count):\n",
      "        time_buckets[id].append(0)\n",
      "        for j in range(curr_index,len(times[id])):\n",
      "            if times[id][j]<bucket_size*(i+1):\n",
      "                time_buckets[id][i]+=1\n",
      "                curr_index+=1\n",
      "171/11: time_buckets[\"l69ulq\"]\n",
      "171/12:\n",
      "bucket_size = 36000\n",
      "bucket_count = 36\n",
      "time_buckets = {}\n",
      "for id in times.keys():\n",
      "    time_buckets[id] = []\n",
      "    curr_index = 0\n",
      "    for i in range(bucket_count):\n",
      "        time_buckets[id].append(0)\n",
      "        for j in range(curr_index,len(times[id])):\n",
      "            if times[id][j]<bucket_size*(i+1):\n",
      "                time_buckets[id][i]+=1\n",
      "                curr_index+=1\n",
      "171/13: time_buckets[\"l69ulq\"]\n",
      "171/14: comments[\"l69ulq\"]\n",
      "171/15: time_buckets[\"l69ulq\"]\n",
      "171/16:\n",
      "bucket_size = 6000\n",
      "bucket_count = 36\n",
      "time_buckets = {}\n",
      "for id in times.keys():\n",
      "    time_buckets[id] = []\n",
      "    curr_index = 0\n",
      "    for i in range(bucket_count):\n",
      "        time_buckets[id].append(0)\n",
      "        for j in range(curr_index,len(times[id])):\n",
      "            if times[id][j]<bucket_size*(i+1):\n",
      "                time_buckets[id][i]+=1\n",
      "                curr_index+=1\n",
      "171/17:\n",
      "bucket_size = 6000\n",
      "bucket_count = 12\n",
      "time_buckets = {}\n",
      "for id in times.keys():\n",
      "    time_buckets[id] = []\n",
      "    curr_index = 0\n",
      "    for i in range(bucket_count):\n",
      "        time_buckets[id].append(0)\n",
      "        for j in range(curr_index,len(times[id])):\n",
      "            if times[id][j]<bucket_size*(i+1):\n",
      "                time_buckets[id][i]+=1\n",
      "                curr_index+=1\n",
      "171/18: time_buckets[\"l69ulq\"]\n",
      "171/19:\n",
      "time_buckets[\"l69ulq\"]\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "x = time_buckets[\"l69ulq\"]\n",
      "y = [x*bucket_size/10/60 for x in range(bucket_count)]\n",
      "\n",
      "plt.plot(x, y)\n",
      "plt.show()\n",
      "171/20:\n",
      "time_buckets[\"l69ulq\"]\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "x = time_buckets[\"l69ulq\"]\n",
      "y = [i*bucket_size/10/60 for i in range(bucket_count)]\n",
      "\n",
      "plt.plot(x, y)\n",
      "plt.show()\n",
      "171/21:\n",
      "time_buckets[\"l69ulq\"]\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "x = time_buckets[\"l69ulq\"]\n",
      "y = [i*bucket_size/10/60 for i in range(bucket_count)]\n",
      "\n",
      "print(x)\n",
      "print(y)\n",
      "\n",
      "plt.plot(x, y)\n",
      "plt.show()\n",
      "171/22:\n",
      "time_buckets[\"l69ulq\"]\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "y = time_buckets[\"l69ulq\"]\n",
      "x = [i*bucket_size/10/60 for i in range(bucket_count)]\n",
      "\n",
      "print(x)\n",
      "print(y)\n",
      "\n",
      "plt.plot(x, y)\n",
      "plt.show()\n",
      "171/23:\n",
      "time_buckets[\"l69ulq\"]\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "y = time_buckets[\"l69ulq\"]\n",
      "x = [i*bucket_size/10/60 for i in range(bucket_count)]\n",
      "\n",
      "print(x)\n",
      "print(y)\n",
      "\n",
      "plt.plot(x, y)\n",
      "plt.bar(x,y)\n",
      "plt.show()\n",
      "171/24:\n",
      "time_buckets[\"l69ulq\"]\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "y = time_buckets[\"l69ulq\"]\n",
      "x = [i*bucket_size/10/60 for i in range(bucket_count)]\n",
      "\n",
      "print(x)\n",
      "print(y)\n",
      "\n",
      "plt.bar(x,y, width = 0.25)\n",
      "plt.plot(x, y)\n",
      "\n",
      "plt.show()\n",
      "171/25:\n",
      "time_buckets[\"l69ulq\"]\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "y = time_buckets[\"l69ulq\"]\n",
      "x = [i*bucket_size/10/60 for i in range(bucket_count)]\n",
      "\n",
      "print(x)\n",
      "print(y)\n",
      "\n",
      "plt.bar(x,y)\n",
      "plt.plot(x, y)\n",
      "\n",
      "plt.show()\n",
      "171/26:\n",
      "time_buckets[\"l69ulq\"]\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "y = time_buckets[\"l69ulq\"]\n",
      "x = [i*bucket_size/10/60 for i in range(bucket_count)]\n",
      "\n",
      "print(x)\n",
      "print(y)\n",
      "\n",
      "plt.bar(x,y)\n",
      "\n",
      "\n",
      "plt.show()\n",
      "171/27:\n",
      "time_buckets[\"l69ulq\"]\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "y = time_buckets[\"l69ulq\"]\n",
      "x = [i*bucket_size/10/60 for i in range(bucket_count)]\n",
      "\n",
      "print(x)\n",
      "print(y)\n",
      "\n",
      "plt.bar(x,y, width = 5)\n",
      "plt.plot(x, y)\n",
      "\n",
      "plt.show()\n",
      "171/28:\n",
      "time_buckets[\"l69ulq\"]\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "y = time_buckets[\"l69ulq\"]\n",
      "x = [i*bucket_size/10/60 for i in range(bucket_count)]\n",
      "\n",
      "print(x)\n",
      "print(y)\n",
      "\n",
      "plt.bar(x,y, width = 5)\n",
      "plt.plot(x, y, color='red')\n",
      "\n",
      "plt.show()\n",
      "171/29:\n",
      "import pandas\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "attributes_comms = pickle.load(open( \"../../pickle/text_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "\n",
      "comments_df = pd.DataFrame(comments.items())\n",
      "attributes = pd.merge(attributes_comms, comments_df, how=\"outer\", on=[\"id\", \"0\"])\n",
      "x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "171/30:\n",
      "import pandas as pd\n",
      "df = pd.read_csv('../../data/reddit_wsb.csv')\n",
      "attributes_comms = pickle.load(open( \"../../pickle/text_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "\n",
      "comments_df = pd.DataFrame(comments.items())\n",
      "attributes = pd.merge(attributes_comms, comments_df, how=\"outer\", on=[\"id\", \"0\"])\n",
      "x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "171/31:\n",
      "import pandas as pd\n",
      "df = pd.read_csv('../../data/reddit_wsb.csv')\n",
      "attributes_comms = pickle.load(open( \"../../pickle/text_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "print(attributes_comms)\n",
      "comments_df = pd.DataFrame(comments.items())\n",
      "attributes = pd.merge(attributes_comms, comments_df, how=\"outer\", on=[\"id\", \"0\"])\n",
      "x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "171/32:\n",
      "import pandas as pd\n",
      "df = pd.read_csv('../../data/reddit_wsb.csv')\n",
      "attributes_comms = pickle.load(open( \"../../pickle/text_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "print(attributes_comms[\"id\"])\n",
      "comments_df = pd.DataFrame(comments.items())\n",
      "attributes = pd.merge(attributes_comms, comments_df, how=\"outer\", on=[\"id\", \"0\"])\n",
      "x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "171/33:\n",
      "import pandas as pd\n",
      "df = pd.read_csv('../../data/reddit_wsb.csv')\n",
      "attributes_comms = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "print(attributes_comms[\"id\"])\n",
      "comments_df = pd.DataFrame(comments.items())\n",
      "attributes = pd.merge(attributes_comms, comments_df, how=\"outer\", on=[\"id\", \"0\"])\n",
      "x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "171/34:\n",
      "import pandas as pd\n",
      "df = pd.read_csv('../../data/reddit_wsb.csv')\n",
      "attributes_comms = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "print(attributes_comms[\"id\"])\n",
      "comments_df = pd.DataFrame(comments.items())\n",
      "attributes = pd.merge(attributes_comms, comments_df, how=\"outer\", on=[\"id\", \"0\"])\n",
      "x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "171/35:\n",
      "import pandas as pd\n",
      "df = pd.read_csv('../../data/reddit_wsb.csv')\n",
      "attributes_comms = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "print(attributes_comms[\"id\"])\n",
      "print(comments[\"0\"])\n",
      "comments_df = pd.DataFrame(comments.items())\n",
      "attributes = pd.merge(attributes_comms, comments_df, how=\"outer\", on=[\"id\", \"0\"])\n",
      "x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "171/36:\n",
      "import pandas as pd\n",
      "df = pd.read_csv('../../data/reddit_wsb.csv')\n",
      "attributes_comms = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "print(attributes_comms[\"id\"])\n",
      "print(comments)\n",
      "comments_df = pd.DataFrame(comments.items())\n",
      "attributes = pd.merge(attributes_comms, comments_df, how=\"outer\", on=[\"id\", \"0\"])\n",
      "x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "171/37:\n",
      "attributes_comms = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "print(attributes_comms[\"id\"])\n",
      "print(time_buckets)\n",
      "time_buckets_df = pd.DataFrame(time_buckets.items())\n",
      "attributes = pd.merge(attributes_comms, time_buckets_df, how=\"outer\", on=[\"id\", \"0\"])\n",
      "x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "171/38:\n",
      "attributes_comms = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "print(attributes_comms[\"id\"])\n",
      "time_buckets_df = pd.DataFrame(time_buckets.items())\n",
      "print(time_buckets_df)\n",
      "attributes = pd.merge(attributes_comms, time_buckets_df, how=\"outer\", on=[\"id\", \"0\"])\n",
      "x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "171/39:\n",
      "attributes_comms = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "print(attributes_comms[\"id\"])\n",
      "time_buckets_df = pd.DataFrame(time_buckets.items())\n",
      "print(time_buckets_df[\"0\"])\n",
      "attributes = pd.merge(attributes_comms, time_buckets_df, how=\"outer\", on=[\"id\", \"0\"])\n",
      "x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "171/40:\n",
      "attributes_comms = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "print(attributes_comms[\"id\"])\n",
      "time_buckets_df = pd.DataFrame(time_buckets.items())\n",
      "print(time_buckets_df[0])\n",
      "attributes = pd.merge(attributes_comms, time_buckets_df, how=\"outer\", on=[\"id\", \"0\"])\n",
      "x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "171/41:\n",
      "attributes_comms = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "print(attributes_comms[\"id\"])\n",
      "time_buckets_df = pd.DataFrame(time_buckets.items())\n",
      "print(time_buckets_df[0])\n",
      "attributes = pd.merge(attributes_comms, time_buckets_df, how=\"outer\", on=[\"id\", 0])\n",
      "x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "171/42:\n",
      "attributes_comms = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "print(attributes_comms[\"id\"])\n",
      "time_buckets_df = pd.DataFrame(time_buckets.items())\n",
      "print(time_buckets_df[0])\n",
      "attributes = pd.merge(attributes_comms, time_buckets_df, on=[\"id\", 0])\n",
      "x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "171/43:\n",
      "attributes_comms = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "print(attributes_comms[\"id\"])\n",
      "time_buckets_df = pd.DataFrame(time_buckets.items())\n",
      "time_buckets_df.columns = [\"id\", \"buckets\"]\n",
      "\n",
      "attributes = pd.merge(attributes_comms, time_buckets_df, on=[\"id\"])\n",
      "x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "171/44:\n",
      "attributes_comms = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "print(attributes_comms[\"id\"])\n",
      "time_buckets_df = pd.DataFrame(time_buckets.items())\n",
      "print(time_buckets_df)\n",
      "time_buckets_df.columns = [\"id\", \"buckets\"]\n",
      "\n",
      "attributes = pd.merge(attributes_comms, time_buckets_df, on=[\"id\"])\n",
      "x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "171/45:\n",
      "attributes_comms = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "print(attributes_comms[\"id\"])\n",
      "time_buckets_df = pd.DataFrame(time_buckets.items())\n",
      "print(time_buckets_df)\n",
      "time_buckets_df.rename(index={0:'id',1:'buckets'}, inplace=True)\n",
      "\n",
      "attributes = pd.merge(attributes_comms, time_buckets_df, on=[\"id\"])\n",
      "x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "171/46:\n",
      "attributes_comms = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "print(attributes_comms[\"id\"])\n",
      "time_buckets_df = pd.DataFrame(time_buckets.items())\n",
      "\n",
      "time_buckets_df.rename(index={0:'id',1:'buckets'}, inplace=True)\n",
      "print(time_buckets_df)\n",
      "attributes = pd.merge(attributes_comms, time_buckets_df, on=[\"id\"])\n",
      "x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "171/47:\n",
      "attributes_comms = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "print(attributes_comms[\"id\"])\n",
      "time_buckets_df = pd.DataFrame(time_buckets.items())\n",
      "\n",
      "time_buckets_df = time_buckets_df.rename(index={0:'id',1:'buckets'}, inplace=True)\n",
      "print(time_buckets_df)\n",
      "attributes = pd.merge(attributes_comms, time_buckets_df, on=[\"id\"])\n",
      "x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "171/48:\n",
      "attributes_comms = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "print(attributes_comms[\"id\"])\n",
      "time_buckets_df = pd.DataFrame(time_buckets.items())\n",
      "\n",
      "time_buckets_df.rename(index={0:'id',1:'buckets'}, inplace=True)\n",
      "\n",
      "print(time_buckets_df)\n",
      "attributes = pd.merge(attributes_comms, time_buckets_df, on=[\"id\"])\n",
      "x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "171/49:\n",
      "attributes_comms = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "print(attributes_comms[\"id\"])\n",
      "time_buckets_df = pd.DataFrame(time_buckets.items())\n",
      "\n",
      "time_buckets_df = time_buckets_df.rename(index={0:'id',1:'buckets'})\n",
      "\n",
      "print(time_buckets_df)\n",
      "attributes = pd.merge(attributes_comms, time_buckets_df, on=[\"id\"])\n",
      "x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "171/50:\n",
      "attributes_comms = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "print(attributes_comms[\"id\"])\n",
      "time_buckets_df = pd.DataFrame(time_buckets.items())\n",
      "\n",
      "time_buckets_df = time_buckets_df.rename(name={0:'id',1:'buckets'})\n",
      "\n",
      "print(time_buckets_df)\n",
      "attributes = pd.merge(attributes_comms, time_buckets_df, on=[\"id\"])\n",
      "x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "171/51:\n",
      "attributes_comms = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "print(attributes_comms[\"id\"])\n",
      "time_buckets_df = pd.DataFrame(time_buckets.items())\n",
      "\n",
      "time_buckets_df = time_buckets_df.rename(columns={0:'id',1:'buckets'})\n",
      "\n",
      "print(time_buckets_df)\n",
      "attributes = pd.merge(attributes_comms, time_buckets_df, on=[\"id\"])\n",
      "x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "171/52:\n",
      "attributes_comms = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "print(attributes_comms[\"id\"])\n",
      "time_buckets_df = pd.DataFrame(time_buckets.items())\n",
      "\n",
      "time_buckets_df = time_buckets_df.rename(columns={0:'id',1:'buckets'})\n",
      "\n",
      "print(time_buckets_df)\n",
      "attributes = pd.merge(attributes_comms, time_buckets_df, on=[\"id\"])\n",
      "171/53:\n",
      "attributes_comms = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "\n",
      "time_buckets_df = pd.DataFrame(time_buckets.items())\n",
      "\n",
      "time_buckets_df = time_buckets_df.rename(columns={0:'id',1:'buckets'})\n",
      "\n",
      "\n",
      "attributes = pd.merge(attributes_comms, time_buckets_df, on=[\"id\"])\n",
      "\n",
      "print(attributes)\n",
      "171/54:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.linear_model import Ridge\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "\n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "clf = MultiOutputRegressor(Ridge(random_state=123)).fit(train_without_exculded, train['buckets'])\n",
      "clf.predict(X[[0]])\n",
      "171/55:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Ridge\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "\n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "clf = MultiOutputRegressor(Ridge(random_state=123)).fit(train_without_exculded, train['buckets'])\n",
      "clf.predict(X[[0]])\n",
      "171/56:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Ridge\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "\n",
      "print( train['buckets'])\n",
      "\n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "clf = MultiOutputRegressor(Ridge(random_state=123)).fit(train_without_exculded, train['buckets'])\n",
      "clf.predict(X[[0]])\n",
      "171/57:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Ridge\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "\n",
      "print( train['buckets'])\n",
      "\n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "clf = MultiOutputRegressor(Ridge(random_state=123)).fit(train_without_exculded, [train['buckets']])\n",
      "clf.predict(X[[0]])\n",
      "171/58:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Ridge\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "\n",
      "print( train['buckets'])\n",
      "\n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "clf = MultiOutputRegressor(Ridge(random_state=123)).fit(train_without_exculded, train['buckets'])\n",
      "clf.predict(X[[0]])\n",
      "171/59:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Ridge\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "\n",
      "print( train['buckets'])\n",
      "\n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "clf = MultiOutputRegressor(Ridge(random_state=123)).fit(train_without_exculded, train['buckets'].tolist())\n",
      "clf.predict(X[[0]])\n",
      "171/60:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Ridge\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "\n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "clf = MultiOutputRegressor(Ridge(random_state=123)).fit(train_without_exculded, train['buckets'].tolist())\n",
      "clf.predict(X[[0]])\n",
      "171/61:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Ridge\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "\n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "clf = MultiOutputRegressor(Ridge(random_state=123)).fit(train_without_exculded, train['buckets'].tolist())\n",
      "clf.predict(test_without_excluded[[0]])\n",
      "171/62:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Ridge\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "\n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "clf = MultiOutputRegressor(Ridge(random_state=123)).fit(train_without_exculded, train['buckets'].tolist())\n",
      "clf.predict(test_without_excluded)\n",
      "171/63:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Ridge\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "\n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "clf = MultiOutputRegressor(Ridge(random_state=123)).fit(train_without_exculded, train['buckets'].tolist())\n",
      "clf.predict([y])\n",
      "171/64:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Ridge\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "\n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "clf = MultiOutputRegressor(Ridge(random_state=123)).fit(train_without_exculded, train['buckets'].tolist())\n",
      "clf.predict(test_without_excluded[test_without_excluded['id']==l69ulq])\n",
      "171/65:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Ridge\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "\n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "clf = MultiOutputRegressor(Ridge(random_state=123)).fit(train_without_exculded, train['buckets'].tolist())\n",
      "clf.predict(test_without_excluded[test['id']==l69ulq])\n",
      "171/66:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Ridge\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "\n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "clf = MultiOutputRegressor(Ridge(random_state=123)).fit(train_without_exculded, train['buckets'].tolist())\n",
      "clf.predict(test_without_excluded[test['id']==\"l69ulq\"])\n",
      "171/67:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Ridge\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "\n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "clf = MultiOutputRegressor(Ridge(random_state=123)).fit(train_without_exculded, train['buckets'].tolist())\n",
      "clf.predict(test[test['id']==\"l69ulq\"])\n",
      "171/68:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Ridge\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "\n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "clf = MultiOutputRegressor(Ridge(random_state=123)).fit(train_without_exculded, train['buckets'].tolist())\n",
      "clf.predict(test[test['id']==\"l69ulq\"][test.columns.difference(EXCLUDED_COLUMNS)])\n",
      "171/69:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Ridge\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "\n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "clf = MultiOutputRegressor(Ridge(random_state=123)).fit(train_without_exculded, train['buckets'].tolist())\n",
      "clf.predict(test[test['id']==\"l69ulq\"][test.columns.difference(EXCLUDED_COLUMNS)])\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "171/70:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Ridge\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "\n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "clf = MultiOutputRegressor(Ridge(random_state=123)).fit(train_without_exculded, train['buckets'].tolist())\n",
      "clf.predict(test[test['id']==\"l69ulq\"][test.columns.difference(EXCLUDED_COLUMNS)])\n",
      "171/71:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Ridge\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "\n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "clf = MultiOutputRegressor(Ridge(random_state=123)).fit(train_without_exculded, train['buckets'].tolist())\n",
      "clf.predict(test[test['id']==\"l69ulq\"][test.columns.difference(EXCLUDED_COLUMNS)])\n",
      "171/72:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Ridge\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "\n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "clf = MultiOutputRegressor(Ridge(random_state=123)).fit(train_without_exculded, train['buckets'].tolist())\n",
      "clf.predict(test[test['id']==\"l69ulq\"])\n",
      "171/73:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Ridge\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "\n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "clf = MultiOutputRegressor(Ridge(random_state=123)).fit(train_without_exculded, train['buckets'].tolist())\n",
      "clf.predict(test[test['id']==\"l69ulq\"][x.columns.difference(EXCLUDED_COLUMNS)])\n",
      "171/74:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Ridge\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "\n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "clf = MultiOutputRegressor(Ridge(random_state=123)).fit(train_without_exculded, train['buckets'].tolist())\n",
      "clf.predict(test[test['id']==\"l69ulq\"][test.columns.difference(EXCLUDED_COLUMNS)])\n",
      "171/75:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Ridge\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "\n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "clf = MultiOutputRegressor(Ridge(random_state=123)).fit(train_without_exculded, train['buckets'].tolist())\n",
      "clf.predict(test[test['id']==\"l69ulq\"][test.columns.difference(EXCLUDED_COLUMNS)])\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "171/76:\n",
      "attributes_comms = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "\n",
      "time_buckets_df = pd.DataFrame(time_buckets.items())\n",
      "\n",
      "time_buckets_df = time_buckets_df.rename(columns={0:'id',1:'buckets'})\n",
      "\n",
      "\n",
      "attributes = pd.merge(attributes_comms, time_buckets_df, on=[\"id\"])\n",
      "\n",
      "print(attributes)\n",
      "171/77:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Ridge\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "\n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "clf = MultiOutputRegressor(Ridge(random_state=123)).fit(train_without_exculded, train['buckets'].tolist())\n",
      "clf.predict(test[test['id']==\"l69ulq\"][test.columns.difference(EXCLUDED_COLUMNS)])\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "171/78:\n",
      "time_buckets[\"l69ulq\"]\n",
      "\n",
      "y = time_buckets[\"l69ulq\"]\n",
      "x = [i*bucket_size/10/60 for i in range(bucket_count)]\n",
      "\n",
      "print(x)\n",
      "print(y)\n",
      "\n",
      "plt.bar(x,y, width = 5)\n",
      "plt.plot(x, y, color='red')\n",
      "\n",
      "plt.show()\n",
      "171/79:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Ridge\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "\n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "clf = MultiOutputRegressor(Ridge(random_state=123)).fit(train_without_exculded, train['buckets'].tolist())\n",
      "clf.predict(test[test['id']==\"l69ulq\"][test.columns.difference(EXCLUDED_COLUMNS)])\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "171/80:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Ridge\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "\n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "clf = MultiOutputRegressor(Ridge(random_state=123)).fit(train_without_exculded, train['buckets'].tolist())\n",
      "print(clf.predict(test[test['id']==\"l69ulq\"][test.columns.difference(EXCLUDED_COLUMNS)]))\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "171/81:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Ridge\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "\n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "clf = MultiOutputRegressor(Ridge(random_state=123)).fit(train_without_exculded, train['buckets'].tolist())\n",
      "clf.predict(test[test['id']==\"l69ulq\"][test.columns.difference(EXCLUDED_COLUMNS)])\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "171/82:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Ridge\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "\n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "clf = MultiOutputRegressor(Ridge(random_state=123)).fit(train_without_exculded, train['buckets'].tolist())\n",
      "171/83:\n",
      "print(clf.predict(test[test['id']==\"l69ulq\"][test.columns.difference(EXCLUDED_COLUMNS)]))\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "171/84:\n",
      "print(clf.predict(test_without_excluded))\n",
      "print(clf.predict(test[test['id']==\"l69ulq\"][test.columns.difference(EXCLUDED_COLUMNS)]))\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "171/85:\n",
      "print(clf.predict(test_without_excluded))\n",
      "print(clf.predict(test[test['id']==\"l69ulq\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "171/86:\n",
      "print(clf.predict(test_without_excluded))\n",
      "print(clf.predict(test[test['id']==\"l69ulq\"][test_without_excluded.columns][0]))\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "171/87:\n",
      "print(test[test['id']==\"l69ulq\"][test_without_excluded.columns])\n",
      "\n",
      "print(clf.predict(test[test['id']==\"l69ulq\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "171/88:\n",
      "print(test[test['id']==\"l69ulq\"][test_without_excluded.columns])\n",
      "\n",
      "print(clf.predict(test[test['id']==\"l69ulq\"]))\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "171/89:\n",
      "print(test[test['id']==\"l69ulq\"])\n",
      "\n",
      "print(clf.predict(test[test['id']==\"l69ulq\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "171/90: df.loc[df['id'] == 'l6ea1b'].iloc[0].created\n",
      "171/91:\n",
      "times = {}\n",
      "for id in comments:\n",
      "    times[id] = []\n",
      "    created = df.loc[df['id'] == id].iloc[0].created\n",
      "    for comment in comments[id]:\n",
      "        times[id].append(comment.created_utc - created)\n",
      "for id in times.keys():\n",
      "    times[id].sort()\n",
      "171/92:\n",
      "bucket_size = 6000\n",
      "bucket_count = 12\n",
      "time_buckets = {}\n",
      "for id in times.keys():\n",
      "    time_buckets[id] = []\n",
      "    curr_index = 0\n",
      "    for i in range(bucket_count):\n",
      "        time_buckets[id].append(0)\n",
      "        for j in range(curr_index,len(times[id])):\n",
      "            if times[id][j]<bucket_size*(i+1):\n",
      "                time_buckets[id][i]+=1\n",
      "                curr_index+=1\n",
      "171/93:\n",
      "time_buckets[\"l69ulq\"]\n",
      "\n",
      "y = time_buckets[\"l69ulq\"]\n",
      "x = [i*bucket_size/10/60 for i in range(bucket_count)]\n",
      "\n",
      "print(x)\n",
      "print(y)\n",
      "\n",
      "plt.bar(x,y, width = 5)\n",
      "plt.plot(x, y, color='red')\n",
      "\n",
      "plt.show()\n",
      "171/94:\n",
      "attributes_comms = pickle.load(open( \"../../pickle/image_attributes_processed_score.pkl\", \"rb\" ))\n",
      "\n",
      "time_buckets_df = pd.DataFrame(time_buckets.items())\n",
      "\n",
      "time_buckets_df = time_buckets_df.rename(columns={0:'id',1:'buckets'})\n",
      "\n",
      "\n",
      "attributes = pd.merge(attributes_comms, time_buckets_df, on=[\"id\"])\n",
      "\n",
      "print(attributes)\n",
      "171/95:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Ridge\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "\n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "clf = MultiOutputRegressor(Ridge(random_state=123)).fit(train_without_exculded, train['buckets'].tolist())\n",
      "171/96:\n",
      "print(test[test['id']==\"l69ulq\"])\n",
      "\n",
      "print(clf.predict(test[test['id']==\"l69ulq\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "171/97:\n",
      "print(test[test['id']==\"l69ulq\"])\n",
      "\n",
      "print(clf.predict(test[test['id']==\"l69ulq\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "171/98:\n",
      "print(test.loc[test['id']==\"l69ulq\"])\n",
      "\n",
      "print(clf.predict(test.loc[test['id']==\"l69ulq\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "171/99:\n",
      "print(test['id'])\n",
      "\n",
      "print(clf.predict(test.loc[test['id']==\"l69ulq\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "171/100:\n",
      "print(test['id']==\"l69ulq\")\n",
      "\n",
      "print(clf.predict(test.loc[test['id']==\"l69ulq\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "171/101:\n",
      "print(test[test['id']==\"l69ulq\"])\n",
      "\n",
      "print(clf.predict(test.loc[test['id']==\"l69ulq\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "171/102:\n",
      "print(test[test['id']==\"l69ulq\"])\n",
      "print(test['id'])\n",
      "\n",
      "print(clf.predict(test.loc[test['id']==\"l69ulq\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "171/103:\n",
      "print(test[test['id']==\"l689yb\"])\n",
      "print(test['id'])\n",
      "\n",
      "print(clf.predict(test.loc[test['id']==\"l69ulq\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "171/104:\n",
      "print(test[test['id']==\"l69ulq\"])\n",
      "print(test['id'])\n",
      "\n",
      "print(clf.predict(test.loc[test['id']==\"l69ulq\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "171/105:\n",
      "print(attributes[attributes['id']==\"l69ulq\"])\n",
      "print(test['id'])\n",
      "\n",
      "print(clf.predict(test.loc[test['id']==\"l69ulq\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "171/106:\n",
      "print(attributes[attributes['id']==\"l69ulq\"])\n",
      "\n",
      "\n",
      "print(clf.predict(attributes[attributes['id']==\"l69ulq\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "171/107:\n",
      "print(clf.predict(attributes[attributes['id']==\"l69ulq\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "171/108:\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.ensemble import AdaBoostRegressor\n",
      "\n",
      "# Create the dataset\n",
      "rng = np.random.RandomState(1)\n",
      "X = np.linspace(0, 6, 100)[:, np.newaxis]\n",
      "y = np.sin(X).ravel() + np.sin(6 * X).ravel() + rng.normal(0, 0.1, X.shape[0])\n",
      "\n",
      "# Fit regression model\n",
      "regr_1 = DecisionTreeRegressor(max_depth=4)\n",
      "\n",
      "regr_2 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4),\n",
      "                          n_estimators=300, random_state=rng)\n",
      "\n",
      "regr_1.fit(X, y)\n",
      "regr_2.fit(X, y)\n",
      "\n",
      "# Predict\n",
      "y_1 = regr_1.predict(X)\n",
      "y_2 = regr_2.predict(X)\n",
      "\n",
      "# Plot the results\n",
      "plt.figure()\n",
      "plt.scatter(X, y, c=\"k\", label=\"training samples\")\n",
      "plt.plot(X, y_1, c=\"g\", label=\"n_estimators=1\", linewidth=2)\n",
      "plt.plot(X, y_2, c=\"r\", label=\"n_estimators=300\", linewidth=2)\n",
      "plt.xlabel(\"data\")\n",
      "plt.ylabel(\"target\")\n",
      "plt.title(\"Boosted Decision Tree Regression\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "171/109:\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.ensemble import AdaBoostRegressor\n",
      "\n",
      "# Create the dataset\n",
      "rng = np.random.RandomState(1)\n",
      "X = np.linspace(0, 6, 100)[:, np.newaxis]\n",
      "y = np.sin(X).ravel() + np.sin(6 * X).ravel() + rng.normal(0, 0.1, X.shape[0])\n",
      "\n",
      "# Fit regression model\n",
      "regr_1 = DecisionTreeRegressor(max_depth=4)\n",
      "\n",
      "regr_2 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4),\n",
      "                          n_estimators=300, random_state=rng)\n",
      "\n",
      "print(X)\n",
      "print(y)\n",
      "\n",
      "regr_1.fit(X, y)\n",
      "regr_2.fit(X, y)\n",
      "\n",
      "# Predict\n",
      "y_1 = regr_1.predict(X)\n",
      "y_2 = regr_2.predict(X)\n",
      "\n",
      "# Plot the results\n",
      "plt.figure()\n",
      "plt.scatter(X, y, c=\"k\", label=\"training samples\")\n",
      "plt.plot(X, y_1, c=\"g\", label=\"n_estimators=1\", linewidth=2)\n",
      "plt.plot(X, y_2, c=\"r\", label=\"n_estimators=300\", linewidth=2)\n",
      "plt.xlabel(\"data\")\n",
      "plt.ylabel(\"target\")\n",
      "plt.title(\"Boosted Decision Tree Regression\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "171/110:\n",
      "print(attributes['id'])\n",
      "\n",
      "print(clf.predict(attributes[attributes['id']==\"l69ulq\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "171/111:\n",
      "print(clf.predict(attributes[attributes['id']==\"l69ulq\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "\n",
      "print(clf.predict(attributes[attributes['id']==\"l0j2uy\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l0j2uy\"])\n",
      "171/112:\n",
      "print(clf.predict(attributes[attributes['id']==\"l69ulq\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "\n",
      "print(clf.predict(attributes[attributes['id']==\"l0j2uy\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l0j2uy\"])\n",
      "\n",
      "print(attributes['id'])\n",
      "171/113:\n",
      "print(clf.predict(attributes[attributes['id']==\"l69ulq\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "\n",
      "print(clf.predict(attributes[attributes['id']==\"lh5731\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"lh5731\"])\n",
      "\n",
      "print(attributes['id'])\n",
      "171/114:\n",
      "print(clf.predict(attributes[attributes['id']==\"l69ulq\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "\n",
      "print(clf.predict(attributes[attributes['id']==\"l0khnn\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l0khnn\"])\n",
      "\n",
      "print(attributes['id'])\n",
      "171/115:\n",
      "print(clf.predict(attributes[attributes['id']==\"l69ulq\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "\n",
      "print(clf.predict(attributes[attributes['id']==\"l0jw5j\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l0jw5j\"])\n",
      "\n",
      "print(attributes['id'])\n",
      "171/116:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "\n",
      "clf = MultiOutputRegressor(Ridge(random_state=123)).fit(train_without_exculded, train['buckets'].tolist())\n",
      "171/117:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "\n",
      "clf = MultiOutputRegressor(RandomForestRegressor(random_state=123)).fit(train_without_exculded, train['buckets'].tolist())\n",
      "171/118:\n",
      "print(clf.predict(attributes[attributes['id']==\"l69ulq\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "\n",
      "print(clf.predict(attributes[attributes['id']==\"l0jw5j\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l0jw5j\"])\n",
      "171/119:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "\n",
      "clf = MultiOutputRegressor(Ridge(random_state=123)).fit(train_without_exculded, train['buckets'].tolist())\n",
      "171/120:\n",
      "print(clf.predict(attributes[attributes['id']==\"l69ulq\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "\n",
      "print(clf.predict(attributes[attributes['id']==\"l0jw5j\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l0jw5j\"])\n",
      "171/121:\n",
      "print(clf.predict(attributes[attributes['id']==\"l69ulq\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "\n",
      "print(clf.predict(attributes[attributes['id']==\"l0jw5j\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l0jw5j\"])\n",
      "171/122:\n",
      "import pandas as pd\n",
      "df = pd.read_csv('../../data/reddit_wsb.csv')\n",
      "171/123:\n",
      "attributes_comms = pickle.load(open( \"../../pickle/text_attributes_processed_score.pkl\", \"rb\" ))\n",
      "\n",
      "time_buckets_df = pd.DataFrame(time_buckets.items())\n",
      "\n",
      "time_buckets_df = time_buckets_df.rename(columns={0:'id',1:'buckets'})\n",
      "\n",
      "\n",
      "attributes = pd.merge(attributes_comms, time_buckets_df, on=[\"id\"])\n",
      "\n",
      "print(attributes)\n",
      "171/124:\n",
      "attributes_comms = pickle.load(open( \"../../pickle/text_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "\n",
      "time_buckets_df = pd.DataFrame(time_buckets.items())\n",
      "\n",
      "time_buckets_df = time_buckets_df.rename(columns={0:'id',1:'buckets'})\n",
      "\n",
      "\n",
      "attributes = pd.merge(attributes_comms, time_buckets_df, on=[\"id\"])\n",
      "\n",
      "print(attributes)\n",
      "171/125:\n",
      "attributes_comms = pickle.load(open( \"../../pickle/text_attributes_processed.pkl\", \"rb\" ))\n",
      "\n",
      "time_buckets_df = pd.DataFrame(time_buckets.items())\n",
      "\n",
      "time_buckets_df = time_buckets_df.rename(columns={0:'id',1:'buckets'})\n",
      "\n",
      "\n",
      "attributes = pd.merge(attributes_comms, time_buckets_df, on=[\"id\"])\n",
      "\n",
      "print(attributes)\n",
      "171/126:\n",
      "attributes_comms = pickle.load(open( \"../../pickle/text_attributes_processed_score.pkl\", \"rb\" ))\n",
      "\n",
      "time_buckets_df = pd.DataFrame(time_buckets.items())\n",
      "\n",
      "time_buckets_df = time_buckets_df.rename(columns={0:'id',1:'buckets'})\n",
      "\n",
      "\n",
      "attributes = pd.merge(attributes_comms, time_buckets_df, on=[\"id\"])\n",
      "\n",
      "print(attributes)\n",
      "177/1:\n",
      "import pandas\n",
      "import pickle\n",
      "import numpy as np\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "import pandas\n",
      "\n",
      "df0 = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "177/2:\n",
      "df = pickle.load(open(\"../../pickle/text_attr.pkl\", \"rb\"))\n",
      "df['comms_num'] = df0['comms_num', 'id']\n",
      "print(df.head())\n",
      "177/3:\n",
      "df = pickle.load(open(\"../../pickle/text_attr.pkl\", \"rb\"))\n",
      "df['comms_num'] = df0['comms_num']\n",
      "df['id'] = df0['id']\n",
      "print(df.head())\n",
      "177/4:\n",
      "import pandas\n",
      "import pickle\n",
      "import numpy as np\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "import pandas\n",
      "\n",
      "df0 = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "177/5:\n",
      "df = pickle.load(open(\"../../pickle/text_attr.pkl\", \"rb\"))\n",
      "df['comms_num'] = df0['comms_num']\n",
      "df['id'] = df0['id']\n",
      "print(df.head())\n",
      "177/6:\n",
      "reduced_attr_count = 120\n",
      "corr_matrix = df.corr(method='spearman')\n",
      "177/7:\n",
      "correlations = []\n",
      "for i,column in enumerate(corr_matrix):\n",
      "  x = corr_matrix[column][i+1:]\n",
      "  for row,v in x.items():\n",
      "    correlations.append([v,column,row])\n",
      "correlations.sort(key = lambda x: -abs(x[0]))\n",
      "177/8:\n",
      "df_score = df.copy()\n",
      "score_corr = corr_matrix['score'][corr_matrix['score']<1]\n",
      "for corr in correlations:\n",
      "  if len(df_score.columns) <= reduced_attr_count: break\n",
      "  c,a1,a2 = corr\n",
      "  if 'score' in [a1,a2]: continue\n",
      "  if a1 in df_score and a2 in df_score:\n",
      "    to_be_removed = a1 if score_corr[a1] < score_corr[a2] else a2\n",
      "    del df_score[to_be_removed]\n",
      "177/9:\n",
      "df_comms_num = df.copy()\n",
      "comms_num_corr = corr_matrix['comms_num'][corr_matrix['comms_num']<1]\n",
      "for corr in correlations:\n",
      "  if len(df_comms_num.columns) <= reduced_attr_count: break\n",
      "  c,a1,a2 = corr\n",
      "  if 'comms_num' in [a1,a2]: continue\n",
      "  if a1 in df_comms_num and a2 in df_comms_num:\n",
      "    to_be_removed = a1 if comms_num_corr[a1] < comms_num_corr[a2] else a2\n",
      "    del df_comms_num[to_be_removed]\n",
      "177/10:\n",
      "final_attr_count = 50\n",
      "rf = RandomForestRegressor()\n",
      "rf.fit(df_score[df_score.columns.difference(['score', 'comms_num'])], df_score['score'])\n",
      "\n",
      "result = sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), df_score.head()), \n",
      "             reverse=True)\n",
      "selected = []\n",
      "for x in result[:final_attr_count]:\n",
      "    selected.append(x[1])\n",
      "selected_attributes = df_score[selected + ['score']]\n",
      "print(selected_attributes)\n",
      "\n",
      "pickle.dump(selected_attributes, open( \"../../pickle/text_attributes_processed_score.pkl\", \"wb\" ))\n",
      "177/11:\n",
      "final_attr_count = 50\n",
      "rf = RandomForestRegressor()\n",
      "rf.fit(df_score[df_score.columns.difference(['score', 'comms_num', 'id'])], df_score['score'])\n",
      "\n",
      "result = sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), df_score.head()), \n",
      "             reverse=True)\n",
      "selected = []\n",
      "for x in result[:final_attr_count]:\n",
      "    selected.append(x[1])\n",
      "selected_attributes = df_score[selected + ['score']]\n",
      "print(selected_attributes)\n",
      "\n",
      "pickle.dump(selected_attributes, open( \"../../pickle/text_attributes_processed_score.pkl\", \"wb\" ))\n",
      "177/12:\n",
      "rf = RandomForestRegressor()\n",
      "rf.fit(df_comms_num[df_comms_num.columns.difference(['score', 'comms_num', 'id'])], df_comms_num['comms_num'])\n",
      "\n",
      "result = sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), df_comms_num.head()), \n",
      "             reverse=True)\n",
      "selected = []\n",
      "for x in result[:final_attr_count]:\n",
      "    selected.append(x[1])\n",
      "selected_attributes = df_comms_num[selected + ['comms_num']]\n",
      "print(selected_attributes)\n",
      "\n",
      "pickle.dump(selected_attributes, open( \"../../pickle/text_attributes_processed_comms_num.pkl\", \"wb\" ))\n",
      "171/127:\n",
      "attributes_comms = pickle.load(open( \"../../pickle/text_attributes_processed_score.pkl\", \"rb\" ))\n",
      "\n",
      "time_buckets_df = pd.DataFrame(time_buckets.items())\n",
      "\n",
      "time_buckets_df = time_buckets_df.rename(columns={0:'id',1:'buckets'})\n",
      "\n",
      "\n",
      "attributes = pd.merge(attributes_comms, time_buckets_df, on=[\"id\"])\n",
      "\n",
      "print(attributes)\n",
      "177/13:\n",
      "rf = RandomForestRegressor()\n",
      "rf.fit(df_comms_num[df_comms_num.columns.difference(['score', 'comms_num', 'id'])], df_comms_num['comms_num'])\n",
      "\n",
      "result = sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), df_comms_num.head()), \n",
      "             reverse=True)\n",
      "selected = []\n",
      "for x in result[:final_attr_count]:\n",
      "    selected.append(x[1])\n",
      "selected_attributes = df_comms_num[selected + ['comms_num', 'id']]\n",
      "print(selected_attributes)\n",
      "\n",
      "pickle.dump(selected_attributes, open( \"../../pickle/text_attributes_processed_comms_num.pkl\", \"wb\" ))\n",
      "177/14:\n",
      "final_attr_count = 50\n",
      "rf = RandomForestRegressor()\n",
      "rf.fit(df_score[df_score.columns.difference(['score', 'comms_num', 'id'])], df_score['score'])\n",
      "\n",
      "result = sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), df_score.head()), \n",
      "             reverse=True)\n",
      "selected = []\n",
      "for x in result[:final_attr_count]:\n",
      "    selected.append(x[1])\n",
      "selected_attributes = df_score[selected + ['score', 'id']]\n",
      "print(selected_attributes)\n",
      "\n",
      "pickle.dump(selected_attributes, open( \"../../pickle/text_attributes_processed_score.pkl\", \"wb\" ))\n",
      "171/128:\n",
      "attributes_comms = pickle.load(open( \"../../pickle/text_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "\n",
      "time_buckets_df = pd.DataFrame(time_buckets.items())\n",
      "\n",
      "time_buckets_df = time_buckets_df.rename(columns={0:'id',1:'buckets'})\n",
      "\n",
      "\n",
      "attributes = pd.merge(attributes_comms, time_buckets_df, on=[\"id\"])\n",
      "\n",
      "print(attributes)\n",
      "171/129:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "\n",
      "clf = MultiOutputRegressor(Ridge(random_state=123)).fit(train_without_exculded, train['buckets'].tolist())\n",
      "171/130:\n",
      "print(clf.predict(attributes[attributes['id']==\"l69ulq\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "\n",
      "print(clf.predict(attributes[attributes['id']==\"l0jw5j\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l0jw5j\"])\n",
      "171/131:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.ensemble import AdaBoostRegressor\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "\n",
      "clf = MultiOutputRegressor(AdaBoostRegressor(random_state=123)).fit(train_without_exculded, train['buckets'].tolist())\n",
      "171/132:\n",
      "print(clf.predict(attributes[attributes['id']==\"l69ulq\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "\n",
      "print(clf.predict(attributes[attributes['id']==\"l0jw5j\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l0jw5j\"])\n",
      "171/133:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "\n",
      "clf = MultiOutputRegressor(RandomForestRegressor(random_state=123)).fit(train_without_exculded, train['buckets'].tolist())\n",
      "171/134:\n",
      "print(clf.predict(attributes[attributes['id']==\"l69ulq\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "\n",
      "print(clf.predict(attributes[attributes['id']==\"l0jw5j\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l0jw5j\"])\n",
      "171/135:\n",
      "print(clf.predict(attributes[attributes['id']==\"l69ulq\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "\n",
      "print(clf.predict(attributes[attributes['id']==\"l0jw5j\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l0jw5j\"])\n",
      "\n",
      "x = [i*bucket_size/10/60 for i in range(bucket_count)]\n",
      "\n",
      "plt.plot(x, time_buckets[\"l69ulq\"], color='red')\n",
      "plt.plot(x, clf.predict(attributes[attributes['id']==\"l69ulq\"][test_without_excluded.columns]), color='blue')\n",
      "plt.show()\n",
      "\n",
      "plt.plot(x, time_buckets[\"l0jw5j\"], color='red')\n",
      "plt.plot(x, clf.predict(attributes[attributes['id']==\"l0jw5j\"][test_without_excluded.columns]), color='blue')\n",
      "plt.show()\n",
      "171/136:\n",
      "print(clf.predict(attributes[attributes['id']==\"l69ulq\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "\n",
      "print(clf.predict(attributes[attributes['id']==\"l0jw5j\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l0jw5j\"])\n",
      "\n",
      "x = [i*bucket_size/10/60 for i in range(bucket_count)]\n",
      "\n",
      "plt.plot(x, time_buckets[\"l69ulq\"], color='red')\n",
      "plt.plot(x, clf.predict(attributes[attributes['id']==\"l69ulq\"][test_without_excluded.columns])[0], color='blue')\n",
      "plt.show()\n",
      "\n",
      "plt.plot(x, time_buckets[\"l0jw5j\"], color='red')\n",
      "plt.plot(x, clf.predict(attributes[attributes['id']==\"l0jw5j\"][test_without_excluded.columns])[0], color='blue')\n",
      "plt.show()\n",
      "171/137:\n",
      "print(clf.predict(attributes[attributes['id']==\"l69ulq\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "\n",
      "print(clf.predict(attributes[attributes['id']==\"l0jw5j\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l0jw5j\"])\n",
      "\n",
      "x = [i*bucket_size/10/60 for i in range(bucket_count)]\n",
      "\n",
      "def compare_results(first, second):\n",
      "    plt.plot(x, first, color='red')\n",
      "    plt.plot(x, second, color='blue')\n",
      "    plt.show()\n",
      "\n",
      "compare_results(first, clf.predict(attributes[attributes['id']==\"l69ulq\"][test_without_excluded.columns])[0])\n",
      "compare_results(first, clf.predict(attributes[attributes['id']==\"l0jw5j\"][test_without_excluded.columns])[0])\n",
      "171/138:\n",
      "print(clf.predict(attributes[attributes['id']==\"l69ulq\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l69ulq\"])\n",
      "\n",
      "print(clf.predict(attributes[attributes['id']==\"l0jw5j\"][test_without_excluded.columns]))\n",
      "print(time_buckets[\"l0jw5j\"])\n",
      "\n",
      "x = [i*bucket_size/10/60 for i in range(bucket_count)]\n",
      "\n",
      "def compare_results(first, second):\n",
      "    plt.plot(x, first, color='red')\n",
      "    plt.plot(x, second, color='blue')\n",
      "    plt.show()\n",
      "\n",
      "compare_results(time_buckets[\"l69ulq\"], clf.predict(attributes[attributes['id']==\"l69ulq\"][test_without_excluded.columns])[0])\n",
      "compare_results(time_buckets[\"l0jw5j\"], clf.predict(attributes[attributes['id']==\"l0jw5j\"][test_without_excluded.columns])[0])\n",
      "178/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "comments = pickle.load(open( \"../../pickle/comments_all/comments_all_38200.pkl\", \"rb\" ))\n",
      "\n",
      "comments.pop('post_id', None)\n",
      "178/2:\n",
      "import pandas as pd\n",
      "df = pd.read_csv('../../data/reddit_wsb.csv')\n",
      "178/3: df.loc[df['id'] == 'l6ea1b'].iloc[0].created\n",
      "178/4:\n",
      "times = {}\n",
      "for id in comments:\n",
      "    times[id] = []\n",
      "    created = df.loc[df['id'] == id].iloc[0].created\n",
      "    for comment in comments[id]:\n",
      "        times[id].append(comment.created_utc - created)\n",
      "for id in times.keys():\n",
      "    times[id].sort()\n",
      "178/5:\n",
      "bucket_size = 6000\n",
      "bucket_count = 12\n",
      "time_buckets = {}\n",
      "for id in times.keys():\n",
      "    time_buckets[id] = []\n",
      "    curr_index = 0\n",
      "    for i in range(bucket_count):\n",
      "        time_buckets[id].append(0)\n",
      "        for j in range(curr_index,len(times[id])):\n",
      "            if times[id][j]<bucket_size*(i+1):\n",
      "                time_buckets[id][i]+=1\n",
      "                curr_index+=1\n",
      "178/6:\n",
      "time_buckets[\"l69ulq\"]\n",
      "\n",
      "y = time_buckets[\"l69ulq\"]\n",
      "x = [i*bucket_size/10/60 for i in range(bucket_count)]\n",
      "\n",
      "print(x)\n",
      "print(y)\n",
      "\n",
      "plt.bar(x,y, width = 5)\n",
      "plt.plot(x, y, color='red')\n",
      "\n",
      "plt.show()\n",
      "178/7:\n",
      "time_buckets[\"l2z98c\"]\n",
      "\n",
      "y = time_buckets[\"l2z98c\"]\n",
      "x = [i*bucket_size/10/60 for i in range(bucket_count)]\n",
      "\n",
      "print(x)\n",
      "print(y)\n",
      "\n",
      "plt.bar(x,y, width = 5)\n",
      "plt.plot(x, y, color='red')\n",
      "\n",
      "plt.show()\n",
      "178/8:\n",
      "attributes_comms = pickle.load(open( \"../../pickle/text_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "\n",
      "time_buckets_df = pd.DataFrame(time_buckets.items())\n",
      "\n",
      "time_buckets_df = time_buckets_df.rename(columns={0:'id',1:'buckets'})\n",
      "\n",
      "\n",
      "attributes = pd.merge(attributes_comms, time_buckets_df, on=[\"id\"])\n",
      "\n",
      "print(attributes)\n",
      "178/9:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "\n",
      "clf = MultiOutputRegressor(RandomForestRegressor(random_state=123)).fit(train_without_exculded, train['buckets'].tolist())\n",
      "178/10:\n",
      "x = [i*bucket_size/10/60 for i in range(bucket_count)]\n",
      "def compare_results(first, second):\n",
      "    plt.plot(x, first, color='red')\n",
      "    plt.plot(x, second, color='blue')\n",
      "    plt.show()\n",
      "\n",
      "compare_results(time_buckets[\"l2z98c\"], clf.predict(attributes[attributes['id']==\"l2z98c\"][test_without_excluded.columns])[0])\n",
      "compare_results(time_buckets[\"l0jw5j\"], clf.predict(attributes[attributes['id']==\"l0jw5j\"][test_without_excluded.columns])[0])\n",
      "179/1:\n",
      "from praw.models import MoreComments\n",
      "import datetime as dt\n",
      "import time\n",
      "import csv\n",
      "import os\n",
      "from dotenv import load_dotenv\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas\n",
      "import pickle\n",
      "\n",
      "df = pandas.read_csv('data/reddit_wsb.csv')\n",
      "df = df.sort_values(by=['comms_num'], ascending=True)\n",
      "179/2:\n",
      "from praw.models import MoreComments\n",
      "import datetime as dt\n",
      "import time\n",
      "import csv\n",
      "import os\n",
      "from dotenv import load_dotenv\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas\n",
      "import pickle\n",
      "\n",
      "df = pandas.read_csv('../data/reddit_wsb.csv')\n",
      "df = df.sort_values(by=['comms_num'], ascending=True)\n",
      "179/3:\n",
      "from praw.models import MoreComments\n",
      "import datetime as dt\n",
      "import time\n",
      "import csv\n",
      "import os\n",
      "from dotenv import load_dotenv\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas\n",
      "import pickle\n",
      "\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "df = df.sort_values(by=['comms_num'], ascending=True)\n",
      "179/4:\n",
      "comments_all = pickle.load(open( \"../../pickle/comments_all/comments_all_new_38000.pkl\", \"rb\" ))\n",
      "comments_last_200 = pickle.load(open( \"../../pickle/comments_all/comments_all_38200.pkl\", \"rb\" ))\n",
      "\n",
      "keys_to_remove =  df['id'][:38000]\n",
      "for key in keys_to_remove:\n",
      "  del comments_last_200[key]\n",
      "\n",
      "comments_mix = {**comments_all, **comments_last_200}\n",
      "comments_mix.pop('post_id', None)\n",
      "\n",
      "ids = df['id'][:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179/5: pickle.dump(comments_mix, open(\"./pickle/comments_all/comments_all.pkl\", \"wb\"))\n",
      "179/6: pickle.dump(comments_mix, open(\"../../pickle/comments_all/comments_all.pkl\", \"wb\"))\n",
      "179/7:\n",
      "comments_all = pickle.load(open( \"../../pickle/comments_all/comments_all_new_38000.pkl\", \"rb\" ))\n",
      "comments_last_200 = pickle.load(open( \"../../pickle/comments_all/comments_all_38200.pkl\", \"rb\" ))\n",
      "\n",
      "keys_to_remove =  df['id'][:38000]\n",
      "for key in keys_to_remove:\n",
      "  del comments_last_200[key]\n",
      "\n",
      "print(len(comments_last_200))\n",
      "\n",
      "# comments_mix = {**comments_all, **comments_last_200}\n",
      "# comments_mix.pop('post_id', None)\n",
      "\n",
      "# ids = df['id'][:]\n",
      "179/8:\n",
      "\n",
      "comments_last_200 = pickle.load(open( \"../../pickle/comments_all/comments_all_38200.pkl\", \"rb\" ))\n",
      "print(len(comments_last_200))\n",
      "keys_to_remove =  df['id'][:38000]\n",
      "for key in keys_to_remove:\n",
      "  del comments_last_200[key]\n",
      "\n",
      "print(len(comments_last_200))\n",
      "\n",
      "# comments_mix = {**comments_all, **comments_last_200}\n",
      "# comments_mix.pop('post_id', None)\n",
      "\n",
      "# ids = df['id'][:]\n",
      "180/1:\n",
      "from praw.models import MoreComments\n",
      "import datetime as dt\n",
      "import time\n",
      "import csv\n",
      "import os\n",
      "from dotenv import load_dotenv\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas\n",
      "import pickle\n",
      "\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "df = df.sort_values(by=['comms_num'], ascending=True)\n",
      "\n",
      "comments_all = pickle.load(open( \"../../pickle/comments_all/comments_all_new_38000.pkl\", \"rb\" ))\n",
      "180/2:\n",
      "\n",
      "comments_last_200 = pickle.load(open( \"../../pickle/comments_all/comments_all_38200.pkl\", \"rb\" ))\n",
      "print(len(comments_last_200))\n",
      "keys_to_remove =  df['id'][:38000]\n",
      "for key in keys_to_remove:\n",
      "  del comments_last_200[key]\n",
      "\n",
      "print(len(comments_last_200))\n",
      "\n",
      "# comments_mix = {**comments_all, **comments_last_200}\n",
      "# comments_mix.pop('post_id', None)\n",
      "\n",
      "# ids = df['id'][:]\n",
      "180/3: # pickle.dump(comments_mix, open(\"../../pickle/comments_all/comments_all.pkl\", \"wb\"))\n",
      "180/4:\n",
      "\n",
      "comments_last_200 = pickle.load(open( \"../../pickle/comments_all/comments_all_38100.pkl\", \"rb\" ))\n",
      "print(len(comments_last_200))\n",
      "keys_to_remove =  df['id'][:38000]\n",
      "for key in keys_to_remove:\n",
      "  del comments_last_200[key]\n",
      "\n",
      "print(len(comments_last_200))\n",
      "\n",
      "# comments_mix = {**comments_all, **comments_last_200}\n",
      "# comments_mix.pop('post_id', None)\n",
      "\n",
      "# ids = df['id'][:]\n",
      "180/5:\n",
      "\n",
      "comments_last_200 = pickle.load(open( \"../../pickle/comments_all/comments_all_38100.pkl\", \"rb\" ))\n",
      "print(len(comments_last_200))\n",
      "keys_to_remove =  df['id'][:38000]\n",
      "for key in keys_to_remove:\n",
      "  del comments_last_200[key]\n",
      "\n",
      "print(len(comments_last_200))\n",
      "\n",
      "# comments_mix = {**comments_all, **comments_last_200}\n",
      "# comments_mix.pop('post_id', None)\n",
      "\n",
      "# ids = df['id'][:]\n",
      "178/11:\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "\n",
      "comments_all = pickle.load(open( \"../../pickle/comments_all/comments_all.pkl\", \"rb\" ))\n",
      "\n",
      "comments_mix = comments_all\n",
      "comments_mix.pop('post_id', None)\n",
      "178/12:\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "\n",
      "comments_all = pickle.load(open( \"../../pickle/comments_all/comments_all.pkl\", \"rb\" ))\n",
      "\n",
      "comments_mix = comments_all\n",
      "comments_mix.pop('post_id', None)\n",
      "178/13:\n",
      "import pandas\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "178/14:\n",
      "times = {}\n",
      "for id in comments_mix:\n",
      "    times[id] = []\n",
      "    created = df.loc[df['id'] == id].iloc[0].created\n",
      "    for comment in comments_mix[id]:\n",
      "        times[id].append(comment.created_utc - created)\n",
      "for id in times.keys():\n",
      "    times[id].sort()\n",
      "178/15:\n",
      "bins_count = 12\n",
      "bin_timeframe = 6000\n",
      "\n",
      "times={}\n",
      "for id in df['id']:\n",
      "    times[id] = []\n",
      "    cur_idx = 0\n",
      "    for i in range(bins_count):\n",
      "        times[id].push(0)\n",
      "        for j in range(cur_idx, len(comments_mix[id])):\n",
      "            if comments_mix[id][j]<bin_timeframe*(i+1):\n",
      "                times[id][i] += 1\n",
      "            else:\n",
      "                cur_idx = j\n",
      "                break\n",
      "\n",
      "print(times)\n",
      "178/16:\n",
      "bins_count = 12\n",
      "bin_timeframe = 6000\n",
      "\n",
      "times={}\n",
      "for id in df['id']:\n",
      "    times[id] = []\n",
      "    cur_idx = 0\n",
      "    for i in range(bins_count):\n",
      "        times[id].append(0)\n",
      "        for j in range(cur_idx, len(comments_mix[id])):\n",
      "            if comments_mix[id][j]<bin_timeframe*(i+1):\n",
      "                times[id][i] += 1\n",
      "            else:\n",
      "                cur_idx = j\n",
      "                break\n",
      "\n",
      "print(times)\n",
      "178/17:\n",
      "times = {}\n",
      "for id in comments_mix:\n",
      "    times[id] = []\n",
      "    created = df.loc[df['id'] == id].iloc[0].created\n",
      "    for comment in comments_mix[id]:\n",
      "        times[id].append(comment.created_utc - created)\n",
      "for id in times.keys():\n",
      "    times[id].sort()\n",
      "178/18:\n",
      "bins_count = 12\n",
      "bin_timeframe = 6000\n",
      "\n",
      "buckets={}\n",
      "for id in df['id']:\n",
      "    buckets[id] = []\n",
      "    cur_idx = 0\n",
      "    for i in range(bins_count):\n",
      "        buckets[id].append(0)\n",
      "        for j in range(cur_idx, len(times[id])):\n",
      "            if times[id][j]<bin_timeframe*(i+1):\n",
      "                buckets[id][i] += 1\n",
      "            else:\n",
      "                cur_idx = j\n",
      "                break\n",
      "\n",
      "print(times)\n",
      "180/6:\n",
      "from praw.models import MoreComments\n",
      "import datetime as dt\n",
      "import time\n",
      "import csv\n",
      "import os\n",
      "from dotenv import load_dotenv\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas\n",
      "import pickle\n",
      "\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "df = df.sort_values(by=['comms_num'], ascending=True)\n",
      "\n",
      "comments_all = pickle.load(open( \"../../pickle/comments_all/comments_all_new_38000.pkl\", \"rb\" ))\n",
      "180/7:\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_all_38200.pkl\", \"rb\" ))\n",
      "keys_to_remove =  df['id'][:38000]\n",
      "for key in keys_to_remove:\n",
      "  del comments_last[key]\n",
      "\n",
      "\n",
      "comments_mix = {**comments_all, **comments_last}\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_all_38400.pkl\", \"rb\" ))\n",
      "\n",
      "comments_mix = {**comments_all, **comments_last}\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_38000_to_38800_10.pkl\", \"rb\" ))\n",
      "\n",
      "for key in comments_mix.keys():\n",
      "    del comments_last[key]\n",
      "    \n",
      "comments_mix = {**comments_all, **comments_last}\n",
      "    \n",
      "comments_mix.pop('post_id', None)\n",
      "181/1:\n",
      "from praw.models import MoreComments\n",
      "import datetime as dt\n",
      "import time\n",
      "import csv\n",
      "import os\n",
      "from dotenv import load_dotenv\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas\n",
      "import pickle\n",
      "\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "df = df.sort_values(by=['comms_num'], ascending=True)\n",
      "\n",
      "comments_all = pickle.load(open( \"../../pickle/comments_all/comments_all_new_38000.pkl\", \"rb\" ))\n",
      "181/2:\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_all_38200.pkl\", \"rb\" ))\n",
      "keys_to_remove =  df['id'][:38000]\n",
      "for key in keys_to_remove:\n",
      "  del comments_last[key]\n",
      "\n",
      "\n",
      "comments_mix = {**comments_all, **comments_last}\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_all_38400.pkl\", \"rb\" ))\n",
      "\n",
      "comments_mix = {**comments_all, **comments_last}\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_38000_to_38800_10.pkl\", \"rb\" ))\n",
      "\n",
      "for key in comments_mix.keys():\n",
      "    del comments_last[key]\n",
      "    \n",
      "comments_mix = {**comments_all, **comments_last}\n",
      "    \n",
      "comments_mix.pop('post_id', None)\n",
      "181/3:\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_all_38200.pkl\", \"rb\" ))\n",
      "keys_to_remove =  df['id'][:38000]\n",
      "for key in keys_to_remove:\n",
      "    if comments_last[key]:\n",
      "        del comments_last[key]\n",
      "\n",
      "\n",
      "comments_mix = {**comments_all, **comments_last}\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_all_38400.pkl\", \"rb\" ))\n",
      "\n",
      "comments_mix = {**comments_all, **comments_last}\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_38000_to_38800_10.pkl\", \"rb\" ))\n",
      "\n",
      "for key in comments_mix.keys():\n",
      "    del comments_last[key]\n",
      "    \n",
      "comments_mix = {**comments_all, **comments_last}\n",
      "    \n",
      "comments_mix.pop('post_id', None)\n",
      "181/4:\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_all_38200.pkl\", \"rb\" ))\n",
      "keys_to_remove =  df['id'][:38000]\n",
      "for key in keys_to_remove:\n",
      "    if comments_last[key]:\n",
      "        del comments_last[key]\n",
      "\n",
      "\n",
      "comments_mix = {**comments_all, **comments_last}\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_all_38400.pkl\", \"rb\" ))\n",
      "\n",
      "comments_mix = {**comments_all, **comments_last}\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_38000_to_38800_10.pkl\", \"rb\" ))\n",
      "\n",
      "for key in comments_mix.keys():\n",
      "    if comments_last[key]:\n",
      "        del comments_last[key]\n",
      "    \n",
      "comments_mix = {**comments_all, **comments_last}\n",
      "    \n",
      "comments_mix.pop('post_id', None)\n",
      "182/1:\n",
      "from praw.models import MoreComments\n",
      "import datetime as dt\n",
      "import time\n",
      "import csv\n",
      "import os\n",
      "from dotenv import load_dotenv\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas\n",
      "import pickle\n",
      "\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "df = df.sort_values(by=['comms_num'], ascending=True)\n",
      "\n",
      "comments_all = pickle.load(open( \"../../pickle/comments_all/comments_all_new_38000.pkl\", \"rb\" ))\n",
      "182/2:\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_all_38200.pkl\", \"rb\" ))\n",
      "print('load')\n",
      "keys_to_remove =  df['id'][:38000]\n",
      "for key in keys_to_remove:\n",
      "    if comments_last[key]:\n",
      "        del comments_last[key]\n",
      "\n",
      "\n",
      "comments_mix = {**comments_all, **comments_last}\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_all_38400.pkl\", \"rb\" ))\n",
      "print('load')\n",
      "\n",
      "comments_mix = {**comments_all, **comments_last}\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_38000_to_38800_10.pkl\", \"rb\" ))\n",
      "print('load')\n",
      "\n",
      "for key in comments_mix.keys():\n",
      "    if comments_last[key]:\n",
      "        del comments_last[key]\n",
      "    \n",
      "comments_mix = {**comments_all, **comments_last}\n",
      "    \n",
      "comments_mix.pop('post_id', None)\n",
      "182/3:\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_all_38200.pkl\", \"rb\" ))\n",
      "print('load')\n",
      "keys_to_remove =  df['id'][:38000]\n",
      "for key in keys_to_remove:\n",
      "    if key in comments_last.keys()::\n",
      "        del comments_last[key]\n",
      "\n",
      "\n",
      "comments_mix = {**comments_all, **comments_last}\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_all_38400.pkl\", \"rb\" ))\n",
      "print('load')\n",
      "\n",
      "comments_mix = {**comments_mix, **comments_last}\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_38000_to_38800_10.pkl\", \"rb\" ))\n",
      "print('load')\n",
      "\n",
      "for key in comments_mix.keys():\n",
      "    if key in comments_last.keys()::\n",
      "        del comments_last[key]\n",
      "    \n",
      "comments_mix = {**comments_mix, **comments_last}\n",
      "    \n",
      "comments_mix.pop('post_id', None)\n",
      "182/4:\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_all_38200.pkl\", \"rb\" ))\n",
      "print('load')\n",
      "keys_to_remove =  df['id'][:38000]\n",
      "for key in keys_to_remove:\n",
      "    if key in comments_last.keys():\n",
      "        del comments_last[key]\n",
      "\n",
      "\n",
      "comments_mix = {**comments_all, **comments_last}\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_all_38400.pkl\", \"rb\" ))\n",
      "print('load')\n",
      "\n",
      "comments_mix = {**comments_mix, **comments_last}\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_38000_to_38800_10.pkl\", \"rb\" ))\n",
      "print('load')\n",
      "\n",
      "for key in comments_mix.keys():\n",
      "    if key in comments_last.keys()::\n",
      "        del comments_last[key]\n",
      "    \n",
      "comments_mix = {**comments_mix, **comments_last}\n",
      "    \n",
      "comments_mix.pop('post_id', None)\n",
      "182/5:\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_all_38200.pkl\", \"rb\" ))\n",
      "print('load')\n",
      "keys_to_remove =  df['id'][:38000]\n",
      "for key in keys_to_remove:\n",
      "    if key in comments_last.keys():\n",
      "        del comments_last[key]\n",
      "\n",
      "\n",
      "comments_mix = {**comments_all, **comments_last}\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_all_38400.pkl\", \"rb\" ))\n",
      "print('load')\n",
      "\n",
      "comments_mix = {**comments_mix, **comments_last}\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_38000_to_38800_10.pkl\", \"rb\" ))\n",
      "print('load')\n",
      "\n",
      "for key in comments_mix.keys():\n",
      "    if key in comments_last.keys():\n",
      "        del comments_last[key]\n",
      "    \n",
      "comments_mix = {**comments_mix, **comments_last}\n",
      "    \n",
      "comments_mix.pop('post_id', None)\n",
      "183/1:\n",
      "from praw.models import MoreComments\n",
      "import datetime as dt\n",
      "import time\n",
      "import csv\n",
      "import os\n",
      "from dotenv import load_dotenv\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas\n",
      "import pickle\n",
      "\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "df = df.sort_values(by=['comms_num'], ascending=True)\n",
      "\n",
      "comments_all = pickle.load(open( \"../../pickle/comments_all/comments_all_new_38000.pkl\", \"rb\" ))\n",
      "183/2:\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_all_38200.pkl\", \"rb\" ))\n",
      "print('load')\n",
      "keys_to_remove =  df['id'][:38000]\n",
      "for key in keys_to_remove:\n",
      "    if key in comments_last.keys():\n",
      "        del comments_last[key]\n",
      "\n",
      "\n",
      "comments_mix = {**comments_all, **comments_last}\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_all_38400.pkl\", \"rb\" ))\n",
      "print('load')\n",
      "\n",
      "comments_mix = {**comments_mix, **comments_last}\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_38000_to_38800_10.pkl\", \"rb\" ))\n",
      "print('load')\n",
      "\n",
      "for key in comments_mix.keys():\n",
      "    if key in comments_last.keys():\n",
      "        del comments_last[key]\n",
      "    \n",
      "comments_mix = {**comments_mix, **comments_last}\n",
      "    \n",
      "comments_mix.pop('post_id', None)\n",
      "183/3: # pickle.dump(comments_mix, open(\"../../pickle/comments_all/comments_all.pkl\", \"wb\"))\n",
      "183/4: pickle.dump(comments_mix, open(\"../../pickle/comments_all/comments_all.pkl\", \"wb\"))\n",
      "183/5:\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_all_38200.pkl\", \"rb\" ))\n",
      "keys_to_remove =  df['id'][:38000]\n",
      "for key in keys_to_remove:\n",
      "    if key in comments_last.keys():\n",
      "        del comments_last[key]\n",
      "\n",
      "print(len(comments_last))\n",
      "comments_mix = {**comments_all, **comments_last}\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_all_38400.pkl\", \"rb\" ))\n",
      "print(len(comments_last))\n",
      "comments_mix = {**comments_mix, **comments_last}\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_38000_to_38800_10.pkl\", \"rb\" ))\n",
      "\n",
      "for key in comments_mix.keys():\n",
      "    if key in comments_last.keys():\n",
      "        del comments_last[key]\n",
      "print(len(comments_last))    \n",
      "comments_mix = {**comments_mix, **comments_last}\n",
      "    \n",
      "comments_mix.pop('post_id', None)\n",
      "print(len(comments_mix))\n",
      "184/1:\n",
      "from praw.models import MoreComments\n",
      "import datetime as dt\n",
      "import time\n",
      "import csv\n",
      "import os\n",
      "from dotenv import load_dotenv\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas\n",
      "import pickle\n",
      "\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "df = df.sort_values(by=['comms_num'], ascending=True)\n",
      "\n",
      "comments_all = pickle.load(open( \"../../pickle/comments_all/comments_all_new_38000.pkl\", \"rb\" ))\n",
      "184/2:\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_all_38200.pkl\", \"rb\" ))\n",
      "print('load')\n",
      "keys_to_remove =  df['id'][:38000]\n",
      "for key in keys_to_remove:\n",
      "    if key in comments_last.keys():\n",
      "        del comments_last[key]\n",
      "\n",
      "print(len(comments_last))\n",
      "comments_mix = {**comments_all, **comments_last}\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_all_38400.pkl\", \"rb\" ))\n",
      "print('load')\n",
      "print(len(comments_last))\n",
      "comments_mix = {**comments_mix, **comments_last}\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_38000_to_38800_10.pkl\", \"rb\" ))\n",
      "print('load')\n",
      "\n",
      "for key in comments_mix.keys():\n",
      "    if key in comments_last.keys():\n",
      "        del comments_last[key]\n",
      "print(len(comments_last))    \n",
      "comments_mix = {**comments_mix, **comments_last}\n",
      "    \n",
      "comments_mix.pop('post_id', None)\n",
      "print(len(comments_mix))\n",
      "184/3: pickle.dump(comments_mix, open(\"../../pickle/comments_all/comments_all.pkl\", \"wb\"))\n",
      "178/19:\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "\n",
      "comments_all = pickle.load(open( \"../../pickle/comments_all/comments_all.pkl\", \"rb\" ))\n",
      "\n",
      "comments_mix = comments_all\n",
      "comments_mix.pop('post_id', None)\n",
      "185/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "\n",
      "comments_all = pickle.load(open( \"../../pickle/comments_all/comments_all.pkl\", \"rb\" ))\n",
      "\n",
      "comments_mix = comments_all\n",
      "comments_mix.pop('post_id', None)\n",
      "185/2:\n",
      "import pandas\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "185/3:\n",
      "times = {}\n",
      "for id in comments_mix:\n",
      "    times[id] = []\n",
      "    created = df.loc[df['id'] == id].iloc[0].created\n",
      "    for comment in comments_mix[id]:\n",
      "        times[id].append(comment.created_utc - created)\n",
      "for id in times.keys():\n",
      "    times[id].sort()\n",
      "185/4:\n",
      "bins_count = 12\n",
      "bin_timeframe = 6000\n",
      "\n",
      "buckets={}\n",
      "for id in comments_all:\n",
      "    buckets[id] = []\n",
      "    cur_idx = 0\n",
      "    for i in range(bins_count):\n",
      "        buckets[id].append(0)\n",
      "        for j in range(cur_idx, len(times[id])):\n",
      "            if times[id][j]<bin_timeframe*(i+1):\n",
      "                buckets[id][i] += 1\n",
      "            else:\n",
      "                cur_idx = j\n",
      "                break\n",
      "\n",
      "print(times)\n",
      "185/5:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.linear_model import Ridge\n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "clf = MultiOutputRegressor(Ridge(random_state=123)).fit(X, y)\n",
      "clf.predict(X[[0]])\n",
      "185/6:\n",
      "import pandas as pd\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "185/7:\n",
      "buckets_df = pd.DataFrame.from_dict(buckets, orient='index',\n",
      "                       columns=['id', 'buckets'])\n",
      "185/8:\n",
      "print(buckets)\n",
      "buckets_df = pd.DataFrame.from_dict(buckets, orient='index',\n",
      "                       columns=['id', 'buckets'])\n",
      "185/9:\n",
      "print(buckets)\n",
      "buckets_df = pd.DataFrame.from_dict(buckets, orient='index')\n",
      "\n",
      "print(buckets_df)\n",
      "185/10:\n",
      "buckets_df = pd.DataFrame.from_dict(buckets)\n",
      "\n",
      "print(buckets_df)\n",
      "185/11:\n",
      "buckets_df = pd.DataFrame.from_dict(buckets, orient='index'))\n",
      "\n",
      "print(buckets_df)\n",
      "185/12:\n",
      "buckets_df = pd.DataFrame.from_dict(buckets, orient='index')\n",
      "\n",
      "print(buckets_df)\n",
      "185/13:\n",
      "buckets_df = pd.DataFrame.from_dict(buckets, orient='index').values.tolist()\n",
      "\n",
      "print(buckets_df)\n",
      "185/14:\n",
      "buckets_df = pd.DataFrame.from_dict(buckets, orient='index').values.tolist()\n",
      "buckets_df['buckets'] = buckets_df.values.tolist()\n",
      "\n",
      "print(buckets_df)\n",
      "185/15:\n",
      "buckets_df = pd.DataFrame.from_dict(buckets, orient='index')\n",
      "buckets_df['buckets'] = buckets_df.values.tolist()\n",
      "\n",
      "print(buckets_df)\n",
      "185/16:\n",
      "buckets_df = pd.DataFrame.from_dict(buckets, orient='index')\n",
      "buckets_df['buckets'] = buckets_df.values.tolist()\n",
      "\n",
      "print(buckets_df[0, 'buckets'])\n",
      "185/17:\n",
      "buckets_df = pd.DataFrame.from_dict(buckets, orient='index')\n",
      "buckets_df['buckets'] = buckets_df.values.tolist()\n",
      "\n",
      "print(buckets_df[[0, 'buckets']])\n",
      "185/18:\n",
      "buckets_df = pd.DataFrame.from_dict(buckets, orient='index')\n",
      "buckets_df['buckets'] = buckets_df.values.tolist()\n",
      "\n",
      "print(buckets_df['buckets'])\n",
      "185/19:\n",
      "buckets_df = pd.DataFrame.from_dict(buckets, orient='index')\n",
      "buckets_df['buckets'] = buckets_df.values.tolist()\n",
      "buckets_df = buckets_df['buckets']\n",
      "print(buckets_df)\n",
      "185/20:\n",
      "print(buckets)\n",
      "\n",
      "buckets_df = pd.DataFrame.from_dict(buckets, orient='index')\n",
      "buckets_df['buckets'] = buckets_df.values.tolist()\n",
      "buckets_df = buckets_df['buckets']\n",
      "print(buckets_df)\n",
      "185/21:\n",
      "\n",
      "\n",
      "buckets_df = pd.DataFrame(buckets.items(), columns=['id', 'buckets'])\n",
      "\n",
      "print(buckets_df)\n",
      "185/22:\n",
      "attributes = pickle.load(open( \"../../pickle/text_attributes_processed_comm_nums.pkl\", \"rb\" ))\n",
      "attributes = pd.merge(attributes, buckets_df, on=\"id\")\n",
      "185/23:\n",
      "attributes = pickle.load(open( \"../../pickle/text_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "attributes = pd.merge(attributes, buckets_df, on=\"id\")\n",
      "185/24:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.linear_model import Ridge\n",
      "\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "    \n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "\n",
      "\n",
      "clf = MultiOutputRegressor(Ridge(random_state=123)).fit(train_without_exculded, train['buckets'])\n",
      "clf.predict(test_without_excluded, test['buckets'])\n",
      "185/25:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "    \n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "\n",
      "\n",
      "clf = MultiOutputRegressor(Ridge(random_state=123)).fit(train_without_exculded, train['buckets'])\n",
      "clf.predict(test_without_excluded, test['buckets'])\n",
      "185/26:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "    \n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "\n",
      "\n",
      "clf = MultiOutputRegressor(Ridge(random_state=123)).fit(train_without_exculded, train['buckets'].to_list())\n",
      "clf.predict(test_without_excluded, test['buckets'])\n",
      "185/27:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "    \n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "\n",
      "\n",
      "clf = MultiOutputRegressor(Ridge(random_state=123)).fit(train_without_exculded, train['buckets'].to_list())\n",
      "clf.predict(test_without_excluded)\n",
      "185/28:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "    \n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "\n",
      "\n",
      "clf = MultiOutputRegressor(RandomForestRegressor(random_state=123)).fit(train_without_exculded, train['buckets'].to_list())\n",
      "clf.predict(test_without_excluded)\n",
      "186/1:\n",
      "from praw.models import MoreComments\n",
      "import datetime as dt\n",
      "import time\n",
      "import csv\n",
      "import os\n",
      "from dotenv import load_dotenv\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas\n",
      "import pickle\n",
      "\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "df = df.sort_values(by=['comms_num'], ascending=True)\n",
      "\n",
      "comments_all = pickle.load(open( \"../../pickle/comments_all/comments_all_new_38000.pkl\", \"rb\" ))\n",
      "186/2:\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_all_38200.pkl\", \"rb\" ))\n",
      "print('load')\n",
      "keys_to_remove =  df['id'][:38000]\n",
      "for key in keys_to_remove:\n",
      "    if key in comments_last.keys():\n",
      "        del comments_last[key]\n",
      "\n",
      "print(len(comments_last))\n",
      "comments_mix = {**comments_all, **comments_last}\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_all_38500.pkl\", \"rb\" ))\n",
      "print('load')\n",
      "print(len(comments_last))\n",
      "comments_mix = {**comments_mix, **comments_last}\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_38000_to_38800_10.pkl\", \"rb\" ))\n",
      "print('load')\n",
      "\n",
      "for key in comments_mix.keys():\n",
      "    if key in comments_last.keys():\n",
      "        del comments_last[key]\n",
      "print(len(comments_last))    \n",
      "comments_mix = {**comments_mix, **comments_last}\n",
      "    \n",
      "comments_mix.pop('post_id', None)\n",
      "print(len(comments_mix))\n",
      "186/3: pickle.dump(comments_mix, open(\"../../pickle/comments_all/comments_all.pkl\", \"wb\"))\n",
      "187/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "\n",
      "comments_all = pickle.load(open( \"../../pickle/comments_all/comments_all.pkl\", \"rb\" ))\n",
      "\n",
      "comments_mix = comments_all\n",
      "comments_mix.pop('post_id', None)\n",
      "187/2:\n",
      "import pandas as pd\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "187/3:\n",
      "import pandas as pd\n",
      "df = pd.read_csv('../../data/reddit_wsb.csv')\n",
      "187/4:\n",
      "times = {}\n",
      "for id in comments_mix:\n",
      "    times[id] = []\n",
      "    created = df.loc[df['id'] == id].iloc[0].created\n",
      "    for comment in comments_mix[id]:\n",
      "        times[id].append(comment.created_utc - created)\n",
      "for id in times.keys():\n",
      "    times[id].sort()\n",
      "187/5:\n",
      "bins_count = 12\n",
      "bin_timeframe = 6000\n",
      "\n",
      "buckets={}\n",
      "for id in comments_all:\n",
      "    buckets[id] = []\n",
      "    cur_idx = 0\n",
      "    for i in range(bins_count):\n",
      "        buckets[id].append(0)\n",
      "        for j in range(cur_idx, len(times[id])):\n",
      "            if times[id][j]<bin_timeframe*(i+1):\n",
      "                buckets[id][i] += 1\n",
      "            else:\n",
      "                cur_idx = j\n",
      "                break\n",
      "\n",
      "print(buckets)\n",
      "187/6:\n",
      "x  = [i*bin_timeframe/10/60 for i in range(bins_count)]\n",
      "\n",
      "plt.plot(x, buckets[l8azdz])\n",
      "plt.bar(x, buckets[l8azdz])\n",
      "plt.show()\n",
      "187/7:\n",
      "x  = [i*bin_timeframe/10/60 for i in range(bins_count)]\n",
      "\n",
      "plt.plot(x, buckets['l8azdz'])\n",
      "plt.bar(x, buckets['l8azdz'])\n",
      "plt.show()\n",
      "187/8:\n",
      "x  = [i*bin_timeframe/10/60 for i in range(bins_count)]\n",
      "\n",
      "plt.plot(x, buckets['l8azdz'])\n",
      "plt.bar(x, buckets['l8azdz'], width:2)\n",
      "plt.show()\n",
      "187/9:\n",
      "x  = [i*bin_timeframe/10/60 for i in range(bins_count)]\n",
      "\n",
      "plt.plot(x, buckets['l8azdz'])\n",
      "plt.bar(x, buckets['l8azdz'], width=2)\n",
      "plt.show()\n",
      "187/10:\n",
      "x  = [i*bin_timeframe/10/60 for i in range(bins_count)]\n",
      "\n",
      "plt.plot(x, buckets['l8azdz'])\n",
      "plt.bar(x, buckets['l8azdz'], width=5)\n",
      "plt.show()\n",
      "187/11:\n",
      "x  = [i*bin_timeframe/10/60 for i in range(bins_count)]\n",
      "\n",
      "plt.plot(x, buckets['l8azdz'])\n",
      "plt.bar(x, buckets['l8azdz'], width=5)\n",
      "plt.show()\n",
      "\n",
      "plt.plot(x, buckets['l922ub'])\n",
      "plt.bar(x, buckets['l922ub'], width=5)\n",
      "plt.show()\n",
      "187/12:\n",
      "buckets_df = pd.DataFrame(buckets.items(), columns=['id', 'buckets'])\n",
      "\n",
      "print(buckets_df)\n",
      "187/13:\n",
      "attributes = pickle.load(open( \"../../pickle/text_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "attributes = pd.merge(attributes, buckets_df, on=\"id\")\n",
      "187/14:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "    \n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "\n",
      "\n",
      "clf = MultiOutputRegressor(RandomForestRegressor(random_state=123)).fit(train_without_exculded, train['buckets'].to_list())\n",
      "clf.predict(test_without_excluded)\n",
      "187/15:\n",
      "plt.plot(x, clf.predict(attributes_without_excluded['l8azdz']), color='blue')\n",
      "plt.plot(x, buckets['l8azdz'], color='red')\n",
      "plt.show()\n",
      "\n",
      "plt.plot(x, clf.predict(attributes_without_excluded['l922ub']), color='blue')\n",
      "plt.plot(x, buckets['l922ub'], color='red')\n",
      "plt.show()\n",
      "187/16:\n",
      "plt.plot(x, clf.predict([attributes_without_excluded['l8azdz']]), color='blue')\n",
      "plt.plot(x, buckets['l8azdz'], color='red')\n",
      "plt.show()\n",
      "\n",
      "plt.plot(x, clf.predict([attributes_without_excluded['l922ub']]), color='blue')\n",
      "plt.plot(x, buckets['l922ub'], color='red')\n",
      "plt.show()\n",
      "187/17:\n",
      "plt.plot(x, clf.predict(attributes_without_excluded[attributes[id]=='l8azdz']), color='blue')\n",
      "plt.plot(x, buckets['l8azdz'], color='red')\n",
      "plt.show()\n",
      "\n",
      "plt.plot(x, clf.predict(attributes_without_excluded['l922ub']), color='blue')\n",
      "plt.plot(x, buckets['l922ub'], color='red')\n",
      "plt.show()\n",
      "187/18:\n",
      "plt.plot(x, clf.predict(attributes_without_excluded[attributes['id']=='l8azdz']), color='blue')\n",
      "plt.plot(x, buckets['l8azdz'], color='red')\n",
      "plt.show()\n",
      "\n",
      "plt.plot(x, clf.predict(attributes_without_excluded['l922ub']), color='blue')\n",
      "plt.plot(x, buckets['l922ub'], color='red')\n",
      "plt.show()\n",
      "187/19:\n",
      "plt.plot(x, clf.predict(attributes_without_excluded[attributes['id']=='l8azdz'])[0], color='blue')\n",
      "plt.plot(x, buckets['l8azdz'], color='red')\n",
      "plt.show()\n",
      "\n",
      "plt.plot(x, clf.predict(attributes_without_excluded[attributes['id']=='l922ub']), color='blue')\n",
      "plt.plot(x, buckets['l922ub'], color='red')\n",
      "plt.show()\n",
      "187/20:\n",
      "plt.plot(x, clf.predict(attributes_without_excluded[attributes['id']=='l8azdz'])[0], color='blue')\n",
      "plt.plot(x, buckets['l8azdz'], color='red')\n",
      "plt.show()\n",
      "\n",
      "plt.plot(x, clf.predict(attributes_without_excluded[attributes['id']=='l922ub'])[0], color='blue')\n",
      "plt.plot(x, buckets['l922ub'], color='red')\n",
      "plt.show()\n",
      "187/21:\n",
      "plt.plot(x, clf.predict(attributes_without_excluded[attributes['id']=='l8azdz'])[0], color='blue')\n",
      "plt.plot(x, buckets['l8azdz'], color='red')\n",
      "plt.show()\n",
      "print(attributes[attributes['id']=='l8azdz']['comms_num'])\n",
      "\n",
      "plt.plot(x, clf.predict(attributes_without_excluded[attributes['id']=='l922ub'])[0], color='blue')\n",
      "plt.plot(x, buckets['l922ub'], color='red')\n",
      "plt.show()\n",
      "187/22:\n",
      "plt.plot(x, clf.predict(attributes_without_excluded[attributes['id']=='l8azdz'])[0], color='blue')\n",
      "plt.plot(x, buckets['l8azdz'], color='red')\n",
      "plt.show()\n",
      "print(attributes[attributes['id']=='l8azdz']['comms_num'])\n",
      "\n",
      "plt.plot(x, clf.predict(attributes_without_excluded[attributes['id']=='l922ub'])[0], color='blue')\n",
      "plt.plot(x, buckets['l922ub'], color='red')\n",
      "plt.show()\n",
      "print(attributes[attributes['id']=='l922ub']['comms_num'])\n",
      "187/23:\n",
      "plt.plot(x, clf.predict(attributes_without_excluded[attributes['id']=='l8azdz'])[0], color='blue')\n",
      "plt.plot(x, buckets['l8azdz'], color='red')\n",
      "plt.show()\n",
      "print(attributes[attributes['id']=='l8azdz']['comms_num'], print(np.sum(attributes[attributes['id']=='l8azdz']['buckets']))\n",
      "\n",
      "plt.plot(x, clf.predict(attributes_without_excluded[attributes['id']=='l922ub'])[0], color='blue')\n",
      "plt.plot(x, buckets['l922ub'], color='red')\n",
      "plt.show()\n",
      "print(attributes[attributes['id']=='l922ub']['comms_num'])\n",
      "187/24:\n",
      "plt.plot(x, clf.predict(attributes_without_excluded[attributes['id']=='l8azdz'])[0], color='blue')\n",
      "plt.plot(x, buckets['l8azdz'], color='red')\n",
      "plt.show()\n",
      "print(attributes[attributes['id']=='l8azdz']['comms_num'], np.sum(attributes[attributes['id']=='l8azdz']['buckets']))\n",
      "\n",
      "plt.plot(x, clf.predict(attributes_without_excluded[attributes['id']=='l922ub'])[0], color='blue')\n",
      "plt.plot(x, buckets['l922ub'], color='red')\n",
      "plt.show()\n",
      "print(attributes[attributes['id']=='l922ub']['comms_num'])\n",
      "187/25:\n",
      "plt.plot(x, clf.predict(attributes_without_excluded[attributes['id']=='l8azdz'])[0], color='blue')\n",
      "plt.plot(x, buckets['l8azdz'], color='red')\n",
      "plt.show()\n",
      "print(attributes[attributes['id']=='l8azdz']['comms_num'], np.count(times['l8azdz']))\n",
      "\n",
      "plt.plot(x, clf.predict(attributes_without_excluded[attributes['id']=='l922ub'])[0], color='blue')\n",
      "plt.plot(x, buckets['l922ub'], color='red')\n",
      "plt.show()\n",
      "print(attributes[attributes['id']=='l922ub']['comms_num'])\n",
      "187/26:\n",
      "plt.plot(x, clf.predict(attributes_without_excluded[attributes['id']=='l8azdz'])[0], color='blue')\n",
      "plt.plot(x, buckets['l8azdz'], color='red')\n",
      "plt.show()\n",
      "print(attributes[attributes['id']=='l8azdz']['comms_num'], len(times['l8azdz']))\n",
      "\n",
      "plt.plot(x, clf.predict(attributes_without_excluded[attributes['id']=='l922ub'])[0], color='blue')\n",
      "plt.plot(x, buckets['l922ub'], color='red')\n",
      "plt.show()\n",
      "print(attributes[attributes['id']=='l922ub']['comms_num'])\n",
      "187/27:\n",
      "plt.plot(x, clf.predict(attributes_without_excluded[attributes['id']=='l8azdz'])[0], color='blue')\n",
      "plt.plot(x, buckets['l8azdz'], color='red')\n",
      "plt.show()\n",
      "print(attributes[attributes['id']=='l8azdz']['comms_num'], len(times['l8azdz']))\n",
      "\n",
      "plt.plot(x, clf.predict(attributes_without_excluded[attributes['id']=='l922ub'])[0], color='blue')\n",
      "plt.plot(x, buckets['l922ub'], color='red')\n",
      "plt.show()\n",
      "print(attributes[attributes['id']=='l922ub']['comms_num'], len(times['l922ub']))\n",
      "187/28:\n",
      "def probe_post(post_id):\n",
      "    plt.plot(x, clf.predict(attributes_without_excluded[attributes['id']==post_id])[0], color='blue')\n",
      "    plt.plot(x, buckets[post_id], color='red')\n",
      "    plt.show()\n",
      "    print(attributes[attributes['id']==post_id]['comms_num'], len(times[post_id]))\n",
      "\n",
      "probe_post('l8azdz')\n",
      "probe_post('l922ub')\n",
      "187/29:\n",
      "def probe_post(post_id):\n",
      "    plt.plot(x, clf.predict(attributes_without_excluded[attributes['id']==post_id])[0], color='blue')\n",
      "    plt.plot(x, buckets[post_id], color='red')\n",
      "    plt.show()\n",
      "    print('reddit comment cound: ', attributes[attributes['id']==post_id]['comms_num'], len(times[post_id]))\n",
      "    print('actual comment cound: ', attributes[attributes['id']==post_id]['comms_num'], len(times[post_id]))\n",
      "\n",
      "probe_post('l8azdz')\n",
      "probe_post('l922ub')\n",
      "187/30:\n",
      "def probe_post(post_id):\n",
      "    plt.plot(x, clf.predict(attributes_without_excluded[attributes['id']==post_id])[0], color='blue')\n",
      "    plt.plot(x, buckets[post_id], color='red')\n",
      "    plt.show()\n",
      "    print('reddit comment cound: ', attributes[attributes['id']==post_id]['comms_num'])\n",
      "    print('actual comment cound: ', len(times[post_id]))\n",
      "\n",
      "probe_post('l8azdz')\n",
      "probe_post('l922ub')\n",
      "187/31:\n",
      "def probe_post(post_id):\n",
      "    plt.plot(x, clf.predict(attributes_without_excluded[attributes['id']==post_id])[0], color='blue')\n",
      "    plt.plot(x, buckets[post_id], color='red')\n",
      "    plt.show()\n",
      "    print('reddit comment cound: ', attributes[attributes['id']==post_id]['comms_num'].to_list())\n",
      "    print('actual comment cound: ', len(times[post_id]))\n",
      "\n",
      "probe_post('l8azdz')\n",
      "probe_post('l922ub')\n",
      "187/32:\n",
      "def probe_post(post_id):\n",
      "    plt.plot(x, clf.predict(attributes_without_excluded[attributes['id']==post_id])[0], color='blue')\n",
      "    plt.plot(x, buckets[post_id], color='red')\n",
      "    plt.show()\n",
      "    print('reddit comment cound: ', attributes[attributes['id']==post_id]['comms_num'].to_list()[0])\n",
      "    print('actual comment cound: ', len(times[post_id]))\n",
      "\n",
      "probe_post('l8azdz')\n",
      "probe_post('l922ub')\n",
      "187/33:\n",
      "def probe_post(post_id):\n",
      "    plt.plot(x, clf.predict(attributes_without_excluded[attributes['id']==post_id])[0], color='blue')\n",
      "    plt.plot(x, buckets[post_id], color='red')\n",
      "    plt.show()\n",
      "    print('reddit comment count: ', attributes[attributes['id']==post_id]['comms_num'].to_list()[0])\n",
      "    print('actual comment count: ', len(times[post_id]))\n",
      "    print('timeslot comment count: ', np.sum(buckets[post_id]))\n",
      "\n",
      "probe_post('l8azdz')\n",
      "probe_post('l922ub')\n",
      "187/34:\n",
      "def probe_post(post_id):\n",
      "    plt.plot(x, clf.predict(attributes_without_excluded[attributes['id']==post_id])[0], color='blue')\n",
      "    plt.plot(x, buckets[post_id], color='red')\n",
      "    plt.show()\n",
      "    print('reddit comment count: ', attributes[attributes['id']==post_id]['comms_num'].to_list()[0])\n",
      "    print('actual comment count: ', len(times[post_id]))\n",
      "    print('timeslot comment count: ', np.sum(buckets[post_id]))\n",
      "\n",
      "probe_post('l8azdz')\n",
      "probe_post('l922ub')\n",
      "187/35:\n",
      "bins_count = 12\n",
      "bin_timeframe = 6000\n",
      "\n",
      "buckets={}\n",
      "for id in comments_all:\n",
      "    buckets[id] = []\n",
      "    cur_idx = 0\n",
      "    for i in range(bins_count):\n",
      "        buckets[id].append(0)\n",
      "        for j in range(cur_idx, len(times[id])):\n",
      "            if times[id][j]<bin_timeframe*(i+1):\n",
      "                buckets[id][i] += 1\n",
      "            else:\n",
      "                cur_idx = j\n",
      "                break\n",
      "                \n",
      "    if np.sum(buckets[id])>len(times[id]):\n",
      "        print(buckets[id])\n",
      "        print(times[id])\n",
      "187/36:\n",
      "bins_count = 12\n",
      "bin_timeframe = 6000\n",
      "\n",
      "buckets={}\n",
      "for id in comments_all:\n",
      "    buckets[id] = []\n",
      "    cur_idx = 0\n",
      "    for i in range(bins_count):\n",
      "        buckets[id].append(0)\n",
      "        for j in range(cur_idx, len(times[id])):\n",
      "            cur_idx = j\n",
      "            if times[id][j]<bin_timeframe*(i+1):\n",
      "                buckets[id][i] += 1\n",
      "            else:\n",
      "                break\n",
      "                \n",
      "    if np.sum(buckets[id])>len(times[id]):\n",
      "        print(buckets[id])\n",
      "        print(times[id])\n",
      "187/37:\n",
      "bins_count = 12\n",
      "bin_timeframe = 6000\n",
      "\n",
      "buckets={}\n",
      "for id in comments_all:\n",
      "    buckets[id] = []\n",
      "    cur_idx = 0\n",
      "    for i in range(bins_count):\n",
      "        buckets[id].append(0)\n",
      "        for j in range(cur_idx, len(times[id])):\n",
      "            if times[id][j]<bin_timeframe*(i+1):\n",
      "                cur_idx += 1\n",
      "                buckets[id][i] += 1\n",
      "            else:\n",
      "                break\n",
      "                \n",
      "    if np.sum(buckets[id])>len(times[id]):\n",
      "        print(buckets[id])\n",
      "        print(times[id])\n",
      "187/38:\n",
      "x  = [i*bin_timeframe/10/60 for i in range(bins_count)]\n",
      "\n",
      "plt.plot(x, buckets['l8azdz'])\n",
      "plt.bar(x, buckets['l8azdz'], width=5)\n",
      "plt.show()\n",
      "\n",
      "plt.plot(x, buckets['l922ub'])\n",
      "plt.bar(x, buckets['l922ub'], width=5)\n",
      "plt.show()\n",
      "187/39:\n",
      "buckets_df = pd.DataFrame(buckets.items(), columns=['id', 'buckets'])\n",
      "\n",
      "print(buckets_df)\n",
      "187/40:\n",
      "attributes = pickle.load(open( \"../../pickle/text_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "attributes = pd.merge(attributes, buckets_df, on=\"id\")\n",
      "187/41:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "    \n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "\n",
      "\n",
      "clf = MultiOutputRegressor(RandomForestRegressor()).fit(train_without_exculded, train['buckets'].to_list())\n",
      "clf.predict(test_without_excluded)\n",
      "187/42:\n",
      "def probe_post(post_id):\n",
      "    plt.plot(x, clf.predict(attributes_without_excluded[attributes['id']==post_id])[0], color='blue')\n",
      "    plt.plot(x, buckets[post_id], color='red')\n",
      "    plt.show()\n",
      "    print('reddit comment count: ', attributes[attributes['id']==post_id]['comms_num'].to_list()[0])\n",
      "    print('actual comment count: ', len(times[post_id]))\n",
      "    print('timeslot comment count: ', np.sum(buckets[post_id]))\n",
      "\n",
      "probe_post('l8azdz')\n",
      "probe_post('l922ub')\n",
      "187/43:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2, random_state=123)\n",
      "\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "    \n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "\n",
      "\n",
      "clf = MultiOutputRegressor(RandomForestRegressor(random_state=123)).fit(train_without_exculded, train['buckets'].to_list())\n",
      "clf.predict(test_without_excluded)\n",
      "187/44:\n",
      "def probe_post(post_id):\n",
      "    plt.plot(x, clf.predict(attributes_without_excluded[attributes['id']==post_id])[0], color='blue')\n",
      "    plt.plot(x, buckets[post_id], color='red')\n",
      "    plt.show()\n",
      "    print('reddit comment count: ', attributes[attributes['id']==post_id]['comms_num'].to_list()[0])\n",
      "    print('actual comment count: ', len(times[post_id]))\n",
      "    print('timeslot comment count: ', np.sum(buckets[post_id]))\n",
      "\n",
      "    \n",
      "probe_post('l8azdz')\n",
      "probe_post('l922ub')\n",
      "187/45:\n",
      "for id in test_without_excluded['id'].to_list():\n",
      "    probe_post(id)\n",
      "187/46:\n",
      "for id in test['id'].to_list():\n",
      "    probe_post(id)\n",
      "187/47:\n",
      "def probe_post(post_id):\n",
      "    plt.plot(x, clf.predict(attributes_without_excluded[attributes['id']==post_id])[0], color='blue')\n",
      "    plt.plot(x, buckets[post_id], color='red')\n",
      "    plt.show()\n",
      "    print(buckets[post_id])\n",
      "    print('reddit comment count: ', attributes[attributes['id']==post_id]['comms_num'].to_list()[0])\n",
      "    print('actual comment count: ', len(times[post_id]))\n",
      "    print('timeslot comment count: ', np.sum(buckets[post_id]))\n",
      "\n",
      "    \n",
      "probe_post('l8azdz')\n",
      "probe_post('l922ub')\n",
      "187/48:\n",
      "for id in test['id'].to_list():\n",
      "    probe_post(id)\n",
      "187/49:\n",
      "def probe_post(post_id):\n",
      "    plt.plot(x, clf.predict(attributes_without_excluded[attributes['id']==post_id])[0], color='blue')\n",
      "    plt.plot(x, buckets[post_id], color='red')\n",
      "    plt.show()\n",
      "    print('reddit comment count: ', attributes[attributes['id']==post_id]['comms_num'].to_list()[0])\n",
      "    print('actual comment count: ', len(times[post_id]))\n",
      "    print('timeslot comment count: ', np.sum(buckets[post_id]))\n",
      "\n",
      "    \n",
      "probe_post('l8azdz')\n",
      "probe_post('l922ub')\n",
      "187/50:\n",
      "for id in test['id'].to_list():\n",
      "    probe_post(id)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/51:\n",
      "def probe_post(post_id):\n",
      "    plt.plot(x, clf.predict(attributes_without_excluded[attributes['id']==post_id])[0], color='blue')\n",
      "    plt.plot(x, buckets[post_id], color='red')\n",
      "    plt.show()\n",
      "    print('reddit comment count: ', attributes[attributes['id']==post_id]['comms_num'].to_list()[0])\n",
      "    print('actual comment count: ', len(times[post_id]))\n",
      "    print('timeslot comment count: ', np.sum(buckets[post_id]))\n",
      "\n",
      "    \n",
      "probe_post('l8azdz')\n",
      "probe_post('l922ub')\n",
      "\n",
      "print(buckets['l922ub'])\n",
      "187/52:\n",
      "def probe_post(post_id):\n",
      "    plt.plot(x, clf.predict(attributes_without_excluded[attributes['id']==post_id])[0], color='blue')\n",
      "    plt.plot(x, buckets[post_id], color='red')\n",
      "    plt.show()\n",
      "    print('reddit comment count: ', attributes[attributes['id']==post_id]['comms_num'].to_list()[0])\n",
      "    print('actual comment count: ', len(times[post_id]))\n",
      "    print('timeslot comment count: ', np.sum(buckets[post_id]))\n",
      "\n",
      "    \n",
      "probe_post('l8azdz')\n",
      "probe_post('l922ub')\n",
      "\n",
      "print(times['l922ub'])\n",
      "187/53:\n",
      "bins_count = 12\n",
      "bin_timeframe = 3600\n",
      "\n",
      "buckets={}\n",
      "for id in comments_all:\n",
      "    buckets[id] = []\n",
      "    cur_idx = 0\n",
      "    for i in range(bins_count):\n",
      "        buckets[id].append(0)\n",
      "        for j in range(cur_idx, len(times[id])):\n",
      "            if times[id][j]<bin_timeframe*(i+1):\n",
      "                cur_idx += 1\n",
      "                buckets[id][i] += 1\n",
      "            else:\n",
      "                break\n",
      "                \n",
      "    if np.sum(buckets[id])>len(times[id]):\n",
      "        print(buckets[id])\n",
      "        print(times[id])\n",
      "187/54:\n",
      "x  = [i*bin_timeframe/60/60 for i in range(bins_count)]\n",
      "\n",
      "plt.plot(x, buckets['l8azdz'])\n",
      "plt.bar(x, buckets['l8azdz'], width=5)\n",
      "plt.show()\n",
      "\n",
      "plt.plot(x, buckets['l922ub'])\n",
      "plt.bar(x, buckets['l922ub'], width=5)\n",
      "plt.show()\n",
      "187/55:\n",
      "x  = [i*bin_timeframe/60/60 for i in range(bins_count)]\n",
      "\n",
      "plt.plot(x, buckets['l8azdz'])\n",
      "plt.bar(x, buckets['l8azdz'], width=1)\n",
      "plt.show()\n",
      "\n",
      "plt.plot(x, buckets['l922ub'])\n",
      "plt.bar(x, buckets['l922ub'], width=1)\n",
      "plt.show()\n",
      "187/56:\n",
      "x  = [i*bin_timeframe/60/60 for i in range(bins_count)]\n",
      "\n",
      "plt.plot(x, buckets['l8azdz'])\n",
      "plt.bar(x, buckets['l8azdz'], width=0.2)\n",
      "plt.show()\n",
      "\n",
      "plt.plot(x, buckets['l922ub'])\n",
      "plt.bar(x, buckets['l922ub'], width=1)\n",
      "plt.show()\n",
      "187/57:\n",
      "x  = [i*bin_timeframe/60/60 for i in range(bins_count)]\n",
      "\n",
      "plt.plot(x, buckets['l8azdz'])\n",
      "plt.bar(x, buckets['l8azdz'], width=0.5)\n",
      "plt.show()\n",
      "\n",
      "plt.plot(x, buckets['l922ub'])\n",
      "plt.bar(x, buckets['l922ub'], width=0.5)\n",
      "plt.show()\n",
      "187/58:\n",
      "buckets_df = pd.DataFrame(buckets.items(), columns=['id', 'buckets'])\n",
      "\n",
      "print(buckets_df)\n",
      "187/59:\n",
      "attributes = pickle.load(open( \"../../pickle/text_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "attributes = pd.merge(attributes, buckets_df, on=\"id\")\n",
      "187/60:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2, random_state=123)\n",
      "\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "    \n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "\n",
      "\n",
      "clf = MultiOutputRegressor(RandomForestRegressor(random_state=123)).fit(train_without_exculded, train['buckets'].to_list())\n",
      "clf.predict(test_without_excluded)\n",
      "187/61:\n",
      "def probe_post(post_id):\n",
      "    plt.plot(x, clf.predict(attributes_without_excluded[attributes['id']==post_id])[0], color='blue')\n",
      "    plt.plot(x, buckets[post_id], color='red')\n",
      "    plt.show()\n",
      "    print('reddit comment count: ', attributes[attributes['id']==post_id]['comms_num'].to_list()[0])\n",
      "    print('actual comment count: ', len(times[post_id]))\n",
      "    print('timeslot comment count: ', np.sum(buckets[post_id]))\n",
      "\n",
      "    \n",
      "probe_post('l8azdz')\n",
      "probe_post('l922ub')\n",
      "\n",
      "print(times['l922ub'])\n",
      "187/62:\n",
      "bins_count = 24\n",
      "bin_timeframe = 1800\n",
      "\n",
      "buckets={}\n",
      "for id in comments_all:\n",
      "    buckets[id] = []\n",
      "    cur_idx = 0\n",
      "    for i in range(bins_count):\n",
      "        buckets[id].append(0)\n",
      "        for j in range(cur_idx, len(times[id])):\n",
      "            if times[id][j]<bin_timeframe*(i+1):\n",
      "                cur_idx += 1\n",
      "                buckets[id][i] += 1\n",
      "            else:\n",
      "                break\n",
      "                \n",
      "    if np.sum(buckets[id])>len(times[id]):\n",
      "        print(buckets[id])\n",
      "        print(times[id])\n",
      "187/63:\n",
      "x  = [i*bin_timeframe/60/60 for i in range(bins_count)]\n",
      "\n",
      "plt.plot(x, buckets['l8azdz'])\n",
      "plt.bar(x, buckets['l8azdz'], width=0.5)\n",
      "plt.show()\n",
      "\n",
      "plt.plot(x, buckets['l922ub'])\n",
      "plt.bar(x, buckets['l922ub'], width=0.5)\n",
      "plt.show()\n",
      "187/64:\n",
      "x  = [i*bin_timeframe/60/60 for i in range(bins_count)]\n",
      "\n",
      "plt.plot(x, buckets['l8azdz'])\n",
      "plt.bar(x, buckets['l8azdz'], width=0.25)\n",
      "plt.show()\n",
      "\n",
      "plt.plot(x, buckets['l922ub'])\n",
      "plt.bar(x, buckets['l922ub'], width=0.25)\n",
      "plt.show()\n",
      "187/65:\n",
      "buckets_df = pd.DataFrame(buckets.items(), columns=['id', 'buckets'])\n",
      "\n",
      "print(buckets_df)\n",
      "187/66:\n",
      "attributes = pickle.load(open( \"../../pickle/text_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "attributes = pd.merge(attributes, buckets_df, on=\"id\")\n",
      "187/67:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2, random_state=123)\n",
      "\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "    \n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "\n",
      "\n",
      "clf = MultiOutputRegressor(RandomForestRegressor(random_state=123)).fit(train_without_exculded, train['buckets'].to_list())\n",
      "clf.predict(test_without_excluded)\n",
      "187/68:\n",
      "bucket_sum = 0\n",
      "for bucket in buckets:\n",
      "    bucket_sum += np.sum(bucket)\n",
      "\n",
      "times_sum = 0\n",
      "for time in times:\n",
      "    times_sum += len(time)\n",
      "\n",
      "print(bucket_sum/times_sum)\n",
      "187/69:\n",
      "bucket_sum = 0\n",
      "for bucket in buckets:\n",
      "    bucket_sum += np.sum(buckets[bucket])\n",
      "\n",
      "times_sum = 0\n",
      "for time in times:\n",
      "    times_sum += len(time)\n",
      "\n",
      "print(bucket_sum/times_sum)\n",
      "187/70:\n",
      "bucket_sum = 0\n",
      "for bucket in buckets:\n",
      "    bucket_sum += np.sum(buckets[bucket])\n",
      "\n",
      "times_sum = 0\n",
      "for time in times:\n",
      "    times_sum += len(times[time])\n",
      "\n",
      "print(bucket_sum/times_sum)\n",
      "187/71:\n",
      "bins_count = 12\n",
      "bin_timeframe = 3600\n",
      "\n",
      "buckets={}\n",
      "for id in comments_all:\n",
      "    buckets[id] = []\n",
      "    cur_idx = 0\n",
      "    for i in range(bins_count):\n",
      "        buckets[id].append(0)\n",
      "        for j in range(cur_idx, len(times[id])):\n",
      "            if times[id][j]<bin_timeframe*(i+1):\n",
      "                cur_idx += 1\n",
      "                buckets[id][i] += 1\n",
      "            else:\n",
      "                break\n",
      "                \n",
      "    if np.sum(buckets[id])>len(times[id]):\n",
      "        print(buckets[id])\n",
      "        print(times[id])\n",
      "187/72:\n",
      "x  = [i*bin_timeframe/60/60 for i in range(bins_count)]\n",
      "\n",
      "plt.plot(x, buckets['l8azdz'])\n",
      "plt.bar(x, buckets['l8azdz'], width=0.25)\n",
      "plt.show()\n",
      "\n",
      "plt.plot(x, buckets['l922ub'])\n",
      "plt.bar(x, buckets['l922ub'], width=0.25)\n",
      "plt.show()\n",
      "187/73:\n",
      "bucket_sum = 0\n",
      "for bucket in buckets:\n",
      "    bucket_sum += np.sum(buckets[bucket])\n",
      "\n",
      "times_sum = 0\n",
      "for time in times:\n",
      "    times_sum += len(times[time])\n",
      "\n",
      "print(bucket_sum/times_sum)\n",
      "187/74:\n",
      "buckets_df = pd.DataFrame(buckets.items(), columns=['id', 'buckets'])\n",
      "\n",
      "print(buckets_df)\n",
      "187/75:\n",
      "attributes = pickle.load(open( \"../../pickle/text_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "attributes = pd.merge(attributes, buckets_df, on=\"id\")\n",
      "187/76:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2, random_state=123)\n",
      "\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "    \n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "\n",
      "\n",
      "clf = MultiOutputRegressor(RandomForestRegressor(random_state=123)).fit(train_without_exculded, train['buckets'].to_list())\n",
      "clf.predict(test_without_excluded)\n",
      "187/77:\n",
      "def probe_post(post_id):\n",
      "    plt.plot(x, clf.predict(attributes_without_excluded[attributes['id']==post_id])[0], color='blue')\n",
      "    plt.plot(x, buckets[post_id], color='red')\n",
      "    plt.show()\n",
      "    print('reddit comment count: ', attributes[attributes['id']==post_id]['comms_num'].to_list()[0])\n",
      "    print('actual comment count: ', len(times[post_id]))\n",
      "    print('timeslot comment count: ', np.sum(buckets[post_id]))\n",
      "\n",
      "    \n",
      "probe_post('l8azdz')\n",
      "probe_post('l922ub')\n",
      "\n",
      "print(times['l922ub'])\n",
      "187/78:\n",
      "exclude_buckets = lambda x: x[x.columns.difference(['buckets'])]\n",
      "clf = RandomForestRegressor(random_state=123).fit(exclude_buckets(train_without_exculded), train['comms_num'].to_list())\n",
      "187/79:\n",
      "exclude_buckets = lambda x: x[x.columns.difference(['buckets'])]\n",
      "clf_com_numms = RandomForestRegressor(random_state=123).fit(exclude_buckets(train_without_exculded), train['comms_num'].to_list())\n",
      "187/80:\n",
      "def probe_post(post_id):\n",
      "    new_prediction = clf.predict(attributes_without_excluded[attributes['id']==post_id])[0]\n",
      "    plt.plot(x, new_prediction, color='blue')\n",
      "    plt.plot(x, buckets[post_id], color='red')\n",
      "    plt.show()\n",
      "    print('reddit comment count: ', attributes[attributes['id']==post_id]['comms_num'].to_list()[0])\n",
      "    print('actual comment count: ', len(times[post_id]))\n",
      "    print('timeslot comment count: ', np.sum(buckets[post_id]))\n",
      "    print('old predicted comments', clf_com_numms.predict(exclude_buckets(attributes_without_excluded[attributes['id']==post_id)))\n",
      "    print('new predicted comments', np.sum(new_prediction))\n",
      "\n",
      "    \n",
      "probe_post('l8azdz')\n",
      "probe_post('l922ub')\n",
      "\n",
      "print(times['l922ub'])\n",
      "187/81:\n",
      "def probe_post(post_id):\n",
      "    new_prediction = clf.predict(attributes_without_excluded[attributes['id']==post_id])[0]\n",
      "    plt.plot(x, new_prediction, color='blue')\n",
      "    plt.plot(x, buckets[post_id], color='red')\n",
      "    plt.show()\n",
      "    print('reddit comment count: ', attributes[attributes['id']==post_id]['comms_num'].to_list()[0])\n",
      "    print('actual comment count: ', len(times[post_id]))\n",
      "    print('timeslot comment count: ', np.sum(buckets[post_id]))\n",
      "    print('old predicted comments', clf_com_numms.predict(exclude_buckets(attributes_without_excluded[attributes['id']==post_id])))\n",
      "    print('new predicted comments', np.sum(new_prediction))\n",
      "\n",
      "    \n",
      "probe_post('l8azdz')\n",
      "probe_post('l922ub')\n",
      "\n",
      "print(times['l922ub'])\n",
      "187/82:\n",
      "def probe_post(post_id):\n",
      "    new_prediction = clf.predict(attributes_without_excluded[attributes['id']==post_id])[0]\n",
      "    plt.plot(x, new_prediction, color='blue')\n",
      "    plt.plot(x, buckets[post_id], color='red')\n",
      "    plt.show()\n",
      "    print('reddit comment count: ', attributes[attributes['id']==post_id]['comms_num'].to_list()[0])\n",
      "    print('actual comment count: ', len(times[post_id]))\n",
      "    print('timeslot comment count: ', np.sum(buckets[post_id]))\n",
      "    print('old predicted comments', clf_com_numms.predict(exclude_buckets(attributes_without_excluded[attributes['id']==post_id]))[0])\n",
      "    print('new predicted comments', np.sum(new_prediction))\n",
      "\n",
      "    \n",
      "probe_post('l8azdz')\n",
      "probe_post('l922ub')\n",
      "\n",
      "print(times['l922ub'])\n",
      "187/83:\n",
      "def probe_post(post_id):\n",
      "    new_prediction = clf.predict(attributes_without_excluded[attributes['id']==post_id])[0]\n",
      "    plt.plot(x, new_prediction, color='blue')\n",
      "    plt.plot(x, buckets[post_id], color='red')\n",
      "    plt.show()\n",
      "    print('reddit comment count: ', attributes[attributes['id']==post_id]['comms_num'].to_list()[0])\n",
      "    print('actual comment count: ', len(times[post_id]))\n",
      "    print('timeslot comment count: ', np.sum(buckets[post_id]))\n",
      "    print('old predicted comments', clf_com_numms.predict(exclude_buckets(attributes_without_excluded[attributes['id']==post_id]))[0])\n",
      "    print('new predicted comments', np.sum(new_prediction))\n",
      "\n",
      "    \n",
      "probe_post('l8azdz')\n",
      "probe_post('l922ub')\n",
      "187/84:\n",
      "buckets_df = pd.DataFrame(buckets.items(), columns=['id', 'buckets'])\n",
      "buckets_df['buckets_1'] = buckets_df['buckets'].apply(lambda x: x[0])\n",
      "buckets_df['buckets_3'] = buckets_df['buckets'].apply(lambda x: x[0:3])\n",
      "buckets_df['buckets_5'] = buckets_df['buckets'].apply(lambda x: x[0:5])\n",
      "print(buckets_df)\n",
      "187/85:\n",
      "attributes = pickle.load(open( \"../../pickle/text_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "attributes = pd.merge(attributes, buckets_df, on=\"id\")\n",
      "187/86:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets', 'buckets_1', 'buckets_3', 'buckets_5']\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2, random_state=123)\n",
      "\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "    \n",
      "X, y = load_linnerud(return_X_y=True)\n",
      "\n",
      "\n",
      "clf = MultiOutputRegressor(RandomForestRegressor(random_state=123)).fit(train_without_exculded, train['buckets'].to_list())\n",
      "clf.predict(test_without_excluded)\n",
      "187/87:\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets', 'buckets_3', 'buckets_5']\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "\n",
      "\n",
      "clf = MultiOutputRegressor(RandomForestRegressor(random_state=123)).fit(train_without_exculded, train['buckets'].to_list())\n",
      "clf.predict(test_without_excluded)\n",
      "187/88:\n",
      "probe_post('l8azdz')\n",
      "probe_post('l922ub')\n",
      "187/89:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2, random_state=123)\n",
      "\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets', 'buckets_1', 'buckets_3', 'buckets_5']\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "\n",
      "\n",
      "clf = MultiOutputRegressor(RandomForestRegressor(random_state=123)).fit(train_without_exculded, train['buckets'].to_list())\n",
      "clf.predict(test_without_excluded)\n",
      "187/90:\n",
      "def probe_post(post_id,clf,attributes_without_excluded ):\n",
      "    new_prediction = clf.predict(attributes_without_excluded[attributes['id']==post_id])[0]\n",
      "    plt.plot(x, new_prediction, color='blue')\n",
      "    plt.plot(x, buckets[post_id], color='red')\n",
      "    plt.show()\n",
      "    print('reddit comment count: ', attributes[attributes['id']==post_id]['comms_num'].to_list()[0])\n",
      "    print('actual comment count: ', len(times[post_id]))\n",
      "    print('timeslot comment count: ', np.sum(buckets[post_id]))\n",
      "    print('old predicted comments', clf_com_numms.predict(exclude_buckets(attributes_without_excluded[attributes['id']==post_id]))[0])\n",
      "    print('new predicted comments', np.sum(new_prediction))\n",
      "\n",
      "    \n",
      "probe_post('l8azdz', clf, attributes_without_excluded)\n",
      "probe_post('l922ub', clf, attributes_without_excluded)\n",
      "187/91: clf_com_numms = RandomForestRegressor(random_state=123).fit(train_without_exculded, train['comms_num'].to_list())\n",
      "187/92:\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets', 'buckets_3', 'buckets_5']\n",
      "train_without_exculded_1 = exclude(train)\n",
      "test_without_excluded_1 = exclude(test)\n",
      "attributes_without_excluded_1 = exclude(attributes)\n",
      "\n",
      "\n",
      "clf_1 = MultiOutputRegressor(RandomForestRegressor(random_state=123)).fit(train_without_exculded_1, train['buckets'].to_list())\n",
      "187/93:\n",
      "probe_post('l8azdz', clf_1, attributes_without_excluded_1)\n",
      "probe_post('l922ub', clf_1, attributes_without_excluded_1)\n",
      "187/94:\n",
      "def probe_post(post_id,clf,attributes_without_excluded ):\n",
      "    new_prediction = clf.predict(attributes_without_excluded[attributes['id']==post_id])[0]\n",
      "    plt.plot(x, new_prediction, color='blue')\n",
      "    plt.plot(x, buckets[post_id], color='red')\n",
      "    plt.show()\n",
      "    print('reddit comment count: ', attributes[attributes['id']==post_id]['comms_num'].to_list()[0])\n",
      "    print('actual comment count: ', len(times[post_id]))\n",
      "    print('timeslot comment count: ', np.sum(buckets[post_id]))\n",
      "    print('old predicted comments', clf_com_numms.predict(attributes_without_excluded[attributes['id']==post_id])[0])\n",
      "    print('new predicted comments', np.sum(new_prediction))\n",
      "\n",
      "    \n",
      "probe_post('l8azdz', clf, attributes_without_excluded)\n",
      "probe_post('l922ub', clf, attributes_without_excluded)\n",
      "187/95:\n",
      "probe_post('l8azdz', clf_1, attributes_without_excluded_1)\n",
      "probe_post('l922ub', clf_1, attributes_without_excluded_1)\n",
      "187/96:\n",
      "def probe_post(post_id,clf,attributes_excluded ):\n",
      "    new_prediction = clf.predict(attributes_excluded[attributes['id']==post_id])[0]\n",
      "    plt.plot(x, new_prediction, color='blue')\n",
      "    plt.plot(x, buckets[post_id], color='red')\n",
      "    plt.show()\n",
      "    print('reddit comment count: ', attributes[attributes['id']==post_id]['comms_num'].to_list()[0])\n",
      "    print('actual comment count: ', len(times[post_id]))\n",
      "    print('timeslot comment count: ', np.sum(buckets[post_id]))\n",
      "    print('old predicted comments', clf_com_numms.predict(attributes_without_excluded[attributes['id']==post_id])[0])\n",
      "    print('new predicted comments', np.sum(new_prediction))\n",
      "\n",
      "    \n",
      "probe_post('l8azdz', clf, attributes_without_excluded)\n",
      "probe_post('l922ub', clf, attributes_without_excluded)\n",
      "187/97:\n",
      "probe_post('l8azdz', clf_1, attributes_without_excluded_1)\n",
      "probe_post('l922ub', clf_1, attributes_without_excluded_1)\n",
      "187/98:\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets', 'buckets_1', 'buckets_5']\n",
      "train_without_exculded_3 = exclude(train)\n",
      "test_without_excluded_3 = exclude(test)\n",
      "attributes_without_excluded_3 = exclude(attributes)\n",
      "\n",
      "\n",
      "clf_3 = MultiOutputRegressor(RandomForestRegressor(random_state=123)).fit(train_without_exculded_3, train['buckets'].to_list())\n",
      "187/99:\n",
      "probe_post('l8azdz', clf_3, attributes_without_excluded_3)\n",
      "probe_post('l922ub', clf_3, attributes_without_excluded_3)\n",
      "187/100:\n",
      "buckets_df = pd.DataFrame(buckets.items(), columns=['id', 'buckets'])\n",
      "buckets_df['buckets_1'] = buckets_df['buckets'].apply(lambda x: x[0])\n",
      "buckets_df['buckets_2'] = buckets_df['buckets'].apply(lambda x: x[1])\n",
      "buckets_df['buckets_3'] = buckets_df['buckets'].apply(lambda x: x[2])\n",
      "buckets_df['buckets_4'] = buckets_df['buckets'].apply(lambda x: x[3])\n",
      "buckets_df['buckets_5'] = buckets_df['buckets'].apply(lambda x: x[4])\n",
      "print(buckets_df)\n",
      "187/101:\n",
      "attributes = pickle.load(open( \"../../pickle/text_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "attributes = pd.merge(attributes, buckets_df, on=\"id\")\n",
      "187/102:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2, random_state=123)\n",
      "\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets', 'buckets_1', 'buckets_2', 'buckets_3', 'buckets_4', 'buckets_5']\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "\n",
      "\n",
      "clf = MultiOutputRegressor(RandomForestRegressor(random_state=123)).fit(train_without_exculded, train['buckets'].to_list())\n",
      "clf.predict(test_without_excluded)\n",
      "187/103:\n",
      "def probe_post(post_id,clf,attributes_excluded ):\n",
      "    new_prediction = clf.predict(attributes_excluded[attributes['id']==post_id])[0]\n",
      "    plt.plot(x, new_prediction, color='blue')\n",
      "    plt.plot(x, buckets[post_id], color='red')\n",
      "    plt.show()\n",
      "    print('reddit comment count: ', attributes[attributes['id']==post_id]['comms_num'].to_list()[0])\n",
      "    print('actual comment count: ', len(times[post_id]))\n",
      "    print('timeslot comment count: ', np.sum(buckets[post_id]))\n",
      "    print('old predicted comments', clf_com_numms.predict(attributes_without_excluded[attributes['id']==post_id])[0])\n",
      "    print('new predicted comments', np.sum(new_prediction))\n",
      "\n",
      "    \n",
      "probe_post('l8azdz', clf, attributes_without_excluded)\n",
      "probe_post('l922ub', clf, attributes_without_excluded)\n",
      "187/104:\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets', 'buckets_2', 'buckets_3', 'buckets_4', 'buckets_5']\n",
      "train_without_exculded_1 = exclude(train)\n",
      "test_without_excluded_1 = exclude(test)\n",
      "attributes_without_excluded_1 = exclude(attributes)\n",
      "\n",
      "\n",
      "clf_1 = MultiOutputRegressor(RandomForestRegressor(random_state=123)).fit(train_without_exculded_1, train['buckets'].to_list())\n",
      "187/105:\n",
      "probe_post('l8azdz', clf_1, attributes_without_excluded_1)\n",
      "probe_post('l922ub', clf_1, attributes_without_excluded_1)\n",
      "187/106:\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets', 'buckets_1', 'buckets_5']\n",
      "train_without_exculded_3 = exclude(train)\n",
      "test_without_excluded_3 = exclude(test)\n",
      "attributes_without_excluded_3 = exclude(attributes)\n",
      "\n",
      "\n",
      "clf_3 = MultiOutputRegressor(RandomForestRegressor(random_state=123)).fit(train_without_exculded_3, train['buckets'].to_list())\n",
      "187/107:\n",
      "probe_post('l8azdz', clf_3, attributes_without_excluded_3)\n",
      "probe_post('l922ub', clf_3, attributes_without_excluded_3)\n",
      "187/108:\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "train_without_exculded_5 = exclude(train)\n",
      "test_without_excluded_5 = exclude(test)\n",
      "attributes_without_excluded_5 = exclude(attributes)\n",
      "\n",
      "\n",
      "clf_5 = MultiOutputRegressor(RandomForestRegressor(random_state=123)).fit(train_without_exculded_5, train['buckets'].to_list())\n",
      "187/109:\n",
      "bins_count = 12\n",
      "bin_timeframe = 6000\n",
      "\n",
      "buckets={}\n",
      "for id in times:\n",
      "    buckets[id] = []\n",
      "    cur_idx = 0\n",
      "    for i in range(bins_count):\n",
      "        buckets[id].append(0)\n",
      "        for j in range(cur_idx, len(times[id])):\n",
      "            if times[id][j]<bin_timeframe*(i+1):\n",
      "                buckets[id][i] += 1\n",
      "            else:\n",
      "                cur_idx = j\n",
      "                break\n",
      "\n",
      "print(times)\n",
      "187/110:\n",
      "bins_count = 12\n",
      "bin_timeframe = 3600\n",
      "\n",
      "buckets={}\n",
      "for id in times:\n",
      "    buckets[id] = []\n",
      "    cur_idx = 0\n",
      "    for i in range(bins_count):\n",
      "        buckets[id].append(0)\n",
      "        for j in range(cur_idx, len(times[id])):\n",
      "            if times[id][j]<bin_timeframe*(i+1):\n",
      "                cur_idx += 1\n",
      "                buckets[id][i] += 1\n",
      "            else:\n",
      "                break\n",
      "\n",
      "print(buckets)\n",
      "187/111:\n",
      "buckets_df = pd.DataFrame(buckets, columns=['id', 'buckets'])\n",
      "attributes = pickle.load(open( \"../../pickle/text_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "attributes = pd.merge(attributes, buckets_df, on=\"id\")\n",
      "187/112:\n",
      "import numpy as np\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import r2_score, mean_squared_error\n",
      "\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2)\n",
      "\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "clf = MultiOutputRegressor(RandomForestRegressor(random_state=123)).fit(train_without_exculded, train_without_exculded['buckets'])\n",
      "187/113:\n",
      "buckets_df = pd.DataFrame(buckets, columns=['id', 'buckets'])\n",
      "attributes = pickle.load(open( \"../../pickle/text_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "attributes = pd.merge(attributes, buckets_df, on=\"id\")\n",
      "print(attributes)\n",
      "%history -g\n",
      "187/114:\n",
      "# buckets_df = pd.DataFrame(buckets, columns=['id', 'buckets'])\n",
      "# attributes = pickle.load(open( \"../../pickle/text_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "# attributes = pd.merge(attributes, buckets_df, on=\"id\")\n",
      "# print(attributes)\n",
      "%history -g\n",
      "187/115: %history -g\n",
      "187/116:\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "\n",
      "comments_all = pickle.load(open( \"../../pickle/comments_all/comments_all.pkl\", \"rb\" ))\n",
      "\n",
      "comments_mix = comments_all\n",
      "comments_mix.pop('post_id', None)\n",
      "188/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "\n",
      "comments_all = pickle.load(open( \"../../pickle/comments_all/comments_all.pkl\", \"rb\" ))\n",
      "\n",
      "comments_mix = comments_all\n",
      "comments_mix.pop('post_id', None)\n",
      "188/2:\n",
      "import pandas as pd\n",
      "df = pd.read_csv('../../data/reddit_wsb.csv')\n",
      "188/3:\n",
      "times = {}\n",
      "for id in comments_mix:\n",
      "    times[id] = []\n",
      "    created = df.loc[df['id'] == id].iloc[0].created\n",
      "    for comment in comments_mix[id]:\n",
      "        times[id].append(comment.created_utc - created)\n",
      "for id in times.keys():\n",
      "    times[id].sort()\n",
      "188/4:\n",
      "bins_count = 12\n",
      "bin_timeframe = 3600\n",
      "\n",
      "buckets={}\n",
      "for id in comments_all:\n",
      "    buckets[id] = []\n",
      "    cur_idx = 0\n",
      "    for i in range(bins_count):\n",
      "        buckets[id].append(0)\n",
      "        for j in range(cur_idx, len(times[id])):\n",
      "            if times[id][j]<bin_timeframe*(i+1):\n",
      "                cur_idx += 1\n",
      "                buckets[id][i] += 1\n",
      "            else:\n",
      "                break\n",
      "                \n",
      "    if np.sum(buckets[id])>len(times[id]):\n",
      "        print(buckets[id])\n",
      "        print(times[id])\n",
      "188/5: %history -g\n",
      "188/6:\n",
      "import numpy as np\n",
      "bins_count = 12\n",
      "bin_timeframe = 3600\n",
      "\n",
      "buckets={}\n",
      "for id in comments_all:\n",
      "    buckets[id] = []\n",
      "    cur_idx = 0\n",
      "    for i in range(bins_count):\n",
      "        buckets[id].append(0)\n",
      "        for j in range(cur_idx, len(times[id])):\n",
      "            if times[id][j]<bin_timeframe*(i+1):\n",
      "                cur_idx += 1\n",
      "                buckets[id][i] += 1\n",
      "            else:\n",
      "                break\n",
      "                \n",
      "    if np.sum(buckets[id])>len(times[id]):\n",
      "        print(buckets[id])\n",
      "        print(times[id])\n",
      "188/7:\n",
      "x  = [i*bin_timeframe/60/60 for i in range(bins_count)]\n",
      "\n",
      "plt.plot(x, buckets['l8azdz'])\n",
      "plt.bar(x, buckets['l8azdz'], width=0.25)\n",
      "plt.show()\n",
      "\n",
      "plt.plot(x, buckets['l922ub'])\n",
      "plt.bar(x, buckets['l922ub'], width=0.25)\n",
      "plt.show()\n",
      "188/8:\n",
      "bucket_sum = 0\n",
      "for bucket in buckets:\n",
      "    bucket_sum += np.sum(buckets[bucket])\n",
      "\n",
      "times_sum = 0\n",
      "for time in times:\n",
      "    times_sum += len(times[time])\n",
      "\n",
      "print(bucket_sum/times_sum)\n",
      "188/9:\n",
      "buckets_df = pd.DataFrame(buckets.items(), columns=['id', 'buckets'])\n",
      "buckets_df['buckets_1'] = buckets_df['buckets'].apply(lambda x: x[0])\n",
      "buckets_df['buckets_2'] = buckets_df['buckets'].apply(lambda x: x[1])\n",
      "buckets_df['buckets_3'] = buckets_df['buckets'].apply(lambda x: x[2])\n",
      "buckets_df['buckets_4'] = buckets_df['buckets'].apply(lambda x: x[3])\n",
      "buckets_df['buckets_5'] = buckets_df['buckets'].apply(lambda x: x[4])\n",
      "print(buckets_df)\n",
      "188/10:\n",
      "attributes = pickle.load(open( \"../../pickle/text_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "attributes = pd.merge(attributes, buckets_df, on=\"id\")\n",
      "188/11:\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2, random_state=123)\n",
      "\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets', 'buckets_1', 'buckets_2', 'buckets_3', 'buckets_4', 'buckets_5']\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "\n",
      "\n",
      "clf = MultiOutputRegressor(RandomForestRegressor(random_state=123)).fit(train_without_exculded, train['buckets'].to_list())\n",
      "clf.predict(test_without_excluded)\n",
      "188/12:  clf_com_numms = RandomForestRegressor(random_state=123).fit(train_without_exculded, train['comms_num'].to_list())\n",
      "188/13:\n",
      "def probe_post(post_id,clf,attributes_excluded ):\n",
      "    new_prediction = clf.predict(attributes_excluded[attributes['id']==post_id])[0]\n",
      "    plt.plot(x, new_prediction, color='blue')\n",
      "    plt.plot(x, buckets[post_id], color='red')\n",
      "    plt.show()\n",
      "    print('reddit comment count: ', attributes[attributes['id']==post_id]['comms_num'].to_list()[0])\n",
      "    print('actual comment count: ', len(times[post_id]))\n",
      "    print('timeslot comment count: ', np.sum(buckets[post_id]))\n",
      "    print('old predicted comments', clf_com_numms.predict(attributes_without_excluded[attributes['id']==post_id])[0])\n",
      "    print('new predicted comments', np.sum(new_prediction))\n",
      "\n",
      "    \n",
      "probe_post('l8azdz', clf, attributes_without_excluded)\n",
      "probe_post('l922ub', clf, attributes_without_excluded)\n",
      "188/14:\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets', 'buckets_2', 'buckets_3', 'buckets_4', 'buckets_5']\n",
      "train_without_exculded_1 = exclude(train)\n",
      "test_without_excluded_1 = exclude(test)\n",
      "attributes_without_excluded_1 = exclude(attributes)\n",
      "\n",
      "clf_1 = MultiOutputRegressor(RandomForestRegressor(random_state=123)).fit(train_without_exculded_1, train['buckets'].to_list())\n",
      "188/15:\n",
      "probe_post('l8azdz', clf_1, attributes_without_excluded_1)\n",
      "probe_post('l922ub', clf_1, attributes_without_excluded_1)\n",
      "188/16:\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets', 'buckets_1', 'buckets_5']\n",
      "train_without_exculded_3 = exclude(train)\n",
      "test_without_excluded_3 = exclude(test)\n",
      "attributes_without_excluded_3 = exclude(attributes)\n",
      "\n",
      "\n",
      "clf_3 = MultiOutputRegressor(RandomForestRegressor(random_state=123)).fit(train_without_exculded_3, train['buckets'].to_list())\n",
      "                                                                                                       107:\n",
      "188/17:\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets', 'buckets_1', 'buckets_5']\n",
      "train_without_exculded_3 = exclude(train)\n",
      "test_without_excluded_3 = exclude(test)\n",
      "attributes_without_excluded_3 = exclude(attributes)\n",
      "\n",
      "clf_3 = MultiOutputRegressor(RandomForestRegressor(random_state=123)).fit(train_without_exculded_3, train['buckets'].to_list())\n",
      "                                                                                                       107:\n",
      "188/18:\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets', 'buckets_1', 'buckets_5']\n",
      "train_without_exculded_3 = exclude(train)\n",
      "test_without_excluded_3 = exclude(test)\n",
      "attributes_without_excluded_3 = exclude(attributes)\n",
      "\n",
      "clf_3 = MultiOutputRegressor(RandomForestRegressor(random_state=123)).fit(train_without_exculded_3, train['buckets'].to_list())\n",
      "188/19:\n",
      "probe_post('l8azdz', clf_3, attributes_without_excluded_3)\n",
      "probe_post('l922ub', clf_3, attributes_without_excluded_3)\n",
      "188/20:\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "train_without_exculded_5 = exclude(train)\n",
      "test_without_excluded_5 = exclude(test)\n",
      "attributes_without_excluded_5 = exclude(attributes)\n",
      "\n",
      "\n",
      "clf_5 = MultiOutputRegressor(RandomForestRegressor(random_state=123)).fit(train_without_exculded_5, train['buckets'].to_list())\n",
      "188/21:\n",
      "probe_post('l8azdz', clf_5, attributes_without_excluded_5)\n",
      "probe_post('l922ub', clf_5, attributes_without_excluded_5)\n",
      "186/4:\n",
      "from praw.models import MoreComments\n",
      "import datetime as dt\n",
      "import time\n",
      "import csv\n",
      "import os\n",
      "from dotenv import load_dotenv\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas\n",
      "import pickle\n",
      "\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "df = df.sort_values(by=['comms_num'], ascending=True)\n",
      "\n",
      "comments_all = pickle.load(open( \"../../pickle/comments_all/comments_all_new_38000.pkl\", \"rb\" ))\n",
      "186/5:\n",
      "from praw.models import MoreComments\n",
      "import datetime as dt\n",
      "import time\n",
      "import csv\n",
      "import os\n",
      "from dotenv import load_dotenv\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas\n",
      "import pickle\n",
      "\n",
      "df = pandas.read_csv('../../data/reddit_wsb.csv')\n",
      "df = df.sort_values(by=['comms_num'], ascending=True)\n",
      "\n",
      "comments_all = pickle.load(open( \"../../pickle/comments_all/comments_all_new_38000.pkl\", \"rb\" ))\n",
      "186/6:\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_all_38200.pkl\", \"rb\" ))\n",
      "print('load')\n",
      "keys_to_remove =  df['id'][:38000]\n",
      "for key in keys_to_remove:\n",
      "    if key in comments_last.keys():\n",
      "        del comments_last[key]\n",
      "\n",
      "print(len(comments_last))\n",
      "comments_mix = {**comments_all, **comments_last}\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_all_38600.pkl\", \"rb\" ))\n",
      "print('load')\n",
      "print(len(comments_last))\n",
      "comments_mix = {**comments_mix, **comments_last}\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_all_38700.pkl\", \"rb\" ))\n",
      "print('load')\n",
      "print(len(comments_last))\n",
      "comments_mix = {**comments_mix, **comments_last}\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_all_exp_38100.pkl\", \"rb\" ))\n",
      "print('load')\n",
      "print(len(comments_last))\n",
      "comments_mix = {**comments_mix, **comments_last}\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_all_new_rev_38700.pkl\", \"rb\" ))\n",
      "print('load')\n",
      "print(len(comments_last))\n",
      "comments_mix = {**comments_mix, **comments_last}\n",
      "\n",
      "# comments_last = pickle.load(open( \"../../pickle/comments_all/comments_38000_to_38800_10.pkl\", \"rb\" ))\n",
      "# print('load')\n",
      "\n",
      "# for key in comments_mix.keys():\n",
      "#     if key in comments_last.keys():\n",
      "#         del comments_last[key]\n",
      "# print(len(comments_last))    \n",
      "# comments_mix = {**comments_mix, **comments_last}\n",
      "    \n",
      "comments_mix.pop('post_id', None)\n",
      "print(len(comments_mix))\n",
      "186/7:\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_all_exp_38100.pkl\", \"rb\" ))\n",
      "print('load')\n",
      "print(len(comments_last))\n",
      "comments_mix = {**comments_mix, **comments_last}\n",
      "\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_all_38200.pkl\", \"rb\" ))\n",
      "print('load')\n",
      "keys_to_remove =  df['id'][:38000]\n",
      "for key in keys_to_remove:\n",
      "    if key in comments_last.keys():\n",
      "        del comments_last[key]\n",
      "\n",
      "print(len(comments_last))\n",
      "comments_mix = {**comments_all, **comments_last}\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_all_38300.pkl\", \"rb\" ))\n",
      "print('load')\n",
      "print(len(comments_last))\n",
      "comments_mix = {**comments_mix, **comments_last}\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_all_38600.pkl\", \"rb\" ))\n",
      "print('load')\n",
      "print(len(comments_last))\n",
      "comments_mix = {**comments_mix, **comments_last}\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_all_38700.pkl\", \"rb\" ))\n",
      "print('load')\n",
      "print(len(comments_last))\n",
      "comments_mix = {**comments_mix, **comments_last}\n",
      "\n",
      "\n",
      "comments_last = pickle.load(open( \"../../pickle/comments_all/comments_all_new_rev_38700.pkl\", \"rb\" ))\n",
      "print('load')\n",
      "print(len(comments_last))\n",
      "comments_mix = {**comments_mix, **comments_last}\n",
      "\n",
      "# comments_last = pickle.load(open( \"../../pickle/comments_all/comments_38000_to_38800_10.pkl\", \"rb\" ))\n",
      "# print('load')\n",
      "\n",
      "# for key in comments_mix.keys():\n",
      "#     if key in comments_last.keys():\n",
      "#         del comments_last[key]\n",
      "# print(len(comments_last))    \n",
      "# comments_mix = {**comments_mix, **comments_last}\n",
      "    \n",
      "comments_mix.pop('post_id', None)\n",
      "print(len(comments_mix))\n",
      "186/8: pickle.dump(comments_mix, open(\"../../pickle/comments_all/comments_all.pkl\", \"wb\"))\n",
      "188/22:\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "\n",
      "comments_all = pickle.load(open( \"../../pickle/comments_all/comments_all.pkl\", \"rb\" ))\n",
      "\n",
      "comments_mix = comments_all\n",
      "comments_mix.pop('post_id', None)\n",
      "   1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pickle\n",
      "\n",
      "comments_all = pickle.load(open( \"../../pickle/comments_all/comments_all.pkl\", \"rb\" ))\n",
      "\n",
      "comments_mix = comments_all\n",
      "comments_mix.pop('post_id', None)\n",
      "   2:\n",
      "import pandas as pd\n",
      "df = pd.read_csv('../../data/reddit_wsb.csv')\n",
      "   3:\n",
      "times = {}\n",
      "for id in comments_mix:\n",
      "    times[id] = []\n",
      "    created = df.loc[df['id'] == id].iloc[0].created\n",
      "    for comment in comments_mix[id]:\n",
      "        times[id].append(comment.created_utc - created)\n",
      "for id in times.keys():\n",
      "    times[id].sort()\n",
      "   4:\n",
      "import numpy as np\n",
      "bins_count = 12\n",
      "bin_timeframe = 3600\n",
      "\n",
      "buckets={}\n",
      "for id in comments_all:\n",
      "    buckets[id] = []\n",
      "    cur_idx = 0\n",
      "    for i in range(bins_count):\n",
      "        buckets[id].append(0)\n",
      "        for j in range(cur_idx, len(times[id])):\n",
      "            if times[id][j]<bin_timeframe*(i+1):\n",
      "                cur_idx += 1\n",
      "                buckets[id][i] += 1\n",
      "            else:\n",
      "                break\n",
      "                \n",
      "    if np.sum(buckets[id])>len(times[id]):\n",
      "        print(buckets[id])\n",
      "        print(times[id])\n",
      "   5:\n",
      "x  = [i*bin_timeframe/60/60 for i in range(bins_count)]\n",
      "\n",
      "plt.plot(x, buckets['l8azdz'])\n",
      "plt.bar(x, buckets['l8azdz'], width=0.25)\n",
      "plt.show()\n",
      "\n",
      "plt.plot(x, buckets['l922ub'])\n",
      "plt.bar(x, buckets['l922ub'], width=0.25)\n",
      "plt.show()\n",
      "   6:\n",
      "bucket_sum = 0\n",
      "for bucket in buckets:\n",
      "    bucket_sum += np.sum(buckets[bucket])\n",
      "\n",
      "times_sum = 0\n",
      "for time in times:\n",
      "    times_sum += len(times[time])\n",
      "\n",
      "print(bucket_sum/times_sum)\n",
      "   7:\n",
      "buckets_df = pd.DataFrame(buckets.items(), columns=['id', 'buckets'])\n",
      "buckets_df['buckets_1'] = buckets_df['buckets'].apply(lambda x: x[0])\n",
      "buckets_df['buckets_2'] = buckets_df['buckets'].apply(lambda x: x[1])\n",
      "buckets_df['buckets_3'] = buckets_df['buckets'].apply(lambda x: x[2])\n",
      "buckets_df['buckets_4'] = buckets_df['buckets'].apply(lambda x: x[3])\n",
      "buckets_df['buckets_5'] = buckets_df['buckets'].apply(lambda x: x[4])\n",
      "print(buckets_df)\n",
      "   8:\n",
      "attributes = pickle.load(open( \"../../pickle/text_attributes_processed_comms_num.pkl\", \"rb\" ))\n",
      "attributes = pd.merge(attributes, buckets_df, on=\"id\")\n",
      "   9:\n",
      "from sklearn.datasets import load_linnerud\n",
      "from sklearn.multioutput import MultiOutputRegressor\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "exclude = lambda x: x[x.columns.difference(EXCLUDED_COLUMNS)]\n",
      "\n",
      "train, test = train_test_split(attributes, test_size=0.2, random_state=123)\n",
      "\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets', 'buckets_1', 'buckets_2', 'buckets_3', 'buckets_4', 'buckets_5']\n",
      "train_without_exculded = exclude(train)\n",
      "test_without_excluded = exclude(test)\n",
      "attributes_without_excluded = exclude(attributes)\n",
      "\n",
      "\n",
      "clf = MultiOutputRegressor(RandomForestRegressor(random_state=123)).fit(train_without_exculded, train['buckets'].to_list())\n",
      "clf.predict(test_without_excluded)\n",
      "  10:  clf_com_numms = RandomForestRegressor(random_state=123).fit(train_without_exculded, train['comms_num'].to_list())\n",
      "  11:\n",
      "def probe_post(post_id,clf,attributes_excluded ):\n",
      "    new_prediction = clf.predict(attributes_excluded[attributes['id']==post_id])[0]\n",
      "    plt.plot(x, new_prediction, color='blue')\n",
      "    plt.plot(x, buckets[post_id], color='red')\n",
      "    plt.show()\n",
      "    print('reddit comment count: ', attributes[attributes['id']==post_id]['comms_num'].to_list()[0])\n",
      "    print('actual comment count: ', len(times[post_id]))\n",
      "    print('timeslot comment count: ', np.sum(buckets[post_id]))\n",
      "    print('old predicted comments', clf_com_numms.predict(attributes_without_excluded[attributes['id']==post_id])[0])\n",
      "    print('new predicted comments', np.sum(new_prediction))\n",
      "\n",
      "    \n",
      "probe_post('l8azdz', clf, attributes_without_excluded)\n",
      "probe_post('l922ub', clf, attributes_without_excluded)\n",
      "  12:\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets', 'buckets_2', 'buckets_3', 'buckets_4', 'buckets_5']\n",
      "train_without_exculded_1 = exclude(train)\n",
      "test_without_excluded_1 = exclude(test)\n",
      "attributes_without_excluded_1 = exclude(attributes)\n",
      "\n",
      "clf_1 = MultiOutputRegressor(RandomForestRegressor(random_state=123)).fit(train_without_exculded_1, train['buckets'].to_list())\n",
      "  13:\n",
      "probe_post('l8azdz', clf_1, attributes_without_excluded_1)\n",
      "probe_post('l922ub', clf_1, attributes_without_excluded_1)\n",
      "  14:\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets', 'buckets_1', 'buckets_5']\n",
      "train_without_exculded_3 = exclude(train)\n",
      "test_without_excluded_3 = exclude(test)\n",
      "attributes_without_excluded_3 = exclude(attributes)\n",
      "\n",
      "clf_3 = MultiOutputRegressor(RandomForestRegressor(random_state=123)).fit(train_without_exculded_3, train['buckets'].to_list())\n",
      "  15:\n",
      "probe_post('l8azdz', clf_3, attributes_without_excluded_3)\n",
      "probe_post('l922ub', clf_3, attributes_without_excluded_3)\n",
      "  16:\n",
      "EXCLUDED_COLUMNS = ['id', 'score', 'comms_num', 'buckets']\n",
      "train_without_exculded_5 = exclude(train)\n",
      "test_without_excluded_5 = exclude(test)\n",
      "attributes_without_excluded_5 = exclude(attributes)\n",
      "\n",
      "\n",
      "clf_5 = MultiOutputRegressor(RandomForestRegressor(random_state=123)).fit(train_without_exculded_5, train['buckets'].to_list())\n",
      "  17:\n",
      "probe_post('l8azdz', clf_5, attributes_without_excluded_5)\n",
      "probe_post('l922ub', clf_5, attributes_without_excluded_5)\n",
      "  18: %history -g\n"
     ]
    }
   ],
   "source": [
    "%history -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
